{"pr_number": 4686, "pr_title": "Issue 4676: (PDP-34 )  Part 1 of 4. - ChunkedSegmentStorage and BaseChunkMetadataStore implementation", "pr_createdAt": "2020-04-10T23:42:45Z", "pr_url": "https://github.com/pravega/pravega/pull/4686", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTExODk5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421118999", "bodyText": "Is this method supposed to be overridden?", "author": "eolivelli", "createdAt": "2020-05-06T22:03:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);\n+    protected static final OpStatsLogger WRITE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_WRITE_LATENCY);\n+    protected static final OpStatsLogger CREATE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CREATE_LATENCY);\n+    protected static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n+    protected static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n+\n+    protected static final Counter READ_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_READ_BYTES);\n+    protected static final Counter WRITE_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_WRITE_BYTES);\n+    protected static final Counter CONCAT_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_BYTES);\n+\n+    protected static final Counter CREATE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CREATE_COUNT);\n+    protected static final Counter DELETE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_DELETE_COUNT);\n+    protected static final Counter CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_COUNT);\n+\n+    protected static final Counter LARGE_CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_LARGE_CONCAT_COUNT);\n+\n+    protected final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    public boolean exists(String chunkName)  throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTIxMTU2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421211566", "bodyText": "This method is defined in ChunkStorageProvider  and implemented here. But the derived classes should not override methods from ChunkStorageProvider interface directly.  Instead, they are supposed to override all the abstract methods with naming pattern doXYZ()  ..(These methods start at line 406)", "author": "sachin-j-joshi", "createdAt": "2020-05-07T03:03:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTExODk5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTYwMzY1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421603653", "bodyText": "@eolivelli I have split a really big PR in multiple parts.\nPart 2 contains actual implementation for the ChunkStorageProvider.\nBelow are the links to the implementation. They all override the abstract methods.\n\n\nHDFS\n\n\nNFS \n\n\nECS \n\n\nMy hope is that this makes the actual implementation pretty simple and straight forward. Please do let me know your thoughts.", "author": "sachin-j-joshi", "createdAt": "2020-05-07T15:41:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTExODk5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTYwNzk3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421607977", "bodyText": "I suggest to make these methods \"final\", this way it is not possible to override them", "author": "eolivelli", "createdAt": "2020-05-07T15:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTExODk5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc4NDgwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421784807", "bodyText": "Fixed. Thanks", "author": "sachin-j-joshi", "createdAt": "2020-05-07T20:49:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTExODk5OQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzAwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421757000", "bodyText": "final", "author": "eolivelli", "createdAt": "2020-05-07T19:56:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkHandle.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * Handle to a chunk.\n+ */\n+public class ChunkHandle {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc5MDM1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421790351", "bodyText": "An implementation may choose to store additional data in the ChunkHandle (Eg etags, tokens, stuff like ledgerId etc for optimization purpose).  I'm not yet sure we don't to eliminate that possibility.", "author": "sachin-j-joshi", "createdAt": "2020-05-07T20:59:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzAwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1MDY4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r435750686", "bodyText": "I am not sure that this way is compatible with the static factory methods you added below.\nIf you leave that methods every implementation will be forced to support both the generic base class and its own specific type.\nIf you do not want to use 'final' please consider dropping those factory methods.", "author": "eolivelli", "createdAt": "2020-06-05T07:53:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzAwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI3NTM0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r436275341", "bodyText": "I think I'll agree with you. To keep the API surface very small and simple, it makes sense to make it final.", "author": "sachin-j-joshi", "createdAt": "2020-06-06T14:59:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzAwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzI3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421757279", "bodyText": "if this class is not supposed to be subclassed please make it final", "author": "eolivelli", "createdAt": "2020-05-07T19:56:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkInfo.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Chunk Information.\n+ */\n+@Builder\n+@Data\n+public class ChunkInfo {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjE0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430592145", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1NzI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1OTYwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421759607", "bodyText": "What happens in case of failure here ?\nthe chunkStorage already performed the write, there is no transaction that spans the two entities.", "author": "eolivelli", "createdAt": "2020-05-07T20:00:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxMzYxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421813610", "bodyText": "Good catch !\nAll segment metadata is now stored in some key-value store (ChunkMetadataStore).\nHowever that metadata store itself needs to store all its data in some segments. These segments I am calling \"system segments\".\nTo avoid circular reference (having metadata about system segments stored in the store itself), all changes to system segments metadata are logged in systemJournal and metadata about system segments is actually not written to the metadata table itself.\nYou are correct that writing to these logs might actually fail even if the transaction is committed.\nHere I think I need to write systemJournal record before committing. (Technically we don't write the metadata about system segments to underlying store at all, so that commit can't fail because of any Io errors. If it fails because of validation, we have bigger problem)", "author": "sachin-j-joshi", "createdAt": "2020-05-07T21:46:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1OTYwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MjM1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430592350", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc1OTYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2MDk2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421760968", "bodyText": "what about a deleteIfExists primitive on checkStorage ?\nsometimes implementations can be more efficient.\nTherefore there is no lock/transaction here, so there can be a race, and two \"threads\" can issue the \"delete\" operation.\nMaybe you can simply call chuckStorage.delete and require it to not fail in case of non existant chunk", "author": "eolivelli", "createdAt": "2020-05-07T20:03:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);\n+                }\n+\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MjkyMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439172923", "bodyText": "If object doesn't exist then code has to throw ChunkNotFoundException.\nFixed it by making it optimistic check.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:24:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2MDk2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2MTQ1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421761459", "bodyText": "what happens in this case ?\nare we leaving garbage forever\nor do we have some kind of garbage collector that sometimes looks for orphan chunks ?", "author": "eolivelli", "createdAt": "2020-05-07T20:03:59Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);\n+                }\n+\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgwOTU4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421809586", "bodyText": "My plan is to implement this in phases\n0. Initial implementation - Best effort, try to delete but leave garbage if you have to.\n\nThis iteration - Add names of deleted chunks to an in-memory queue and periodically delete chunks.\nNext iteration - Mark chunks deleted in metadata and run Background garbage collection job that scans metadata and deletes chunk.", "author": "sachin-j-joshi", "createdAt": "2020-05-07T21:37:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2MTQ1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2Mzk3NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421763975", "bodyText": "What about supporting a chunkStorage that is able to perform async read operations ?\nThis method looks like it is capable of working with an async data source as it returns a CompletableFuture but is actually performing blocking reads.", "author": "eolivelli", "createdAt": "2020-05-07T20:09:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);\n+                }\n+\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<String> chunksToConcat = new ArrayList<>();\n+            ArrayList<Long> lengths = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(targetChunkName);\n+            lengths.add(targetSizeAfterConcat);\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && minSizeLimitForNativeConcat < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > maxSizeLimitForNativeConcat) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(nextChunkName);\n+                targetSizeAfterConcat += next.getLength();\n+                lengths.add(next.getLength());\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+                long[] lengthsArr = Longs.toArray(lengths);\n+\n+                ChunkHandle[] chunksArr = new ChunkHandle[chunksToConcat.size()];\n+                chunksArr[0] = ChunkHandle.writeHandle(chunksToConcat.get(0));\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksArr[i] = ChunkHandle.readHandle(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunksArr, lengthsArr);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunksArr, lengthsArr);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunksArr.length; i++) {\n+                    txn.delete(chunksArr[i].getChunkName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                                                                    offset,\n+                                                                    segmentMetadata.getFirstChunk(),\n+                                                                    startOffset));\n+                }\n+\n+                for (String toDelete : chunksToDelete) {\n+                    chunkStorage.delete(chunkStorage.openWrite(toDelete));\n+                }\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"truncate - segment={} offset={} latency={}.\", handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset() ) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (readIndex.size() > 0) {\n+                    val floorEntry = readIndex.floorEntry(offset);\n+                    if (null != floorEntry) {\n+                        startOffsetForCurrentChunk = floorEntry.getKey();\n+                        currentChunkName = floorEntry.getValue();\n+                    }\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (   startOffsetForCurrentChunk <= currentOffset\n+                        && startOffsetForCurrentChunk + chunkToReadFrom.getLength() > currentOffset) {\n+                        // we have found a chunk that contains first byte we want to read\n+                        log.debug(\"read: found chunk to read segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        break;\n+                    }\n+                    currentChunkName = chunkToReadFrom.getNextChunk();\n+                    startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+\n+                    // Update read index with newly visited chunk.\n+                    if (null != currentChunkName) {\n+                        readIndex.put(startOffsetForCurrentChunk, currentChunkName);\n+                    }\n+                }\n+\n+                // Now read.\n+                while (bytesRemaining > 0 && null != currentChunkName) {\n+                    int bytesToRead = Math.min(bytesRemaining, Math.toIntExact(chunkToReadFrom.getLength() - (currentOffset - startOffsetForCurrentChunk)));\n+                    //assert bytesToRead != 0;\n+\n+                    if (currentOffset >= startOffsetForCurrentChunk + chunkToReadFrom.getLength()) {\n+                        // The current chunk is over. Move to the next one.\n+                        currentChunkName = chunkToReadFrom.getNextChunk();\n+                        if (null != currentChunkName) {\n+                            startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+                            chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                            log.debug(\"read: reading from next chunk segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        }\n+                    } else {\n+                        Preconditions.checkState(bytesToRead != 0, \"bytesToRead is 0\");\n+                        // Read data from the chunk.\n+                        ChunkHandle chunkHandle = chunkStorage.openRead(chunkToReadFrom.getName());\n+                        int bytesRead = chunkStorage.read(chunkHandle, currentOffset - startOffsetForCurrentChunk, bytesToRead, buffer, currentBufferOffset);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgwNTg4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421805888", "bodyText": "That was the original plan. However, the required changes were getting bit complicated.  I wasn't sure this added complexity was actually buying a lot of performance. (Java threads doing IO do get blocked anyway. ) So I decided to do it in small steps. In this iteration still keeping the sync interface. In next iteration or two I do plan to turn this into async operation.", "author": "sachin-j-joshi", "createdAt": "2020-05-07T21:29:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2Mzk3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NDM4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421764380", "bodyText": "Do we have to deal with InterruptedException ?", "author": "eolivelli", "createdAt": "2020-05-07T20:09:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);\n+                }\n+\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<String> chunksToConcat = new ArrayList<>();\n+            ArrayList<Long> lengths = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(targetChunkName);\n+            lengths.add(targetSizeAfterConcat);\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && minSizeLimitForNativeConcat < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > maxSizeLimitForNativeConcat) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(nextChunkName);\n+                targetSizeAfterConcat += next.getLength();\n+                lengths.add(next.getLength());\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+                long[] lengthsArr = Longs.toArray(lengths);\n+\n+                ChunkHandle[] chunksArr = new ChunkHandle[chunksToConcat.size()];\n+                chunksArr[0] = ChunkHandle.writeHandle(chunksToConcat.get(0));\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksArr[i] = ChunkHandle.readHandle(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunksArr, lengthsArr);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunksArr, lengthsArr);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunksArr.length; i++) {\n+                    txn.delete(chunksArr[i].getChunkName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                                                                    offset,\n+                                                                    segmentMetadata.getFirstChunk(),\n+                                                                    startOffset));\n+                }\n+\n+                for (String toDelete : chunksToDelete) {\n+                    chunkStorage.delete(chunkStorage.openWrite(toDelete));\n+                }\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"truncate - segment={} offset={} latency={}.\", handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset() ) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (readIndex.size() > 0) {\n+                    val floorEntry = readIndex.floorEntry(offset);\n+                    if (null != floorEntry) {\n+                        startOffsetForCurrentChunk = floorEntry.getKey();\n+                        currentChunkName = floorEntry.getValue();\n+                    }\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (   startOffsetForCurrentChunk <= currentOffset\n+                        && startOffsetForCurrentChunk + chunkToReadFrom.getLength() > currentOffset) {\n+                        // we have found a chunk that contains first byte we want to read\n+                        log.debug(\"read: found chunk to read segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        break;\n+                    }\n+                    currentChunkName = chunkToReadFrom.getNextChunk();\n+                    startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+\n+                    // Update read index with newly visited chunk.\n+                    if (null != currentChunkName) {\n+                        readIndex.put(startOffsetForCurrentChunk, currentChunkName);\n+                    }\n+                }\n+\n+                // Now read.\n+                while (bytesRemaining > 0 && null != currentChunkName) {\n+                    int bytesToRead = Math.min(bytesRemaining, Math.toIntExact(chunkToReadFrom.getLength() - (currentOffset - startOffsetForCurrentChunk)));\n+                    //assert bytesToRead != 0;\n+\n+                    if (currentOffset >= startOffsetForCurrentChunk + chunkToReadFrom.getLength()) {\n+                        // The current chunk is over. Move to the next one.\n+                        currentChunkName = chunkToReadFrom.getNextChunk();\n+                        if (null != currentChunkName) {\n+                            startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+                            chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                            log.debug(\"read: reading from next chunk segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        }\n+                    } else {\n+                        Preconditions.checkState(bytesToRead != 0, \"bytesToRead is 0\");\n+                        // Read data from the chunk.\n+                        ChunkHandle chunkHandle = chunkStorage.openRead(chunkToReadFrom.getName());\n+                        int bytesRead = chunkStorage.read(chunkHandle, currentOffset - startOffsetForCurrentChunk, bytesToRead, buffer, currentBufferOffset);\n+\n+                        bytesRemaining -= bytesRead;\n+                        currentOffset += bytesRead;\n+                        currentBufferOffset += bytesRead;\n+                        totalBytesRead += bytesRead;\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"read - segment={} offset={} bytesRead={} latency={}.\", handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+                return totalBytesRead;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets current information about a StreamSegment.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested about the StreamSegment.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata ) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+\n+                val retValue = StreamSegmentInformation.builder()\n+                        .name(streamSegmentName)\n+                        .sealed(segmentMetadata.isSealed())\n+                        .length(segmentMetadata.getLength())\n+                        .startOffset(segmentMetadata.getStartOffset())\n+                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                        .build();\n+                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                return retValue;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Determines whether the given StreamSegment exists or not.\n+     *\n+     * @param streamSegmentName The name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested. If the operation failed,\n+     * it will contain the cause of the failure.\n+     */\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                val retValue = segmentMetadata == null ? false : segmentMetadata.isActive();\n+                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                return retValue;\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n+            }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n+\n+    /**\n+     *\n+     * @param streamSegmentName\n+     * @return\n+     */\n+    private ConcurrentSkipListMap<Long, String> getReadIndex(String streamSegmentName) {\n+        ConcurrentSkipListMap<Long, String> readIndex;\n+        if (cachedReadIndex.containsKey(streamSegmentName)) {\n+            readIndex = cachedReadIndex.get(streamSegmentName);\n+        } else {\n+            val newReadIndex = new ConcurrentSkipListMap<Long, String>();\n+            val oldReadIndex = cachedReadIndex.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation     The function to execute.\n+     * @param <R>           Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> execute(Callable<R> operation) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (Exception e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAwNzI4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445007286", "bodyText": "I don't think we need to handle it here.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T16:07:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NDM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NDQ4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421764486", "bodyText": "static ?", "author": "eolivelli", "createdAt": "2020-05-07T20:10:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    // Claim ownership.\n+                    // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+                    segmentMetadata.setOwnerEpoch(this.epoch);\n+                    segmentMetadata.setOwnershipChanged(true);\n+\n+                    // Get the last chunk\n+                    String lastChunkName = segmentMetadata.getLastChunk();\n+                    if (null != lastChunkName) {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+                        ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+                        Preconditions.checkState( chunkInfo != null);\n+                        Preconditions.checkState( lastChunk != null);\n+                        // Adjust its length;\n+                        if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                            Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                            // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                            lastChunk.setLength((int) chunkInfo.getLength());\n+                            segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                            txn.update(lastChunk);\n+                            log.debug(\"openWrite:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                                    segmentMetadata.getName(),\n+                                    lastChunk.getName(),\n+                                    chunkInfo.getLength());\n+                        }\n+                    }\n+                    // Update and commit\n+                    // If This instance is fenced this update will fail.\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    systemJournal.commitRecords(systemLogRecords);\n+                }\n+\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<String> chunksToConcat = new ArrayList<>();\n+            ArrayList<Long> lengths = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(targetChunkName);\n+            lengths.add(targetSizeAfterConcat);\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && minSizeLimitForNativeConcat < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > maxSizeLimitForNativeConcat) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(nextChunkName);\n+                targetSizeAfterConcat += next.getLength();\n+                lengths.add(next.getLength());\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+                long[] lengthsArr = Longs.toArray(lengths);\n+\n+                ChunkHandle[] chunksArr = new ChunkHandle[chunksToConcat.size()];\n+                chunksArr[0] = ChunkHandle.writeHandle(chunksToConcat.get(0));\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksArr[i] = ChunkHandle.readHandle(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunksArr, lengthsArr);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunksArr, lengthsArr);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunksArr.length; i++) {\n+                    txn.delete(chunksArr[i].getChunkName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                                                                    offset,\n+                                                                    segmentMetadata.getFirstChunk(),\n+                                                                    startOffset));\n+                }\n+\n+                for (String toDelete : chunksToDelete) {\n+                    chunkStorage.delete(chunkStorage.openWrite(toDelete));\n+                }\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"truncate - segment={} offset={} latency={}.\", handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset() ) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (readIndex.size() > 0) {\n+                    val floorEntry = readIndex.floorEntry(offset);\n+                    if (null != floorEntry) {\n+                        startOffsetForCurrentChunk = floorEntry.getKey();\n+                        currentChunkName = floorEntry.getValue();\n+                    }\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (   startOffsetForCurrentChunk <= currentOffset\n+                        && startOffsetForCurrentChunk + chunkToReadFrom.getLength() > currentOffset) {\n+                        // we have found a chunk that contains first byte we want to read\n+                        log.debug(\"read: found chunk to read segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        break;\n+                    }\n+                    currentChunkName = chunkToReadFrom.getNextChunk();\n+                    startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+\n+                    // Update read index with newly visited chunk.\n+                    if (null != currentChunkName) {\n+                        readIndex.put(startOffsetForCurrentChunk, currentChunkName);\n+                    }\n+                }\n+\n+                // Now read.\n+                while (bytesRemaining > 0 && null != currentChunkName) {\n+                    int bytesToRead = Math.min(bytesRemaining, Math.toIntExact(chunkToReadFrom.getLength() - (currentOffset - startOffsetForCurrentChunk)));\n+                    //assert bytesToRead != 0;\n+\n+                    if (currentOffset >= startOffsetForCurrentChunk + chunkToReadFrom.getLength()) {\n+                        // The current chunk is over. Move to the next one.\n+                        currentChunkName = chunkToReadFrom.getNextChunk();\n+                        if (null != currentChunkName) {\n+                            startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+                            chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                            log.debug(\"read: reading from next chunk segment: {} chunk: {}\", streamSegmentName, chunkToReadFrom);\n+                        }\n+                    } else {\n+                        Preconditions.checkState(bytesToRead != 0, \"bytesToRead is 0\");\n+                        // Read data from the chunk.\n+                        ChunkHandle chunkHandle = chunkStorage.openRead(chunkToReadFrom.getName());\n+                        int bytesRead = chunkStorage.read(chunkHandle, currentOffset - startOffsetForCurrentChunk, bytesToRead, buffer, currentBufferOffset);\n+\n+                        bytesRemaining -= bytesRead;\n+                        currentOffset += bytesRead;\n+                        currentBufferOffset += bytesRead;\n+                        totalBytesRead += bytesRead;\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"read - segment={} offset={} bytesRead={} latency={}.\", handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+                return totalBytesRead;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets current information about a StreamSegment.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested about the StreamSegment.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata ) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+\n+                val retValue = StreamSegmentInformation.builder()\n+                        .name(streamSegmentName)\n+                        .sealed(segmentMetadata.isSealed())\n+                        .length(segmentMetadata.getLength())\n+                        .startOffset(segmentMetadata.getStartOffset())\n+                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                        .build();\n+                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                return retValue;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Determines whether the given StreamSegment exists or not.\n+     *\n+     * @param streamSegmentName The name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested. If the operation failed,\n+     * it will contain the cause of the failure.\n+     */\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                val retValue = segmentMetadata == null ? false : segmentMetadata.isActive();\n+                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                return retValue;\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n+            }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n+\n+    /**\n+     *\n+     * @param streamSegmentName\n+     * @return\n+     */\n+    private ConcurrentSkipListMap<Long, String> getReadIndex(String streamSegmentName) {\n+        ConcurrentSkipListMap<Long, String> readIndex;\n+        if (cachedReadIndex.containsKey(streamSegmentName)) {\n+            readIndex = cachedReadIndex.get(streamSegmentName);\n+        } else {\n+            val newReadIndex = new ConcurrentSkipListMap<Long, String>();\n+            val oldReadIndex = cachedReadIndex.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation     The function to execute.\n+     * @param <R>           Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> execute(Callable<R> operation) {\n+        return CompletableFuture.supplyAsync(() -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (Exception e) {\n+                throw new CompletionException(e);\n+            }\n+        }, this.executor);\n+    }\n+\n+    private String getNewChunkName(String segmentName, long offset) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxNjUyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421816529", "bodyText": "Actually planning to include container epoch in segment chunk name. (uuids are terrible for debugging)", "author": "sachin-j-joshi", "createdAt": "2020-05-07T21:52:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NDQ4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NTYzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421765637", "bodyText": "what about having an async API ?\nCompletableFuture create(String chunkName)\nthe same at least for delete,read,write, openRead and openRead, concat....\nif you have a fully async storage you will be able to leverage its features.\nbut if we start with a blocking interface we will never  be able to leverage async features of backing storage", "author": "eolivelli", "createdAt": "2020-05-07T20:12:37Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTgxOTI2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421819265", "bodyText": "That was the original plan. However, the required changes were getting bit complicated. I wasn't sure this added complexity was actually buying a lot of performance. (Java threads doing IO do get blocked anyway. ) So I decided to do it in small steps. In this iteration still keeping the sync interface. In next iteration or two I do plan to turn this into async operation.", "author": "sachin-j-joshi", "createdAt": "2020-05-07T21:58:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NTYzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NjM2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r421766367", "bodyText": "1000 ?\nis this the size of any internal threadpool ?\nisn't it too big ?", "author": "eolivelli", "createdAt": "2020-05-07T20:13:59Z", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/StorageTestBase.java", "diffHunk": "@@ -51,14 +50,15 @@\n     protected static final int APPENDS_PER_SEGMENT = 10;\n     protected static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n     private static final int SEGMENT_COUNT = 4;\n+    protected final Random rnd = new Random(0);\n \n     @Getter(AccessLevel.PROTECTED)\n     @Setter(AccessLevel.PROTECTED)\n     private boolean isTestingSystemSegment = false;\n \n     @Override\n     protected int getThreadPoolSize() {\n-        return 5;\n+        return 1000;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwODA4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426708088", "bodyText": "Please revert this.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NjM2Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MTk0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430681940", "bodyText": "done.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:15:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMTc2NjM2Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2MTgyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426661821", "bodyText": "Define transaction and use examples.", "author": "fpj", "createdAt": "2020-05-18T14:20:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5MzI0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430593240", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:43:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2MTgyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2Mzc3NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426663775", "bodyText": "This leading comment is not really adding anything. Please explain what this is and how it fits into the whole LTS architecture. Add detail to the level that a person not knowing the code well understands what this is doing here and why it is important.", "author": "fpj", "createdAt": "2020-05-18T14:22:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NDI5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430594298", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:45:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2Mzc3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg3MjkzMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442872930", "bodyText": "Way to go, nice addition.", "author": "fpj", "createdAt": "2020-06-19T14:28:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2Mzc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDA2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426664064", "bodyText": "Key is undefined.", "author": "fpj", "createdAt": "2020-05-18T14:23:22Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MjU2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430682568", "bodyText": "added comments", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:16:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDM0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426664347", "bodyText": "Why do we need pinning? Another undefined term.", "author": "fpj", "createdAt": "2020-05-18T14:23:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MDcxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426780710", "bodyText": "Pinned record is not evicted from memory and not written to underlying key-value store. The metadata about the segments in which metadata store itself stores the data, that metadata creates a problem of circular reference. This is solved by not reading and writing metadata about these segments from the metadata itself, but by pinning them to memory.\nNow for handling failover - Layout changes (the addition/deleting of new chunks) to metadata store segments are written to a separate dedicated journal chunk which is read at the time of startup to re create in-memory state of these pinned segments.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T17:20:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDM0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkwNjY2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427906660", "bodyText": "Is this description anywhere in the code as a comment or javadoc?", "author": "fpj", "createdAt": "2020-05-20T10:31:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDM0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4Mjc3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430682770", "bodyText": "added comments.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:17:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NDM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NTg2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426665867", "bodyText": "Detail what this metadata is about, in particular, describe the fields and how they fit into the big picture.", "author": "fpj", "createdAt": "2020-05-18T14:25:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadata.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents chunk metadata.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NTgxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430595810", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NTg2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NzAxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426667017", "bodyText": "We need way more detail here. Explain what this is, and what it does as part of the big picture. Define all properties that are expected from chunk storage so that it is clear what the contract is.", "author": "fpj", "createdAt": "2020-05-18T14:27:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MjM0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430752341", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-05-26T22:57:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY2NzAxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3MzkzNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426673936", "bodyText": "What's an example of a system that supports native concat vs. basic concat?", "author": "fpj", "createdAt": "2020-05-18T14:37:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc2MzU2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426763566", "bodyText": "HDFS FileSystem.concat is one example https://hadoop.apache.org/docs/r2.8.2/api/org/apache/hadoop/fs/FileSystem.html#concat(org.apache.hadoop.fs.Path,%20org.apache.hadoop.fs.Path[]) , Using MPU with Extended S3 to create bigger chunks out of small objects is another example. ( BigChunk= Chunk1 + Chunk2 + Chunk3...).\nNFS does not support native concat , but the provider can support concat by using appends (writes).\nIf the provider does not support concat, then the segment concat operation is purely a metadata operation (with no data movements or calls to underlying storage provider).", "author": "sachin-j-joshi", "createdAt": "2020-05-18T16:49:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3MzkzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMDUyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427920525", "bodyText": "I don't really like this terminology, what if we call the operations append and concatenation? Append is what you are calling basic concat while concatenation is what you are calling native concatenation.", "author": "fpj", "createdAt": "2020-05-20T10:57:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3MzkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NDMyMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426674322", "bodyText": "Is this supposed to be idempotent? What is the chunk with this name already exists?", "author": "fpj", "createdAt": "2020-05-18T14:37:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc3MDM0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426770348", "bodyText": "No. I'll update the comment.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T17:01:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NDMyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NTk3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430595971", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:48:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NDMyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NDk0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426674945", "bodyText": "What would be the reason to return false here? If it is an error, then I would expect it to be propagated through an exception.", "author": "fpj", "createdAt": "2020-05-18T14:38:38Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NjE0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430596143", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:48:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NDk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NjU2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426676560", "bodyText": "I suppose chunks and lengths must have the same size and there is a 1:1 mapping from one array to the other? The abstraction is not very clean and it is prone to errors. Why have you chosen to do it rather than pass pairs?", "author": "fpj", "createdAt": "2020-05-18T14:40:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean delete(ChunkHandle handle) throws IOException;\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to read from.\n+     * @param fromOffset Offset in the file from which to start reading.\n+     * @param length Number of bytes to read.\n+     * @param buffer Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws NullPointerException  If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException;\n+\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    int write(ChunkHandle handle, long offset, int length, InputStream data) throws IOException;\n+\n+    /**\n+     * Concatenates two or more chunks using native facility. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @param lengths Lengths of chunks.\n+     * @return int Number of bytes concatenated.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    int concatNatively(ChunkHandle[] chunks, long[] lengths) throws IOException, UnsupportedOperationException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc2MTEzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426761135", "bodyText": "Agree. However there is no standard Pair interface in java. Should we use Apache commons ImmutablePair<L,R> here?", "author": "sachin-j-joshi", "createdAt": "2020-05-18T16:45:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NjU2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkxMTIyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427911227", "bodyText": "I noticed that we are importing commons across the code. I'm not sure if it is being used only for pairs, in which case I'd claim that we are loading a dependency unnecessarily. If it is already there, then you can use it, but you can also create a data type (e.g., ChunkInfo) with two fields.", "author": "fpj", "createdAt": "2020-05-20T10:39:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NjU2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5NjM1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430596350", "bodyText": "Added new class ConcatArgument", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:49:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY3NjU2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426692510", "bodyText": "This is the first time we are talking about attributes for chunks in this class, which makes me wonder whether we should be leaking this concept of attributes for chunks at all.", "author": "fpj", "createdAt": "2020-05-18T15:02:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean delete(ChunkHandle handle) throws IOException;\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to read from.\n+     * @param fromOffset Offset in the file from which to start reading.\n+     * @param length Number of bytes to read.\n+     * @param buffer Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws NullPointerException  If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException;\n+\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    int write(ChunkHandle handle, long offset, int length, InputStream data) throws IOException;\n+\n+    /**\n+     * Concatenates two or more chunks using native facility. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @param lengths Lengths of chunks.\n+     * @return int Number of bytes concatenated.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    int concatNatively(ChunkHandle[] chunks, long[] lengths) throws IOException, UnsupportedOperationException;\n+\n+    /**\n+     * Concatenates two or more chunks using append operation. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @param lengths Lengths of chunks.\n+     * @return int Number of bytes concatenated.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    int concatWithAppend(ChunkHandle[] chunks, long[] lengths) throws IOException, UnsupportedOperationException;\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the storage object to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    boolean truncate(ChunkHandle handle, long offset) throws IOException, UnsupportedOperationException;\n+\n+    /**\n+     * Sets readonly attribute for the chunk.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNDIwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426804200", "bodyText": "This method allows to mark chunk as readonly. It is useful to make chunks readonly when you are done writing to them. But not absolutely essential either.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:03:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkxMTg2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427911863", "bodyText": "It makes sense, the confusing part is mentioning attributes, which is not a concept that has appeared in this interface until here. It makes it feel like there are other attributes, and we don't say what they are here.", "author": "fpj", "createdAt": "2020-05-20T10:41:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4Mjk3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430682972", "bodyText": "removed word attribute", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:17:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4NTI1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426685259", "bodyText": "private final", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:53:05Z", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentTruncatedException.java", "diffHunk": "@@ -9,12 +9,20 @@\n  */\n package io.pravega.segmentstore.contracts;\n \n+import lombok.Getter;\n+\n /**\n  * An Exception that indicates a StreamSegment has been truncated and certain offsets cannot be accessed anymore.\n  */\n public class StreamSegmentTruncatedException extends StreamSegmentException {\n     private static final long serialVersionUID = 1L;\n \n+    /**\n+     * Lowest accessible offset for the segment.\n+     */\n+    @Getter\n+    long startOffset;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MzA3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430683077", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:17:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4NTI1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4NjA2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426686063", "bodyText": "Tip: Use @Data on the class. This will create constructor, getter and tostring for it. Then annotate chunkname with @NotNull (the lombok one).", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:54:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkHandle.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+\n+/**\n+ * Handle to a chunk.\n+ */\n+public class ChunkHandle {\n+    /**\n+     * Name of the chunk.\n+     */\n+    private final String chunkName;\n+\n+    /**\n+     *  Whether the segment is read only or not.\n+     */\n+    private final boolean isReadOnly;\n+\n+    /**\n+     * Creates a new instance of ChunkHandle.\n+     *\n+     * @param chunkName Name of the segment.\n+     * @param isReadOnly  Whether the segment is read only or not.\n+     */\n+    public ChunkHandle(String chunkName, boolean isReadOnly) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5ODg3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430598873", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4NjA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4NzMzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426687335", "bodyText": "private final for all these fields.", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:55:47Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkInfo.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Chunk Information.\n+ */\n+@Builder\n+@Data\n+final public class ChunkInfo {\n+    /**\n+     * Length of the chunk.\n+     */\n+    long length;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4Nzg2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426687863", "bodyText": "Tip: replace truncate with link to actual method: {@link #truncate}", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:56:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4ODQ0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426688443", "bodyText": "Fix this for all.", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:57:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4Nzg2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDU5ODk5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430598999", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:53:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4Nzg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4ODI3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426688274", "bodyText": "Please no more than 1 consecutive blank lines. This PR already has enough lines :)", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:57:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MzI3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430683279", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:18:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4ODI3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4ODk0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426688948", "bodyText": "whether the given chunk exists. There is no notion of file or object here.\nAlso, why does it throw IOException? I haven't seen any exists method throw an exception...\nAlso, I would prefer if at this point, the adapter converts the exception into Pravega exceptions. IOException may bubble up from FileSystem, but not from S3. After reading the rest of this file, I see that it may be worthwhile to just throw the Pravega exceptions. There are plenty of those, and the adapters themselves can translate appropriately. A blanket IOException is not very informative for the user, since the calling code may not know what to expect and thus code properly to the API.", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:57:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMjEzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430612138", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:14:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY4ODk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MDA2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426690060", "bodyText": "chunk.\nPlease fix this everywhere in the file.", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:59:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjE1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430776155", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:17:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MDA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MDIzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426690238", "bodyText": "remove String", "author": "andreipaduroiu", "createdAt": "2020-05-18T14:59:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMjMyMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430612323", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:14:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MDIzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426692518", "bodyText": "Since we are in a good effort of reducing redundant memory allocations and copies, is there a way we can eliminate this byte[] argument?\nI am not saying we change anything now, but perhaps pass in a ByteBuffer.\nOf the 3 adapters that we have so far, how do they actually perform the read? Do they expect an array? Do they return an InputStream? Something else?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:02:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean delete(ChunkHandle handle) throws IOException;\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to read from.\n+     * @param fromOffset Offset in the file from which to start reading.\n+     * @param length Number of bytes to read.\n+     * @param buffer Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws NullPointerException  If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjIyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426806221", "bodyText": "Currently  SyncStorage has following methods . I have tried to keep byte[] and InputStream . But if there is enough urgency we can change this to accept ByteBuffer\n int read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length) throws StreamSegmentException;\n void write(SegmentHandle handle, long offset, InputStream data, int length) throws StreamSegmentException;", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:08:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMjU1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430612558", "bodyText": "We'll do this a separate effort.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:14:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MjUxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MzA1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426693053", "bodyText": "Same comment here. I know we use InputStream currently and I don't want to change that, but if we pass in a ByteBuffer, would the existing underlying adapters make better use of it?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:03:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean exists(String chunkName) throws IOException;\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    ChunkHandle create(String chunkName) throws IOException;\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    boolean delete(ChunkHandle handle) throws IOException;\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException;\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to read from.\n+     * @param fromOffset Offset in the file from which to start reading.\n+     * @param length Number of bytes to read.\n+     * @param buffer Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws NullPointerException  If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException;\n+\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    int write(ChunkHandle handle, long offset, int length, InputStream data) throws IOException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjI3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430776276", "bodyText": "postponed to other issue.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:17:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5MzA1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NDgwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426694804", "bodyText": "@Data", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:06:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SegmentStorageHandle.java", "diffHunk": "@@ -0,0 +1,69 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+\n+/**\n+ * Defines a Handle that can be used to operate on Segments in Storage.\n+ */\n+public class SegmentStorageHandle implements SegmentHandle {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMjcyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430612720", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:15:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NDgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NTA2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426695063", "bodyText": "private final", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:06:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadata.java", "diffHunk": "@@ -0,0 +1,53 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents chunk metadata.\n+ */\n+@Builder(toBuilder = true)\n+@Data\n+public class ChunkMetadata implements StorageMetadata {\n+    /**\n+     * Name of this chunk.\n+     */\n+    String name;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NjM3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430776372", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:17:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NTA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NTQ1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426695457", "bodyText": "complete", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:06:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMjg2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430612862", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:15:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NTQ1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NjYxMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426696612", "bodyText": "So... what is the difference between this method and beginTransaction? When should either be called and what is the effect on the underlying metadata store.\nPlease explain this clearly in this class' Javaodoc as it is not clear for how to use this.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:08:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMzAyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430613021", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:15:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NjYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NzU2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426697563", "bodyText": "What is a key?\nI am confused by the Javadoc in this file. The create method says it creates a record; this method says it deletes the record given the key. What is the key ?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:09:48Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMzE5MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430613190", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5NzU2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODA4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426698086", "bodyText": "You need to clearly define what lazy behavior is. When does it actually get written.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:10:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODQ0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426698443", "bodyText": "What's the difference between this and the overloaded method above?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:11:00Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @param skipStoreCheck true if data is not to be reloaded from store.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMzQyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430613425", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:16:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODQ0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODUzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426698537", "bodyText": "And here.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:11:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @param skipStoreCheck true if data is not to be reloaded from store.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Commits given transaction.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODg0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426698846", "bodyText": "What is the behavior of this method? What does it do? What exceptions should I expect?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:11:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Storage Metadata store.\n+ */\n+public interface ChunkMetadataStore extends AutoCloseable {\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @param skipStoreCheck true if data is not to be reloaded from store.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException;\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException;\n+\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    void commit(MetadataTransaction txn) throws StorageMetadataException;\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    void abort(MetadataTransaction txn) throws StorageMetadataException;\n+\n+    /**\n+     * Fences the store.\n+     */\n+    void markFenced();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTU3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430761571", "bodyText": "updated.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T23:27:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5ODg0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTExMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426699113", "bodyText": "private final\nat least make all your fields private.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:11:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implemenation of the MetadataTransaction.\n+ * This implementation delegates all calls to underlying storage.\n+ */\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    ChunkMetadataStore store;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMzY1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430613651", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:16:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTY3NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426699675", "bodyText": "This class is not thread safe.\n\nDo we expect concurrent access?\nDo we expect to pass it between multiple threads (even in sequence)?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:12:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implemenation of the MetadataTransaction.\n+ * This implementation delegates all calls to underlying storage.\n+ */\n+public class MetadataTransaction implements AutoCloseable {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc1NzY3NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426757675", "bodyText": "This data is not supposed to be passed between threads.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T16:39:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTY3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzUyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433517520", "bodyText": "Then mark it as @NotThreadSafe", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:11:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTY3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4Nzc2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443987766", "bodyText": "Can you also add final ?", "author": "eolivelli", "createdAt": "2020-06-23T06:23:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjY5OTY3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMDIyMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426700222", "bodyText": "Exposing your internals is a bad practice. Make this a regular hash map, keep it internal, and provide your own APIs to modify and query it.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:13:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implemenation of the MetadataTransaction.\n+ * This implementation delegates all calls to underlying storage.\n+ */\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTY3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430761673", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T23:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMDIyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426701067", "bodyText": "Where is this used?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:14:37Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implemenation of the MetadataTransaction.\n+ * This implementation delegates all calls to underlying storage.\n+ */\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;\n+\n+    /**\n+     * Optional external commit operation that is executed during the commit.\n+     * The transaction commit operation fails if this operation fails.\n+     */\n+    @Getter\n+    @Setter\n+    private Callable<Void> externalCommitStep;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNDA4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430614088", "bodyText": "It is used with storage metadata segments.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:17:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTA2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTU2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426701564", "bodyText": "Document these values. Also, do they need to be public?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:15:22Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents segment metadata.\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+public class SegmentMetadata implements StorageMetadata {\n+    public static final int ACTIVE  = 0x0001;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTYyMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426701622", "bodyText": "private final", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:15:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents segment metadata.\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+public class SegmentMetadata implements StorageMetadata {\n+    public static final int ACTIVE  = 0x0001;\n+    public static final int SEALED  = 0x0002;\n+    public static final int DELETED = 0x0004;\n+    public static final int OWNERSHIP_CHANGED = 0x0008;\n+\n+    /**\n+     * Name of the segment.\n+     */\n+    String name;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc2MTc1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430761751", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T23:28:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMTYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMjUyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426702524", "bodyText": "validate?\nYou should add messages to all these checks explaining what's wrong. A simple IllegalStateException is not very useful when debugging.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:16:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents segment metadata.\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+public class SegmentMetadata implements StorageMetadata {\n+    public static final int ACTIVE  = 0x0001;\n+    public static final int SEALED  = 0x0002;\n+    public static final int DELETED = 0x0004;\n+    public static final int OWNERSHIP_CHANGED = 0x0008;\n+\n+    /**\n+     * Name of the segment.\n+     */\n+    String name;\n+\n+    /**\n+     * Length of the segment.\n+     */\n+    long length;\n+\n+    /**\n+     * Start offset of the segment. This is offset of the first byte available for read.\n+     */\n+    long startOffset;\n+\n+    /**\n+     * Status bit flags.\n+     */\n+    int status;\n+\n+    /**\n+     * Maximum Rolling length of the segment.\n+     */\n+    long maxRollinglength;\n+\n+    /**\n+     * Name of the first chunk.\n+     */\n+    String firstChunk;\n+\n+    /**\n+     * Name of the last chunk.\n+     */\n+    String lastChunk;\n+\n+    /**\n+     * Last modified time.\n+     */\n+    long lastModified;\n+\n+    /**\n+     * Offset of the first byte of the first chunk.\n+     * This is NOT the same as start offset of the segment. Byte at this offset may not be available for read.\n+     * With arbitrary truncates the effective start offset might be in the middle of the first chunk.\n+     *\n+     */\n+    long firstChunkStartOffset;\n+\n+    /**\n+     * Offset of the first byte of the first chunk.\n+     */\n+    long lastChunkStartOffset;\n+\n+    /**\n+     * Epoch of the container that last owned it.\n+     */\n+    long ownerEpoch;\n+\n+    /**\n+     * Retrieves the key associated with the metadata, which is the name of the segment.\n+     *\n+     * @return Name of the segment.\n+     */\n+    @Override\n+    public String getKey() {\n+        return name;\n+    }\n+\n+    /**\n+     * Creates a copy of this instance.\n+     *\n+     * @return\n+     */\n+    @Override\n+    public StorageMetadata copy() {\n+        return toBuilder().build();\n+    }\n+\n+    /**\n+     * Sets active status.\n+     * @param value Value to set.\n+     * @return This instance so that these calls can be chained.\n+     */\n+    public SegmentMetadata setActive(boolean value) {\n+        return setFlag(ACTIVE, value);\n+    }\n+\n+    /**\n+     * Sets sealed or unsealed status.\n+     * @param value Value to set.\n+     * @return This instance so that these calls can be chained.\n+     */\n+    public SegmentMetadata setSealed(boolean value) {\n+        return setFlag(SEALED, value);\n+    }\n+\n+    /**\n+     * Sets whether ownership was recently changed.\n+     * This value indicates that a new chunk must be created for new owner. The flag is cleared after first write by new owner.\n+     * @param value Value to set.\n+     * @return This instance so that these calls can be chained.\n+     */\n+    public SegmentMetadata setOwnershipChanged(boolean value) {\n+        return setFlag(OWNERSHIP_CHANGED, value);\n+    }\n+\n+    /**\n+     * Gets active status.\n+     * @return True if active, false otherwise.\n+     */\n+    public boolean isActive() {\n+        return getFlag(ACTIVE);\n+    }\n+\n+    /**\n+     * Gets sealed/unsealed status.\n+     * @return True if sealed, false otherwise.\n+     */\n+    public boolean isSealed() {\n+        return getFlag(SEALED);\n+    }\n+\n+    /**\n+     * Gets whether ownership was changed.\n+     * @return True if changed, false otherwise.\n+     */\n+    public boolean isOwnershipChanged() {\n+        return getFlag(OWNERSHIP_CHANGED);\n+    }\n+\n+    /**\n+     * Checks the invariants that must be held for a segment.\n+     */\n+    public void checkInvariants() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMzY4NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426703684", "bodyText": "Actually, instead of this method, I would suggest writing your own setters and validate input as it comes in. That will guarantee that this object is always in a good state and that you won't \"forget\" to validate it.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMjUyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTI2NDI4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431264280", "bodyText": "The segment metadata must have some invariant respected. While we are modifying the metadata some of these could be temporarily not satisfied. We want to make sure any operation on metadata leaves it in state that satisfies all its invariant.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T16:07:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMjUyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMzA4MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426703081", "bodyText": "Do not make this public. Anyone can read (and misinterpret) and worse, anyone can set it to arbitrary values.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:17:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents segment metadata.\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+public class SegmentMetadata implements StorageMetadata {\n+    public static final int ACTIVE  = 0x0001;\n+    public static final int SEALED  = 0x0002;\n+    public static final int DELETED = 0x0004;\n+    public static final int OWNERSHIP_CHANGED = 0x0008;\n+\n+    /**\n+     * Name of the segment.\n+     */\n+    String name;\n+\n+    /**\n+     * Length of the segment.\n+     */\n+    long length;\n+\n+    /**\n+     * Start offset of the segment. This is offset of the first byte available for read.\n+     */\n+    long startOffset;\n+\n+    /**\n+     * Status bit flags.\n+     */\n+    int status;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNDg4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430614880", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwMzA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNDAxOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426704019", "bodyText": "Why do all these classes have a copy method?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:18:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,30 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ */\n+public interface StorageMetadata extends Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     * @return key.\n+     */\n+    String getKey();\n+\n+    /**\n+     * Creates a copy of this instance.\n+     * @return\n+     */\n+    StorageMetadata copy();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNTA3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430615073", "bodyText": "renamed to deepCopy", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:19:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNDAxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTg4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426705887", "bodyText": "I assume this is a mock that is never used in production code, correct?\nEven so, unless you move this class to the test package, I would suggest transforming these fields into abstract/virtual methods and force the derived classes to override them. In the state that they are right now, anyone can instantiate your class and change these fields arbitrarily.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:21:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/AbstractInMemoryChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,96 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorageProvider;\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+/**\n+ * Base class for InMemory mock implementations.\n+ * Allows to simulate ChunkStorageProvider with different supported properties.\n+ */\n+abstract public class AbstractInMemoryChunkStorageProvider extends BaseChunkStorageProvider {\n+    /**\n+     * value to return when {@link AbstractInMemoryChunkStorageProvider#supportsTruncation()} is called.\n+     */\n+    @Getter", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0MTE1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431341151", "bodyText": "No I need them to be accessible to test code. I'll probably move this to test package.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:03:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNTg4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNjU2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426706562", "bodyText": "This is the last time I comment on this:\n\nMake all your fields private final\nDo not expose maps or other internal data structures.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:22:37Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,289 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * In-Memory mock for ChunkStorageProvider. Contents is destroyed when object is garbage collected.\n+ */\n+public class InMemoryChunkStorageProvider extends AbstractInMemoryChunkStorageProvider {\n+    ConcurrentHashMap<String, InMemoryChunk> chunks = new ConcurrentHashMap<String, InMemoryChunk>();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNjc1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426706758", "bodyText": "final?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:22:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryMetadataStore.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.metadata.BaseMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import lombok.val;\n+\n+import java.util.Collection;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * InMemoryMetadataStore stores the key-values in memory.\n+ */\n+public class InMemoryMetadataStore extends BaseMetadataStore {\n+\n+    private AtomicBoolean entryTracker = new AtomicBoolean(false);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ0OTk0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431449946", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T21:22:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNjc1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzM4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426707380", "bodyText": "well?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:23:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/noop/NoOpChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.noop;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.mocks.AbstractInMemoryChunkStorageProvider;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/*\n+ NoOp implementation.\n+ */\n+public class NoOpChunkStorageProvider extends AbstractInMemoryChunkStorageProvider {\n+    @Getter\n+    @Setter\n+    ConcurrentHashMap<String, ChunkData> chunkMetadata = new ConcurrentHashMap<>();\n+\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkInfo.builder().name(chunkName).length(chunkData.length).build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null != chunkData) {\n+            throw new FileAlreadyExistsException(chunkName);\n+        }\n+        chunkMetadata.put(chunkName, new ChunkData());\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected boolean doesExist(String chunkName) throws IOException, IllegalArgumentException {\n+        return chunkMetadata.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected boolean doDelete(ChunkHandle handle) throws IOException, IllegalArgumentException {\n+        Preconditions.checkNotNull(null != handle, \"handle\");\n+        Preconditions.checkNotNull(handle.getChunkName(), \"handle\");\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(handle.getChunkName());\n+        }\n+        if (chunkData.isReadonly) {\n+            throw new IOException(\"chunk is readonly\");\n+        }\n+        chunkMetadata.remove(handle.getChunkName());\n+        return true;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException {\n+        // TODO: Add validation", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0MTczMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431341730", "bodyText": "validation was already added.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:04:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzY3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426707674", "bodyText": "Use the appropriate method in Preconditions or Exceptions.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:24:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/noop/NoOpChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,200 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.noop;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.mocks.AbstractInMemoryChunkStorageProvider;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/*\n+ NoOp implementation.\n+ */\n+public class NoOpChunkStorageProvider extends AbstractInMemoryChunkStorageProvider {\n+    @Getter\n+    @Setter\n+    ConcurrentHashMap<String, ChunkData> chunkMetadata = new ConcurrentHashMap<>();\n+\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkInfo.builder().name(chunkName).length(chunkData.length).build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null != chunkData) {\n+            throw new FileAlreadyExistsException(chunkName);\n+        }\n+        chunkMetadata.put(chunkName, new ChunkData());\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected boolean doesExist(String chunkName) throws IOException, IllegalArgumentException {\n+        return chunkMetadata.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected boolean doDelete(ChunkHandle handle) throws IOException, IllegalArgumentException {\n+        Preconditions.checkNotNull(null != handle, \"handle\");\n+        Preconditions.checkNotNull(handle.getChunkName(), \"handle\");\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(handle.getChunkName());\n+        }\n+        if (chunkData.isReadonly) {\n+            throw new IOException(\"chunk is readonly\");\n+        }\n+        chunkMetadata.remove(handle.getChunkName());\n+        return true;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws IOException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException {\n+        // TODO: Add validation\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new FileNotFoundException(handle.getChunkName());\n+        }\n+\n+        if (fromOffset >= chunkData.length || fromOffset + length > chunkData.length) {\n+            throw new IndexOutOfBoundsException(\"fromOffset\");\n+        }\n+\n+        if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MDM4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439170386", "bodyText": "Th exceptions thrown by those methods are slightly different.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:13:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcwNzY3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDM2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426710367", "bodyText": "What changed here to trigger this?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:27:59Z", "path": "test/testcommon/src/main/java/io/pravega/test/common/LeakDetectorTestSuite.java", "diffHunk": "@@ -39,15 +39,15 @@\n     private ResourceLeakDetector.Level originalLevel;\n \n     @Before\n-    public void before() {\n+    public void before() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNzQ1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430617451", "bodyText": "Base class before() needed signature change.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:23:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDM2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDQ3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426710476", "bodyText": "same here", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:28:09Z", "path": "test/testcommon/src/main/java/io/pravega/test/common/ThreadPooledTestSuite.java", "diffHunk": "@@ -24,13 +24,13 @@\n     private ScheduledExecutorService executorService = null;\n \n     @Before\n-    public void before() {\n+    public void before() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNzYyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430617620", "bodyText": "same as above.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:23:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDc3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426710777", "bodyText": "@Slf4j ???", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:28:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwOTI2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426809265", "bodyText": "ok remembered . @slf4j creates a private field and I wanted it to be protected so that derived classes can use them.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:14:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDc3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkxMzEyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427913121", "bodyText": "I'm not sure what you gain by doing it, it is best to use the annotation, also on the derived classes.", "author": "fpj", "createdAt": "2020-05-20T10:43:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDc3Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNzc2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430617762", "bodyText": "ok. Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:24:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMDc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMTE1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426711152", "bodyText": "Put these metrics in a separate, static class. Please follow the pattern used throughout other parts of the system.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:29:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxNzg3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430617873", "bodyText": "done.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:24:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMTE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMjU4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426712587", "bodyText": "Where is this used? Please wire it in.\nMake it private\nIf you want to provide access to subclasses, consider creating a protected method named ensureNotClosed which will throw if closed.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:31:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);\n+    protected static final OpStatsLogger WRITE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_WRITE_LATENCY);\n+    protected static final OpStatsLogger CREATE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CREATE_LATENCY);\n+    protected static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n+    protected static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n+\n+    protected static final Counter READ_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_READ_BYTES);\n+    protected static final Counter WRITE_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_WRITE_BYTES);\n+    protected static final Counter CONCAT_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_BYTES);\n+\n+    protected static final Counter CREATE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CREATE_COUNT);\n+    protected static final Counter DELETE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_DELETE_COUNT);\n+    protected static final Counter CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_COUNT);\n+\n+    protected static final Counter LARGE_CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_LARGE_CONCAT_COUNT);\n+\n+    protected final AtomicBoolean closed;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM0MjM0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431342345", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:05:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMjU4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426713199", "bodyText": "There's no point in doing trace enter-leave if you log-debug everything. I would suggest cleaning it up.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:32:03Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);\n+    protected static final OpStatsLogger WRITE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_WRITE_LATENCY);\n+    protected static final OpStatsLogger CREATE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CREATE_LATENCY);\n+    protected static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n+    protected static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n+\n+    protected static final Counter READ_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_READ_BYTES);\n+    protected static final Counter WRITE_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_WRITE_BYTES);\n+    protected static final Counter CONCAT_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_BYTES);\n+\n+    protected static final Counter CREATE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CREATE_COUNT);\n+    protected static final Counter DELETE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_DELETE_COUNT);\n+    protected static final Counter CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_COUNT);\n+\n+    protected static final Counter LARGE_CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_LARGE_CONCAT_COUNT);\n+\n+    protected final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+        return retValue;\n+    }\n+\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        CREATE_COUNT.inc();\n+\n+        log.debug(\"Create chunk={} latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public boolean delete(ChunkHandle handle) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        boolean retValue = doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkxODExOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427918118", "bodyText": "It might be. There is information at debug and trace levels. Whatever we output at debug will also be output at trace, so the trace information should complement the debug one. I'd say it is fine to have both as long as the trace information is complementary.", "author": "fpj", "createdAt": "2020-05-20T10:53:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE1MDY1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428150658", "bodyText": "log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\nhas latency information which trace does not include.\nRecently, we found such information to be useful for debugging with ECS.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T16:34:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxMzE5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDExNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426714116", "bodyText": "This check is superfluous.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:33:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);\n+    protected static final OpStatsLogger WRITE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_WRITE_LATENCY);\n+    protected static final OpStatsLogger CREATE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CREATE_LATENCY);\n+    protected static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n+    protected static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n+\n+    protected static final Counter READ_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_READ_BYTES);\n+    protected static final Counter WRITE_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_WRITE_BYTES);\n+    protected static final Counter CONCAT_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_BYTES);\n+\n+    protected static final Counter CREATE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CREATE_COUNT);\n+    protected static final Counter DELETE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_DELETE_COUNT);\n+    protected static final Counter CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_COUNT);\n+\n+    protected static final Counter LARGE_CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_LARGE_CONCAT_COUNT);\n+\n+    protected final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+        return retValue;\n+    }\n+\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        CREATE_COUNT.inc();\n+\n+        log.debug(\"Create chunk={} latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public boolean delete(ChunkHandle handle) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        boolean retValue = doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+        return retValue;\n+    }\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        READ_LATENCY.reportSuccessEvent(elapsed);\n+        READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @param lengths Lengths of chunks.\n+     * @return int Number of bytes concatenated.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concatNatively(ChunkHandle[] chunks, long[] lengths) throws IOException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(null != lengths, \"lengths must not be null\");\n+        Preconditions.checkArgument(chunks.length == lengths.length, \"chunks and lengths should have same length\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(!chunks[0].isReadOnly(), \"target chunk must readonly.\");\n+        Preconditions.checkArgument(lengths[0] >= 0, \"target chunk lenth must be non negative.\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxODA3NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430618075", "bodyText": "outdated.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:24:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDYwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426714607", "bodyText": "What is segment storage?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:34:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM1MjI2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431352260", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:22:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDYwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNDc4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426714786", "bodyText": "final", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:34:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTE0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426715143", "bodyText": "Remove the setter. Pass this via config.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:35:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxODMzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430618334", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:25:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTMwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426715308", "bodyText": "Do not expose this.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:35:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMjkwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426722908", "bodyText": "Also how do you maintain this? What is the eviction policy? Please describe that in its javadoc.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTMwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM1MjE2OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431352169", "bodyText": "eviction logic coming in next iteration.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:22:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTMwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTQzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426715435", "bodyText": "Fix your todo", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:35:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxODc5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430618798", "bodyText": "done.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:25:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTQzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNTc0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426715741", "bodyText": "Why do you set closed here too? Isn't the constructor sufficient?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:35:57Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNzQwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426717405", "bodyText": "Is there a single ChunkStorageManager? From what I understand there is one per  container.\nIf the latter, please adjust all of your logging to output in our standard format:\nEither:\n\n<component-name>[<container-id> OR\n<component-name>[<container-id>-<segment-id>]\n\nSee how it was done in other components.\nThis will help log parsing.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:38:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3MzI0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430773248", "bodyText": "done [", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:07:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNzQwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNzg3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426717876", "bodyText": "Why do you even bother to catch if you're just going to rethrow? Can you just bubble it up?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:38:55Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxOTIwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430619206", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:26:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxNzg3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODA2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426718064", "bodyText": ". instead of ..", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:39:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxOTMyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430619328", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODIyNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426718226", "bodyText": "What does defrag mean?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:39:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxOTYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430619617", "bodyText": "common industry term for defragmentation", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:27:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODIyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODcxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426718710", "bodyText": "How do you know that?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:40:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODMyMjYyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428322625", "bodyText": "OwnerEpoch value on saved metadata is already smaller than this epoch\nif (segmentMetadata.getOwnerEpoch() < this.epoch) {\n                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n                    claimOwnership(txn, segmentMetadata);\n                }", "author": "sachin-j-joshi", "createdAt": "2020-05-20T21:38:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxODcxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTAxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426719011", "bodyText": "here too", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:40:29Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxOTcxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430619714", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:27:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTQ2MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426719461", "bodyText": "This method is 200 lines long. Please break it down into smaller ones.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:41:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMTI2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426811263", "bodyText": "Java doesn't have out parameters . So extracting smaller methods requires creating method objects etc. So instead of making code simple it makes it unnecessarily complicated and in my opinion defeats the purpose. If there is a simple way to break this method I'll definitely do that.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjE2MDI0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442160247", "bodyText": "I don't know if I agree with that statement, @sachin-j-joshi. Making smaller, single-purpose methods to implement parts of the functionality of a complex method is better for readability. Also, such smaller functions are easier to test in isolation.", "author": "RaulGracia", "createdAt": "2020-06-18T11:34:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTQ2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcxOTYxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426719610", "bodyText": "why?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:41:21Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDI2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426720267", "bodyText": "Method too long.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:42:26Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMTM3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426811374", "bodyText": "Java doesn't have out parameters . So extracting smaller methods requires creating method objects etc. So instead of making code simple it makes it unnecessarily complicated and in my opinion defeats the purpose. If there is a simple way to break this method I'll definitely do that.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:18:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDY1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426720659", "bodyText": "Remove this.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:42:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYyMDEwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430620101", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:27:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDY1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDk4MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426720981", "bodyText": "You need to explain in great detail what this does. Please put it in the Javadoc (instead of a comment inside the method).", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:43:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMTUzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426811537", "bodyText": "sure.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:18:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMDk4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMTE3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426721179", "bodyText": "method too long", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:43:48Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1375 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import com.google.common.primitives.Longs;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataVersionMismatchException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Default SegmentRollingPolicy to use.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy defaultRollingPolicy;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concatNatively.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;\n+\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered for concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long maxSizeLimitForNativeConcat = Long.MAX_VALUE;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    @Getter\n+    private ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+        this.maxSizeLimitForNativeConcat = defaultRollingPolicy.getMaxLength(); //TODO : Pick this up from config.\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param defaultRollingPolicy A SegmentRollingPolicy to apply to every StreamSegment that does not have its own policy\n+     *                             defined.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, SegmentRollingPolicy defaultRollingPolicy) {\n+        this.defaultRollingPolicy = Preconditions.checkNotNull(defaultRollingPolicy, \"defaultRollingPolicy\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+        this.closed.set(false);\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+        this.closed.set(false);\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required..\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } catch (Exception ex) {\n+                throw ex;\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (FileNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<String> chunksToConcat = new ArrayList<>();\n+            ArrayList<Long> lengths = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(targetChunkName);\n+            lengths.add(targetSizeAfterConcat);\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && minSizeLimitForNativeConcat < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > maxSizeLimitForNativeConcat) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(nextChunkName);\n+                targetSizeAfterConcat += next.getLength();\n+                lengths.add(next.getLength());\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+                long[] lengthsArr = Longs.toArray(lengths);\n+\n+                ChunkHandle[] chunksArr = new ChunkHandle[chunksToConcat.size()];\n+                chunksArr[0] = ChunkHandle.writeHandle(chunksToConcat.get(0));\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksArr[i] = ChunkHandle.readHandle(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunksArr, lengthsArr);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunksArr, lengthsArr);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunksArr.length; i++) {\n+                    txn.delete(chunksArr[i].getChunkName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataVersionMismatchException ex) {\n+                throw ex;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMTU4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426811583", "bodyText": "Java doesn't have out parameters . So extracting smaller methods requires creating method objects etc. So instead of making code simple it makes it unnecessarily complicated and in my opinion defeats the purpose. If there is a simple way to break this method I'll definitely do that.", "author": "sachin-j-joshi", "createdAt": "2020-05-18T18:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMTE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMzY1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426723657", "bodyText": "You used the word store three times in the first sentence. It's hard to understand. Can you try to rephrase it please?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:47:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMzk5MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426723990", "bodyText": "No such thing as tier-1 in the codebase.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:47:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYyMDM1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430620359", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:28:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyMzk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDQxOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426724419", "bodyText": "fix todo", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:48:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDgxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426724810", "bodyText": "Do not expose your inner data structures, even as protected. Write APIs to access and/or mutate them.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:48:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNTA3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426725078", "bodyText": "Same with the next few fields.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDgxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNDkyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426724921", "bodyText": "Shared lock objects are bad.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:49:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNTE5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426725198", "bodyText": "private final", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:49:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNjM4NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426726385", "bodyText": "You execute quite a number of external APIs, some of them a bit shaky in nature (like this one) while holding your lock. The more APIs you execute externally, and the more async they are in nature, the more likely it is for your application to perform poorly or deadlock. Can you reduce the scope of the encompassing lock?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:51:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData: toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTM1MTY2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431351667", "bodyText": "The scope of the lock can not be reduced. However I have plans to take locks per segment instead of global locks.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T18:21:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNjM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNjU5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426726599", "bodyText": "Same comment here about log message standardization.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:51:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzMxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426727317", "bodyText": "Too much logging here. Use only one debug statement. IF you want more, change one to trace.", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:52:37Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData: toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException  {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().copy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    @VisibleForTesting\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.debug(\"Loading key from the store key = {}\", key);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NDY5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430774699", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:12:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzMxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NDc3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430774774", "bodyText": "made it trace", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:12:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzMxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzY4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426727689", "bodyText": "extra space", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:53:06Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData: toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException  {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().copy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    @VisibleForTesting\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.debug(\"Loading key from the store key = {}\", key);\n+        TransactionData fromDb = read(key);\n+        Preconditions.checkState(null != fromDb);\n+        log.debug(\"Done Loading key from the store key = {}\", key);\n+\n+        TransactionData copyForBuffer = fromDb.toBuilder()\n+                .key(key)\n+                .build();\n+\n+        if (null != fromDb.getValue()) {\n+            Preconditions.checkState(0 != fromDb.getVersion(), \"Version is not initialized\");\n+            // Make sure it is a deep copy.\n+            copyForBuffer.setValue(fromDb.getValue().copy());\n+        }\n+        // Put this value in bufferedTxnData buffer.\n+        synchronized (lock) {\n+            // If some other transaction beat us then use that value.\n+            TransactionData oldValue = bufferedTxnData.putIfAbsent(key, copyForBuffer);\n+            if (oldValue != null) {\n+                copyForBuffer = oldValue;\n+            }\n+        }\n+        return copyForBuffer;\n+    }\n+\n+    /**\n+     * Reads a metadata record for the given key.\n+     *\n+     * @param key Key for the metadata record.\n+     * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected TransactionData read(String key) throws StorageMetadataException;\n+\n+    /**\n+     * Writes transaction data from a given list to the metadata store.\n+     * @param dataList List of transaction data to write.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected void writeAll(Collection<TransactionData> dataList) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(metadata);\n+        data.setPersisted(false);\n+        Preconditions.checkState(txn.getVersion() >= data.getVersion());\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Map<String, TransactionData> txnData = txn.getData();\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+\n+        data.setValue(metadata);\n+        data.setPinned(true);\n+        data.setVersion(txn.getVersion());\n+\n+        pinnedKeys.add(metadata.getKey());\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+        txnData.put(metadata.getKey(), TransactionData.builder()\n+                .key(metadata.getKey())\n+                .value(metadata)\n+                .version(txn.getVersion())\n+                .build() );", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4MzQ0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430683442", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:18:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzY4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzkxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426727914", "bodyText": "Didn't you validate this inside the metadata object?", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:53:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData: toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException  {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().copy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    @VisibleForTesting\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.debug(\"Loading key from the store key = {}\", key);\n+        TransactionData fromDb = read(key);\n+        Preconditions.checkState(null != fromDb);\n+        log.debug(\"Done Loading key from the store key = {}\", key);\n+\n+        TransactionData copyForBuffer = fromDb.toBuilder()\n+                .key(key)\n+                .build();\n+\n+        if (null != fromDb.getValue()) {\n+            Preconditions.checkState(0 != fromDb.getVersion(), \"Version is not initialized\");\n+            // Make sure it is a deep copy.\n+            copyForBuffer.setValue(fromDb.getValue().copy());\n+        }\n+        // Put this value in bufferedTxnData buffer.\n+        synchronized (lock) {\n+            // If some other transaction beat us then use that value.\n+            TransactionData oldValue = bufferedTxnData.putIfAbsent(key, copyForBuffer);\n+            if (oldValue != null) {\n+                copyForBuffer = oldValue;\n+            }\n+        }\n+        return copyForBuffer;\n+    }\n+\n+    /**\n+     * Reads a metadata record for the given key.\n+     *\n+     * @param key Key for the metadata record.\n+     * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected TransactionData read(String key) throws StorageMetadataException;\n+\n+    /**\n+     * Writes transaction data from a given list to the metadata store.\n+     * @param dataList List of transaction data to write.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected void writeAll(Collection<TransactionData> dataList) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(metadata);\n+        data.setPersisted(false);\n+        Preconditions.checkState(txn.getVersion() >= data.getVersion());\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Map<String, TransactionData> txnData = txn.getData();\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+\n+        data.setValue(metadata);\n+        data.setPinned(true);\n+        data.setVersion(txn.getVersion());\n+\n+        pinnedKeys.add(metadata.getKey());\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4Mzk5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430683999", "bodyText": "no that was in other places.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:19:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyNzkxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyODE2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r426728163", "bodyText": "final everywhere", "author": "andreipaduroiu", "createdAt": "2020-05-18T15:53:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,510 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * tier-1 fencing is used to establish ownership.\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * 1. It provides simple transactions API with snapshot isolation that is independent of underlying key-value store API .\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed.\n+ * In addition they are atomic - either all changes in the transaction are committed or none at all.\n+ * Snapshot isolation means that transaction is fails if any of the metadata records read and during the transactions are changed after they were read.\n+ *\n+ * 2. It buffers frequently or recently updated metadata keys to optimize read/write to underlying.\n+ * Especially it provides \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg when only length of chunk is changed.)\n+ * (Note that otherwise for each commit the data is written to underlying storage.)\n+ *\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000; //TODO : load from the config\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    protected final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    protected final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    protected final AtomicBoolean fenced;\n+\n+    /**\n+     *  Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    protected final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    protected final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return\n+     *\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException  {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     * @param txn transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.debug(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.debug(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList =  modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.debug(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData: toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException  {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return Metadata for given key. Null if key was not found.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().copy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    @VisibleForTesting\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.debug(\"Loading key from the store key = {}\", key);\n+        TransactionData fromDb = read(key);\n+        Preconditions.checkState(null != fromDb);\n+        log.debug(\"Done Loading key from the store key = {}\", key);\n+\n+        TransactionData copyForBuffer = fromDb.toBuilder()\n+                .key(key)\n+                .build();\n+\n+        if (null != fromDb.getValue()) {\n+            Preconditions.checkState(0 != fromDb.getVersion(), \"Version is not initialized\");\n+            // Make sure it is a deep copy.\n+            copyForBuffer.setValue(fromDb.getValue().copy());\n+        }\n+        // Put this value in bufferedTxnData buffer.\n+        synchronized (lock) {\n+            // If some other transaction beat us then use that value.\n+            TransactionData oldValue = bufferedTxnData.putIfAbsent(key, copyForBuffer);\n+            if (oldValue != null) {\n+                copyForBuffer = oldValue;\n+            }\n+        }\n+        return copyForBuffer;\n+    }\n+\n+    /**\n+     * Reads a metadata record for the given key.\n+     *\n+     * @param key Key for the metadata record.\n+     * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected TransactionData read(String key) throws StorageMetadataException;\n+\n+    /**\n+     * Writes transaction data from a given list to the metadata store.\n+     * @param dataList List of transaction data to write.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @VisibleForTesting\n+    abstract protected void writeAll(Collection<TransactionData> dataList) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(metadata);\n+        data.setPersisted(false);\n+        Preconditions.checkState(txn.getVersion() >= data.getVersion());\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Map<String, TransactionData> txnData = txn.getData();\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+\n+        data.setValue(metadata);\n+        data.setPinned(true);\n+        data.setVersion(txn.getVersion());\n+\n+        pinnedKeys.add(metadata.getKey());\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn Transaction.\n+     * @param metadata  metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException  {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+        txnData.put(metadata.getKey(), TransactionData.builder()\n+                .key(metadata.getKey())\n+                .value(metadata)\n+                .version(txn.getVersion())\n+                .build() );\n+    }\n+\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void delete(MetadataTransaction txn, String key) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != key);\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(null);\n+        data.setPersisted(false);\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * {@link AutoCloseable#close()} implementation.\n+     */\n+    @Override\n+    public void close() throws Exception {\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        bufferedTxnData.entrySet().stream().filter(entry -> !entry.getValue().isPersisted() && !entry.getValue().isPinned()).forEach(entry -> modifiedValues.add(entry.getValue()));\n+        if (modifiedValues.size() > 0) {\n+            writeAll(modifiedValues);\n+        }\n+    }\n+\n+    /**\n+     * Marks the store as fenced.\n+     */\n+    public void markFenced() {\n+        this.fenced.set(true);\n+    }\n+\n+    /**\n+     * Stores the transaction data.\n+     */\n+    @Builder(toBuilder = true)\n+    @Data\n+    public static class TransactionData {\n+        /**\n+         * Version\n+         */\n+        private long version;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwMDQyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430600425", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T17:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyODE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzAzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433517038", "bodyText": "I don't think so.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyODE2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2OTg5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439169892", "bodyText": "This field is changes during lifetime of this object.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:11:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjcyODE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkxODkzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427918938", "bodyText": "Empty new line before return statements, please.", "author": "fpj", "createdAt": "2020-05-20T10:54:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,592 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.shared.MetricsNames;\n+import io.pravega.shared.metrics.Counter;\n+import io.pravega.shared.metrics.MetricsProvider;\n+import io.pravega.shared.metrics.OpStatsLogger;\n+import io.pravega.shared.metrics.StatsLogger;\n+\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality for\n+ * Delegates to specific implemntations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    protected static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(BaseChunkStorageProvider.class);\n+\n+    protected static final StatsLogger STATS_LOGGER = MetricsProvider.createStatsLogger(\"BaseChunkStorageProvider\");\n+\n+\n+    protected static final OpStatsLogger READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_READ_LATENCY);\n+    protected static final OpStatsLogger WRITE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_WRITE_LATENCY);\n+    protected static final OpStatsLogger CREATE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CREATE_LATENCY);\n+    protected static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n+    protected static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n+\n+    protected static final Counter READ_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_READ_BYTES);\n+    protected static final Counter WRITE_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_WRITE_BYTES);\n+    protected static final Counter CONCAT_BYTES = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_BYTES);\n+\n+    protected static final Counter CREATE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CREATE_COUNT);\n+    protected static final Counter DELETE_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_DELETE_COUNT);\n+    protected static final Counter CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_CONCAT_COUNT);\n+\n+    protected static final Counter LARGE_CONCAT_COUNT = STATS_LOGGER.createCounter(MetricsNames.STORAGE_LARGE_CONCAT_COUNT);\n+\n+    protected final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+        return retValue;\n+    }\n+\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        CREATE_COUNT.inc();\n+\n+        log.debug(\"Create chunk={} latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public boolean delete(ChunkHandle handle) throws IOException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        boolean retValue = doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+        return retValue;\n+    }\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws IOException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws IOException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws IOException, NullPointerException, IndexOutOfBoundsException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        READ_LATENCY.reportSuccessEvent(elapsed);\n+        READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+        return bytesRead;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMzI5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427923293", "bodyText": "This also needs more detail. Explain what system segments are and why we need a journal for layout changes.\nI actually checked and the PDP design document does not mention a system journal at all.", "author": "fpj", "createdAt": "2020-05-20T11:03:23Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ1MDI5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431450294", "bodyText": "added bit more details.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T21:22:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMzI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMzQyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427923428", "bodyText": "Remove unnecessary new line.", "author": "fpj", "createdAt": "2020-05-20T11:03:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYxMDI1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430610257", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:10:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyMzQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyNjQ5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427926496", "bodyText": "I'm not sure why we are synchronizing and holding a big lock at the same time. Also, the execution of this method naturally splits into separate steps, could we create multiple private methods to improve readability?", "author": "fpj", "createdAt": "2020-05-20T11:09:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyNzUwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427927506", "bodyText": "I'm not sure what the semantics of pinning a txn is.", "author": "fpj", "createdAt": "2020-05-20T11:11:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {\n+        synchronized (LOCK) {\n+            try (val txn = metadataStore.beginTransaction()) {\n+                // Step 1: Create segment metadata.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                            .name(systemSegment)\n+                            .ownerEpoch(epoch)\n+                            .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                            .build();\n+                    segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                    segmentMetadata.checkInvariants();\n+                    txn.create(segmentMetadata);\n+                    txn.markPinned(segmentMetadata);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MjEyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430752120", "bodyText": "updated comments", "author": "sachin-j-joshi", "createdAt": "2020-05-26T22:57:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkyNzUwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzMjk2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r427932966", "bodyText": "Why is it necessary to synchronize and LOCK?", "author": "fpj", "createdAt": "2020-05-20T11:22:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE4MTMxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428181310", "bodyText": "Thanks! I think I forgot to take synchronized out.\nI'll try to reduce scope all locks in that file.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:21:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzMjk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwOTM2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430609366", "bodyText": "fixed. Thanks", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:09:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzkzMjk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAyNTM5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428025391", "bodyText": "String", "author": "fpj", "createdAt": "2020-05-20T13:48:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwOTIwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430609201", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:08:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAyNTM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAyODUwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428028504", "bodyText": "Isn't it only the write that has side-effects and consequently needs to be synchronized? If so, then reduce the scope of the lock.", "author": "fpj", "createdAt": "2020-05-20T13:51:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwOTA2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430609066", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:08:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAyODUwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzMTc5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428031794", "bodyText": "We are not creating a file here, so there is no side-effect, other than the global variable update. Is it why you are protecting under a lock?", "author": "fpj", "createdAt": "2020-05-20T13:55:57Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3OTgzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428179837", "bodyText": "Yes. The writes to journal need to serialized. (yes - systemJournalOffset needs being protected.)", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODAzMTc5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NDQ1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428054455", "bodyText": "Could you remind me why we need to start from zero every time? Is it because there is no notion of compaction and this is bootstrapping from the beginning of times?", "author": "fpj", "createdAt": "2020-05-20T14:23:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {\n+        synchronized (LOCK) {\n+            try (val txn = metadataStore.beginTransaction()) {\n+                // Step 1: Create segment metadata.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                            .name(systemSegment)\n+                            .ownerEpoch(epoch)\n+                            .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                            .build();\n+                    segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                    segmentMetadata.checkInvariants();\n+                    txn.create(segmentMetadata);\n+                    txn.markPinned(segmentMetadata);\n+                }\n+\n+                // We only need to apply the final truncate offset.\n+                val finalTruncateOffsets = new HashMap<String, Long>();\n+                val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+                val chunkStartOffsets = new HashMap<String, Long>();\n+\n+                // Step 2: For each epoch, find the corresponding system journal files.\n+                for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE3NTY2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428175664", "bodyText": "Yes you are correct. There is no notion of compaction of system journal (yet). I am planning to implement it in next iteration.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:12:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA1NDQ1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2MzM0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428063347", "bodyText": "Is it efficient to journal strings? Is there a more compact, efficient way of appending to this journal?", "author": "fpj", "createdAt": "2020-05-20T14:35:00Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5NjczNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428196737", "bodyText": "Agree. The compact serialization is needed and will be added in next iteration. The strings are extremely useful during debugging and tracing.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:47:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA2MzM0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5NTEyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428095125", "bodyText": "Could you elaborate on why we are updating the length like this? it sounds like the segment metadata can be outdated with respect to the system segment data, but it is not entirely clear to me how it can get there.", "author": "fpj", "createdAt": "2020-05-20T15:15:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {\n+        synchronized (LOCK) {\n+            try (val txn = metadataStore.beginTransaction()) {\n+                // Step 1: Create segment metadata.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                            .name(systemSegment)\n+                            .ownerEpoch(epoch)\n+                            .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                            .build();\n+                    segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                    segmentMetadata.checkInvariants();\n+                    txn.create(segmentMetadata);\n+                    txn.markPinned(segmentMetadata);\n+                }\n+\n+                // We only need to apply the final truncate offset.\n+                val finalTruncateOffsets = new HashMap<String, Long>();\n+                val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+                val chunkStartOffsets = new HashMap<String, Long>();\n+\n+                // Step 2: For each epoch, find the corresponding system journal files.\n+                for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+                    // Start scan with file index 0.\n+                    int fileIndexToRecover = 0;\n+                    while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                        val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                        val info = chunkStorage.getInfo(systemLogName);\n+                        val h = chunkStorage.openRead(systemLogName);\n+                        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                        long fromOffset = 0;\n+                        int remaining = contents.length;\n+                        while (remaining > 0) {\n+                            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                            remaining -= bytesRead;\n+                            fromOffset += bytesRead;\n+                        }\n+                        BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                        String line;\n+\n+                        while ((line = reader.readLine()) != null) {\n+                            // Handle only whole records.\n+                            if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                                String[] records = line.split(RECORD_SEPARATOR);\n+                                for (String record : records) {\n+                                    String[] parts = record.split(\":\");\n+                                    if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                        String segmentName = parts[1];\n+                                        String oldChunkName = parts[2];\n+                                        String newChunkName = parts[3];\n+                                        long offset = Long.parseLong(parts[4]);\n+\n+                                        applyChunkAddition(txn, chunkStartOffsets, segmentName, oldChunkName, newChunkName, offset);\n+                                    }\n+                                    if (TRUNCATE_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                        String segmentName = parts[1];\n+                                        long truncateAt = Long.parseLong(parts[2]);\n+                                        String firstChunkName = parts[3];\n+                                        long truncateStartAt = Long.parseLong(parts[4]);\n+                                        finalTruncateOffsets.put(segmentName, truncateAt);\n+                                        finalFirstChunkStartsAtOffsets.put(segmentName, truncateStartAt);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                        // Move to next file.\n+                        fileIndexToRecover++;\n+                    }\n+                }\n+\n+                // Step 3: Adjust the length of the last chunk.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+                    segmentMetadata.checkInvariants();\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                        long length = chunkInfo.getLength();\n+\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                        Preconditions.checkState(null != lastChunk);\n+                        lastChunk.setLength(Math.toIntExact(length));\n+                        txn.update(lastChunk);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + length);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5NTU5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428195592", "bodyText": "In the presence of zombie segment store, the new and old instances are running  concurrently. The length of last chunk is held in memory of zombie process and not written out to file. The new instance knows what is the last chunk but not its current length. This is why we check the length directly and update. Now the zombie instance might continue to write even after this but that does not matter because in the view of new instance that chunk ended at offset new instance decides to save. The rule here is that the new instance always writes to a new chunk.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:45:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA5NTEyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzMjE2OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428132169", "bodyText": "Remind me what the naming conventions are (you can point me to where it is documented). I'm trying to understand why we need an old and a new name is this add record.", "author": "fpj", "createdAt": "2020-05-20T16:06:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {\n+        synchronized (LOCK) {\n+            try (val txn = metadataStore.beginTransaction()) {\n+                // Step 1: Create segment metadata.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                            .name(systemSegment)\n+                            .ownerEpoch(epoch)\n+                            .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                            .build();\n+                    segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                    segmentMetadata.checkInvariants();\n+                    txn.create(segmentMetadata);\n+                    txn.markPinned(segmentMetadata);\n+                }\n+\n+                // We only need to apply the final truncate offset.\n+                val finalTruncateOffsets = new HashMap<String, Long>();\n+                val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+                val chunkStartOffsets = new HashMap<String, Long>();\n+\n+                // Step 2: For each epoch, find the corresponding system journal files.\n+                for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+                    // Start scan with file index 0.\n+                    int fileIndexToRecover = 0;\n+                    while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                        val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                        val info = chunkStorage.getInfo(systemLogName);\n+                        val h = chunkStorage.openRead(systemLogName);\n+                        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                        long fromOffset = 0;\n+                        int remaining = contents.length;\n+                        while (remaining > 0) {\n+                            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                            remaining -= bytesRead;\n+                            fromOffset += bytesRead;\n+                        }\n+                        BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                        String line;\n+\n+                        while ((line = reader.readLine()) != null) {\n+                            // Handle only whole records.\n+                            if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                                String[] records = line.split(RECORD_SEPARATOR);\n+                                for (String record : records) {\n+                                    String[] parts = record.split(\":\");\n+                                    if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                        String segmentName = parts[1];\n+                                        String oldChunkName = parts[2];", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODE5OTA0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428199047", "bodyText": "Having both old and new chunk names makes it independent of the naming conventions itself. (Eg. In non backward compatible mode we'll use UUID as chunk name)", "author": "sachin-j-joshi", "createdAt": "2020-05-20T17:51:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzMjE2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzMzE2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428133167", "bodyText": "do we need to get before we delete? I'm not clear that the order is correct.", "author": "fpj", "createdAt": "2020-05-20T16:07:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,450 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+\n+    /**\n+     * Sting prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws IOException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws IOException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws IOException Exception in case of any error.\n+     */\n+    synchronized public void commitRecords(Collection<String> records) throws IOException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+        synchronized (LOCK) {\n+            // Open the underlying chunk to write.\n+            ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+            StringBuffer stringBuffer = new StringBuffer();\n+            stringBuffer.append(\"\\n\");\n+            stringBuffer.append(START_TOKEN);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+            for (String logLine : records) {\n+                stringBuffer.append(logLine);\n+                stringBuffer.append(RECORD_SEPARATOR);\n+            }\n+            stringBuffer.append(END_TOKEN);\n+\n+            // Persist\n+            byte[] bytes = stringBuffer.toString().getBytes();\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    synchronized public void bootstrap() throws Exception {\n+        synchronized (LOCK) {\n+            try (val txn = metadataStore.beginTransaction()) {\n+                // Step 1: Create segment metadata.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                            .name(systemSegment)\n+                            .ownerEpoch(epoch)\n+                            .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                            .build();\n+                    segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                    segmentMetadata.checkInvariants();\n+                    txn.create(segmentMetadata);\n+                    txn.markPinned(segmentMetadata);\n+                }\n+\n+                // We only need to apply the final truncate offset.\n+                val finalTruncateOffsets = new HashMap<String, Long>();\n+                val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+                val chunkStartOffsets = new HashMap<String, Long>();\n+\n+                // Step 2: For each epoch, find the corresponding system journal files.\n+                for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+                    // Start scan with file index 0.\n+                    int fileIndexToRecover = 0;\n+                    while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                        val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                        val info = chunkStorage.getInfo(systemLogName);\n+                        val h = chunkStorage.openRead(systemLogName);\n+                        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                        long fromOffset = 0;\n+                        int remaining = contents.length;\n+                        while (remaining > 0) {\n+                            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                            remaining -= bytesRead;\n+                            fromOffset += bytesRead;\n+                        }\n+                        BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                        String line;\n+\n+                        while ((line = reader.readLine()) != null) {\n+                            // Handle only whole records.\n+                            if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                                String[] records = line.split(RECORD_SEPARATOR);\n+                                for (String record : records) {\n+                                    String[] parts = record.split(\":\");\n+                                    if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                        String segmentName = parts[1];\n+                                        String oldChunkName = parts[2];\n+                                        String newChunkName = parts[3];\n+                                        long offset = Long.parseLong(parts[4]);\n+\n+                                        applyChunkAddition(txn, chunkStartOffsets, segmentName, oldChunkName, newChunkName, offset);\n+                                    }\n+                                    if (TRUNCATE_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                        String segmentName = parts[1];\n+                                        long truncateAt = Long.parseLong(parts[2]);\n+                                        String firstChunkName = parts[3];\n+                                        long truncateStartAt = Long.parseLong(parts[4]);\n+                                        finalTruncateOffsets.put(segmentName, truncateAt);\n+                                        finalFirstChunkStartsAtOffsets.put(segmentName, truncateStartAt);\n+                                    }\n+                                }\n+                            }\n+                        }\n+                        // Move to next file.\n+                        fileIndexToRecover++;\n+                    }\n+                }\n+\n+                // Step 3: Adjust the length of the last chunk.\n+                for (String systemSegment : systemSegments) {\n+                    SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+                    segmentMetadata.checkInvariants();\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                        long length = chunkInfo.getLength();\n+\n+                        ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                        Preconditions.checkState(null != lastChunk);\n+                        lastChunk.setLength(Math.toIntExact(length));\n+                        txn.update(lastChunk);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + length);\n+                    }\n+                    Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+                    segmentMetadata.checkInvariants();\n+                    txn.update(segmentMetadata);\n+                }\n+\n+                // Step 4: Apply the truncate offsets.\n+                for (String systemSegment : systemSegments) {\n+                    if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                        val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                        val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                        applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+                    }\n+                }\n+\n+                // Step 5: Finally commit all data.\n+                txn.commit(true, true);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODIwNTE0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r428205140", "bodyText": "Yes. We are iterating over the list of chunks.  this line ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete); retrieved the record \"pointed\" by toDelete and the next line toDelete = chunkToDelete.getNextChunk(); moves the pointer.  That is why we first get a reference to copy of the record and then delete it from the transaction. Once deleted we won't be able to find that record.", "author": "sachin-j-joshi", "createdAt": "2020-05-20T18:01:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODEzMzE2Nw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE0OTE3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429149176", "bodyText": "If I'm not wrong, according to PDP-34, the \"concat\" operation can be implemented only in terms of metadata, which completely removes the need to managing actual data in Tier 2. This is great, and I actually expect this to make our transaction commits much faster, for instance. For this reason, I think that even if the underlying system supports concatenating files, we should not go for that. Empirically, our observations of some Tier 2 system regarding the performance of this kind of operations suggest that even if it is available, we may not want to use it. If possible, it would be a great thing to keep the APIs as small as possible and exploit the goodness of K/V tables (as you currently do) to make things simpler, faster and more efficient.", "author": "RaulGracia", "createdAt": "2020-05-22T09:44:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ *     <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ *     <li>{@link ChunkStorageProvider#create(String)}  and {@link ChunkStorageProvider#delete(ChunkHandle)} are not idempotent.</li>\n+ *     <li>{@link ChunkStorageProvider#exists(String)} and {@link ChunkStorageProvider#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *</div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorageProvider}  support appending to existing chunks? For vanilla S3 compatible this would return false. This is indicated by {@link ChunkStorageProvider#supportsAppend()}. </li>\n+ * <li> Does {@link ChunkStorageProvider}  support for concatenating chunks (either natively or simulated with append)? This is indicated by {@link ChunkStorageProvider#supportsConcat()}.\n+ * If this is false then concat will be only a metadata operation for Pravega - that is we only merge metadata about chunks not the data.</li>\n+ * <li> Does {@link ChunkStorageProvider}  support for concatenating chunks natively? This is indicated by {@link ChunkStorageProvider#supportsNativeConcat()}.\n+ * If this is true then native concat operation concatNatively will be invoked otherwise concatWithAppend is invoked.</li>\n+ * <li>In addition {@link ChunkStorageProvider} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorageProvider#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorageProvider supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorageProvider} supports both native and append, ChunkStorageManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports {@link ChunkStorageProvider#truncate(ChunkHandle, long)} operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    boolean supportsNativeConcat();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1Mjc4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429152788", "bodyText": "Too many blank lines, perhaps.", "author": "RaulGracia", "createdAt": "2020-05-22T09:52:00Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NDE4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430684189", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:19:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1Mjc4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1MjkwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429152901", "bodyText": "Too many blank lines.", "author": "RaulGracia", "createdAt": "2020-05-22T09:52:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NDMzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430684334", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:20:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1MjkwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NjI3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429156271", "bodyText": "These preconditions are repeated in concatNatively too. I think that you could organize groups of preconditions into separate methods and reuse thee methods, like ensureConcatPreconditions().", "author": "RaulGracia", "createdAt": "2020-05-22T09:59:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create chunk={} latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageProviderMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkInfo to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concatNatively(ChunkInfo[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i-1].getName()), \"duplicate chunk found\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcatNatively(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"Concat target={} latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageProviderMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageProviderMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageProviderMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks using append operation. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concatWithAppend(ChunkInfo[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks,  \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDYwNDQ5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430604497", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T18:00:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NjI3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NjY4NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429156685", "bodyText": "setReadonly -> setReadOnly?", "author": "RaulGracia", "createdAt": "2020-05-22T10:00:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,575 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports native merge operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsNativeConcat();\n+\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName)  throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create chunk={} latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageProviderMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write chunk={} offset={} bytesWritten={} latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkInfo to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concatNatively(ChunkInfo[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i-1].getName()), \"duplicate chunk found\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcatNatively(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"Concat target={} latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageProviderMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageProviderMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageProviderMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks using append operation. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ChunkHandle to existing chunks to be appended together. The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concatWithAppend(ChunkInfo[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks,  \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i-1].getName()), \"duplicate chunk found\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcatWithAppend(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"Concat target={} latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageProviderMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageProviderMetrics.CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadonly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NDQyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430684421", "bodyText": "ok", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:20:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NjY4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NzgwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429157800", "bodyText": "Maybe the collection fields can be defined with the interface, that is ConcurrentHashMap -> Map.", "author": "RaulGracia", "createdAt": "2020-05-22T10:02:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ1MDg1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431450856", "bodyText": "Need methods on ConcurrentMap", "author": "sachin-j-joshi", "createdAt": "2020-05-27T21:24:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1NzgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODAzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429158038", "bodyText": "Extra blank line.", "author": "RaulGracia", "createdAt": "2020-05-22T10:03:31Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTE4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430775183", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:13:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODAzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODQ0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429158447", "bodyText": "Maybe the preconditions can be checked before event executing anything here.", "author": "RaulGracia", "createdAt": "2020-05-22T10:04:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MzY5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439173694", "bodyText": "I think I had tried that very initially, but some of the callers can only handle CompletionException. Not worth making all that change for optimizing exceptional cases.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:27:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1ODQ0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1OTYwMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429159602", "bodyText": "Could all these preconditions be checked before we submit anything to the executor?", "author": "RaulGracia", "createdAt": "2020-05-22T10:07:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTMxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430775310", "bodyText": "requires change to method signature", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:14:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1OTYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE1OTcwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429159705", "bodyText": "Extra space.", "author": "RaulGracia", "createdAt": "2020-05-22T10:07:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MDEwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429160100", "bodyText": "Extra space.", "author": "RaulGracia", "createdAt": "2020-05-22T10:08:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ1MTExOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431451118", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-27T21:24:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MDEwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMTQ1MTE5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r431451199", "bodyText": "i think", "author": "sachin-j-joshi", "createdAt": "2020-05-27T21:24:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MDEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MDE4NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429160185", "bodyText": "Extra space.", "author": "RaulGracia", "createdAt": "2020-05-22T10:08:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MTU4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429161589", "bodyText": "I think that Andrei already said that, but we need to keep a consistent format for every log-related message with this new Tier 2. For instance, we see on another class of this PR:\nlog.debug(\"Delete chunk={} latency={}.\", handle.getChunkName(), elapsed.toMillis());\nand on this class\nlog.debug(\"seal - segment={}.\", handle.getSegmentName());\nLet's define a consistent format for all the logging messages related to Tier 2 operations, if possible.", "author": "RaulGracia", "createdAt": "2020-05-22T10:11:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc3NTY0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430775640", "bodyText": "done", "author": "sachin-j-joshi", "createdAt": "2020-05-27T00:15:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MTU4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MTkwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429161904", "bodyText": "Same comment about preconditions out of the execute method.", "author": "RaulGracia", "createdAt": "2020-05-22T10:12:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MzkyMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430753921", "bodyText": "issue with need to change the method signature.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T23:02:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MTkwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MjkyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429162928", "bodyText": "There are many if blocks to basically throw the same exception. I wonder if this kind of logic could be compacted on a more concise way.", "author": "RaulGracia", "createdAt": "2020-05-22T10:14:59Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MjU5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430752599", "bodyText": "done.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T22:58:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2MjkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2NjE2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429166166", "bodyText": "Preconditions could be outside execute method.", "author": "RaulGracia", "createdAt": "2020-05-22T10:22:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ChunkInfo[] chunkInfos = new ChunkInfo[chunksToConcat.size()];\n+                chunkInfos = chunksToConcat.toArray(chunkInfos);\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunkInfos);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunkInfos);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunkInfos.length; i++) {\n+                    txn.delete(chunkInfos[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1MzAzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430753032", "bodyText": "Remembered why - I think that requires interface signature change and modification to all call sites. Not worth it.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T22:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2NjE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2NjQ3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429166479", "bodyText": "Extra spaces.", "author": "RaulGracia", "createdAt": "2020-05-22T10:23:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"write: First write after failover. segment={}\", streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"write: New chunk added. segment={} chunk={} offset={} \", streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"write - segment={} offset={} length={} latency={}.\", handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"collectGarbage - chunk={}.\", chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"Could not delete garbage chunk {}\", chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"seal - segment={}.\", handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                if (null == targetSegmentMetadata || !targetSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(targetSegmentName);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                if (null == sourceSegmentMetadata || !sourceSegmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(sourceSegment);\n+                }\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (chunkStorage.supportsConcat() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"concat - target={} source={} offset={} latency={}.\", targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    /**\n+     * Defragments the combined chunks after concatenation.\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    @VisibleForTesting\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ChunkInfo[] chunkInfos = new ChunkInfo[chunksToConcat.size()];\n+                chunkInfos = chunksToConcat.toArray(chunkInfos);\n+                if (!useAppend && chunkStorage.supportsNativeConcat()) {\n+                    int length = chunkStorage.concatNatively(chunkInfos);\n+                } else {\n+                    int length = chunkStorage.concatWithAppend(chunkInfos);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < chunkInfos.length; i++) {\n+                    txn.delete(chunkInfos[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"delete - segment={} latency={}.\", handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                            offset,\n+                            segmentMetadata.getFirstChunk(),\n+                            finalStartOffset));\n+                        return null;\n+                    });\n+\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"truncate - segment={} offset={} latency={}.\", handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openRead:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset() ) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (readIndex.size() > 0) {\n+                    val floorEntry = readIndex.floorEntry(offset);\n+                    if (null != floorEntry) {\n+                        startOffsetForCurrentChunk = floorEntry.getKey();\n+                        currentChunkName = floorEntry.getValue();\n+                    }\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (   startOffsetForCurrentChunk <= currentOffset", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDY4NDU5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430684599", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-05-26T20:20:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE2NjQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3MjMzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r429172337", "bodyText": "One of the limitations of today tier 2 model comes for a 1 segment stream: there is only 1 thread from the tier 2 that moves the chunks to storage. An important question is: with this new implementation, could we also parallelize movement of chunks for 1 Segment case or things will remain the same in this sense?", "author": "RaulGracia", "createdAt": "2020-05-22T10:37:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements segment storage using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"openWrite:Segment needs ownership change. segment: {} \", segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"claimOwnership:Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"create - segment={} rollingPolicy={} latency={}.\", streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDc1Mzc3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r430753772", "bodyText": "Yes. That is the intention to parallelized writes and reads as much as possible. But will have to do that incrementally in next couple milestone.", "author": "sachin-j-joshi", "createdAt": "2020-05-26T23:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTE3MjMzNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMDM4NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433500385", "bodyText": "Do you actually need all these overloaded constructors? This shouldn't be throw from too many places; is there an opportunity for consolidation so we only have 1-2 such constructors?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:28:09Z", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentTruncatedException.java", "diffHunk": "@@ -33,5 +42,17 @@ public StreamSegmentTruncatedException(String segmentName, String message, Throw\n      */\n     public StreamSegmentTruncatedException(long startOffset) {\n         super(\"\", String.format(\"Segment truncated: Lowest accessible offset is %d.\", startOffset));\n+        this.startOffset = startOffset;\n+    }\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentTruncatedException class.\n+     * @param segmentName Name of the truncated Segment.\n+     * @param startOffset First valid offset of the StreamSegment.\n+     * @param requestedOffset Requested offset.\n+     */\n+    public StreamSegmentTruncatedException(String segmentName, long startOffset, long requestedOffset) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2NjU1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439166556", "bodyText": "The new overload has important information other overloads are missing.\nHowever the overloads are used in different places. I don't want to change all the places other overloads are used in existing code.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T01:57:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMDM4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMTY5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433501694", "bodyText": "You do not seem to be enforcing this anywhere. Any public method should check Exceptions.checkNotClosed as its first line. Additionally, any long-running computations (i.e., loops, sync or async) should abort as soon as this is set to true.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:31:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,520 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4MDI2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433580266", "bodyText": "good catch . thanks.\nI had an execute method that executed every method and that method checked it in one single place. When I refactored the code I got rid of separate execute method and seems like I forgot to add back Exceptions.checkNotClosed. I'll put that check back.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T01:59:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMTY5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA5MjQwOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r437092409", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-09T01:49:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMTY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMjAxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433502013", "bodyText": "Please have these constructors call one another so you don't have to copy-paste the string format for each one of them.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:32:00Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkAlreadyExistsException.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+/**\n+ * Exception thrown when chunk with given name already exists.\n+ */\n+public class ChunkAlreadyExistsException extends ChunkStorageException {\n+    /**\n+     * Creates a new instance of the exception.\n+     *\n+     * @param chunkName The name of the chunk.\n+     */\n+    public ChunkAlreadyExistsException(String chunkName) {\n+        super(chunkName, String.format(\"Chunk %s already exists.\", chunkName));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MjM1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439172355", "bodyText": "I removed one of the overloads.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMjAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMjQ2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433502467", "bodyText": "here too", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:33:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkNotFoundException.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+/**\n+ * Exception thrown when chunk with given name does not exist.\n+ */\n+public class ChunkNotFoundException extends ChunkStorageException {\n+    /**\n+     * Creates a new instance of the exception.\n+     *\n+     * @param chunkName The name of the chunk.\n+     */\n+    public ChunkNotFoundException(String chunkName) {\n+        super(chunkName, String.format(\"Chunk %s not found\", chunkName));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MjI5MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439172290", "bodyText": "I removed one of the overloads.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:21:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMjQ2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzQ5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433503491", "bodyText": "Part of this class is thread safe and part is not. You use a ConcurrentHashMap, yet your non-final fields are not protected.\nPlease use the Atomic... classes to hold references to those objects.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:35:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxMjExNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434212115", "bodyText": "Non-final fields are modified in either init or constructor. as explained above we need two step initialization. so these fields can't be final. However they are not modified outside of init sequence.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T22:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzQ5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzU5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433503598", "bodyText": "What it you are already initialized?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:36:01Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzcwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433503707", "bodyText": "Also can't you pass in the container id in the constructor?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:36:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzU5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIxMDcxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434210711", "bodyText": "no. Storage instance is created in a factory method and hooked up to StreamSegmentContainer in initializeSecondaryServices.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T22:31:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzU5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzk5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433503994", "bodyText": "Ok, why do you have 2 initialization methods. I understand you need to override this one here, but anyone trying to use this class will be confused as to why there are two such methods.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:37:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU3ODI4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433578280", "bodyText": "The reason for two initialize overload is that segment container needs storage initialized before it can use table segments and and storage needs table segments to function. To break the circular dependency, the storage is initialized in multiple steps. may be we can have distinct names than multiple overloads. Most of the non final fields are also coming from this multi step initialization.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T01:51:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwMzk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDMyNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433504326", "bodyText": "You are not enforcing this anywhere. See my comment in the other class about how to do it.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:37:50Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU3OTYwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433579607", "bodyText": "private <R> CompletableFuture<R> execute(Callable<R> operation) {\n        return CompletableFuture.supplyAsync(() -> {\n            Exceptions.checkNotClosed(this.closed.get(), this);\n\nIt is checked in execute", "author": "sachin-j-joshi", "createdAt": "2020-06-02T01:56:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDMyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDc5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433504797", "bodyText": "For cleanliness, I stored ChunkStorageManager[...] in a final member variable. That way I do not have to reformat it every time and I will make these log lines shorter.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:39:01Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDg4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433504888", "bodyText": "typo in addjusted", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:39:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNTE2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433505168", "bodyText": "and Please watch Your character Casing.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:39:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyNjYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434826617", "bodyText": "claimOwnership is name of the method", "author": "sachin-j-joshi", "createdAt": "2020-06-03T20:13:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDgyNjc2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434826764", "bodyText": "fixed the typo", "author": "sachin-j-joshi", "createdAt": "2020-06-03T20:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNDc5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNTc3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433505777", "bodyText": "Please be consistent in your logging syntax here you don't use any punctuation neither am I in this comment", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:41:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA5MjQ4Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r437092482", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-09T01:49:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNTc3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNjU2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433506560", "bodyText": "doesn't chunkStorage.getInfo make a call to LTS? This in itself can fail and the user will get a weird exception back.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:43:01Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"ChunkStorageManager[{}] write - First write after failover. segment={}\", containerId, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"ChunkStorageManager[{}] write - New chunk added. segment={} chunk={} offset={} \", containerId, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyODAxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442328013", "bodyText": "This comment is unresolved.", "author": "fpj", "createdAt": "2020-06-18T15:49:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNjU2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAwNjUzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445006539", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T16:05:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNjU2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzAzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433507031", "bodyText": "Can you please differentiate between these two log messages? At least include the name and message of the exception.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:44:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"ChunkStorageManager[{}] write - First write after failover. segment={}\", containerId, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"ChunkStorageManager[{}] write - New chunk added. segment={} chunk={} offset={} \", containerId, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] write - segment={} offset={} length={} latency={}.\", containerId, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"ChunkStorageManager[{}] collectGarbage - chunk={}.\", containerId, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MzkyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439173924", "bodyText": "Fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:28:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzI5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433507298", "bodyText": "Include the reason why in the exception.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:44:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"ChunkStorageManager[{}] write - First write after failover. segment={}\", containerId, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"ChunkStorageManager[{}] write - New chunk added. segment={} chunk={} offset={} \", containerId, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] write - segment={} offset={} length={} latency={}.\", containerId, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"ChunkStorageManager[{}] collectGarbage - chunk={}.\", containerId, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"ChunkStorageManager[{}] seal - segment={}.\", containerId, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzkwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433507904", "bodyText": "You should implement this as #4710 has been merged in.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:46:16Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"ChunkStorageManager[{}] write - First write after failover. segment={}\", containerId, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"ChunkStorageManager[{}] write - New chunk added. segment={} chunk={} offset={} \", containerId, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] write - segment={} offset={} length={} latency={}.\", containerId, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"ChunkStorageManager[{}] collectGarbage - chunk={}.\", containerId, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"ChunkStorageManager[{}] seal - segment={}.\", containerId, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] concat - target={} source={} offset={} latency={}.\", containerId, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return chunkStorage.supportsAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     *  <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] delete - segment={} latency={}.\", containerId, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) throws StreamSegmentNotExistsException {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new StreamSegmentNotExistsException(streamSegmentName);\n+        }\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                            offset,\n+                            segmentMetadata.getFirstChunk(),\n+                            finalStartOffset));\n+                        return null;\n+                    });\n+\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] truncate - segment={} offset={} latency={}.\", containerId, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Lists all the segments stored on the storage device.\n+     *\n+     * @return Iterator that can be used to enumerate and retrieve properties of all the segments.\n+     * @throws IOException if exception occurred while listing segments.\n+     */\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() throws IOException {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3NDE5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439174195", "bodyText": "Implementing this change is separate task", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:29:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzkwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMTM2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442401362", "bodyText": "Is there an issue for implementing it?", "author": "fpj", "createdAt": "2020-06-18T17:51:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzkwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3ODEyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442578129", "bodyText": "listSegments was not planned for the first iteration. Listing file on LTS is a meaningless operation until we have a naming convention that supports reverse metadata creation. Until then the metadata stored in the storage metadata segment has all the layout info.  In DR scenario the storage metadata table segment may not be fully flushed. So what we find on LTS is all we have.\nI would wait until we have have clarity regarding approach to handle DR and PDP-40.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T01:04:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzkwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3ODIxOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442578218", "bodyText": "I'll create a github issue for tracking that.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T01:04:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwNzkwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwODQ3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433508476", "bodyText": "I am pretty sure the Storage.openRead contract says that it will not claim ownership. Why are you doing this here?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:47:47Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1417 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>> cachedReadIndex = new ConcurrentHashMap<String, ConcurrentSkipListMap<Long, String>>();\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor    An Executor for async operations.\n+     * @param config Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openWrite - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState( chunkInfo != null);\n+            Preconditions.checkState( lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState( chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"ChunkStorageManager[{}] claimOwnership - Length of last chunk addjusted. segment: {}, last chunk: {} Length: {}\",\n+                        containerId,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue =  SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] create - segment={} rollingPolicy={} latency={}.\", containerId, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            HashMap<Long, String> newReadIndexEntries = new HashMap<Long, String>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover =  segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if ( null == lastChunkMetadata\n+                        || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                        || isFirstWriteAfterFailover\n+                        || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.put(segmentMetadata.getLength(), newChunkName);\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"ChunkStorageManager[{}] write - First write after failover. segment={}\", containerId, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"ChunkStorageManager[{}] write - New chunk added. segment={} chunk={} offset={} \", containerId, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                for (val entry : newReadIndexEntries.entrySet()) {\n+                    readIndex.put(entry.getKey(), entry.getValue());\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] write - segment={} offset={} length={} latency={}.\", containerId, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.values());\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset Offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+            offset,\n+            oldChunkName == null ? null : oldChunkName,\n+            newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"ChunkStorageManager[{}] collectGarbage - chunk={}.\", containerId, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"ChunkStorageManager[{}] collectGarbage - Could not delete garbage chunk {}\", containerId, chunkTodelete);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"ChunkStorageManager[{}] seal - segment={}.\", containerId, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk ) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] concat - target={} source={} offset={} latency={}.\", containerId, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return chunkStorage.supportsAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     *  <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     * @param txn Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName Name of the first chunk to start defragmentation.\n+     * @param lastChunkName Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                cachedReadIndex.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] delete - segment={} latency={}.\", containerId, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) throws StreamSegmentNotExistsException {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new StreamSegmentNotExistsException(streamSegmentName);\n+        }\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(systemJournal.getSegmentTruncatedRecord(streamSegmentName,\n+                            offset,\n+                            segmentMetadata.getFirstChunk(),\n+                            finalStartOffset));\n+                        return null;\n+                    });\n+\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                val readIndex = getReadIndex(streamSegmentName);\n+                if (null != readIndex) {\n+                    val headMap = readIndex.headMap(segmentMetadata.getStartOffset());\n+                    if (null != headMap) {\n+                        ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                        keysToRemove.addAll(headMap.keySet());\n+                        for (val keyToRemove : keysToRemove) {\n+                            cachedReadIndex.remove(keyToRemove);\n+                        }\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"ChunkStorageManager[{}] truncate - segment={} offset={} latency={}.\", containerId, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Lists all the segments stored on the storage device.\n+     *\n+     * @return Iterator that can be used to enumerate and retrieve properties of all the segments.\n+     * @throws IOException if exception occurred while listing segments.\n+     */\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() throws IOException {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute( () -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"ChunkStorageManager[{}] openRead - Segment needs ownership change. segment: {} \", containerId, segmentMetadata.getName());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU3NzM1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433577350", "bodyText": "This prevents the bug where we write to a chunk and that chunk is not rolled over but in this case we do not update metadata in tier-1. If now segment store fails and first request to new segment store instance is read then we have lost the data at the end. This check prevents that data loss.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T01:47:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwODQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUwOTI0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433509241", "bodyText": "Why aren't these final?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:49:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManagerConfig.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+\n+/**\n+ * Configuration for {@link ChunkStorageManager}.\n+ */\n+@AllArgsConstructor\n+@Builder(toBuilder = true)\n+public class ChunkStorageManagerConfig {\n+    /**\n+     * Default configuration for {@link ChunkStorageManager}.\n+     */\n+    public static final ChunkStorageManagerConfig DEFAULT_CONFIG = ChunkStorageManagerConfig.builder()\n+            .minSizeLimitForNativeConcat(0)\n+            .maxSizeLimitForNativeConcat(Long.MAX_VALUE)\n+            .defaultRollingPolicy(SegmentRollingPolicy.NO_ROLLING)\n+            .maxBufferSizeForChunkDataTransfer(1024 * 1024)\n+            .build();\n+\n+    /**\n+     * Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concat.\n+     */\n+    @Getter\n+    @Setter\n+    private long minSizeLimitForNativeConcat;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMDYyNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433510626", "bodyText": "Why do you have a static mutex? This is very, very dangerous and will kill your performance.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:53:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg0ODQwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434848405", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-03T20:56:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMDYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMDgxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433510817", "bodyText": "I don't see any reasons to not make these fields final. How does your object react if someone just changes these fields after it's been initialized?", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:53:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3NDI5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439174294", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:29:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMDgxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMTUzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433511531", "bodyText": "You need to explicitly define the charset here. Otherwise it will use whatever happens to be default on the host, and defaults may vary across hosts and OS-es.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:55:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMjAyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433512028", "bodyText": "Also here you're making a double-copy. Once into a String and once to a byte array.\nCan you rewrite this method so that it does exactly one copy? I may suggest doing some binary encoding, using VersionedSerializer (which will give you versioning too) or some other technique to simplify things.", "author": "andreipaduroiu", "createdAt": "2020-06-01T21:56:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMTUzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMzI5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433513297", "bodyText": "Actually you also waste cycles formatting the strings.\nI would consider keeping these records as objects and using some serializer (like VersionedSerializer) to write them all in one pass into something like an EnhancedByteArrayOutputStream, then invoke getData() on it and finally getReader on that result. This should only do one copy.\nAlternatively, consider serializing each record into a ByteBuffer (BufferView) and using BufferView.wrap to compose them into a single instance and invoke getReader on the result.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMTUzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2NzQ2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439167465", "bodyText": "At the current moment I need debug friendly format.\nOnce I am done with all changes, I do plan to implement serialization at the very end as part of the last PR in this group.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:00:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMTUzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwMjI3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439502276", "bodyText": "ok for the debug-friendly format, but at least we should use a defined encoding.\nOr are you planning to commit only the full PDP-34 implementation at once ?\nIf we are committing it one step at a time each commit should be as much good as possible", "author": "eolivelli", "createdAt": "2020-06-12T15:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMTUzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMzgzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433513835", "bodyText": "IT is more efficient to use a tokenizer which returns an iterator vs doing split.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:01:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offests at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments without any chunk information.\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 0.\n+            int fileIndexToRecover = 0;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                val info = chunkStorage.getInfo(systemLogName);\n+                val h = chunkStorage.openRead(systemLogName);\n+                byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                long fromOffset = 0;\n+                int remaining = contents.length;\n+                while (remaining > 0) {\n+                    int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                    remaining -= bytesRead;\n+                    fromOffset += bytesRead;\n+                }\n+                BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                String line;\n+\n+                while ((line = reader.readLine()) != null) {\n+                    // Handle only whole records.\n+                    if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                        String[] records = line.split(RECORD_SEPARATOR);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2Nzc2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439167767", "bodyText": "At the current moment I need debug friendly format.\nOnce I am done with all changes, I do plan to implement serialization at the very end as part of the last PR in this group.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:02:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxMzgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNDE4Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433514182", "bodyText": "After reading this file, I believe it will will benefit from serialization versioning. Consider using VersionedSerializer, couple with binary encoding to both reduce the size of your serializations and future-proof them from change.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:02:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2NzEzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439167135", "bodyText": "At the current moment I need debug friendly format.\nOnce I am done with all changes, I do plan to implement serialization at the very end as part of the last PR in this group.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T01:59:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNDE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNDUyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433514525", "bodyText": "Is this an initialization? If so, name this initialize.\nAlso ensure nobody can call this more than once. What would happen if this got invoked after you did a bunch of work?", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:03:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNDkwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433514908", "bodyText": "The call to exists is unnecessary. Rework your loop to make use of getInfo/openRead and stop on a not-exist exception.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:04:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offests at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments without any chunk information.\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 0.\n+            int fileIndexToRecover = 0;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2NzY0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439167644", "bodyText": "At the current moment I need debug friendly easy code.\nOnce I am done with all changes, I do plan to implement a few more optimization at the very end as part of the last PR in this group.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:01:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNDkwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTI1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433515253", "bodyText": "Be optimistic. Do openWrite and only if it fails with the appropriate exception invoke create.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:05:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offests at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments without any chunk information.\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 0.\n+            int fileIndexToRecover = 0;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                val info = chunkStorage.getInfo(systemLogName);\n+                val h = chunkStorage.openRead(systemLogName);\n+                byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                long fromOffset = 0;\n+                int remaining = contents.length;\n+                while (remaining > 0) {\n+                    int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                    remaining -= bytesRead;\n+                    fromOffset += bytesRead;\n+                }\n+                BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                String line;\n+\n+                while ((line = reader.readLine()) != null) {\n+                    // Handle only whole records.\n+                    if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                        String[] records = line.split(RECORD_SEPARATOR);\n+                        for (String record : records) {\n+                            String[] parts = record.split(\":\");\n+                            if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                String oldChunkName = parts[2];\n+                                String newChunkName = parts[3];\n+                                long offset = Long.parseLong(parts[4]);\n+\n+                                applyChunkAddition(txn, chunkStartOffsets, segmentName, oldChunkName, newChunkName, offset);\n+                            }\n+                            if (TRUNCATE_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                long truncateAt = Long.parseLong(parts[2]);\n+                                String firstChunkName = parts[3];\n+                                long truncateStartAt = Long.parseLong(parts[4]);\n+                                finalTruncateOffsets.put(segmentName, truncateAt);\n+                                finalFirstChunkStartsAtOffsets.put(segmentName, truncateStartAt);\n+                            }\n+                        }\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(Math.toIntExact(length));\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLength() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param txn\n+     * @param finalTruncateOffsets\n+     * @param finalFirstChunkStartsAtOffsets\n+     * @throws StorageMetadataException\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(Math.toIntExact(offset - oldLength));\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {\n+        return getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private String getSystemJournalFileName(int containerId, long epoch, long currentFileIndex) {\n+        return NameUtils.getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private ChunkHandle getChunkHandleForSystemJournal() throws ChunkStorageException {\n+        ChunkHandle h;\n+        val systemLogName = getSystemJournalFileName();\n+        if (!chunkStorage.exists(systemLogName)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2ODA2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439168063", "bodyText": "At the current moment I need debug friendly easy code.\nOnce I am done with all changes, I do plan to implement a few more optimization at the very end as part of the last PR in this group.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:03:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTI1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2ODIzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439168232", "bodyText": "Note that this code is called only during container startup.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:04:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTI1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTg3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433515878", "bodyText": "This method seems slow.", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:07:06Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offests at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments without any chunk information.\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 0.\n+            int fileIndexToRecover = 0;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                val info = chunkStorage.getInfo(systemLogName);\n+                val h = chunkStorage.openRead(systemLogName);\n+                byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                long fromOffset = 0;\n+                int remaining = contents.length;\n+                while (remaining > 0) {\n+                    int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                    remaining -= bytesRead;\n+                    fromOffset += bytesRead;\n+                }\n+                BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                String line;\n+\n+                while ((line = reader.readLine()) != null) {\n+                    // Handle only whole records.\n+                    if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                        String[] records = line.split(RECORD_SEPARATOR);\n+                        for (String record : records) {\n+                            String[] parts = record.split(\":\");\n+                            if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                String oldChunkName = parts[2];\n+                                String newChunkName = parts[3];\n+                                long offset = Long.parseLong(parts[4]);\n+\n+                                applyChunkAddition(txn, chunkStartOffsets, segmentName, oldChunkName, newChunkName, offset);\n+                            }\n+                            if (TRUNCATE_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                long truncateAt = Long.parseLong(parts[2]);\n+                                String firstChunkName = parts[3];\n+                                long truncateStartAt = Long.parseLong(parts[4]);\n+                                finalTruncateOffsets.put(segmentName, truncateAt);\n+                                finalFirstChunkStartsAtOffsets.put(segmentName, truncateStartAt);\n+                            }\n+                        }\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(Math.toIntExact(length));\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLength() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param txn\n+     * @param finalTruncateOffsets\n+     * @param finalFirstChunkStartsAtOffsets\n+     * @throws StorageMetadataException\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(Math.toIntExact(offset - oldLength));\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {\n+        return getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private String getSystemJournalFileName(int containerId, long epoch, long currentFileIndex) {\n+        return NameUtils.getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private ChunkHandle getChunkHandleForSystemJournal() throws ChunkStorageException {\n+        ChunkHandle h;\n+        val systemLogName = getSystemJournalFileName();\n+        if (!chunkStorage.exists(systemLogName)) {\n+            h = chunkStorage.create(systemLogName);\n+        } else {\n+            h = chunkStorage.openWrite(systemLogName);\n+        }\n+        return h;\n+    }\n+\n+    /**\n+     * Apply truncate action to the segment metadata.\n+     */\n+    private void applyTruncate(MetadataTransaction txn, String segmentName, long truncateAt, long firstChunkStartsAt) throws StorageMetadataException {\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+        String currentChunkName = segmentMetadata.getFirstChunk();\n+        ChunkMetadata currentMetadata;\n+        long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+        while (null != currentChunkName) {\n+            currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+            // If for given chunk start <= truncateAt < end  then we have found the chunk that will be the first chunk.\n+            if ((startOffset <= truncateAt) && (startOffset + currentMetadata.getLength() > truncateAt)) {\n+                break;\n+            }\n+\n+            startOffset += currentMetadata.getLength();\n+            // move to next chunk\n+            currentChunkName = currentMetadata.getNextChunk();\n+            txn.delete(currentMetadata.getName());\n+        }\n+        Preconditions.checkState(firstChunkStartsAt == startOffset);\n+        segmentMetadata.setFirstChunk(currentChunkName);\n+        segmentMetadata.setStartOffset(truncateAt);\n+        segmentMetadata.setFirstChunkStartOffset(firstChunkStartsAt);\n+        segmentMetadata.checkInvariants();\n+\n+    }\n+\n+    /**\n+     * Indicates whether given segment is a system segment.\n+     *\n+     * @param segmentName Name of the sgement to check.\n+     * @return True if given segment is a system segment.\n+     */\n+    public boolean isStorageSystemSegment(String segmentName) {\n+        if (segmentName.startsWith(systemSegmentsPrefix)) {\n+            for (String systemSegment: systemSegments) {\n+                if (segmentName.equals(systemSegment)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU4MTMzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433581339", "bodyText": "In reality it checks if segment starts with _system prefix and then checks if it is one of the four segments.\nBut I think adding a flag in segment metadata itself makes more sense.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T02:04:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTg3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE2OTQxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439169411", "bodyText": "Fixed - this method is not used in main ChunkStorageManager code. Left behind just because method useful only for validation purposes.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:09:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTg3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNTk1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433515950", "bodyText": "space before :", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:07:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,483 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import java.io.BufferedReader;\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkStorageManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    private static final String ADD_RECORD = \"ADD\";\n+    private static final String TRUNCATE_RECORD = \"TRUNCATE\";\n+    private static final String RECORD_SEPARATOR = \",\";\n+    private static final String START_TOKEN = \"BEGIN\";\n+    private static final String END_TOKEN = \"END\";\n+\n+    private static final Object LOCK = new Object();\n+\n+    @Getter\n+    private ChunkStorageProvider chunkStorage;\n+\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Default {@link SegmentRollingPolicy} for the system segments.\n+     */\n+    @Getter\n+    private SegmentRollingPolicy segmentRollingPolicy;\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId Container id of the owner container.\n+     * @param epoch Epoch of the current container instance.\n+     * @param chunkStorage ChunkStorageProvider instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param segmentRollingPolicy Default SegmentRollingPolicy for system segments.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, SegmentRollingPolicy segmentRollingPolicy) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.segmentRollingPolicy = Preconditions.checkNotNull(segmentRollingPolicy, \"segmentRollingPolicy\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param logLine Log line to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(String logLine) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(logLine));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<String> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        StringBuffer stringBuffer = new StringBuffer();\n+        stringBuffer.append(\"\\n\");\n+        stringBuffer.append(START_TOKEN);\n+        stringBuffer.append(RECORD_SEPARATOR);\n+        for (String logLine : records) {\n+            stringBuffer.append(logLine);\n+            stringBuffer.append(RECORD_SEPARATOR);\n+        }\n+        stringBuffer.append(END_TOKEN);\n+\n+        // Persist\n+        byte[] bytes = stringBuffer.toString().getBytes();\n+        synchronized (LOCK) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.length, new ByteArrayInputStream(bytes));\n+            Preconditions.checkState(bytesWritten == bytes.length);\n+            systemJournalOffset += bytesWritten;\n+\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Formats a log record for newly added chunk.\n+     *\n+     * @param segmentName Name of the segment.\n+     * @param offset offset at which new chunk was added.\n+     * @param oldChunkName Name of the previous last chunk.\n+     * @param newChunkName Name of the new last chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getChunkAddedRecord(String segmentName, long offset, String oldChunkName, String newChunkName) {\n+        return String.format(\"%s:%s:%s:%s:%d\",\n+                ADD_RECORD,\n+                segmentName,\n+                oldChunkName == null ? \"\" : oldChunkName,\n+                newChunkName == null ? \"\" : newChunkName,\n+                offset);\n+    }\n+\n+    /**\n+     * Formats a log record for trucate operation on given segment.\n+     * @param segmentName Name of the segment.\n+     * @param offset Offset at which the segment is truncated.\n+     * @param firstChunkName Name of the new first chunk.\n+     * @param startOffset Offset at which new chunk starts. The actual start offset of the segment may be anywhere in that chunk.\n+     * @return Formatted log record string.\n+     */\n+    public String getSegmentTruncatedRecord(String segmentName, long offset, String firstChunkName, long startOffset) {\n+        return String.format(\"%s:%s:%d:%s:%d\",\n+                TRUNCATE_RECORD,\n+                segmentName,\n+                offset,\n+                firstChunkName,\n+                startOffset);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about critical storage segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offests at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments without any chunk information.\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(segmentRollingPolicy.getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true).setOwnershipChanged(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (int epochToRecover = 0; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 0.\n+            int fileIndexToRecover = 0;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                val info = chunkStorage.getInfo(systemLogName);\n+                val h = chunkStorage.openRead(systemLogName);\n+                byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+                long fromOffset = 0;\n+                int remaining = contents.length;\n+                while (remaining > 0) {\n+                    int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+                    remaining -= bytesRead;\n+                    fromOffset += bytesRead;\n+                }\n+                BufferedReader reader = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(contents)));\n+                String line;\n+\n+                while ((line = reader.readLine()) != null) {\n+                    // Handle only whole records.\n+                    if (line.startsWith(START_TOKEN) && line.endsWith(END_TOKEN)) {\n+                        String[] records = line.split(RECORD_SEPARATOR);\n+                        for (String record : records) {\n+                            String[] parts = record.split(\":\");\n+                            if (ADD_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                String oldChunkName = parts[2];\n+                                String newChunkName = parts[3];\n+                                long offset = Long.parseLong(parts[4]);\n+\n+                                applyChunkAddition(txn, chunkStartOffsets, segmentName, oldChunkName, newChunkName, offset);\n+                            }\n+                            if (TRUNCATE_RECORD.equals(parts[0]) && parts.length == 5) {\n+                                String segmentName = parts[1];\n+                                long truncateAt = Long.parseLong(parts[2]);\n+                                String firstChunkName = parts[3];\n+                                long truncateStartAt = Long.parseLong(parts[4]);\n+                                finalTruncateOffsets.put(segmentName, truncateAt);\n+                                finalFirstChunkStartsAtOffsets.put(segmentName, truncateStartAt);\n+                            }\n+                        }\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(Math.toIntExact(length));\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLength() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     *\n+     * @param txn\n+     * @param finalTruncateOffsets\n+     * @param finalFirstChunkStartsAtOffsets\n+     * @throws StorageMetadataException\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(Math.toIntExact(offset - oldLength));\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {\n+        return getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private String getSystemJournalFileName(int containerId, long epoch, long currentFileIndex) {\n+        return NameUtils.getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private ChunkHandle getChunkHandleForSystemJournal() throws ChunkStorageException {\n+        ChunkHandle h;\n+        val systemLogName = getSystemJournalFileName();\n+        if (!chunkStorage.exists(systemLogName)) {\n+            h = chunkStorage.create(systemLogName);\n+        } else {\n+            h = chunkStorage.openWrite(systemLogName);\n+        }\n+        return h;\n+    }\n+\n+    /**\n+     * Apply truncate action to the segment metadata.\n+     */\n+    private void applyTruncate(MetadataTransaction txn, String segmentName, long truncateAt, long firstChunkStartsAt) throws StorageMetadataException {\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+        String currentChunkName = segmentMetadata.getFirstChunk();\n+        ChunkMetadata currentMetadata;\n+        long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+        while (null != currentChunkName) {\n+            currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+            // If for given chunk start <= truncateAt < end  then we have found the chunk that will be the first chunk.\n+            if ((startOffset <= truncateAt) && (startOffset + currentMetadata.getLength() > truncateAt)) {\n+                break;\n+            }\n+\n+            startOffset += currentMetadata.getLength();\n+            // move to next chunk\n+            currentChunkName = currentMetadata.getNextChunk();\n+            txn.delete(currentMetadata.getName());\n+        }\n+        Preconditions.checkState(firstChunkStartsAt == startOffset);\n+        segmentMetadata.setFirstChunk(currentChunkName);\n+        segmentMetadata.setStartOffset(truncateAt);\n+        segmentMetadata.setFirstChunkStartOffset(firstChunkStartsAt);\n+        segmentMetadata.checkInvariants();\n+\n+    }\n+\n+    /**\n+     * Indicates whether given segment is a system segment.\n+     *\n+     * @param segmentName Name of the sgement to check.\n+     * @return True if given segment is a system segment.\n+     */\n+    public boolean isStorageSystemSegment(String segmentName) {\n+        if (segmentName.startsWith(systemSegmentsPrefix)) {\n+            for (String systemSegment: systemSegments) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzIwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433517206", "bodyText": "final", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:10:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadata.java", "diffHunk": "@@ -0,0 +1,59 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents chunk metadata.\n+ * Following metadata is stored.\n+ * <ul>\n+ *     <li>Name of the chunk.</li>\n+ *     <li>Length of the chunk.</li>\n+ *     <li>Name of the next chunk in list.</li>\n+ * </ul>\n+ */\n+@Builder(toBuilder = true)\n+@Data\n+public class ChunkMetadata implements StorageMetadata {\n+    /**\n+     * Name of this chunk.\n+     */\n+    private String name;", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzYxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433517611", "bodyText": "final", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:12:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,223 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import lombok.Builder;\n+import lombok.Data;\n+\n+/**\n+ * Represents segment metadata.\n+ Following metadata is stored.\n+ * <ul>\n+ *     <li>Name of the segment.</li>\n+ *     <li>Length of the segment.</li>\n+ *     <li>Epoch of the container that last owned it.</li>\n+ *     <li>Start offset of the segment. This is offset of the first byte available for read.</li>\n+ *     <li>Status bit flags.</li>\n+ *     <li>Maximum Rolling length of the segment.</li>\n+ *     <li>Name of the first chunk.</li>\n+ *     <li>Name of the last chunk.</li>\n+ *     <li>Offset corresponding to the the first byte of the first chunk.This is NOT the same as start offset of the segment.\n+ *      With arbitrary truncates the effective start offset might be in the middle of the first chunk. Byte at this offset may not be available for read.</li>\n+ *     <li>Offset of the first byte of the last chunk.</li>\n+ * </ul>\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+public class SegmentMetadata implements StorageMetadata {\n+    /**\n+     * Flag to indicate whether the segment is active or not.\n+     */\n+    private static final int ACTIVE  = 0x0001;\n+\n+    /**\n+     * Flag to indicate whether the segment is sealed or not.\n+     */\n+    private static final int SEALED  = 0x0002;\n+\n+    /**\n+     * Flag to indicate whether the segment is deleted or not.\n+     */\n+    private static final int DELETED = 0x0004;\n+\n+    /**\n+     * Flag to indicate whether followup actions (like adding new chunks) after ownership changes are needed or not.\n+     */\n+    private static final int OWNERSHIP_CHANGED = 0x0008;\n+\n+    /**\n+     * Name of the segment.\n+     */\n+    private String name;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwOTI5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434209299", "bodyText": "ok", "author": "sachin-j-joshi", "createdAt": "2020-06-02T22:27:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzYxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzkwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r433517906", "bodyText": "final?", "author": "andreipaduroiu", "createdAt": "2020-06-01T22:12:55Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/noop/NoOpChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ * <p>\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.noop;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import io.pravega.segmentstore.storage.mocks.AbstractInMemoryChunkStorageProvider;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * NoOp implementation.\n+ */\n+@Slf4j\n+public class NoOpChunkStorageProvider extends AbstractInMemoryChunkStorageProvider {\n+    @Getter\n+    @Setter\n+    ConcurrentHashMap<String, ChunkData> chunkMetadata = new ConcurrentHashMap<>();\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(chunkName);\n+        }\n+        return ChunkInfo.builder().name(chunkName).length(chunkData.length).build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null != chunkData) {\n+            throw new ChunkAlreadyExistsException(chunkName);\n+        }\n+        chunkMetadata.put(chunkName, new ChunkData());\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected boolean doesExist(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return chunkMetadata.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(null != handle, \"handle\");\n+        Preconditions.checkNotNull(handle.getChunkName(), \"handle\");\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(handle.getChunkName());\n+        }\n+        if (chunkData.isReadonly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunkMetadata.remove(handle.getChunkName());\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(chunkName);\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(handle.getChunkName());\n+        }\n+\n+        if (fromOffset >= chunkData.length || fromOffset + length > chunkData.length) {\n+            throw new IndexOutOfBoundsException(\"fromOffset\");\n+        }\n+\n+        if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+            throw new ArrayIndexOutOfBoundsException(String.format(\n+                    \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                    fromOffset, bufferOffset, length, buffer.length));\n+        }\n+\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(handle.getChunkName());\n+        }\n+        if (offset != chunkData.length) {\n+            throw new IndexOutOfBoundsException(\"\");\n+        }\n+        if (chunkData.isReadonly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunkData.length = offset + length;\n+        chunkMetadata.put(handle.getChunkName(), chunkData);\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int total = 0;\n+        for (ConcatArgument chunk : chunks) {\n+            val chunkData = chunkMetadata.get(chunk.getName());\n+            Preconditions.checkState(null != chunkData);\n+            Preconditions.checkState(chunkData.length >= chunk.getLength());\n+            total += chunk.getLength();\n+        }\n+\n+        val targetChunkData = chunkMetadata.get(chunks[0].getName());\n+        targetChunkData.length = total;\n+        for (int i = 1; i < chunks.length; i++) {\n+            chunkMetadata.remove(chunks[i].getName());\n+        }\n+\n+        return total;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        ChunkData chunkData = chunkMetadata.get(handle.getChunkName());\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(handle.getChunkName());\n+        }\n+        if (offset < chunkData.length) {\n+            chunkData.length = offset;\n+            return true;\n+        }\n+        return false;\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        Preconditions.checkNotNull(null != handle, \"handle\");\n+        Preconditions.checkNotNull(handle.getChunkName(), \"handle\");\n+        String chunkName = handle.getChunkName();\n+        ChunkData chunkData = chunkMetadata.get(chunkName);\n+        if (null == chunkData) {\n+            throw new ChunkNotFoundException(chunkName);\n+        }\n+        chunkData.isReadonly = isReadOnly;\n+        return false;\n+    }\n+\n+    @Override\n+    public void addChunk(String chunkName, long length) {\n+        ChunkData chunkData = new ChunkData();\n+        chunkData.length = length;\n+        chunkMetadata.put(chunkName, chunkData);\n+    }\n+\n+    /**\n+     * Stores the chunk data.\n+     */\n+    public static class ChunkData {\n+        public long length;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDIwODg0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r434208844", "bodyText": "This field gets modified every time we write.", "author": "sachin-j-joshi", "createdAt": "2020-06-02T22:25:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUxNzkwNg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "url": "https://github.com/pravega/pravega/commit/a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Core functionality.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-04T21:39:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1MjIzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r435752231", "bodyText": "this list is only added elements", "author": "eolivelli", "createdAt": "2020-06-05T07:56:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1407 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+                offset,\n+                oldChunkName == null ? null : oldChunkName,\n+                newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"{} collectGarbage - chunk={}.\", logPrefix, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);", "originalCommit": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI3NjMyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r436276320", "bodyText": "Yes, you are right. No background threads for additional processing yet. They are coming in next PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-06T15:14:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1MjIzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r435754379", "bodyText": "how big is expected to be this buffer ?\nshall we consider reusing the same buffer, with a fixed size ?", "author": "eolivelli", "createdAt": "2020-06-05T08:00:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1407 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<String> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !chunkStorage.supportsAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isStorageSystemSegment(streamSegmentName)) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isStorageSystemSegment(streamSegmentName) && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(String streamSegmentName) {\n+        return null != systemJournal && systemJournal.isStorageSystemSegment(streamSegmentName);\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<String> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        val systemLogRecord = systemJournal.getChunkAddedRecord(streamSegmentName,\n+                offset,\n+                oldChunkName == null ? null : oldChunkName,\n+                newChunkName);\n+        systemLogRecords.add(systemLogRecord);\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                if (chunkStorage.exists(chunkTodelete)) {\n+                    chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                    log.debug(\"{} collectGarbage - chunk={}.\", logPrefix, chunkTodelete);\n+                }\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            // This is a critical assumption at this point which should not be broken,\n+            if (null != systemJournal) {\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(targetSegmentName));\n+                Preconditions.checkState(!systemJournal.isStorageSystemSegment(sourceSegment));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return chunkStorage.supportsAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];", "originalCommit": "a9d7cf04741f5b13c229025a8c694b31bc7e45c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjI3NjUzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r436276539", "bodyText": "Currently it is set to be 1MB. (ChunkStorageManagerConfig.maxBufferSizeForChunkDataTransfer).", "author": "sachin-j-joshi", "createdAt": "2020-06-06T15:17:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3MjA0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439172046", "bodyText": "There are several threads and this method should be called rarely. (only called when data is too small for calling native concat eg. for HDFS or ExtendedS3)\nDefinitely need to  measure and fix it in future if required.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T02:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTc1NDM3OQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "url": "https://github.com/pravega/pravega/commit/8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Serialize with VersionedSerializer.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-12T01:49:06Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439482841", "bodyText": "what is the impact of changing this value ?\nis there any backward compatibility concern ?", "author": "eolivelli", "createdAt": "2020-06-12T15:18:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.\n+     */\n+    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE / 4 );", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0OTc3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439649770", "bodyText": "For new installations that shouldn't be problem. For migration we will have to  set it to new  max rolling length. From practical point of view max rolling length will now be 2305843009213693951 bytes which is still really big.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T21:29:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI2MTkwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442261903", "bodyText": "I don't understand what the context for changing this constant.", "author": "fpj", "createdAt": "2020-06-18T14:18:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTk4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442505983", "bodyText": "For serialization we want to use compact long representation. If not we'll require 8 bytes for each length related field.  Using compact serialization allows us to save size of records. However compact can handle up to 2^62.  Long.MAX_VALUE is an arbitrary limit there is no real reason the default value has to be that high. With 2^62 the new limit is 2305843009213693951 which is still really big default value,", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Mjg0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4NDQ3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439484478", "bodyText": "what happens in case that data returns less than length bytes ?\nwhat happens in case that offset points to a part of the chunk that has already be written ?\nis it supposed to be overwritten ?\nor is it only expected that we are appending data to the end of the chunk ?", "author": "eolivelli", "createdAt": "2020-06-12T15:21:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwMzA1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439503052", "bodyText": "It can write less bytes than length. In this case ChunkStorageManeger will make another call.\nThe writes are supposed to create same effect as if they are append only. The upper layers will guarantee that either data is never overwritten or that same data is written back at same offset.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T15:55:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4NDQ3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439486770", "bodyText": "I apologize,\nI am trying to understand better the final API that we are defining.\nIt looks like that this ChunkHandle is only a wrapper for a chunkName.\nIt looks mostly useless.\nAs you previously said, it would be really useful that concrete implementations subclass this class and enrich it with custom properties.", "author": "eolivelli", "createdAt": "2020-06-12T15:25:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkHandle.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Data;\n+import lombok.NonNull;\n+\n+/**\n+ * Handle to a chunk.\n+ */\n+@Data\n+final public class ChunkHandle {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwOTk0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439509944", "bodyText": "I agree - In fact it is useless, and I'm now inclined to remove the concept of ChunkHandle .\nThe problem with extensible ChunkHandle is that now to create read/write handle instances in the code , we have to  add openRead and openWrite as new apis on ChunkStorageProvider. (so that we create an instance of correct concrete type).\nIt initially seemed useful to have ability to put implementation specific data in ChunkHandle.\nHowever then problem is that when you have two separate ChunkHandle instances pointing to the same underlying object, the data stored in them diverges and can have unintended side effects .\nIn this case most likely implementation choice is to keep single copy of internal state in some other data structure and handle merely points to that data. So having extensible ChunkHandle isn't really helping here.\nGiven that ChunkHandle creates more problem than it solves - I am not sure having it is useful.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:08:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzQ2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443937465", "bodyText": "For now keeping the ChunkHandle.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T03:14:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4Njc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4ODgwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439488804", "bodyText": "What is the purpose of this class ?\nGiven a chunkname you can get its current length\nis it expected to be used by tools or only internally ?", "author": "eolivelli", "createdAt": "2020-06-12T15:28:50Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkInfo.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.NonNull;\n+\n+/**\n+ * Chunk Information.\n+ */\n+@Builder\n+@Data\n+final public class ChunkInfo {", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyNDY5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439524698", "bodyText": "The purpose of this class is to get information related to chunk from ChunkStorageProvider.\nThe only relevant property today is length, but in future their could be more.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ4ODgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439492365", "bodyText": "this cast  looks very dangerous.\nA chunk is expected to be longer than Integer.MAX_VALUE", "author": "eolivelli", "createdAt": "2020-06-12T15:35:32Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1432 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength((int) chunkInfo.getLength());", "originalCommit": "8a3a4e02c66a7b68d5b0dd4ee4303e0571768510", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUyMzQ5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439523492", "bodyText": "Good catch. That cast is not needed at all. leftover from old version of code. (will fix)", "author": "sachin-j-joshi", "createdAt": "2020-06-12T16:30:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTYxNzI4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439617280", "bodyText": "I am actually surprised that spotbugs didn't catch it.", "author": "sachin-j-joshi", "createdAt": "2020-06-12T20:00:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTY0NjU5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r439646593", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-12T21:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTQ5MjM2NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MTAzMzE4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r441033183", "bodyText": "Nit: extra space after 4.", "author": "RaulGracia", "createdAt": "2020-06-16T17:48:38Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.\n+     */\n+    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE / 4 );", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNDM4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442324383", "bodyText": "Are the defaults for all these properties and default values explained in config.properties?", "author": "RaulGracia", "createdAt": "2020-06-18T15:44:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManagerConfig.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.NonNull;\n+\n+/**\n+ * Configuration for {@link ChunkStorageManager}.\n+ */\n+@AllArgsConstructor\n+@Builder(toBuilder = true)\n+public class ChunkStorageManagerConfig {\n+    /**\n+     * Default configuration for {@link ChunkStorageManager}.\n+     */\n+    public static final ChunkStorageManagerConfig DEFAULT_CONFIG = ChunkStorageManagerConfig.builder()\n+            .minSizeLimitForNativeConcat(0)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI0NTc4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443245788", "bodyText": "I'll need to update the 3rd PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-21T18:44:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNDM4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyODE0OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442328149", "bodyText": "Nit: in general, the comments of this PR lack from consistent format: sometimes there is a black line between the description of the method and the parameters, and sometimes there is none, there are descriptions of parameters/return values from methods that end with a dot and others don't. As this is an important change and people will need to look at the documentation a lot, having a consistent format would help.", "author": "RaulGracia", "createdAt": "2020-06-18T15:49:57Z", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -237,6 +265,29 @@ public static String getMetadataSegmentName(int containerId) {\n         return String.format(METADATA_SEGMENT_NAME_FORMAT, containerId);\n     }\n \n+    /**\n+     * Gets the name of the Segment that is used to store the Container's Segment Metadata. There is one such Segment\n+     * per container.\n+     *\n+     * @param containerId The Id of the Container.\n+     * @return The Metadata Segment name.\n+     */\n+    public static String getStorageMetadataSegmentName(int containerId) {\n+        Preconditions.checkArgument(containerId >= 0, \"containerId must be a non-negative number.\");\n+        return String.format(STORAGE_METADATA_SEGMENT_NAME_FORMAT, containerId);\n+    }\n+\n+    /**\n+     * Gets file name of SystemJournal for given container instance.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzNDYxOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442534618", "bodyText": "My bad. Did not realize that in IntelliJ you need press (shift + alt + L) multiple times to take effect.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:25:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyODE0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMjQzMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442332433", "bodyText": "Why are you doing this here? Would it be better to make clases like SegmentMetadata or ChunkMetadata to implement equals() and put this logic there?", "author": "RaulGracia", "createdAt": "2020-06-18T15:56:11Z", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/TestUtils.java", "diffHunk": "@@ -0,0 +1,259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadata;\n+import lombok.val;\n+import org.junit.Assert;\n+\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+\n+/**\n+ * Test utility.\n+ */\n+public class TestUtils {\n+    /**\n+     * Checks the bounds for the given segment.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param segmentName Name of the segment.\n+     * @param expectedStartOffset Expected start offset.\n+     * @param expectedLength Expected length.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static void checkSegmentBounds(ChunkMetadataStore metadataStore, String segmentName, long expectedStartOffset, long expectedLength) throws Exception {\n+        val segmentMetadata = getSegmentMetadata(metadataStore, segmentName);\n+        Assert.assertNotNull(segmentMetadata);\n+        Assert.assertEquals(expectedLength, segmentMetadata.getLength());\n+        Assert.assertEquals(expectedStartOffset, segmentMetadata.getStartOffset());\n+    }\n+\n+    /**\n+     * Checks the layout of the chunks for given segment.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param segmentName Name of the segment.\n+     * @param lengthOfChunk Length of each chunk.\n+     * @param numberOfchunks Number of chunks.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static void checkSegmentLayout(ChunkMetadataStore metadataStore, String segmentName, long lengthOfChunk, int numberOfchunks) throws Exception {\n+        val segmentMetadata = getSegmentMetadata(metadataStore, segmentName);\n+        Assert.assertNotNull(segmentMetadata);\n+        // Assert\n+        Assert.assertNotNull(segmentMetadata.getFirstChunk());\n+        Assert.assertNotNull(segmentMetadata.getLastChunk());\n+\n+        int i = 0;\n+        val chunks = getChunkList(metadataStore, segmentName);\n+        for (val chunk : chunks) {\n+            Assert.assertEquals(lengthOfChunk, chunk.getLength());\n+            i++;\n+        }\n+        Assert.assertEquals(numberOfchunks, chunks.size());\n+    }\n+\n+    /**\n+     * Checks the layout of the chunks for given segment.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param segmentName Name of the segment.\n+     * @param expectedLengths Array of expected lengths.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static void checkSegmentLayout(ChunkMetadataStore metadataStore, String segmentName, long[] expectedLengths) throws Exception {\n+        checkSegmentLayout(metadataStore, segmentName, expectedLengths, expectedLengths[expectedLengths.length - 1]);\n+    }\n+\n+    /**\n+     * Checks the layout of the chunks for given segment.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param segmentName Name of the segment.\n+     * @param expectedLengths Array of expected lengths.\n+     * @param lastChunkLengthInStorage Length of the last chunk in storage.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static void checkSegmentLayout(ChunkMetadataStore metadataStore, String segmentName, long[] expectedLengths, long lastChunkLengthInStorage) throws Exception {\n+        val segmentMetadata = getSegmentMetadata(metadataStore, segmentName);\n+        Assert.assertNotNull(segmentMetadata);\n+\n+        // Assert\n+        Assert.assertNotNull(segmentMetadata.getFirstChunk());\n+        Assert.assertNotNull(segmentMetadata.getLastChunk());\n+        int expectedLength = 0;\n+        int i = 0;\n+        val chunks = getChunkList(metadataStore, segmentName);\n+        for (val chunk : chunks) {\n+            Assert.assertEquals(\"Chunk \" + Integer.toString(i) + \" has unexpected length\",\n+                    i == expectedLengths.length-1 ? lastChunkLengthInStorage : expectedLengths[i],\n+                    chunk.getLength());\n+            expectedLength += chunk.getLength();\n+            i++;\n+        }\n+        Assert.assertEquals(expectedLengths.length, chunks.size());\n+\n+        Assert.assertEquals(expectedLengths.length, i);\n+        Assert.assertEquals(expectedLength, segmentMetadata.getLength());\n+        Assert.assertEquals(expectedLengths.length, segmentMetadata.getChunkCount());\n+    }\n+\n+    /**\n+     * Retrieves the {@link StorageMetadata} with given key from given {@link ChunkMetadataStore}.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param key Key.\n+     * @return {@link StorageMetadata} if found, null otherwise.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static StorageMetadata get(ChunkMetadataStore metadataStore, String key) throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            return txn.get(key);\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the {@link SegmentMetadata} with given key from given {@link ChunkMetadataStore}.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param key Key.\n+     * @return {@link SegmentMetadata} if found, null otherwise.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static SegmentMetadata getSegmentMetadata(ChunkMetadataStore metadataStore, String key) throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            return (SegmentMetadata) txn.get(key);\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the {@link ChunkMetadata} with given key from given {@link ChunkMetadataStore}.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param key Key.\n+     * @return {@link ChunkMetadata} if found, null otherwise.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static ChunkMetadata getChunkMetadata(ChunkMetadataStore metadataStore, String key) throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            return (ChunkMetadata) txn.get(key);\n+        }\n+    }\n+\n+    /**\n+     * Gets the list of chunks for the given segment.\n+     *\n+     * @param metadataStore Metadata store to query.\n+     * @param key Key.\n+     * @return List of {@link ChunkMetadata} for the segment.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static ArrayList<ChunkMetadata> getChunkList(ChunkMetadataStore metadataStore, String key) throws Exception {\n+        try (val txn = metadataStore.beginTransaction()) {\n+            val segmentMetadata = getSegmentMetadata(metadataStore, key);\n+            Assert.assertNotNull(segmentMetadata);\n+            ArrayList<ChunkMetadata> chunkList = new ArrayList<ChunkMetadata>();\n+            String current = segmentMetadata.getFirstChunk();\n+            while (null != current) {\n+                val chunk = (ChunkMetadata) txn.get(current);\n+                chunkList.add(chunk);\n+                current = chunk.getNextChunk();\n+            }\n+            return chunkList;\n+        }\n+    }\n+\n+    /**\n+     * Checks if all chunks actually exist in storage for given segment.\n+     * @param storageProvider {@link ChunkStorageProvider} instance to check.\n+     * @param metadataStore {@link ChunkMetadataStore} instance to check.\n+     * @param segmentName Segment name to check.\n+     * @throws Exception Exceptions are thrown in case of any errors.\n+     */\n+    public static void checkChunksExistInStorage(ChunkStorageProvider storageProvider, ChunkMetadataStore metadataStore, String segmentName) throws Exception {\n+        int chunkCount = 0;\n+        long dataSize = 0;\n+        val segmentMetadata = getSegmentMetadata(metadataStore, segmentName);\n+        HashSet<String> visited = new HashSet<>();\n+        val chunkList = getChunkList(metadataStore, segmentName);\n+        for (ChunkMetadata chunkMetadata : chunkList) {\n+            Assert.assertTrue(storageProvider.exists(chunkMetadata.getName()));\n+            val info = storageProvider.getInfo(chunkMetadata.getName());\n+            Assert.assertTrue(String.format(\"Actual %s, Expected %d\", chunkMetadata, info.getLength()),\n+                    chunkMetadata.getLength() <= info.getLength());\n+            chunkCount++;\n+            Assert.assertTrue(\"Chunk length should be non negative\", info.getLength() >= 0);\n+            Assert.assertTrue(info.getLength() <= segmentMetadata.getMaxRollinglength());\n+            Assert.assertTrue(info.getLength() >= chunkMetadata.getLength());\n+            Assert.assertFalse(\"All chunks should be unique\", visited.contains(info.getName()));\n+            visited.add(info.getName());\n+            dataSize += chunkMetadata.getLength();\n+        }\n+        Assert.assertEquals(chunkCount, segmentMetadata.getChunkCount());\n+        Assert.assertEquals(dataSize, segmentMetadata.getLength() - segmentMetadata.getFirstChunkStartOffset());\n+    }\n+\n+    /**\n+     * Asserts that SegmentMetadata and its associated list of ChunkMetadata matches expected values.\n+     *\n+     * @param expectedSegmentMetadata Expected {@link SegmentMetadata}.\n+     * @param expectedChunkMetadataList Expected list of {@link ChunkMetadata}.\n+     * @param actualSegmentMetadata Actual {@link SegmentMetadata}.\n+     * @param actualChunkMetadataList Actual list of {@link ChunkMetadata}.\n+     * @throws Exception {@link AssertionError} is thrown in case of mismatch.\n+     */\n+    public static void assertEquals(SegmentMetadata expectedSegmentMetadata, ArrayList<ChunkMetadata> expectedChunkMetadataList, SegmentMetadata actualSegmentMetadata, ArrayList<ChunkMetadata> actualChunkMetadataList) throws Exception {\n+        assertEquals(expectedSegmentMetadata, actualSegmentMetadata);\n+        Assert.assertEquals(expectedChunkMetadataList.size(), actualChunkMetadataList.size());\n+\n+        for (int i = 0; i < expectedChunkMetadataList.size(); i++) {\n+            val expectedChunkMetadata = expectedChunkMetadataList.get(i);\n+            val actualChunkMetadata = actualChunkMetadataList.get(i);\n+            assertEquals(expectedChunkMetadata, actualChunkMetadata);\n+        }\n+    }\n+\n+    /**\n+     * Asserts that given {@link ChunkMetadata} matches expected value.\n+     *\n+     * @param expectedChunkMetadata Expected {@link ChunkMetadata}.\n+     * @param actualChunkMetadata Actual {@link ChunkMetadata}.\n+     * @throws Exception {@link AssertionError} is thrown in case of mismatch.\n+     */\n+    private static void assertEquals(ChunkMetadata expectedChunkMetadata, ChunkMetadata actualChunkMetadata) {\n+        Assert.assertEquals(expectedChunkMetadata.getName(), actualChunkMetadata.getName());\n+        Assert.assertEquals(expectedChunkMetadata.getLength(), actualChunkMetadata.getLength());\n+        Assert.assertEquals(expectedChunkMetadata.getNextChunk(), actualChunkMetadata.getNextChunk());\n+    }\n+\n+    /**\n+     * Asserts that given {@link SegmentMetadata} matches expected value.\n+     *\n+     * @param expectedSegmentMetadata Expected {@link SegmentMetadata}.\n+     * @param actualSegmentMetadata Actual {@link SegmentMetadata}.\n+     * @throws Exception {@link AssertionError} is thrown in case of mismatch.\n+     */\n+    private static void assertEquals(SegmentMetadata expectedSegmentMetadata, SegmentMetadata actualSegmentMetadata) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4OTk1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444989955", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:41:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMjQzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM0NzY1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442347657", "bodyText": "create -> creates, these segment -> these segments", "author": "RaulGracia", "createdAt": "2020-06-18T16:19:37Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,935 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This create a circular dependency while reading or writing the data about these segment from the metadata segments..", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4OTc4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444989786", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:41:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM0NzY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI1ODY2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442258663", "bodyText": "This change seems gratuitous, there is no other change to the test case other than and a throws Exception to the signature.", "author": "fpj", "createdAt": "2020-06-18T14:14:12Z", "path": "bindings/src/test/java/io/pravega/storage/IdempotentStorageTestBase.java", "diffHunk": "@@ -132,9 +132,10 @@ public void testWrite() throws Exception {\n \n     /**\n      * This test case simulates two hosts writing at the same offset at the same time.\n+     * @throws Exception if an unexpected error occurred.\n      */\n     @Test(timeout = 30000)\n-    public void testParallelWriteTwoHosts() {\n+    public void testParallelWriteTwoHosts() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwMzI5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442503299", "bodyText": "SimpleStorageTests::testParallelWriteTwoHosts overrides it and needs a throws clause.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:07:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI1ODY2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3MTc5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442271797", "bodyText": "If one is to implement a new binding, then is this one of the classes that needs to be extended? A quick text search only shows me the in-memory provider extending it. Independent of the answer, it would be good to have somewhere the guidelines for implementing a new binding, and in particular, the classes and interfaces that the developer needs to pay attention to.\nI have mixed feelings about the pointer to the PDP. On the one hand, it is good to have a single source we can point to. On the other, it doesn't point to a specific section of the document, so is it saying that the developer should read it all to figure out where this specific information is? Not ideal.\nIn general, I'm ok with avoiding duplication, but I'm also a big fan of giving as much insight as possible using comments in the code.", "author": "fpj", "createdAt": "2020-06-18T14:31:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU0MjU4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442542589", "bodyText": "We are breaking a big change into 3/4 smaller PRs.\nActually there are 3 other providers which are part of this change.\nsachin-j-joshi@5bb2129\nI'll try to update the class", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:50:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3MTc5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4OTAwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444989000", "bodyText": "Updated the comments.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:39:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3MTc5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3NzY1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442277653", "bodyText": "We do not collect metrics for open operations like we do for other operations? I'm wondering why we are not doing it consistently for all operations.", "author": "fpj", "createdAt": "2020-06-18T14:38:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4ODY5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444988697", "bodyText": "We collect metrics only in two cases-\n\nLatency and counts for data operation (read/write/concat)\nCount of Creation/deletion of chunks.\n\nNot sure  operations like open, checkExists need to be tracked.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3NzY1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442279712", "bodyText": "What's the goal of \"closing\" a provider? Can you explain why this is something you don't want to be publicly visible?", "author": "fpj", "createdAt": "2020-06-18T14:41:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageProviderMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageProviderMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageProviderMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageProviderMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.\n+     * @throws Exception In case of any error.\n+     */\n+    @Override\n+    public void close() throws Exception {\n+        this.closed.set(true);\n+    }\n+\n+    /**\n+     * Checks whether this instance is closed or not.\n+     * @return True if this instance is closed, false otherwise.\n+     */\n+    protected boolean isClosed() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzNzk4Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442537982", "bodyText": "Initially I had this.closed as protected member so that derived class can access it.\nClose is called by container on shutdown.\nSo instead of exposing the field I am just exposing it as isClosed() method. This useful for derived class to check if this instance is closed or not.\nThere is no need to expose this otherwise.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc3NzM4Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442777386", "bodyText": "What happens if some code tries to use a closed provider?", "author": "fpj", "createdAt": "2020-06-19T11:04:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg5NDE2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442894167", "bodyText": "All BaseChunkStorageProvider methods check this to prevent executing code on closed instance.\nExceptions.checkNotClosed(this.closed.get(), this);\nHDFS provider also uses isClosed but on second thought it now seems redundant .\nI'll probably remove it.\nprivate void ensureInitializedAndNotClosed() {\n        Exceptions.checkNotClosed(this.isClosed(), this);\n        Preconditions.checkState(this.fileSystem != null, \"HDFSStorage is not initialized.\");\n    }\n\n@Override\n    public void close() {\n        if (!isClosed()) {\n            if (this.fileSystem != null) {\n                try {\n                    this.fileSystem.close();\n                    this.fileSystem = null;\n                } catch (IOException e) {\n                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n                }\n            }\n        }\n    }\n\n@eolivelli Did you find isClosed useful at all ? Otherwise I'll just remove it.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T15:06:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY0ODc3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443648774", "bodyText": "@sachin-j-joshi\nI think that this isClosed()  method is not useful to the real implementation.\nbecause if the implementation needs to handle its own close protocol it must do it internally and it cannot rely on this flag.\nSo if this isClosed() method is not useful anywhere else I would drop it.\nWe must require from the implementation that you can call close() as many times as you want.\nWe may expect  from the implementation that any calls to a closed implementation instance fails.\nbtw having our own guard in BaseCheckStoreProvider is needed because we have to handle the resources managed by this class, but it is a internal detail, that we should hide from the user.", "author": "eolivelli", "createdAt": "2020-06-22T15:36:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3MzkwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443673903", "bodyText": "ok.", "author": "sachin-j-joshi", "createdAt": "2020-06-22T16:13:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI3OTcxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI4MDY5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442280692", "bodyText": "doesExist is a funny method name, we are \"doing exist\" here, it is more like we are checking, verifying or validating that it exists. I get that you are trying to be consistent in the use of to do, but that feels off here.", "author": "fpj", "createdAt": "2020-06-18T14:42:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = doesExist(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     *\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageProviderMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageProviderMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageProviderMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageProviderMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageProviderMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageProviderMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.\n+     * @throws Exception In case of any error.\n+     */\n+    @Override\n+    public void close() throws Exception {\n+        this.closed.set(true);\n+    }\n+\n+    /**\n+     * Checks whether this instance is closed or not.\n+     * @return True if this instance is closed, false otherwise.\n+     */\n+    protected boolean isClosed() {\n+        return this.closed.get();\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    abstract protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException;\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName String name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    abstract protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException;\n+\n+    /**\n+     * Determines whether named chunk exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    abstract protected boolean doesExist(String chunkName) throws ChunkStorageException, IllegalArgumentException;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzODkwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442538901", "bodyText": "Agree. checkExists might be a better name.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI4MDY5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI0ODY0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443248642", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-21T19:17:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI4MDY5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI4NjQ5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442286495", "bodyText": "I'm wondering why there is no explicit call to append in this interface. If it supports append, then shouldn't I expect an append call to be exposed?", "author": "fpj", "createdAt": "2020-06-18T14:50:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorageProvider}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorageProvider implements ChunkStorageProvider {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     *\n+     */\n+    public BaseChunkStorageProvider() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzNTMwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442535304", "bodyText": "write is append.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:27:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjI4NjQ5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwNjk0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442306943", "bodyText": "What's the plan for this executor? E.g., what is the recommended size for the pool? Maybe that doesn't change with respect to the previous implementation.", "author": "fpj", "createdAt": "2020-06-18T15:19:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNzAyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442507027", "bodyText": "At least as of now, there is no planned change to thread pool size.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:15:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwNjk0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442308343", "bodyText": "I'm not sure what this is initializing, all I see are assignments, why can't we do it as part of the constructor?", "author": "fpj", "createdAt": "2020-06-18T15:21:23Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMjg0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442512840", "bodyText": "Now there is a circular dependency between table store and storage. I need to plug into existing initialization logic in multiple steps.\nThis is used like below in 3rd PR.\nsachin-j-joshi@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208\nsachin-j-joshi@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208\nsachin-j-joshi@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:28:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MDExOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442840119", "bodyText": "I'll try to understand why you have this circular dependency and whether there is a way around it. Circular dependencies often are an indication of poor design, so I'm reluctant at this point in accepting that we need to have this. If you can explain your reasoning, it might help.", "author": "fpj", "createdAt": "2020-06-19T13:27:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk3NTk0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442975942", "bodyText": "The current design splits metadata management (ChunkMetadataStore) from the actual data path (ChunkStorageProvider).\n\n\nThe specific metadata store can be any KV store that implements version based concurrency control and conditional updates based on version. (Most KV stores do.)\n(In theory fencing can be implemented by keeping an epoch KV pair)\n\n\nMy preference is that we build KV store that directly uses ChunkStorageProvider to store its pages.\n\n\nHowever we chose to use internal Table segments to store this metadata. Mainly because it is already implemented and available. Additionally we have BK fencing built into the implementation.\n\n\nBut that also has a major down side - the table segment itself writes and reads data using StorageWriter - and already assumes that segment abstraction is provided by Storage interface.\nWhen table segment needs to read/write its data to underlying segments, it will end up calling into ChunkStorageManager.  But then ChunkStorageManager itself needs  to read/write all  KV metadata pairs to table segments.\nAll metadata about all segments and chunks is stored in table store by ChunkStorageManager - which is all fine , except what about those segments used by table segment itself?.\nI call these special table segments as \"storage system segments\" or \"critical segments\" . Without information about these segments initialized, the table segments can't read or write , and without table segments we can't read/write metadata about other segments.\nThose table segments themselves need to be stored in chunks and then metadata data about those segments/chunks needs to be managed.\nSo that is the origin of circular dependency.\nWe break this dependency in following way.\n\n\nThe metadata records corresponding to \"storage system segments\" is pinned to memory and not written to table segments themselves. (marking record pinned means that KV pair is never evicted and for those records we don't look them up in underlying table segments)\n\n\nInstead changes to these metadata is logged into SystemJournal. The system journal is written directly to underlying ChunkStorageProvider.\n\n\nDuring initialization , we replay the SystemJournal log to recreate the state of  segments that make the table segments to store metadata.\n\n\nWith this background, during segment container initialization the storage has to be initialized before  table store extension , but then table segments need to be initialized and passed back to storage. So in the end everything works.\nThe obvious question at this point is that - should Storage have a dependency on table segments?\nAnd in retrospect I feel like that definitely complicates things.\nThe metadata can either be stored externally in some other store or locally in ChunkStorageProvider itself.\nAlternative to using full table segment functionality is to implement Log Structured  Merge Tree (LSMT) based KV store on top of ChunkStorageProvider and also deal with fencing there.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T17:58:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYyMDU1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443620558", "bodyText": "I'm not sure we can split the behavior of ChunkMetadataStore, but I wonder if we could do it for ChunkStorageManager. With pinning and such, it sound like system segments are being treated differently, and as such, they might belong in a separate class even if the underlying storage is common. If we can have this separation while not duplicating code, then it might be a nice way to break the circular dependency.\nWhen starting a segment container, the idea would be to:\n1- Start system storage manager\n2- Start metadata store, which depends on 1\n3- Start chunk manager that depends on 2\nWould anything like this work?", "author": "fpj", "createdAt": "2020-06-22T14:55:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY0NjMxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443646317", "bodyText": "One thing I do not like here is that we pass yet another metadata store instance via this method, after we pass one to the constructor. Can we refactor this so that both the metadata store and the container id are passed at construction time?\nAnother thing is that you have two initialized methods. You did explain below why, but this is utterly confusing to whomever is calling this. This particular initialize method should be put entirely within the constructor; feel free to make use of factories if that will help with things.\nThen when the other initialize method is called, you can call whatever method is required on these classes to initialize themselves.", "author": "andreipaduroiu", "createdAt": "2020-06-22T15:32:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY0Nzg5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443647896", "bodyText": "The metadata can either be stored externally in some other store or locally in ChunkStorageProvider itself.\n\nWe do not want to store metadata externally. That means we'd have to deploy yet another service with Pravega, which will make things more complicated. If you want to store it locally, how would do you do that?  You mentioned LSM Trees. Do you believe that doing something like that will be simpler than using this method? If you do it, you'd have to reimplement your fencing mechanism (no BK, so it doesn't come for free), deal with LTS problems, and on top of that, implement your own LTS layout. So you'd just be deferring the problem to another one.", "author": "andreipaduroiu", "createdAt": "2020-06-22T15:34:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY4NDA3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443684077", "bodyText": "I don't understand why we had an initialization method on Storage interface ( ReadOnlyStorage) . But it is what it is - this interface is used all over the place. I can not change this at this point - Also I don't want to change all existing storage implementations everywhere at this point.\nI have already explained why table segment can not be passed in constructor in real container.\nThere are two constructor overloads. On of the overload is used in segment container class. The other one is used in lots of tests.\nHowever I can definitely rename the second initialize() method.\n/**\n * Defines a Read-Only abstraction for Permanent Storage with async operations.\n */\npublic interface ReadOnlyStorage extends AutoCloseable {\n    /**\n     * Initializes this Storage instance with the given ContainerEpoch.\n     *\n     * @param containerEpoch The Container Epoch to initialize with.\n     */\n    void initialize(long containerEpoch);", "author": "sachin-j-joshi", "createdAt": "2020-06-22T16:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4MzczNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444983734", "bodyText": "Based on our conversation, resolving the discussion.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:32:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMwODM0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDAxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442310011", "bodyText": "It is awkward that we use multiple initialize calls like this, initialization typically include some expensive operation that require IO or calls to services, and these are just doing assignment. This is also not checking that the object has already been initialized.", "author": "fpj", "createdAt": "2020-06-18T15:23:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMzE1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442513156", "bodyText": "Now there is a circular dependency between table store and storage. I need to plug into existing initialization logic in multiple steps.\nThis is used like below in 3rd PR.\nsachin-j-joshi/pravega@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208\nsachin-j-joshi/pravega@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208\nsachin-j-joshi/pravega@1cc42a4#diff-b32d413ea6aa7bb2017179c6a1ca4511R189-R208", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:29:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDAxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYyNTEwOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442625109", "bodyText": "link didn't work , but all the inits are called in StreamSegmentContainer.java", "author": "sachin-j-joshi", "createdAt": "2020-06-19T04:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDAxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDU2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442310565", "bodyText": "Is it a typo that it says \"... storage adapter...\"?", "author": "fpj", "createdAt": "2020-06-18T15:24:36Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMzc3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442313773", "bodyText": "Also, could you please properly format this javadoc. Make the lines shorter and separate into paragraphs clearly.", "author": "fpj", "createdAt": "2020-06-18T15:29:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDU2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTQ3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442515470", "bodyText": "ChunkStorageManager implements Storage.  the method is from Storage.  The IDE copied these comments from already defined Storage interface which is not changing at all and so I just accepted the comments as they were.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:34:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDU2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MTY5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442841695", "bodyText": "It doesn't matter if you the IDE did it or you did it manually, those are new lines added and they need to comply with our guidelines. But, you should keep in mind that we don't strictly require javadocs for overriden methods, see our contribution guidelines:\nhttps://github.com/pravega/pravega/wiki/Contributing#javadoc\nYou may want to remove the javadocs for all overriden methods, and create an issue to fix in the javadocs in the corresponding interface, and assign it to yourself.", "author": "fpj", "createdAt": "2020-06-19T13:30:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxMDU2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNTA1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442315059", "bodyText": "This sentence is vague and it is unclear what the point is of this observation on different ways to implement read-write locks. Is it trying to explain what we do?", "author": "fpj", "createdAt": "2020-06-18T15:31:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTU1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442515557", "bodyText": "ChunkStorageManager implements Storage. the method is from Storage. The IDE copied these comments from already defined Storage interface which is not changing at all and so I just accepted the comments as they were.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:34:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNTA1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4NjAzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444986035", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:35:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNTA1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNjI5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442316299", "bodyText": "Here we are using an HTML paragraph markup, but not above. I believe we have in general not been using them, but whatever you choose to do, please do it consistently.", "author": "fpj", "createdAt": "2020-06-18T15:32:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTc1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442515757", "bodyText": "ChunkStorageManager implements Storage. the method is from Storage. The IDE copied these comments from already defined Storage interface which is not changing at all and so I just accepted the comments as they were.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:35:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxNjI5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxOTI2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442319265", "bodyText": "Is it claimOwnership throwing this?", "author": "fpj", "createdAt": "2020-06-18T15:37:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNjAxMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442516012", "bodyText": "yes.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:35:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxOTI2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxOTgxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442319813", "bodyText": "It's not good practice to throw generic exceptions, why are we doing it?", "author": "fpj", "createdAt": "2020-06-18T15:37:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk4NjE5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444986199", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:35:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMxOTgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMDExNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442320117", "bodyText": "Missing space before So.", "author": "fpj", "createdAt": "2020-06-18T15:38:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMTI0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442321245", "bodyText": "Is it the update that fails or the commit? If the update call is making the change permanent, then why do we need a commit?", "author": "fpj", "createdAt": "2020-06-18T15:40:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNzQ2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442517468", "bodyText": "Update call make changes o local copy.\nCommit makes change permanent.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:39:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMTI0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYyMTU3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442621576", "bodyText": "Update updates only the copy of data local to this specific transaction. this value is not reflected until committed. However within a transaction you get the same value you previously wrote when you read it. (Read your own writes).\nTransaction is implemented as a dictionary of key value pairs. Any changes made are thus local. This dictionary is then written to underlying store in a single update  during commit. This simplifies the main logic considerably - changes are all atomic - all modified keys are committed or none at all. Also all changes to the keys are isolated from other transactions.  On first read the value is retrieved from the underlying table and put into the dictionary.\nI did not want to implement very complicated dirty/modified key/vales tracking logic. Instead after modifying the object , update is called to mark the modified object as dirty (so that we can write it at commit time).", "author": "sachin-j-joshi", "createdAt": "2020-06-19T04:15:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMTI0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMTYyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442321628", "bodyText": "Same comment about the use of HTML markups.", "author": "fpj", "createdAt": "2020-06-18T15:40:39Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNzY3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442517679", "bodyText": "Same as above", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:39:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyMTYyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNDMzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442324338", "bodyText": "The ownership can change immediately after we check this, is it a problem and if not, then why?", "author": "fpj", "createdAt": "2020-06-18T15:44:29Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMjAwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442522004", "bodyText": "All changes to metadata are done in transaction. Transaction will fail to commit if new epoch has claimed ownership and fenced this instance. (Transaction commit writes to the table segment which writes to tier-1 and it will fail because of fencing.)\n\nIn case of transaction failure, the newly added chunks are deleted. The metadata in table is not updated as transaction failed.\nOn fencing, the new instance will start a new chunk. And update length of old segment by reading length from LTS. So any data written by this instance to existing chunks is simply ignored.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNDMzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNzIyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442327227", "bodyText": "This comment is referring to two factors in the boolean expression for this if construct, but there are really for. I want to understand the reasoning behind the two last factors.", "author": "fpj", "createdAt": "2020-06-18T15:48:36Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMjY5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442522696", "bodyText": "Every time we fail over we must start a new chunk\nIf the underlying chunk storage can not support append then we must create and write to new chunk.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNzIyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMyNzUwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442327508", "bodyText": "Missing comment.", "author": "fpj", "createdAt": "2020-06-18T15:49:02Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMDk0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442330945", "bodyText": "Critical needs to be defined.", "author": "fpj", "createdAt": "2020-06-18T15:54:01Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMTEwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442331100", "bodyText": "Typo in Metadata.", "author": "fpj", "createdAt": "2020-06-18T15:54:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMzAwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442333000", "bodyText": "Is seal not an idempotent operation? If it is, then we don't need the verification. If it isn't, then I wonder what happens to the transaction that never commits. Is it autoclosed and that's like an abort?", "author": "fpj", "createdAt": "2020-06-18T15:57:02Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMzU0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442523540", "bodyText": "Yes transaction auto closes. (implicit abort.)", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:54:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMzAwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYyMzQ1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442623453", "bodyText": "If it is sealed, then we don't have to modify and write the metadata back to store.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T04:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjMzMzAwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4ODA5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442388092", "bodyText": "Surviving is a funny term, what about \"After this operation completes, the target segment incorporates the source segment, and only the target segment remains.\"", "author": "fpj", "createdAt": "2020-06-18T17:27:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMzk3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442523976", "bodyText": "IDE magic, it copied exact comment from already present Storage interface.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:55:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4ODA5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI1MTgyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443251829", "bodyText": "#4886", "author": "sachin-j-joshi", "createdAt": "2020-06-21T19:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM4ODA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MDgwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442390806", "bodyText": "Looking at shouldDefrag, I'm not sure what the logic is that leads defragmentation. Specifically, the javadoc for the defrag call says that it expects a list of chunks, so I'm not clear on what the condition is.", "author": "fpj", "createdAt": "2020-06-18T17:31:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNDk2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442524966", "bodyText": "List as in range between - String startChunkName, String lastChunkName.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T21:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MDgwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYyMzIwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442623201", "bodyText": "Currently it is a simple condition - if underlying store supports concat or supports appends to same chunk. However in future it could be more complex condition.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T04:22:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MDgwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MTU4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442391580", "bodyText": "such elgible -> of eligible", "author": "fpj", "createdAt": "2020-06-18T17:33:16Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MTg2OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442391869", "bodyText": "Long line, please break it.", "author": "fpj", "createdAt": "2020-06-18T17:33:47Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MzMwMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442393302", "bodyText": "Rephrase this:\n\nObviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n\nto something like:\n\nWe want to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically only metadata operations, reducing the overall cost of the merging them together.", "author": "fpj", "createdAt": "2020-06-18T17:36:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5MzQ1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442393457", "bodyText": "Another line break needed.", "author": "fpj", "createdAt": "2020-06-18T17:36:42Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NDc2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442394762", "bodyText": "A suggested rephrase:\nIdeally, we run defragmentation in the background and periodically rather than in the write/concat path.\nbut at the same time, it sounds speculative as this is not what we are doing. Even if right, we shouldn't be speculating in a comment.", "author": "fpj", "createdAt": "2020-06-18T17:39:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NTA3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442395074", "bodyText": "Line breaks.", "author": "fpj", "createdAt": "2020-06-18T17:39:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NTEyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442395129", "bodyText": "Line breaks.", "author": "fpj", "createdAt": "2020-06-18T17:40:02Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NTY3Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442395677", "bodyText": "If there is a background task doing defragmentation, then do we still need a metadata transaction for context?", "author": "fpj", "createdAt": "2020-06-18T17:41:02Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNTg3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442525871", "bodyText": "No background task today. The defrag is called inline at the end of concat.\nI do plan to add background task in near future. (May be 0.9 time frame).\nIn that can the background task will make changes in its own transaction.\n(A new transaction will not see changes made by other transactions until other transaction is committed.)", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:00:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NTY3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NjAzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442396035", "bodyText": "is exited nextChunkName -> exits, nextChunkName", "author": "fpj", "createdAt": "2020-06-18T17:41:39Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NjUxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442396513", "bodyText": "If there are chunks that can be appended together, then concat them.", "author": "fpj", "createdAt": "2020-06-18T17:42:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI0NDE3NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443244174", "bodyText": "?", "author": "sachin-j-joshi", "createdAt": "2020-06-21T18:24:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NjUxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NzQ2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442397464", "bodyText": "What if the storage system supports append, but not write at offset?", "author": "fpj", "createdAt": "2020-06-18T17:44:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNjU4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442526589", "bodyText": "Then it should throw exception related to bad offset.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:02:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NzQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3NDc4MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443674781", "bodyText": "Ok, but then does it mean that the storage write will fail? Is it the case that for practical purposes it can't use appends to concat?", "author": "fpj", "createdAt": "2020-06-22T16:15:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NzQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc0NTAzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443745035", "bodyText": "Both ECS and FileSystem can support overwriting old data.\nHDFS is truncating the chunks at end before concat. (like below)\nHowever as a general contract I need to handle this case.\n public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n        ensureInitializedAndNotClosed();\n        int length = 0;\n        try {\n            val sources = new Path[chunks.length - 1];\n            for (int i = 1; i < chunks.length; i++) {\n                val chunkLength = chunks[i].getLength();\n                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n                sources[i - 1] = getFilePath(chunks[i].getName());\n                length += chunkLength;\n            }\n            // Concat source file into target.\n            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n        } catch (IOException e) {\n            throwException(chunks[0].getName(), \"doConcat\", e);\n        }\n        return length;\n    }", "author": "sachin-j-joshi", "createdAt": "2020-06-22T18:21:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM5NzQ2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMTAyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442401027", "bodyText": "Why do we need to treat system logs differently?", "author": "fpj", "createdAt": "2020-06-18T17:50:40Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                readIndexCache.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) throws StreamSegmentNotExistsException {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new StreamSegmentNotExistsException(streamSegmentName);\n+        }\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+                    segmentMetadata.decrementChunkCount();\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyODQwNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442528404", "bodyText": "The table segment for storage metadata and container metadata are special segments, they store data in storage and also used for keeping metadata.\nTo avoid this circular dependency at boot time (unfortunately very tricky to handle), we write log record for any change to these segment's layout into the system log. At boot time we replay it. So that we have complete info about metadata data tables.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMTAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMzE1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442403152", "bodyText": "I'm not sure we need a transaction here, can you justify?", "author": "fpj", "createdAt": "2020-06-18T17:54:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                readIndexCache.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) throws StreamSegmentNotExistsException {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new StreamSegmentNotExistsException(streamSegmentName);\n+        }\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+                    segmentMetadata.decrementChunkCount();\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (isStorageSystemSegment(segmentMetadata)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(\n+                                SystemJournal.TruncationRecord.builder()\n+                                        .segmentName(streamSegmentName)\n+                                        .offset(offset)\n+                                        .firstChunkName(segmentMetadata.getFirstChunk())\n+                                        .startOffset(finalStartOffset)\n+                                        .build());\n+                        return null;\n+                    });\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Lists all the segments stored on the storage device.\n+     *\n+     * @return Iterator that can be used to enumerate and retrieve properties of all the segments.\n+     * @throws IOException if exception occurred while listing segments.\n+     */\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() throws IOException {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset()) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+                Timer timer1 = new Timer();\n+                int cntScanned = 0;\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val floorEntry = readIndexCache.findFloor(streamSegmentName, offset);\n+                if (null != floorEntry) {\n+                    startOffsetForCurrentChunk = floorEntry.getOffset();\n+                    currentChunkName = floorEntry.getChunkName();\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (startOffsetForCurrentChunk <= currentOffset\n+                            && startOffsetForCurrentChunk + chunkToReadFrom.getLength() > currentOffset) {\n+                        // we have found a chunk that contains first byte we want to read\n+                        log.debug(\"{} read - found chunk to read - segment={}, chunk={}, startOffset={}, length={}, readOffset={}.\",\n+                                logPrefix, streamSegmentName, chunkToReadFrom, startOffsetForCurrentChunk, chunkToReadFrom.getLength(), currentOffset);\n+                        break;\n+                    }\n+                    currentChunkName = chunkToReadFrom.getNextChunk();\n+                    startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+\n+                    // Update read index with newly visited chunk.\n+                    if (null != currentChunkName) {\n+                        readIndexCache.addIndexEntry(streamSegmentName, currentChunkName, startOffsetForCurrentChunk);\n+                    }\n+                    cntScanned++;\n+                }\n+                log.debug(\"{} read - chunk lookup - segment={}, offset={}, scanned={}, latency={}.\",\n+                        logPrefix, handle.getSegmentName(), offset, cntScanned, timer1.getElapsed().toMillis());\n+\n+                // Now read.\n+                while (bytesRemaining > 0 && null != currentChunkName) {\n+                    int bytesToRead = Math.min(bytesRemaining, Math.toIntExact(chunkToReadFrom.getLength() - (currentOffset - startOffsetForCurrentChunk)));\n+                    //assert bytesToRead != 0;\n+\n+                    if (currentOffset >= startOffsetForCurrentChunk + chunkToReadFrom.getLength()) {\n+                        // The current chunk is over. Move to the next one.\n+                        currentChunkName = chunkToReadFrom.getNextChunk();\n+                        if (null != currentChunkName) {\n+                            startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+                            chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                            log.debug(\"{} read - reading from next chunk - segment={}, chunk={}\", logPrefix, streamSegmentName, chunkToReadFrom);\n+                        }\n+                    } else {\n+                        Preconditions.checkState(bytesToRead != 0, \"bytesToRead is 0\");\n+                        // Read data from the chunk.\n+                        ChunkHandle chunkHandle = chunkStorage.openRead(chunkToReadFrom.getName());\n+                        int bytesRead = chunkStorage.read(chunkHandle, currentOffset - startOffsetForCurrentChunk, bytesToRead, buffer, currentBufferOffset);\n+\n+                        bytesRemaining -= bytesRead;\n+                        currentOffset += bytesRead;\n+                        currentBufferOffset += bytesRead;\n+                        totalBytesRead += bytesRead;\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} read - segment={}, offset={}, bytesRead={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+                return totalBytesRead;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets current information about a StreamSegment.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested about the StreamSegment.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMTU0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442531541", "bodyText": "Transaction is also used for both read and writing of the data. This is so that while the transaction is still pending it needs to get the same value for key that it wrote earlier.\nFor first read it will read from the table sgement/cache  and then this value will be cached in transaction. This avoids multiple calls to table segments to read same value.\nI did not opt for creating separate mechanism for read only read. All transactions auto close. So if it is not committed then it is aborted.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:15:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMzE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwMzQ5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442403493", "bodyText": "Same question about needing a transaction here.", "author": "fpj", "createdAt": "2020-06-18T17:55:06Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        //\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                    .segmentName(streamSegmentName)\n+                    .offset(offset)\n+                    .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                    .newChunkName(newChunkName)\n+                    .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Seals a StreamSegment. No further modifications are allowed on the StreamSegment after this operation completes.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Seal.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed. If the operation\n+     * failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Concatenates two StreamSegments together. The Source StreamSegment will be appended as one atomic block at the end\n+     * of the Target StreamSegment (but only if its length equals the given offset), after which the Source StreamSegment\n+     * will cease to exist. Prior to this operation, the Source StreamSegment must be sealed.\n+     *\n+     * @param targetHandle  A read-write SegmentHandle that points to the Target StreamSegment. After this operation\n+     *                      is complete, this is the surviving StreamSegment.\n+     * @param offset        The offset in the Target StreamSegment to concat at.\n+     * @param sourceSegment The Source StreamSegment. This StreamSegment will be concatenated to the Target StreamSegment.\n+     *                      After this operation is complete, this StreamSegment will no longer exist.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the target segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the either the source Segment or the target Segment do not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for the target Segment (it was fenced out).\n+     * <li> IllegalStateException: When the Source Segment is not Sealed.\n+     * </ul>\n+     * @throws IllegalArgumentException If targetHandle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * Obviously both ECS and S3 have MPU and is supposed to be metadata only operation for them.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForNativeConcat : Size of chunk in bytes above which it is no longer considered a small object. For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForNativeConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which native concat is not efficient.(minSizeLimitForNativeConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForNativeConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForNativeConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForNativeConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Deletes a StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to Delete.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                readIndexCache.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) throws StreamSegmentNotExistsException {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new StreamSegmentNotExistsException(streamSegmentName);\n+        }\n+    }\n+\n+    /**\n+     * Truncates all data in the given StreamSegment prior to the given offset. This does not fill the truncated data\n+     * in the segment with anything, nor does it \"shift\" the remaining data to the beginning. After this operation is\n+     * complete, any attempt to access the truncated data will result in an exception.\n+     * <p>\n+     * Notes:\n+     * * Depending on implementation, this may not truncate at the exact offset. It may truncate at some point prior to\n+     * the given offset, but it will never truncate beyond the offset.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to truncate to.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+                    segmentMetadata.decrementChunkCount();\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (isStorageSystemSegment(segmentMetadata)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(\n+                                SystemJournal.TruncationRecord.builder()\n+                                        .segmentName(streamSegmentName)\n+                                        .offset(offset)\n+                                        .firstChunkName(segmentMetadata.getFirstChunk())\n+                                        .startOffset(finalStartOffset)\n+                                        .build());\n+                        return null;\n+                    });\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation can truncate Segments.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n+\n+    /**\n+     * Lists all the segments stored on the storage device.\n+     *\n+     * @return Iterator that can be used to enumerate and retrieve properties of all the segments.\n+     * @throws IOException if exception occurred while listing segments.\n+     */\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() throws IOException {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n+\n+    /**\n+     * Opens the given Segment in read-only mode without acquiring any locks or blocking on any existing write-locks and\n+     * makes it available for use for this instance of Storage.\n+     * Multiple read-only Handles can coexist at any given time and allow concurrent read-only access to the Segment,\n+     * regardless of whether there is another non-read-only SegmentHandle that modifies the segment at that time.\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened in read-only mode.\n+     * @return A CompletableFuture that, when completed, will contain a read-only SegmentHandle that can be used to\n+     * access the segment for non-modify activities (ex: read, get). If the operation failed, it will be failed with the\n+     * cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                return retValue;\n+\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the StreamSegment.\n+     *\n+     * @param handle       A SegmentHandle (read-only or read-write) that points to a Segment to read from.\n+     * @param offset       The offset in the StreamSegment to read data from.\n+     * @param buffer       A buffer to use for reading data.\n+     * @param bufferOffset The offset in the buffer to start writing data to.\n+     * @param length       The number of bytes to read.\n+     * @param timeout      Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the number of bytes read. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     * @throws ArrayIndexOutOfBoundsException If bufferOffset or bufferOffset + length are invalid for the buffer.\n+     */\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            Preconditions.checkNotNull(buffer, \"buffer\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+\n+            Exceptions.checkArrayRange(bufferOffset, length, buffer.length, \"bufferOffset\", \"length\");\n+\n+            if (offset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        offset, bufferOffset, length, buffer.length));\n+            }\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                Preconditions.checkArgument(offset < segmentMetadata.getLength(), \"Offset %s is beyond the last offset %s of the segment %s.\",\n+                        offset, segmentMetadata.getLength(), streamSegmentName);\n+\n+                if (offset < segmentMetadata.getStartOffset()) {\n+                    throw new StreamSegmentTruncatedException(streamSegmentName, segmentMetadata.getStartOffset(), offset);\n+                }\n+\n+                if (length == 0) {\n+                    return 0;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata chunkToReadFrom = null;\n+\n+                Preconditions.checkState(null != currentChunkName);\n+\n+                int bytesRemaining = length;\n+                int currentBufferOffset = bufferOffset;\n+                long currentOffset = offset;\n+                int totalBytesRead = 0;\n+\n+                // Find the first chunk that contains the data.\n+                long startOffsetForCurrentChunk = segmentMetadata.getFirstChunkStartOffset();\n+                Timer timer1 = new Timer();\n+                int cntScanned = 0;\n+                // Find the name of the chunk in the cached read index that is floor to required offset.\n+                val floorEntry = readIndexCache.findFloor(streamSegmentName, offset);\n+                if (null != floorEntry) {\n+                    startOffsetForCurrentChunk = floorEntry.getOffset();\n+                    currentChunkName = floorEntry.getChunkName();\n+                }\n+\n+                // Navigate to the chunk that contains the first byte of requested data.\n+                while (currentChunkName != null) {\n+                    chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != chunkToReadFrom, \"chunkToReadFrom is null\");\n+                    if (startOffsetForCurrentChunk <= currentOffset\n+                            && startOffsetForCurrentChunk + chunkToReadFrom.getLength() > currentOffset) {\n+                        // we have found a chunk that contains first byte we want to read\n+                        log.debug(\"{} read - found chunk to read - segment={}, chunk={}, startOffset={}, length={}, readOffset={}.\",\n+                                logPrefix, streamSegmentName, chunkToReadFrom, startOffsetForCurrentChunk, chunkToReadFrom.getLength(), currentOffset);\n+                        break;\n+                    }\n+                    currentChunkName = chunkToReadFrom.getNextChunk();\n+                    startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+\n+                    // Update read index with newly visited chunk.\n+                    if (null != currentChunkName) {\n+                        readIndexCache.addIndexEntry(streamSegmentName, currentChunkName, startOffsetForCurrentChunk);\n+                    }\n+                    cntScanned++;\n+                }\n+                log.debug(\"{} read - chunk lookup - segment={}, offset={}, scanned={}, latency={}.\",\n+                        logPrefix, handle.getSegmentName(), offset, cntScanned, timer1.getElapsed().toMillis());\n+\n+                // Now read.\n+                while (bytesRemaining > 0 && null != currentChunkName) {\n+                    int bytesToRead = Math.min(bytesRemaining, Math.toIntExact(chunkToReadFrom.getLength() - (currentOffset - startOffsetForCurrentChunk)));\n+                    //assert bytesToRead != 0;\n+\n+                    if (currentOffset >= startOffsetForCurrentChunk + chunkToReadFrom.getLength()) {\n+                        // The current chunk is over. Move to the next one.\n+                        currentChunkName = chunkToReadFrom.getNextChunk();\n+                        if (null != currentChunkName) {\n+                            startOffsetForCurrentChunk += chunkToReadFrom.getLength();\n+                            chunkToReadFrom = (ChunkMetadata) txn.get(currentChunkName);\n+                            log.debug(\"{} read - reading from next chunk - segment={}, chunk={}\", logPrefix, streamSegmentName, chunkToReadFrom);\n+                        }\n+                    } else {\n+                        Preconditions.checkState(bytesToRead != 0, \"bytesToRead is 0\");\n+                        // Read data from the chunk.\n+                        ChunkHandle chunkHandle = chunkStorage.openRead(chunkToReadFrom.getName());\n+                        int bytesRead = chunkStorage.read(chunkHandle, currentOffset - startOffsetForCurrentChunk, bytesToRead, buffer, currentBufferOffset);\n+\n+                        bytesRemaining -= bytesRead;\n+                        currentOffset += bytesRead;\n+                        currentBufferOffset += bytesRead;\n+                        totalBytesRead += bytesRead;\n+                    }\n+                }\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} read - segment={}, offset={}, bytesRead={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+                return totalBytesRead;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets current information about a StreamSegment.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested about the StreamSegment.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null == segmentMetadata) {\n+                    throw new StreamSegmentNotExistsException(streamSegmentName);\n+                }\n+                segmentMetadata.checkInvariants();\n+\n+                val retValue = StreamSegmentInformation.builder()\n+                        .name(streamSegmentName)\n+                        .sealed(segmentMetadata.isSealed())\n+                        .length(segmentMetadata.getLength())\n+                        .startOffset(segmentMetadata.getStartOffset())\n+                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                        .build();\n+                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                return retValue;\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Determines whether the given StreamSegment exists or not.\n+     *\n+     * @param streamSegmentName The name of the StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain the information requested. If the operation failed,\n+     * it will contain the cause of the failure.\n+     */\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwNDA5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442404095", "bodyText": "We have removed the term \"Native Concat\", please rename accordingly.", "author": "fpj", "createdAt": "2020-06-18T17:56:14Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManagerConfig.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.NonNull;\n+\n+/**\n+ * Configuration for {@link ChunkStorageManager}.\n+ */\n+@AllArgsConstructor\n+@Builder(toBuilder = true)\n+public class ChunkStorageManagerConfig {\n+    /**\n+     * Default configuration for {@link ChunkStorageManager}.\n+     */\n+    public static final ChunkStorageManagerConfig DEFAULT_CONFIG = ChunkStorageManagerConfig.builder()\n+            .minSizeLimitForNativeConcat(0)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMTc5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442531798", "bodyText": "Thanks. Good catch.", "author": "sachin-j-joshi", "createdAt": "2020-06-18T22:16:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwNDA5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzI0ODYxOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443248619", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-21T19:17:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQwNDA5NQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MjQyMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442842423", "bodyText": "Attention to the line breaks, please.", "author": "fpj", "createdAt": "2020-06-19T13:32:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.</li>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MjczMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442842730", "bodyText": "... on a per record basis...", "author": "fpj", "createdAt": "2020-06-19T13:32:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * Within a transaction you can perform following actions on per record basis.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzM3Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442843373", "bodyText": "When does the automatic abort happen, is that based on time?", "author": "fpj", "createdAt": "2020-06-19T13:34:03Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4ODU5Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442988597", "bodyText": "MetadataTransaction object just accumulates the changes and on calling commit it writes to underlying KV store.\nIf commit is not called the data is never written anywhere and simply discarded/thrown away. The java GC will take care of the rest.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T18:29:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzM3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5NDM2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442994364", "bodyText": "MetadataTransaction - keeps track of all KV pairs read by the transaction and keeps in memory copy of all the data. This allows isolation. Until transaction is committed , nothing is write to underlying store.\nBaseMetadataStore - all reads and write to KV store go through this layer. So it always has complete view of all things read/modified/committed etc.\nIt actually accepts the transaction object and then writes it to underlying KV store. Every commit results in write to the store. This is critical - data is not written to underlying store until commit is called - until then all data is in memory only.\nBaseMetadataStore also caches most recently committed records so that for frequent reads it doesn't have to read them again from KV store. There is bit of logic to make sure when we commit new records we also update the cached records. This layer also validates the commit to make sure it is not based on invalid versions  etc.\nThere are two optimizations\n\n\nlazy commit is when we update the record and use this updated copy in subsequent transactions but don't actually write to underlying KV store. Only valid use of this is when the length of the chunk is updated. Here we avoid making frequent writes to KV store, this critically depends on the fact that when we open a segment we check the actual length on LTS and update the in memory copy.\n\n\nSecond optimization is pinned records - which are never written to underlying KV store. These are only applicable for storage system segments. There is a separate SystemJournal to recreate the state of these segments.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T18:44:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg0MzM3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg1MTE0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442851148", "bodyText": "What does it mean to write laziliy with respect to the durability guarantee of the transaction?", "author": "fpj", "createdAt": "2020-06-19T13:48:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * </div>\n+ *\n+ * <div>\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ * </div>\n+ */\n+@NotThreadSafe\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    private final ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    @Getter\n+    private boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    @Getter\n+    private boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    private final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;\n+\n+    /**\n+     * Optional external commit operation that is executed during the commit.\n+     * The transaction commit operation fails if this operation fails.\n+     */\n+    @Getter\n+    @Setter\n+    private Callable<Void> externalCommitStep;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param store   Underlying metadata store.\n+     * @param version Version number of the transactions.\n+     */\n+    public MetadataTransaction(ChunkMetadataStore store, long version) {\n+        this.store = Preconditions.checkNotNull(store, \"store\");\n+        this.version = version;\n+        data = new ConcurrentHashMap<String, BaseMetadataStore.TransactionData>();\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public StorageMetadata get(String key) throws StorageMetadataException {\n+        return store.get(this, key);\n+    }\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void update(StorageMetadata metadata) throws StorageMetadataException {\n+        store.update(this, metadata);\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void create(StorageMetadata metadata) throws StorageMetadataException {\n+        store.create(this, metadata);\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void markPinned(StorageMetadata metadata) throws StorageMetadataException {\n+        store.markPinned(this, metadata);\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void delete(String key) throws StorageMetadataException {\n+        store.delete(this, key);\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @throws Exception If transaction could not be commited.\n+     */\n+    public void commit() throws Exception {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws Exception If transaction could not be commited.\n+     */\n+    public void commit(boolean lazyWrite) throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzNjU4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443036587", "bodyText": "In case of crash,  the changes are  lost. However this option should be used only when data can be recreated independently during recovery.\nMore specifically - the length of the chunk is updated lazily and we don't write updated\nrecord to the store every time we update chunk length. However in case of fail over this field is updated by looking at the actual length of the chunk on the LTS.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T20:43:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg1MTE0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg1MjYwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442852601", "bodyText": "It is not good that we throw a generic exception here. I expect the commit to fail in the case it can't satisfy snapshot isolation as explained above, and as such, I'd expect an exception to be thrown indication that the commit failed. Otherwise, what are the exceptions that I should expect here? Are there exceptions indicating both recoverable and unrecoverable scenarios?", "author": "fpj", "createdAt": "2020-06-19T13:51:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * </div>\n+ *\n+ * <div>\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ * </div>\n+ */\n+@NotThreadSafe\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    private final ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    @Getter\n+    private boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    @Getter\n+    private boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    private final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;\n+\n+    /**\n+     * Optional external commit operation that is executed during the commit.\n+     * The transaction commit operation fails if this operation fails.\n+     */\n+    @Getter\n+    @Setter\n+    private Callable<Void> externalCommitStep;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param store   Underlying metadata store.\n+     * @param version Version number of the transactions.\n+     */\n+    public MetadataTransaction(ChunkMetadataStore store, long version) {\n+        this.store = Preconditions.checkNotNull(store, \"store\");\n+        this.version = version;\n+        data = new ConcurrentHashMap<String, BaseMetadataStore.TransactionData>();\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public StorageMetadata get(String key) throws StorageMetadataException {\n+        return store.get(this, key);\n+    }\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void update(StorageMetadata metadata) throws StorageMetadataException {\n+        store.update(this, metadata);\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void create(StorageMetadata metadata) throws StorageMetadataException {\n+        store.create(this, metadata);\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void markPinned(StorageMetadata metadata) throws StorageMetadataException {\n+        store.markPinned(this, metadata);\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void delete(String key) throws StorageMetadataException {\n+        store.delete(this, key);\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @throws Exception If transaction could not be commited.\n+     */\n+    public void commit() throws Exception {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzk3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443937971", "bodyText": "Fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-23T03:16:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg1MjYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg1MjkwMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442852902", "bodyText": "Typo in snapshot.", "author": "fpj", "createdAt": "2020-06-19T13:51:38Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,222 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.</li>", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkyOTYyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442929624", "bodyText": "I'd not expect a manager abstraction to have critical path operations like reads and writes. Either we are conflating operations that do not belong in the same role or this class should not be called a manager. Why can't we call it simply ChunkStorage?", "author": "fpj", "createdAt": "2020-06-19T16:14:26Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1435 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon {@link MetadataTransaction#commit()}\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    /**\n+     * Initializes this instance with the given ContainerEpoch.\n+     *\n+     * @param containerEpoch The Container Epoch to initialize with.\n+     */\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    /**\n+     * Attempts to open the given Segment in read-write mode and make it available for use for this instance of the Storage\n+     * adapter.\n+     * A single active read-write SegmentHandle can exist at any given time for a particular Segment, regardless of owner,\n+     * while a read-write SegmentHandle can coexist with any number of read-only SegmentHandles for that Segment (obtained\n+     * by calling openRead()).\n+     * This can be accomplished in a number of different ways based on the actual implementation of the Storage\n+     * interface, but it can be compared to acquiring an exclusive lock on the given segment).\n+     *\n+     * @param streamSegmentName Name of the StreamSegment to be opened.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * the segment for read and write activities (ex: read, get, write, seal, concat).\n+     * If the segment is sealed, then a Read-Only handle is returned.\n+     * <p>\n+     * If the operation failed, it will be failed with the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    /**\n+     * Creates a new StreamSegment in this Storage Layer with the given Rolling Policy.\n+     *\n+     * @param streamSegmentName The full name of the StreamSegment.\n+     * @param rollingPolicy     The Rolling Policy to apply to this StreamSegment.\n+     * @param timeout           Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will contain a read-write SegmentHandle that can be used to access\n+     * * the segment for read and write activities (ex: read, get, write, seal, concat). If the operation failed, it will contain the cause of the\n+     * failure. Notable exceptions:\n+     * <ul>\n+     * <li> StreamSegmentExistsException: When the given Segment already exists in Storage.\n+     * </ul>\n+     */\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Writes the given data to the StreamSegment.\n+     *\n+     * @param handle  A read-write SegmentHandle that points to a Segment to write to.\n+     * @param offset  The offset in the StreamSegment to write data at.\n+     * @param data    An InputStream representing the data to write.\n+     * @param length  The length of the InputStream.\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate the operation succeeded. If the operation failed,\n+     * it will contain the cause of the failure. Notable exceptions:\n+     * <ul>\n+     * <li> BadOffsetException: When the given offset does not match the actual length of the segment in storage.\n+     * <li> StreamSegmentNotExistsException: When the given Segment does not exist in Storage.\n+     * <li> StorageNotPrimaryException: When this Storage instance is no longer primary for this Segment (it was fenced out).\n+     * </ul>\n+     * @throws IllegalArgumentException If handle is read-only.\n+     */\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk1NDU3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442954570", "bodyText": "ChunkStorageProvider - provides the actual operations on chunks.\nChunkStorageManager - manages the chunks and creates the abstraction of segment out of these chunks. ChunkManager doesn't actually write or read anything. It manages the metadata for segment layout and it maps and forwards calls to ChunkStorageProvider.\nName ChunkStorage adds ambiguity - does it mean it actually reads/write and then why we have this another thing called ChunkStorageProvider ?\nThe current name removes that ambiguity.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T17:08:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkyOTYyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTYwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442931605", "bodyText": "This class does not sound like a provider, the way I understand is a class related to a factory or factories, while I'm not seeing this at all. Why are we calling it a provider?", "author": "fpj", "createdAt": "2020-06-19T16:19:00Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorageProvider#create(String)}  and {@link ChunkStorageProvider#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorageProvider#exists(String)} and {@link ChunkStorageProvider#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorageProvider}  support appending to existing chunks? For vanilla S3 compatible this would return false. This is indicated by {@link ChunkStorageProvider#supportsAppend()}. </li>\n+ * <li> Does {@link ChunkStorageProvider}  support for concatenating chunks natively? This is indicated by {@link ChunkStorageProvider#supportsConcat()}.\n+ * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+ * <li>In addition {@link ChunkStorageProvider} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorageProvider#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorageProvider supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorageProvider} supports both native and append, ChunkStorageManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ */\n+public interface ChunkStorageProvider extends AutoCloseable {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzMzQ5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443033498", "bodyText": "The usage is somewhat similar to \"FileSystemProvider\" being the concrete implementation that \"provides\" the functionality.\nWe don't have methods that return Input/output streams but instead have read and write methods.\nhttps://docs.oracle.com/javase/8/docs/technotes/guides/io/fsp/filesystemprovider.html\nhttps://docs.oracle.com/javase/8/docs/api/java/nio/file/spi/FileSystemProvider.html#newInputStream-java.nio.file.Path-java.nio.file.OpenOption...-", "author": "sachin-j-joshi", "createdAt": "2020-06-19T20:35:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTYwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAzNTE0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443035147", "bodyText": "Just calling it ChunkStorage is little bit ambiguous.\nInstead I have two long names.\nChunkStorageManager - manages chunks.\nChunkStorageProvider - provides chunks.\nChunkManager/ChunkStorage might be an alternative naming scheme.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T20:39:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzMTYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzNTg3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r442935876", "bodyText": "Do we really need to make deep copies of StorageMetadata objects and how often do you expect to create them?", "author": "fpj", "createdAt": "2020-06-19T16:26:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,86 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.segmentstore.storage.mocks.MockStorageMetadata;\n+import lombok.Data;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ * All storage related metadata is stored in {@link ChunkMetadataStore} using set of key-value pairs.\n+ * The String value returned by {@link StorageMetadata#getKey()} is used as key.\n+ * Notable derived classes are {@link SegmentMetadata} and {@link ChunkMetadata} which form the core of metadata related\n+ * to {@link io.pravega.segmentstore.storage.chunklayer.ChunkStorageManager} functionality.\n+ */\n+@Data\n+public abstract class StorageMetadata implements Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     *\n+     * @return key.\n+     */\n+    public abstract String getKey();\n+\n+    /**\n+     * Creates a deep copy of this instance.\n+     *\n+     * @return\n+     */\n+    public abstract StorageMetadata deepCopy();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzAyOTQzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443029431", "bodyText": "Because java is reference based language, two transactions can end up pointing to the same record and overwrite each others changes. Therefore for complete isolation we do need  completely separate copies.\nYou are right - currently for both SegmentMetadata and ChunkMetadata don't have any embedded objects - they are flat objects with few fields of primitive data types (long/string/int).\nBut I think the name deepCopy captures the intention. The alternative are - copy() or clone(). Although clone is perhaps close enough to intended meaning .\nDeep copy is made whenever with in any transaction a record is read from the underlying store or buffer for the first time.", "author": "sachin-j-joshi", "createdAt": "2020-06-19T20:24:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjkzNTg3Ng=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYwNDQzNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443604436", "bodyText": "Do not reformat the license header.", "author": "fpj", "createdAt": "2020-06-22T14:34:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -5,7 +5,7 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ * http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYwNDc5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443604799", "bodyText": "Do not reformat license header.", "author": "fpj", "createdAt": "2020-06-22T14:34:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorageProvider.java", "diffHunk": "@@ -0,0 +1,531 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk5MTMxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444991314", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:43:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYwNDc5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYwOTA5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443609093", "bodyText": "Do not format the license header.", "author": "fpj", "createdAt": "2020-06-22T14:40:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYyMjc0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443622748", "bodyText": "We are using \"native concat\" again. :-)", "author": "fpj", "createdAt": "2020-06-22T14:58:33Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        // Clear flag for OwnershipChanged once first chunk after ownership change is written.\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a segment to used to storage system metadata.\n+     *\n+     * @param segmentMetadata Metadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist of eligible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * We want to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are\n+     * typically only metadata operations, reducing the overall cost of the merging them together.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and\n+     * append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support\n+     * either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather\n+     * efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat.\n+     * (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable.\n+     * Conversely there is a size below which native concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYyMzMwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443623303", "bodyText": "... and here.", "author": "fpj", "createdAt": "2020-06-22T14:59:20Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        // Clear flag for OwnershipChanged once first chunk after ownership change is written.\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a segment to used to storage system metadata.\n+     *\n+     * @param segmentMetadata Metadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist of eligible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually\n+     * want to exploit that. Especially when this operation is supposed to be \"metadata only operation\" even for them.\n+     * We want to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are\n+     * typically only metadata operations, reducing the overall cost of the merging them together.\n+     * HDFS also has native concat (I think metadata only). NFS has no concept of native concat.\n+     * As chunks become larger, it no longer makes sense to concat them using append writes (read source completely and\n+     * append -ie. write- it back at the end of target.)\n+     * We do not always use native concat to implement concat. We also use appends.\n+     * </li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke native concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorageProvider needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorageProvider support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorageProvider support for concatenating chunks natively? This is indicated by supportsConcat.\n+     * If this is true then native concat operation concat will be invoked otherwise concatWithAppend is invoked.</li>\n+     * <li>There are some obvious constraints - For ChunkStorageProvider support any concat functionality it must support\n+     * either append or native concat.</li>\n+     * <li>Also when ChunkStorageProvider supports both native and append, ChunkStorageManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when native concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, concatWithAppend is used instead of using concat. (For really small txn it is rather\n+     * efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat.\n+     * (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable.\n+     * Conversely there is a size below which native concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws Exception In case of any errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws Exception {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using native concat if available.\n+        // To implement it using single loop we toggle between concat with append and native concat modes. (Instead of two passes.)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYyNjEzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443626139", "bodyText": "A suggested rephrase for this paragraph:\nIn the absence of defragmentation, the number of chunks for individual\nsegments keeps on increasing. When we have too many small chunks\n(say because many transactions with little data on some segments),\nthe segment is fragmented - this may impact both the read throughput\nand the performance of the metadata store. This problem is further\nintensified when we have stores that do not support append semantics\n(e.g., stock S3) and each write becomes a separate chunk.", "author": "fpj", "createdAt": "2020-06-22T15:03:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        // Clear flag for OwnershipChanged once first chunk after ownership change is written.\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a segment to used to storage system metadata.\n+     *\n+     * @param segmentMetadata Metadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist of eligible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzYzNDQ1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443634459", "bodyText": "Suggested rephrase of the paragraph:\nIf the underlying storage provides some facility to stitch together smaller chunks\ninto larger chunks, then we do actually want to exploit that, specially when the\nunderlying implementation is only a metadata operation. We want to leverage\nmulti-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS)\nas they are typically only metadata operations, reducing the overall cost of the\nmerging them together. HDFS also supports merges, whereas NFS has no concept\nof merging natively.\n\nAs chunks become larger, append writes (read source completely and append\n-i.e., write- it back at the end of target) become inefficient. Consequently, a native\noption for merging is desirable. We use such native merge capability when available,\nand if not available, then we use appends.\n\nI tried to rephrase in a way that makes sense to me as the original text is confusing. Please review and correct accordingly.\nAlso, please update the javadocs in BaseChunkStorageProvider for supportAppend and supportConcat to elaborate on what they mean to you. We have talked about them offline and there is a good amount of context that is not said there.", "author": "fpj", "createdAt": "2020-06-22T15:15:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManager.java", "diffHunk": "@@ -0,0 +1,1259 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorageProvider} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkStorageManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkStorageManager instance.\n+     */\n+    @Getter\n+    private final ChunkStorageManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorageProvider} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorageProvider chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkStorageManager#initialize(int, ChunkMetadataStore, SystemJournal)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage ChunkStorageProvider instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkStorageManager class.\n+     *\n+     * @param chunkStorage  ChunkStorageProvider instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkStorageManager instance.\n+     */\n+    public ChunkStorageManager(ChunkStorageProvider chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkStorageManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkStorageManager.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @param systemJournal SystemJournal that keeps track of changes to system segments and helps with bootstrap.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void initialize(int containerId, ChunkMetadataStore metadataStore, SystemJournal systemJournal) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkStorageManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = Preconditions.checkNotNull(systemJournal, \"systemJournal\");\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws Exception In case of any errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws Exception {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.checkInvariants();\n+\n+                if (segmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(streamSegmentName);\n+                }\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                // Write data  to the last segment.\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                ChunkMetadata chunkWrittenMetadata = null;\n+\n+                while (bytesRemaining > 0) {\n+                    // Validate that offset is correct.\n+                    if ((segmentMetadata.getLength()) != currentOffset) {\n+                        throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), currentOffset);\n+                    }\n+\n+                    // Get the last chunk segmentMetadata for the segment.\n+                    if (null != segmentMetadata.getLastChunk()) {\n+                        lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                    }\n+\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+\n+                        chunkWrittenMetadata = ChunkMetadata.builder()\n+                                .name(newChunkName)\n+                                .build();\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        // Record the creation of new chunk.\n+                        chunksAddedCount++;\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    lastChunkMetadata == null ? null : lastChunkMetadata.getName(),\n+                                    newChunkName);\n+                            txn.markPinned(chunkWrittenMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        // update first and last chunks.\n+                        segmentMetadata.setLastChunk(newChunkName);\n+                        if (lastChunkMetadata == null) {\n+                            segmentMetadata.setFirstChunk(newChunkName);\n+                        } else {\n+                            lastChunkMetadata.setNextChunk(newChunkName);\n+                        }\n+                        segmentMetadata.incrementChunkCount();\n+\n+                        // Update the transaction.\n+                        txn.update(chunkWrittenMetadata);\n+                        if (lastChunkMetadata != null) {\n+                            txn.update(lastChunkMetadata);\n+                        }\n+                        txn.update(segmentMetadata);\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+                        // Clear flag for OwnershipChanged once first chunk after ownership change is written.\n+                        if (isFirstWriteAfterFailover) {\n+                            segmentMetadata.setOwnerEpoch(this.epoch);\n+                            isFirstWriteAfterFailover = false;\n+                            segmentMetadata.setOwnershipChanged(false);\n+                            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, streamSegmentName);\n+                        }\n+                        didSegmentLayoutChange = true;\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\", logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkWrittenMetadata = lastChunkMetadata;\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int bytesToWrite = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+                    Preconditions.checkState(0 != bytesToWrite, \"Attempt to write zero bytes\");\n+\n+                    try {\n+                        int bytesWritten;\n+                        // Finally write the data.\n+                        try (BoundedInputStream bis = new BoundedInputStream(data, bytesToWrite)) {\n+                            bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesToWrite, bis);\n+                        }\n+\n+                        // Update the counts\n+                        bytesRemaining -= bytesWritten;\n+                        currentOffset += bytesWritten;\n+\n+                        // Update the metadata for segment and chunk.\n+                        Preconditions.checkState(bytesWritten >= 0);\n+                        segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+                        chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+                        txn.update(chunkWrittenMetadata);\n+                        txn.update(segmentMetadata);\n+                    } catch (IndexOutOfBoundsException e) {\n+                        throw new BadOffsetException(streamSegmentName, chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+                    }\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Gets whether given segment is a segment to used to storage system metadata.\n+     *\n+     * @param segmentMetadata Metadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(streamSegmentName);\n+                }\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                // Validate preconditions.\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                if (targetSegmentMetadata.isSealed()) {\n+                    throw new StreamSegmentSealedException(targetSegmentName);\n+                }\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                if (!sourceSegmentMetadata.isSealed()) {\n+                    throw new IllegalStateException();\n+                }\n+\n+                if (targetSegmentMetadata.getOwnerEpoch() > this.epoch) {\n+                    throw new StorageNotPrimaryException(targetSegmentMetadata.getName());\n+                }\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist of eligible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In general without defrag the number of chunks in the system just keeps on increasing.\n+     * In addition when we have too many small chunks (say because too many small transactions), the segment is fragmented -\n+     * this may impact the read throughput but also performance of metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (eg. vanilla S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If underlying storage provides some facility to stitch together smaller chunks into larger chunks then we do actually", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3NjExNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443676116", "bodyText": "Why only 1024 by default?", "author": "fpj", "createdAt": "2020-06-22T16:17:36Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageManagerConfig.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import lombok.AllArgsConstructor;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.NonNull;\n+\n+/**\n+ * Configuration for {@link ChunkStorageManager}.\n+ */\n+@AllArgsConstructor\n+@Builder(toBuilder = true)\n+public class ChunkStorageManagerConfig {\n+    /**\n+     * Default configuration for {@link ChunkStorageManager}.\n+     */\n+    public static final ChunkStorageManagerConfig DEFAULT_CONFIG = ChunkStorageManagerConfig.builder()\n+            .minSizeLimitForConcat(0)\n+            .maxSizeLimitForConcat(Long.MAX_VALUE)\n+            .defaultRollingPolicy(SegmentRollingPolicy.NO_ROLLING)\n+            .maxBufferSizeForChunkDataTransfer(1024 * 1024)\n+            .maxIndexedSegments(1024)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDk5NzI1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444997256", "bodyText": "The number is on a conservative side to avoid memory consumption. We'll measure and update this to more aggressively eventually.", "author": "sachin-j-joshi", "createdAt": "2020-06-24T15:51:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3NjExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3OTE2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443679166", "bodyText": "Why do we need to increment the generation on eviction? Is it to implement an LRU-like cache?", "author": "fpj", "createdAt": "2020-06-22T16:22:26Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzczNDY0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443734641", "bodyText": "yes.", "author": "sachin-j-joshi", "createdAt": "2020-06-22T18:01:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY3OTE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzY4MDkwNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443680907", "bodyText": "Typo: given a given", "author": "fpj", "createdAt": "2020-06-22T16:25:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for given a given chunk in a given segment.", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443709456", "bodyText": "What's the cost of iterating through all entries upon eviction?", "author": "fpj", "createdAt": "2020-06-22T17:14:59Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for given a given chunk in a given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(currentGeneration.get())\n+                            .build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(currentGeneration.get()).build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+                floorEntry.getValue().setGeneration(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().getGeneration() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzczNTUxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443735511", "bodyText": "This loop needs improvement. Let me think about this.", "author": "sachin-j-joshi", "createdAt": "2020-06-22T18:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc0NjE0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443746140", "bodyText": "Although chunks are evicted on per segment basis.", "author": "sachin-j-joshi", "createdAt": "2020-06-22T18:23:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNDEwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444514108", "bodyText": "I'll re-implement the readIndex in next iteration.\nCurrently the operation is O(number of chunks in segment that are cached).\nThis is bounded by the max number of  cached index entries per segment 1024 .\nSo far in my testing I have seen very low number of actual chunks per segments for NFS.\nImmutable storage like S3 will have higher number of chunks per segment and this index needs to improve before that.   I'll create a issue to track this.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T21:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyNzY3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447027671", "bodyText": "I'm unresolving this thread as it doesn't look like it has a resolution. I mentioned this thread in the first comment of this file.", "author": "fpj", "createdAt": "2020-06-29T14:46:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1Njg2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447056868", "bodyText": "I was planning to re-implement this cache in a separate PR after this PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:26:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg2NzI3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448867279", "bodyText": "Can you create an issue and post the link here, please?", "author": "fpj", "createdAt": "2020-07-02T09:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAxMTA1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449011056", "bodyText": "#4902", "author": "sachin-j-joshi", "createdAt": "2020-07-02T13:42:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzcwOTQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4ODM3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443988376", "bodyText": "If there is an error in store.abort, is the transaction to be considered aborted ?", "author": "eolivelli", "createdAt": "2020-06-23T06:25:16Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload\n+ * of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata\n+ * records read during the transactions are changed outside the transaction after they were read.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * Within a transaction you can perform following actions on a per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * </div>\n+ *\n+ * <div>\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover\n+ * from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data\n+ * is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store.\n+ * They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to\n+ * underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip\n+ * validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ * </div>\n+ */\n+@NotThreadSafe\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    private final ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    @Getter\n+    private boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    @Getter\n+    private boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    private final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;\n+\n+    /**\n+     * Optional external commit operation that is executed during the commit.\n+     * The transaction commit operation fails if this operation fails.\n+     */\n+    @Getter\n+    @Setter\n+    private Callable<Void> externalCommitStep;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param store   Underlying metadata store.\n+     * @param version Version number of the transactions.\n+     */\n+    public MetadataTransaction(ChunkMetadataStore store, long version) {\n+        this.store = Preconditions.checkNotNull(store, \"store\");\n+        this.version = version;\n+        data = new ConcurrentHashMap<String, BaseMetadataStore.TransactionData>();\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public StorageMetadata get(String key) throws StorageMetadataException {\n+        return store.get(this, key);\n+    }\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void update(StorageMetadata metadata) throws StorageMetadataException {\n+        store.update(this, metadata);\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void create(StorageMetadata metadata) throws StorageMetadataException {\n+        store.create(this, metadata);\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void markPinned(StorageMetadata metadata) throws StorageMetadataException {\n+        store.markPinned(this, metadata);\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void delete(String key) throws StorageMetadataException {\n+        store.delete(this, key);\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit() throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit(boolean lazyWrite) throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this, lazyWrite);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @param lazyWrite      true if data can be written lazily.\n+     * @param skipStoreCheck true if data is not to be reloaded from store.\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit(boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this, lazyWrite, skipStoreCheck);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Aborts the transaction.\n+     *\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void abort() throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        isAborted = true;\n+        store.abort(this);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNDM1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444324356", "bodyText": "Data is written to underlying store only when transaction is committed. Until then the underlying store or cache has no knowledge of changes in your transaction. So if you fail to commit, it is an implicit abort. In fact the abort implementation in BaseMetadataStore is empty. There is no action required. TableBasedMetadataStore or InMemoryMetadataStore do not override it. If some derived class overrides it then it may throw an error. But transaction is already marked isAborted = true;", "author": "sachin-j-joshi", "createdAt": "2020-06-23T15:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4ODM3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyODEwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444328101", "bodyText": "So transactions are only in memory until you commit them. and if you reboot the service you will lose your transaction data. is that correct ?", "author": "eolivelli", "createdAt": "2020-06-23T15:50:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4ODM3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMzNTIzMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444335231", "bodyText": "Yes. That is correct.\nThese are short lived transactions. ChunkManager should either succeed in committing or in case of failed/rebooted service leave no side effect. Therefore the next instance of the service will not have to deal with any partial changes to metadata.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T16:00:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4ODM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTE1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443989158", "bodyText": "shall we log that we are automatically trying to abort the transaction ?\nwhat happens in case of an error ?\nwhat happens if you are calling this method twice ? will the store throw an error ?", "author": "eolivelli", "createdAt": "2020-06-23T06:27:19Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MetadataTransaction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+\n+import lombok.Getter;\n+import lombok.Setter;\n+\n+import javax.annotation.concurrent.NotThreadSafe;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * Default implementation of the storage metadata transaction.\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ * This implementation delegates all calls to underlying {@link ChunkMetadataStore}.\n+ * <div>\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ * <ul>\n+ * <li>Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload\n+ * of{@link MetadataTransaction#commit()}.</li>\n+ * <li>Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.</li>\n+ * <li>Transactions are atomic - either all changes in the transaction are committed or none at all.</li>\n+ * <li>In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata\n+ * records read during the transactions are changed outside the transaction after they were read.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * Within a transaction you can perform following actions on a per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * </div>\n+ *\n+ * <div>\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover\n+ * from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data\n+ * is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store.\n+ * They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to\n+ * underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip\n+ * validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ * </div>\n+ */\n+@NotThreadSafe\n+public class MetadataTransaction implements AutoCloseable {\n+    /**\n+     * {@link ChunkMetadataStore} that stores the actual metadata.\n+     */\n+    private final ChunkMetadataStore store;\n+\n+    /**\n+     * Indicates whether the transaction is commited or not.\n+     */\n+    @Getter\n+    private boolean isCommitted = false;\n+\n+    /**\n+     * Indicates whether the transaction is aborted or not.\n+     */\n+    @Getter\n+    private boolean isAborted = false;\n+\n+    /**\n+     * The version of the transaction.\n+     */\n+    @Getter\n+    private final long version;\n+\n+    /**\n+     * Local data in the transaction.\n+     */\n+    @Getter\n+    private final ConcurrentHashMap<String, BaseMetadataStore.TransactionData> data;\n+\n+    /**\n+     * Optional external commit operation that is executed during the commit.\n+     * The transaction commit operation fails if this operation fails.\n+     */\n+    @Getter\n+    @Setter\n+    private Callable<Void> externalCommitStep;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param store   Underlying metadata store.\n+     * @param version Version number of the transactions.\n+     */\n+    public MetadataTransaction(ChunkMetadataStore store, long version) {\n+        this.store = Preconditions.checkNotNull(store, \"store\");\n+        this.version = version;\n+        data = new ConcurrentHashMap<String, BaseMetadataStore.TransactionData>();\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public StorageMetadata get(String key) throws StorageMetadataException {\n+        return store.get(this, key);\n+    }\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void update(StorageMetadata metadata) throws StorageMetadataException {\n+        store.update(this, metadata);\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void create(StorageMetadata metadata) throws StorageMetadataException {\n+        store.create(this, metadata);\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void markPinned(StorageMetadata metadata) throws StorageMetadataException {\n+        store.markPinned(this, metadata);\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    public void delete(String key) throws StorageMetadataException {\n+        store.delete(this, key);\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit() throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit(boolean lazyWrite) throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this, lazyWrite);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Commits the transaction.\n+     *\n+     * @param lazyWrite      true if data can be written lazily.\n+     * @param skipStoreCheck true if data is not to be reloaded from store.\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void commit(boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        store.commit(this, lazyWrite, skipStoreCheck);\n+        isCommitted = true;\n+    }\n+\n+    /**\n+     * Aborts the transaction.\n+     *\n+     * @throws StorageMetadataException If transaction could not be commited.\n+     */\n+    public void abort() throws StorageMetadataException {\n+        Preconditions.checkState(!isCommitted, \"Transaction is already committed\");\n+        Preconditions.checkState(!isAborted, \"Transaction is already aborted\");\n+        isAborted = true;\n+        store.abort(this);\n+    }\n+\n+    /**\n+     * {@link AutoCloseable#close()} implementation.\n+     *\n+     * @throws Exception In case of any errors.\n+     */\n+    public void close() throws Exception {\n+        if (!isCommitted || isAborted) {\n+            store.abort(this);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMyNjA0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444326047", "bodyText": "The abort implementation in BaseMetadataStore is empty. There is no action required. TableBasedMetadataStore or InMemoryMetadataStore do not override it. If some derived class overrides it then it may throw an error. In this case I would expect store.abort(); to be idempotent.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T15:47:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTE1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTcwOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443989709", "bodyText": "Just for curiosity, why are we packing the status in a bitset ? is it for saving resources ?\nis it some form of convention in Pravega project ?", "author": "eolivelli", "createdAt": "2020-06-23T06:28:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/SegmentMetadata.java", "diffHunk": "@@ -0,0 +1,334 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents segment metadata.\n+ Following metadata is stored.\n+ * <ul>\n+ *     <li>Name of the segment.</li>\n+ *     <li>Length of the segment.</li>\n+ *     <li>Epoch of the container that last owned it.</li>\n+ *     <li>Start offset of the segment. This is offset of the first byte available for read.</li>\n+ *     <li>Status bit flags.</li>\n+ *     <li>Maximum Rolling length of the segment.</li>\n+ *     <li>Name of the first chunk.</li>\n+ *     <li>Name of the last chunk.</li>\n+ *     <li>Offset corresponding to the the first byte of the first chunk.This is NOT the same as start offset of the segment.\n+ *      With arbitrary truncates the effective start offset might be in the middle of the first chunk. Byte at this offset may not be available for read.</li>\n+ *     <li>Offset of the first byte of the last chunk.</li>\n+ * </ul>\n+ */\n+@Data\n+@Builder(toBuilder = true)\n+@EqualsAndHashCode(callSuper = true)\n+public class SegmentMetadata extends StorageMetadata {\n+    /**\n+     * Flag to indicate whether the segment is active or not.\n+     */\n+    private static final int ACTIVE = 0x0001;\n+\n+    /**\n+     * Flag to indicate whether the segment is sealed or not.\n+     */\n+    private static final int SEALED = 0x0002;\n+\n+    /**\n+     * Flag to indicate whether the segment is deleted or not.\n+     */\n+    private static final int DELETED = 0x0004;\n+\n+    /**\n+     * Flag to indicate whether followup actions (like adding new chunks) after ownership changes are needed or not.\n+     */\n+    private static final int OWNERSHIP_CHANGED = 0x0008;\n+\n+    /**\n+     * Flag to indicate whether the segment is storage system segment.\n+     */\n+    private static final int SYSTEM_SEGMENT = 0x0010;\n+\n+    /**\n+     * Name of the segment.\n+     */\n+    private final String name;\n+\n+    /**\n+     * Length of the segment.\n+     */\n+    private long length;\n+\n+    /**\n+     * Number of chunks.\n+     */\n+    private int chunkCount;\n+\n+    /**\n+     * Start offset of the segment. This is offset of the first byte available for read.\n+     */\n+    private long startOffset;\n+\n+    /**\n+     * Status bit flags.\n+     */\n+    private int status;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNzM1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444317356", "bodyText": "Yes - it is to conserve amount of data store and also memory footprint. Each boolean takes space. But putting it in status helps save at least there.\nUnlike C#, unfortunately Java enums are not very flexible or that useful here. (https://docs.microsoft.com/en-us/dotnet/api/system.flagsattribute?view=netcore-3.1)", "author": "sachin-j-joshi", "createdAt": "2020-06-23T15:34:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTcwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxODk4NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444318985", "bodyText": "In fact if I don't have more than 8 flags, I'll change this field to byte.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T15:36:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk4OTcwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDE5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443990196", "bodyText": "Why don't you put these methods into \"pravega common\" module ?", "author": "eolivelli", "createdAt": "2020-06-23T06:30:01Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkManager;\n+import io.pravega.segmentstore.storage.mocks.MockStorageMetadata;\n+import lombok.Data;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ * All storage related metadata is stored in {@link ChunkMetadataStore} using set of key-value pairs.\n+ * The String value returned by {@link StorageMetadata#getKey()} is used as key.\n+ * Notable derived classes are {@link SegmentMetadata} and {@link ChunkMetadata} which form the core of metadata related\n+ * to {@link ChunkManager} functionality.\n+ */\n+@Data\n+public abstract class StorageMetadata implements Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     *\n+     * @return key.\n+     */\n+    public abstract String getKey();\n+\n+    /**\n+     * Creates a deep copy of this instance.\n+     *\n+     * @return\n+     */\n+    public abstract StorageMetadata deepCopy();\n+\n+    /**\n+     * Helper method that converts empty string to null value.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String toNullableString(String toConvert) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MTE3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444551172", "bodyText": "Not sure. Doesn't look very useful outside limited use in this class/module.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T22:52:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDE5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDIzMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443990233", "bodyText": "Why don't you put these methods into \"pravega common\" module ?", "author": "eolivelli", "createdAt": "2020-06-23T06:30:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkManager;\n+import io.pravega.segmentstore.storage.mocks.MockStorageMetadata;\n+import lombok.Data;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ * All storage related metadata is stored in {@link ChunkMetadataStore} using set of key-value pairs.\n+ * The String value returned by {@link StorageMetadata#getKey()} is used as key.\n+ * Notable derived classes are {@link SegmentMetadata} and {@link ChunkMetadata} which form the core of metadata related\n+ * to {@link ChunkManager} functionality.\n+ */\n+@Data\n+public abstract class StorageMetadata implements Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     *\n+     * @return key.\n+     */\n+    public abstract String getKey();\n+\n+    /**\n+     * Creates a deep copy of this instance.\n+     *\n+     * @return\n+     */\n+    public abstract StorageMetadata deepCopy();\n+\n+    /**\n+     * Helper method that converts empty string to null value.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String toNullableString(String toConvert) {\n+        if (toConvert.length() == 0) {\n+            return null;\n+        }\n+        return toConvert;\n+    }\n+\n+    /**\n+     * Helper method that converts null value to empty string.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String fromNullableString(String toConvert) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MDkyMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444550920", "bodyText": "Not sure. Doesn't look very useful outside limited use in this class.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T22:51:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDIzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDcyNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443990726", "bodyText": "This class does not bring much value.\nwhy do we need it ?", "author": "eolivelli", "createdAt": "2020-06-23T06:31:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/AbstractInMemoryChunkStorage.java", "diffHunk": "@@ -0,0 +1,81 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+/**\n+ * Base class for InMemory mock implementations.\n+ * Allows to simulate ChunkStorage with different supported properties.\n+ */\n+@Slf4j\n+abstract public class AbstractInMemoryChunkStorage extends BaseChunkStorage {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDI5MDg5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444290898", "bodyText": "This class is extremely useful for testing.\nIt has shouldSupportAppend and shouldSupportConcat fields which allows me to control behavior. This is used extensively in tests for chunk manager.  For example ChunkManagerTests::testSimpleScenarioWithNonAppendProvider\nI have two concrete implementations InMemoryChunkStorage that stores all data in memory and NoOpChunkStorage which only stores metadata and ignores the actual data.\n **\n * Base class for InMemory mock implementations.\n * Allows to simulate ChunkStorage with different supported properties.\n */\n@Slf4j\nabstract public class AbstractInMemoryChunkStorage extends BaseChunkStorage {\n    /**\n     * value to return when {@link AbstractInMemoryChunkStorage#supportsTruncation()} is called.\n     */\n    @Getter\n    @Setter\n    boolean shouldSupportTruncation = true;\n\n    /**\n     * value to return when {@link AbstractInMemoryChunkStorage#supportsAppend()} is called.\n     */\n    @Getter\n    @Setter\n    boolean shouldSupportAppend = true;\n\n    /**\n     * value to return when {@link AbstractInMemoryChunkStorage#supportsConcat()} is called.\n     */\n    @Getter\n    @Setter\n    boolean shouldSupportConcat = false;\n\n    /**", "author": "sachin-j-joshi", "createdAt": "2020-06-23T14:57:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNDY2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444314667", "bodyText": "okay", "author": "eolivelli", "createdAt": "2020-06-23T15:30:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MDcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MTQ4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443991487", "bodyText": "we could move this block into a method in InMemoryChunkData", "author": "eolivelli", "createdAt": "2020-06-23T06:33:28Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryChunkStorage.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * In-Memory mock for ChunkStorage. Contents is destroyed when object is garbage collected.\n+ */\n+@Slf4j\n+public class InMemoryChunkStorage extends AbstractInMemoryChunkStorage {\n+    private final ConcurrentHashMap<String, InMemoryChunk> chunks = new ConcurrentHashMap<String, InMemoryChunk>();\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        InMemoryChunk chunk = chunks.get(chunkName);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doGetInfo\");\n+        }\n+        return ChunkInfo.builder()\n+                .length(chunk.getLength())\n+                .name(chunkName)\n+                .build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        if (null != chunks.putIfAbsent(chunkName, new InMemoryChunk(chunkName))) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"InMemoryChunkStorage::doCreate\");\n+        }\n+        return new ChunkHandle(chunkName, false);\n+    }\n+\n+    @Override\n+    protected boolean checkExist(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return chunks.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(handle.getChunkName(), \"InMemoryChunkStorage::doDelete\");\n+        }\n+        if (chunk.isReadOnly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunks.remove(handle.getChunkName());\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, true);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenRead\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, false);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenWrite\");\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (fromOffset >= chunk.getLength()) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read at wrong offset\");\n+        }\n+        if (length == 0) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read 0 bytes\");\n+        }\n+\n+        // This is implemented this way to simulate possibility of partial read.\n+        InMemoryChunkData matchingData = null;\n+\n+        // TODO : This is OK for now. This is just test code, but need binary search here.\n+        for (InMemoryChunkData data : chunk.inMemoryChunkDataList) {\n+            if (data.start <= fromOffset && data.start + data.length > fromOffset) {\n+                matchingData = data;\n+                break;\n+\n+            }\n+        }\n+\n+        if (null != matchingData) {\n+            int startIndex = (int) (fromOffset - matchingData.start);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxMjI3Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444312276", "bodyText": "This the only thing that doRead really does and the method is already small enough.", "author": "sachin-j-joshi", "createdAt": "2020-06-23T15:27:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MTQ4Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDMxNTYzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444315632", "bodyText": "I am telling this just to have a cleaner encapsulation for InMemoryChuckData", "author": "eolivelli", "createdAt": "2020-06-23T15:31:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MTQ4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MjE2Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r443992162", "bodyText": "maybe you could move this block into  chunk.append\nDon't we have IO utility methods that read a given amount of bytes from an InputStream ?", "author": "eolivelli", "createdAt": "2020-06-23T06:35:07Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryChunkStorage.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * In-Memory mock for ChunkStorage. Contents is destroyed when object is garbage collected.\n+ */\n+@Slf4j\n+public class InMemoryChunkStorage extends AbstractInMemoryChunkStorage {\n+    private final ConcurrentHashMap<String, InMemoryChunk> chunks = new ConcurrentHashMap<String, InMemoryChunk>();\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        InMemoryChunk chunk = chunks.get(chunkName);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doGetInfo\");\n+        }\n+        return ChunkInfo.builder()\n+                .length(chunk.getLength())\n+                .name(chunkName)\n+                .build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        if (null != chunks.putIfAbsent(chunkName, new InMemoryChunk(chunkName))) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"InMemoryChunkStorage::doCreate\");\n+        }\n+        return new ChunkHandle(chunkName, false);\n+    }\n+\n+    @Override\n+    protected boolean checkExist(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return chunks.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(handle.getChunkName(), \"InMemoryChunkStorage::doDelete\");\n+        }\n+        if (chunk.isReadOnly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunks.remove(handle.getChunkName());\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, true);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenRead\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, false);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenWrite\");\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (fromOffset >= chunk.getLength()) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read at wrong offset\");\n+        }\n+        if (length == 0) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read 0 bytes\");\n+        }\n+\n+        // This is implemented this way to simulate possibility of partial read.\n+        InMemoryChunkData matchingData = null;\n+\n+        // TODO : This is OK for now. This is just test code, but need binary search here.\n+        for (InMemoryChunkData data : chunk.inMemoryChunkDataList) {\n+            if (data.start <= fromOffset && data.start + data.length > fromOffset) {\n+                matchingData = data;\n+                break;\n+\n+            }\n+        }\n+\n+        if (null != matchingData) {\n+            int startIndex = (int) (fromOffset - matchingData.start);\n+            byte[] source = matchingData.getData();\n+            int i;\n+            for (i = 0; i < length && bufferOffset + i < buffer.length && startIndex + i < source.length; i++) {\n+                buffer[bufferOffset + i] = source[startIndex + i];\n+            }\n+            return i;\n+        }\n+        throw new ChunkStorageException(handle.getChunkName(), \"No data was read\");\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws IndexOutOfBoundsException, ChunkStorageException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        long oldLength = chunk.getLength();\n+        if (offset != chunk.getLength()) {\n+            throw new IndexOutOfBoundsException(\"Attempt to write at wrong offset\");\n+        }\n+        if (length == 0) {\n+            throw new IndexOutOfBoundsException(\"Attempt to write 0 bytes\");\n+        }\n+        if (chunk.isReadOnly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+\n+        ByteArrayOutputStream out = new ByteArrayOutputStream(length);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU1MDA2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r444550063", "bodyText": "Chunk doesn't need to know about InputStreams.\nI don't think we have such methods in this repository. We mostly read and write from ByteBuffer or BufferView/ArrayView (defined in io.pravega.common.util)", "author": "sachin-j-joshi", "createdAt": "2020-06-23T22:48:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk5MjE2Mg=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": "d023bb71a22e0be2b1585566468e76cd89806831", "url": "https://github.com/pravega/pravega/commit/d023bb71a22e0be2b1585566468e76cd89806831", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Renaming and other review changes.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-23T23:57:01Z", "type": "forcePushed"}, {"oid": "c1504d075982965e6a767ca931b47caa5fe60987", "url": "https://github.com/pravega/pravega/commit/c1504d075982965e6a767ca931b47caa5fe60987", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-24T21:02:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445896446", "bodyText": "This is actually 2^61, not 2^62. Long.MAX_VALUE is 2^63-1.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:44:05Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.\n+     */\n+    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE / 4);", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkxOTc0Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445919743", "bodyText": "I want 62 bit number : Long.MAX_VALUE / 4 makes it 2 bits less than 64.\nSo comment is wrong - 2^61 , but logic is not.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1ODU5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445958598", "bodyText": "fixed comment.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:17:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyOTk2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447029960", "bodyText": "Long is a signed type, so its values are from -2^63 to 2^63-1. Therefore Long.MAX_VALUE is 2^63-1. Dividing that number by 4 gives you the wrong result.\nTry this code, for example:\n        System.out.println(1L << 63);\n        System.out.println(Long.MAX_VALUE);\n        System.out.println(Long.MAX_VALUE / 4);\n        System.out.println(1L << 61);\n        System.out.println(1L << 62);\n\nYou'll get\n-9223372036854775808\n9223372036854775807\n2305843009213693951\n2305843009213693952\n4611686018427387904\n\nSo:\n\n2^63 overflows Long (it is 1 + Long.MAX_VALUE)\nLong.MAX_VALUE/4 == 2^61 - 1", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NDU5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447044592", "bodyText": "System.out.println(Long.MAX_VALUE / 4);\nSystem.out.println(1L << 61);\n2305843009213693951\n2305843009213693952\n\n\nWhy do you think Long.MAX_VALUE / 4 requires more than 62 bits? (61 + 1 sign bit)", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:09:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA2ODI2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447068265", "bodyText": "Ok, I got confused about your comment.\nThen please set the explicit value to this field (1L << 61 -1). Do not rely on integer division to do the work for you.\nWhy do you need a sign bit? If offset is always >=0, then you can safely serialize up to 2^62 (excluded). so you can set that as your upper bound.", "author": "andreipaduroiu", "createdAt": "2020-06-29T15:41:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIyMDA4NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447220084", "bodyText": "updated.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T20:01:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzI5Mjc3OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447292779", "bodyText": "Using RevisionDataOutput.COMPACT_LONG_MAX as max segment length.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T22:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjY4OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445896688", "bodyText": "a new chunk", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:45:04Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,564 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk1ODY1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445958659", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:17:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NjY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5NzAxNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445897016", "bodyText": "No need to declare throws exception. You are less restrictive than the interface you are overriding so it will compile even without.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:46:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,564 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds or offset is not a valid offset in the underlying file/object.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * <ul>\n+     * <li>It is expected that in cases where it can not overwrite the existing data at given offset, the implementation should throw IndexOutOfBoundsException.</li>\n+     * For storage where underlying files/objects are immutable once written, the implementation should return false on {@link ChunkStorage#supportsAppend()}.\n+     * <li>In such cases only valid offset is 0.</li>\n+     * <li>For storages where underlying files/objects can only be appended but not overwritten, it must match actual current length of underlying file/object.</li>\n+     * <li>In all cases the offset can not be greater that actual current length of underlying file/object. </li>\n+     * </ul>\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+        if (!supportsAppend()) {\n+            Preconditions.checkArgument(offset == 0, \"offset must be 0 because storage does not support appends.\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle     ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.\n+     *\n+     * @throws Exception In case of any error.\n+     */\n+    @Override\n+    public void close() throws Exception {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODA5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445898093", "bodyText": "getOrDefault will return null if it doesn't exist. Use that to avoid having to make 2 calls into the map.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:50:13Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTI2Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961263", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:29:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445898957", "bodyText": "This type of eviction is very similar to how we keep and evict Cache Entries. So I have a few questions:\n\nIs there a way we can use the existing Cache Manager to perform eviction?\nCan we actually use the active segments from the Container Metadata? That thing already has a way to track all segments and evicts them based on an LRU criteria. There should be hooks available in the Segment Container's Metadata Cleaner to do that.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:53:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyNzgwMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445927800", "bodyText": "That will help with eviction of segments but not chunks.\nThis data is used only for finding first chunk to read from in case of a read. (and also helps with recently written last chunks so even if the segment is evicted out of memory it can still do quick look up for recently evicted segments.\nIt has a bit different use case and some specific logic related to chunks that is not covered by Cache Entries.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyODUwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445928506", "bodyText": "Anyways we'll revisit this soon #4902 as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:56:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTg5ODk1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDE5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900191", "bodyText": "Is this the most appropriate data type for this? Could we use one of the following:\n\nTreeMap (you'll have to synchronize).\nSortedIndex (in Pravega Common). This is an AVL tree that uses less memory and is a bit faster than TreeMap.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:57:44Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(currentGeneration.get())\n+                            .build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(currentGeneration.get()).build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+                floorEntry.getValue().setGeneration(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().getGeneration() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().getGeneration();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.\n+        long deletedUpToGen = 0;\n+        int runningCount = 0;\n+        for (val entry : counts.entrySet()) {\n+            runningCount += entry.getValue();\n+            deletedUpToGen = entry.getKey();\n+            if (runningCount >= toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        // Step 3: Now remove keys.\n+        int removed = 0;\n+        val iterator2 = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator2.hasNext() && removed < toRemoveCount) {\n+            val entry = iterator2.next();\n+            val gen = entry.getValue().getGeneration();\n+            if (gen <= deletedUpToGen) {\n+                iterator2.remove();\n+                removed++;\n+                counts.put(gen, counts.get(gen) - 1);\n+            }\n+            if (removed == toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        if (removed > 0) {\n+            long newOldGenvalue = previousGen + 1;\n+            for (val entry : counts.entrySet()) {\n+                if (entry.getValue() != 0) {\n+                    newOldGenvalue = entry.getKey();\n+                    break;\n+                }\n+            }\n+            if (newOldGenvalue != oldGen) {\n+                oldestGeneration.compareAndSet(oldGen, newOldGenvalue);\n+            }\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Per segment read index.\n+     * The index contains mapping from start offset to entries containing chunk names.\n+     * The underlying data structure uses ConcurrentSkipListMap.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndex {\n+        private final ConcurrentSkipListMap<Long, SegmentReadIndexEntry> chunkIndex;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMzg5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445923899", "bodyText": "It is thread safe . It is log(n) and already provides floor() method. It it standard library.\nIt is implemented with less locking (lock free).  This data structure is used by lots of threads.\nI think it's good enough for this purpose.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:36:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDE5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900430", "bodyText": "This class is not thread safe yet you are modifying this generation field from various different threads. Make it an AtomicLong (with appropriate getters and setters) or synchronize it.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:58:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index correpsonding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex;\n+        if (segmentsToReadIndexMap.containsKey(streamSegmentName)) {\n+            readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        } else {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(currentGeneration.get())\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(currentGeneration.get())\n+                            .build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(currentGeneration.get()).build());\n+            segmentReadIndex.setGeneration(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+                floorEntry.getValue().setGeneration(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.setGeneration(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().getGeneration() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().getGeneration();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.\n+        long deletedUpToGen = 0;\n+        int runningCount = 0;\n+        for (val entry : counts.entrySet()) {\n+            runningCount += entry.getValue();\n+            deletedUpToGen = entry.getKey();\n+            if (runningCount >= toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        // Step 3: Now remove keys.\n+        int removed = 0;\n+        val iterator2 = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator2.hasNext() && removed < toRemoveCount) {\n+            val entry = iterator2.next();\n+            val gen = entry.getValue().getGeneration();\n+            if (gen <= deletedUpToGen) {\n+                iterator2.remove();\n+                removed++;\n+                counts.put(gen, counts.get(gen) - 1);\n+            }\n+            if (removed == toRemoveCount) {\n+                break;\n+            }\n+        }\n+\n+        if (removed > 0) {\n+            long newOldGenvalue = previousGen + 1;\n+            for (val entry : counts.entrySet()) {\n+                if (entry.getValue() != 0) {\n+                    newOldGenvalue = entry.getKey();\n+                    break;\n+                }\n+            }\n+            if (newOldGenvalue != oldGen) {\n+                oldestGeneration.compareAndSet(oldGen, newOldGenvalue);\n+            }\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Per segment read index.\n+     * The index contains mapping from start offset to entries containing chunk names.\n+     * The underlying data structure uses ConcurrentSkipListMap.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndex {\n+        private final ConcurrentSkipListMap<Long, SegmentReadIndexEntry> chunkIndex;\n+        private long generation;\n+    }\n+\n+    /**\n+     * Entry for chunk in a segment read index.\n+     * The generation is used to cleanup unused entries.\n+     */\n+    @Builder\n+    @Data\n+    private static class SegmentReadIndexEntry {\n+        private final String chunkName;\n+        private long generation;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDUxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900514", "bodyText": "Same with the class above this.", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MjYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445962617", "bodyText": "This was on purpose.\nThe left side of the assignment was atomic long. What's the probability of thread getting suspended there and also has been like that for the long time for generation value to matter.\nIn worst case, we have early eviction of object. Okay to have that if its rarely , but otherwise  causes overhead.\nWell - that was the thinking.\nBut I fixed it now with atomic anyway.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:35:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMzQ2Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447033467", "bodyText": "2 threads accessing the same object concurrently is not the only thing you need to worry about in Java. If you set a value to a field, that value may be visible to the current thread, but may not be immediately visible to other threads, even if subsequent threads access the object after the original thread is done with it. You need to use one of the Atomic* classes or a mutex (i.e., synchronized) to ensure the memory location for that field has been properly set.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwNTA4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448005080", "bodyText": "Tracking this issue here #4902 This will be done as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T22:03:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445900851", "bodyText": "These fields are mutable. Is this class thread safe? Is it passed around different threads?", "author": "andreipaduroiu", "createdAt": "2020-06-25T23:59:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadata.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+\n+import java.io.IOException;\n+\n+/**\n+ * Represents chunk metadata.\n+ * Following metadata is stored.\n+ * <ul>\n+ * <li>Name of the chunk.</li>\n+ * <li>Length of the chunk.</li>\n+ * <li>Name of the next chunk in list.</li>\n+ * </ul>\n+ */\n+@Builder(toBuilder = true)\n+@Data\n+@EqualsAndHashCode(callSuper = true)\n+public class ChunkMetadata extends StorageMetadata {\n+    /**\n+     * Name of this chunk.\n+     */\n+    private final String name;\n+\n+    /**\n+     * Length of the chunk.\n+     */\n+    private long length;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyNDcxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445924713", "bodyText": "It is modified by only one thread.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:40:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNDUxMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447034511", "bodyText": "Then mark it as @NotThreadSafe. Same with all your other classes that are not thread safe.\nSimilarly, mark your thread-safe classes with @ThreadSafe.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:55:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NzY2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449077664", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:19:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMDg1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901617", "bodyText": "Why does this have a setter?", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:02:45Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTQ3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961471", "bodyText": "It was for tests. Fixed tests", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:30:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNTAzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447035034", "bodyText": "It still has a setter.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:55:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjUwMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447052502", "bodyText": "No it is fixed 90c2dd4", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:20:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901789", "bodyText": "It's best if you keep this collection internal and add APIs to mutate it. Otherwise external classes will need to know the internals of this class which can lead to trouble.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:03:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTYwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961606", "bodyText": "it was for tests. Fixed tests", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:31:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNTIxNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447035214", "bodyText": "It still is exposed.", "author": "andreipaduroiu", "createdAt": "2020-06-29T14:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MjM0Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447052347", "bodyText": "No it is fixed 90c2dd4", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:19:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTc4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTk4OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445901989", "bodyText": "Collections.singletonList", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:04:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,943 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.fromNullableString;\n+import static io.pravega.segmentstore.storage.metadata.StorageMetadata.toNullableString;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    @Setter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    @Setter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Arrays.asList(record));", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTE0MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961141", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:29:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMTk4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMjg1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445902857", "bodyText": "Is this the most appropriate data structure?", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:07:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,635 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * DurableLog's native fencing is used to establish ownership and single writer pattern.)\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ *\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ *\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.\n+ * Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.\n+ * Transactions are atomic - either all changes in the transaction are committed or none at all.\n+ * In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.\n+ *\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * <div>\n+ * <pre>\n+ *  // Start a transaction.\n+ * try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+ *      // Retrieve the data from transaction\n+ *      SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+ *\n+ *      // Modify retrieved record\n+ *      // seal if it is not already sealed.\n+ *      segmentMetadata.setSealed(true);\n+ *\n+ *      // put it back transaction\n+ *      txn.update(segmentMetadata);\n+ *\n+ *      // Commit\n+ *      txn.commit();\n+ *  } catch (StorageMetadataException ex) {\n+ *      // Handle Exceptions\n+ *  }\n+ *  </pre>\n+ * </div>\n+ *\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000;\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    private final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMzgwNg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445923806", "bodyText": "Removed. It was not needed any more.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMjg1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzI5MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445903291", "bodyText": "Is this class thread safe? If more than one thread touches it during its lifetime, even if not concurrently, it must be made thread safe.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:09:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -0,0 +1,635 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListSet;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements base metadata store that provides core functionality of metadata store by encapsulating underlying key value store.\n+ * Derived classes override {@link BaseMetadataStore#read(String)} and {@link BaseMetadataStore#writeAll(Collection)} to write to underlying storage.\n+ * The minimum requirement for underlying key-value store is to provide optimistic concurrency ( Eg. using versions numbers or etags.)\n+ *\n+ *\n+ * Within a segment store instance there should be only one instance that exclusively writes to the underlying key value store.\n+ * For distributed systems the single writer pattern must be enforced through external means. (Eg. for table segment based implementation\n+ * DurableLog's native fencing is used to establish ownership and single writer pattern.)\n+ *\n+ * This implementation provides following features that simplify metadata management.\n+ *\n+ * All access to and modifications to the metadata the {@link ChunkMetadataStore} must be done through a transaction.\n+ *\n+ * A transaction is created by calling {@link ChunkMetadataStore#beginTransaction()}\n+ *\n+ * Changes made to metadata inside a transaction are not visible until a transaction is committed using any overload of{@link MetadataTransaction#commit()}.\n+ * Transaction is aborted automatically unless committed or when {@link MetadataTransaction#abort()} is called.\n+ * Transactions are atomic - either all changes in the transaction are committed or none at all.\n+ * In addition, Transactions provide snaphot isolation which means that transaction fails if any of the metadata records read during the transactions are changed outside the transaction after they were read.\n+ *\n+ * Within a transaction you can perform following actions on per record basis.\n+ * <ul>\n+ * <li>{@link MetadataTransaction#get(String)} Retrieves metadata using for given key.</li>\n+ * <li>{@link MetadataTransaction#create(StorageMetadata)} Creates a new record.</li>\n+ * <li>{@link MetadataTransaction#delete(String)} Deletes records for given key.</li>\n+ * <li>{@link MetadataTransaction#update(StorageMetadata)} Updates the transaction local copy of the record.\n+ * For each record modified inside the transaction update must be called to mark the record as dirty.</li>\n+ * </ul>\n+ * <div>\n+ * <pre>\n+ *  // Start a transaction.\n+ * try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+ *      // Retrieve the data from transaction\n+ *      SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+ *\n+ *      // Modify retrieved record\n+ *      // seal if it is not already sealed.\n+ *      segmentMetadata.setSealed(true);\n+ *\n+ *      // put it back transaction\n+ *      txn.update(segmentMetadata);\n+ *\n+ *      // Commit\n+ *      txn.commit();\n+ *  } catch (StorageMetadataException ex) {\n+ *      // Handle Exceptions\n+ *  }\n+ *  </pre>\n+ * </div>\n+ *\n+ * Underlying implementation might buffer frequently or recently updated metadata keys to optimize read/write performance.\n+ * To further optimize it may provide \"lazy committing\" of changes where there is application specific way to recover from failures.(Eg. when only length of chunk is changed.)\n+ * In this case {@link MetadataTransaction#commit(boolean)} can be called.Note that otherwise for each commit the data is written to underlying key-value store.\n+ *\n+ * There are two special methods provided to handle metadata about data segments for the underlying key-value store. They are useful in avoiding circular references.\n+ * <ul>\n+ * <li>A record marked as pinned by calling {@link MetadataTransaction#markPinned(StorageMetadata)} is never written to underlying storage.</li>\n+ * <li>In addition transaction can be committed using {@link MetadataTransaction#commit(boolean, boolean)} to skip validation step that reads any recently evicted changes from underlying storage.</li>\n+ * </ul>\n+ */\n+@Slf4j\n+abstract public class BaseMetadataStore implements ChunkMetadataStore {\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    private static final int MAX_ENTRIES_IN_TXN_BUFFER = 5000;\n+\n+    /**\n+     * List of pinned keys. These keys should not be read or written.\n+     */\n+    private final ConcurrentSkipListSet<String> pinnedKeys = new ConcurrentSkipListSet<>();\n+\n+    /**\n+     * Lock for synchronization.\n+     */\n+    private final Object lock = new Object();\n+\n+    /**\n+     * Indicates whether this instance is fenced or not.\n+     */\n+    private final AtomicBoolean fenced;\n+\n+    /**\n+     * Monotonically increasing number. Keeps track of versions independent of external persistence or transaction mechanism.\n+     */\n+    private final AtomicLong version;\n+\n+    /**\n+     * Buffer for reading and writing transaction data entries to underlying KV store.\n+     * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     */\n+    @GuardedBy(\"lock\")\n+    private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n+\n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n+\n+    /**\n+     * Constructs a BaseMetadataStore object.\n+     */\n+    public BaseMetadataStore() {\n+        version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n+        fenced = new AtomicBoolean(false);\n+        bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+    }\n+\n+    /**\n+     * Begins a new transaction.\n+     *\n+     * @return Returns a new instance of MetadataTransaction.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n+        // Each transaction gets a unique number which is monotinically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet());\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn       transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n+        commit(txn, lazyWrite, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn transaction to commit.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n+        commit(txn, false, false);\n+    }\n+\n+    /**\n+     * Commits given transaction.\n+     *\n+     * @param txn       transaction to commit.\n+     * @param lazyWrite true if data can be written lazily.\n+     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     */\n+    @Override\n+    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        if (fenced.get()) {\n+            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        ArrayList<String> modifiedKeys = new ArrayList<>();\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+\n+        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+        // This step is kind of thread safe\n+        for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+            String key = entry.getKey();\n+            if (skipStoreCheck || entry.getValue().isPinned()) {\n+                log.trace(\"Skipping loading key from the store key = {}\", key);\n+            } else {\n+                // This check is safe to be outside the lock\n+                if (!bufferedTxnData.containsKey(key)) {\n+                    loadFromStore(key);\n+                }\n+            }\n+        }\n+\n+        // Step 2 : Check whether transaction is safe to commit.\n+        // This check needs to be atomic, with absolutely no possibility of re-entry\n+        synchronized (lock) {\n+            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                String key = entry.getKey();\n+                val transactionData = entry.getValue();\n+                Preconditions.checkState(null != transactionData.getKey());\n+\n+                // See if this entry was modified in this transaction.\n+                if (transactionData.getVersion() == txn.getVersion()) {\n+                    modifiedKeys.add(key);\n+                    transactionData.setPersisted(false);\n+                    modifiedValues.add(transactionData);\n+                }\n+                // make sure none of the keys used in this transaction have changed.\n+                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n+                if (null != dataFromBuffer) {\n+                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                        throw new StorageMetadataVersionMismatchException(\n+                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+                    }\n+\n+                    // Pin it if it is already pinned.\n+                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+\n+                    // Set the database object.\n+                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n+                }\n+            }\n+\n+            // Step 3: Commit externally.\n+            // This operation may call external storage.\n+            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n+                log.trace(\"Persisting all modified keys (except pinned)\");\n+                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+                writeAll(toWriteList);\n+                log.trace(\"Done persisting all modified keys\");\n+\n+                // Mark written keys as persisted.\n+                for (val writtenData : toWriteList) {\n+                    writtenData.setPersisted(true);\n+                }\n+            }\n+\n+            // Execute external commit step.\n+            try {\n+                if (null != txn.getExternalCommitStep()) {\n+                    txn.getExternalCommitStep().call();\n+                }\n+            } catch (Exception e) {\n+                log.error(\"Exception during execution of external commit step\", e);\n+                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+            }\n+\n+            // If we reach here then it means transaction is safe to commit.\n+            // Step 4: Insert\n+            long committedVersion = version.incrementAndGet();\n+            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n+            for (String key : modifiedKeys) {\n+                TransactionData data = txnData.get(key);\n+                data.setVersion(committedVersion);\n+                toAdd.put(key, data);\n+            }\n+            bufferedTxnData.putAll(toAdd);\n+        }\n+\n+        //  Step 5 : evict if required.\n+        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n+            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+        }\n+\n+        //  Step 6: finally clear\n+        txnData.clear();\n+    }\n+\n+    /**\n+     * Aborts given transaction.\n+     *\n+     * @param txn transaction to abort.\n+     * @throws StorageMetadataException If there are any errors.\n+     */\n+    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+        // Do nothing\n+    }\n+\n+    /**\n+     * Retrieves the metadata for given key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @return Metadata for given key. Null if key was not found.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        TransactionData dataFromBuffer = null;\n+        if (null == key) {\n+            return null;\n+        }\n+\n+        Map<String, TransactionData> txnData = txn.getData();\n+        TransactionData data = txnData.get(key);\n+\n+        // Search in the buffer.\n+        if (null == data) {\n+            synchronized (lock) {\n+                dataFromBuffer = bufferedTxnData.get(key);\n+            }\n+            // If we did not find in buffer then load it from store\n+            if (null == dataFromBuffer) {\n+                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+                loadFromStore(key);\n+                dataFromBuffer = bufferedTxnData.get(key);\n+                Preconditions.checkState(null != dataFromBuffer);\n+            }\n+\n+            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n+                // Make copy.\n+                data = dataFromBuffer.toBuilder()\n+                        .key(key)\n+                        .value(dataFromBuffer.getValue().deepCopy())\n+                        .build();\n+                txnData.put(key, data);\n+            }\n+        }\n+\n+        if (data != null) {\n+            return data.getValue();\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Loads value from store\n+     *\n+     * @param key Key to load\n+     * @return Value if found null otherwise.\n+     * @throws StorageMetadataException Any exceptions.\n+     */\n+    private TransactionData loadFromStore(String key) throws StorageMetadataException {\n+        // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n+        log.trace(\"Loading key from the store key = {}\", key);\n+        TransactionData fromDb = read(key);\n+        Preconditions.checkState(null != fromDb);\n+        log.trace(\"Done Loading key from the store key = {}\", key);\n+\n+        TransactionData copyForBuffer = fromDb.toBuilder()\n+                .key(key)\n+                .build();\n+\n+        if (null != fromDb.getValue()) {\n+            Preconditions.checkState(0 != fromDb.getVersion(), \"Version is not initialized\");\n+            // Make sure it is a deep copy.\n+            copyForBuffer.setValue(fromDb.getValue().deepCopy());\n+        }\n+        // Put this value in bufferedTxnData buffer.\n+        synchronized (lock) {\n+            // If some other transaction beat us then use that value.\n+            TransactionData oldValue = bufferedTxnData.putIfAbsent(key, copyForBuffer);\n+            if (oldValue != null) {\n+                copyForBuffer = oldValue;\n+            }\n+        }\n+        return copyForBuffer;\n+    }\n+\n+    /**\n+     * Reads a metadata record for the given key.\n+     *\n+     * @param key Key for the metadata record.\n+     * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    abstract protected TransactionData read(String key) throws StorageMetadataException;\n+\n+    /**\n+     * Writes transaction data from a given list to the metadata store.\n+     *\n+     * @param dataList List of transaction data to write.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    abstract protected void writeAll(Collection<TransactionData> dataList) throws StorageMetadataException;\n+\n+    /**\n+     * Updates existing metadata.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(metadata);\n+        data.setPersisted(false);\n+        Preconditions.checkState(txn.getVersion() >= data.getVersion());\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * Marks given record as pinned.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Map<String, TransactionData> txnData = txn.getData();\n+        String key = metadata.getKey();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+\n+        data.setValue(metadata);\n+        data.setPinned(true);\n+        data.setVersion(txn.getVersion());\n+\n+        pinnedKeys.add(metadata.getKey());\n+    }\n+\n+    /**\n+     * Creates a new metadata record.\n+     *\n+     * @param txn      Transaction.\n+     * @param metadata metadata record.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != metadata);\n+        Preconditions.checkArgument(null != metadata.getKey());\n+        Map<String, TransactionData> txnData = txn.getData();\n+        txnData.put(metadata.getKey(), TransactionData.builder()\n+                .key(metadata.getKey())\n+                .value(metadata)\n+                .version(txn.getVersion())\n+                .build());\n+    }\n+\n+    /**\n+     * Deletes a metadata record given the key.\n+     *\n+     * @param txn Transaction.\n+     * @param key key to use to retrieve metadata.\n+     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     */\n+    @Override\n+    public void delete(MetadataTransaction txn, String key) throws StorageMetadataException {\n+        Preconditions.checkArgument(null != txn);\n+        Preconditions.checkArgument(null != key);\n+        Map<String, TransactionData> txnData = txn.getData();\n+\n+        TransactionData data = TransactionData.builder().key(key).build();\n+        TransactionData oldData = txnData.putIfAbsent(key, data);\n+        if (null != oldData) {\n+            data = oldData;\n+        }\n+        data.setValue(null);\n+        data.setPersisted(false);\n+        data.setVersion(txn.getVersion());\n+    }\n+\n+    /**\n+     * {@link AutoCloseable#close()} implementation.\n+     */\n+    @Override\n+    public void close() throws Exception {\n+        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        bufferedTxnData.entrySet().stream().filter(entry -> !entry.getValue().isPersisted() && !entry.getValue().isPinned()).forEach(entry -> modifiedValues.add(entry.getValue()));\n+        if (modifiedValues.size() > 0) {\n+            writeAll(modifiedValues);\n+        }\n+    }\n+\n+    /**\n+     * Explicitly marks the store as fenced.\n+     * Once marked fenced no modifications to data should be allowed.\n+     */\n+    public void markFenced() {\n+        this.fenced.set(true);\n+    }\n+\n+    /**\n+     * Return whether record for the given key is pinned or not.\n+     *\n+     * @param key Key to check.\n+     * @return True if record for the given key is pinned, false otherwise.\n+     */\n+    protected boolean isPinnedKey(String key) {\n+        return pinnedKeys.contains(key);\n+    }\n+\n+    /**\n+     * Retrieves the current version number.\n+     *\n+     * @return current version number.\n+     */\n+    protected long getVersion() {\n+        return version.get();\n+    }\n+\n+    /**\n+     * Sets the current version number.\n+     *\n+     * @param version Version to set.\n+     */\n+    protected void setVersion(long version) {\n+        this.version.set(version);\n+    }\n+\n+    /**\n+     * Stores the transaction data.\n+     */\n+    @Builder(toBuilder = true)\n+    @Data\n+    public static class TransactionData implements Serializable {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMDUzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445920532", "bodyText": "This class is thread safe. Instance of these objects are not shared between transactions.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:21:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzI5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzg3MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445903871", "bodyText": "Check out Guava's Strings class. I'm sure at least one of these methods is in there.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:11:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadata.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkManager;\n+import io.pravega.segmentstore.storage.mocks.MockStorageMetadata;\n+import lombok.Data;\n+\n+import java.io.Serializable;\n+\n+/**\n+ * Storage Metadata.\n+ * All storage related metadata is stored in {@link ChunkMetadataStore} using set of key-value pairs.\n+ * The String value returned by {@link StorageMetadata#getKey()} is used as key.\n+ * Notable derived classes are {@link SegmentMetadata} and {@link ChunkMetadata} which form the core of metadata related\n+ * to {@link ChunkManager} functionality.\n+ */\n+@Data\n+public abstract class StorageMetadata implements Serializable {\n+\n+    /**\n+     * Retrieves the key associated with the metadata.\n+     *\n+     * @return key.\n+     */\n+    public abstract String getKey();\n+\n+    /**\n+     * Creates a deep copy of this instance.\n+     *\n+     * @return A deep copy of this instance.\n+     */\n+    public abstract StorageMetadata deepCopy();\n+\n+    /**\n+     * Helper method that converts empty string to null value.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String toNullableString(String toConvert) {\n+        if (toConvert.length() == 0) {\n+            return null;\n+        }\n+        return toConvert;\n+    }\n+\n+    /**\n+     * Helper method that converts null value to empty string.\n+     *\n+     * @param toConvert String to convert.\n+     * @return If toConvert is null then it returns empty string. Otherwise returns original string.\n+     */\n+    public static String fromNullableString(String toConvert) {", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk0MDg4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445940887", "bodyText": "Nice.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T02:50:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwMzg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445904913", "bodyText": "Why not put it in _system/containers? At least that way we'll keep all container-related stuff together.", "author": "andreipaduroiu", "createdAt": "2020-06-26T00:15:42Z", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -68,11 +68,26 @@\n      */\n     private static final String EPOCH_DELIMITER = \".#epoch.\";\n \n+    /**\n+     * Format for chunk name with segment name , epoch and offset.\n+     */\n+    private static final String CHUNK_NAME_FORMAT_WITH_EPOCH_OFFSET = \"%s.E-%d-O-%d.%s\";\n+\n     /**\n      * Format for Container Metadata Segment name.\n      */\n     private static final String METADATA_SEGMENT_NAME_FORMAT = \"_system/containers/metadata_%d\";\n \n+    /**\n+     * Format for Storage Metadata Segment name.\n+     */\n+    private static final String STORAGE_METADATA_SEGMENT_NAME_FORMAT = \"_system/containers/storage_metadata_%d\";\n+\n+    /**\n+     * Format for Container System Journal file name.\n+     */\n+    private static final String SYSJOURNAL_NAME_FORMAT = \"_sysjournal.epoch%d.container%d.file%d\";", "originalCommit": "c1504d075982965e6a767ca931b47caa5fe60987", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkyMDY1Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445920653", "bodyText": "ok", "author": "sachin-j-joshi", "createdAt": "2020-06-26T01:22:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTk2MTExNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r445961117", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T04:28:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTkwNDkxMw=="}], "type": "inlineReview"}, {"oid": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "url": "https://github.com/pravega/pravega/commit/90c2dd483f8507bb02048504a166fe71ae6a82ae", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-06-26T04:28:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446097542", "bodyText": "I'm not sure I understand the implication of this evaluating always to true. Why is it correct to implement it this way independent of the underlying storage?", "author": "fpj", "createdAt": "2020-06-26T10:17:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+     * when available, and if not available, then we use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorage needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+     * append or concat.</li>\n+     * <li>Also when ChunkStorage supports both concat and append, ChunkManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){\n+     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+     *     For (List<chunk> list : s){\n+     *        ConcatChunks (list);\n+     *     }\n+     * }\n+     * </pre>\n+     * </li>\n+     * </ul>\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n+     * @param startChunkName  Name of the first chunk to start defragmentation.\n+     * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, String startChunkName, String lastChunkName) throws StorageMetadataException, ChunkStorageException {\n+        // The algorithm is actually very simple.\n+        // It tries to concat all small chunks using appends first.\n+        // Then it tries to concat remaining chunks using concat if available.\n+        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n+        boolean useAppend = true;\n+        String targetChunkName = startChunkName;\n+\n+        // Iterate through chunk list\n+        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n+            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n+\n+            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+            long targetSizeAfterConcat = target.getLength();\n+\n+            // Add target to the list of chunks\n+            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n+\n+            String nextChunkName = target.getNextChunk();\n+            ChunkMetadata next = null;\n+\n+            // Gather list of chunks that can be appended together.\n+            while (null != nextChunkName) {\n+                next = (ChunkMetadata) txn.get(nextChunkName);\n+\n+                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n+                    break;\n+                }\n+\n+                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n+                    break;\n+                }\n+\n+                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n+                targetSizeAfterConcat += next.getLength();\n+\n+                nextChunkName = next.getNextChunk();\n+            }\n+            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n+            // Which means target should now point to it as next after concat is complete.\n+\n+            // If there are chunks that can be appended together then concat them.\n+            if (chunksToConcat.size() > 1) {\n+                // Concat\n+\n+                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n+                for (int i = 0; i < chunksToConcat.size(); i++) {\n+                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n+                }\n+\n+                if (!useAppend && chunkStorage.supportsConcat()) {\n+                    int length = chunkStorage.concat(concatArgs);\n+                } else {\n+                    concatUsingAppend(concatArgs);\n+                }\n+\n+                // Set the pointers\n+                target.setLength(targetSizeAfterConcat);\n+                target.setNextChunk(nextChunkName);\n+\n+                // If target is the last chunk after this then update metadata accordingly\n+                if (null == nextChunkName) {\n+                    segmentMetadata.setLastChunk(target.getName());\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n+                }\n+\n+                // Update metadata for affected chunks.\n+                for (int i = 1; i < concatArgs.length; i++) {\n+                    txn.delete(concatArgs[i].getName());\n+                    segmentMetadata.decrementChunkCount();\n+                }\n+                txn.update(target);\n+                txn.update(segmentMetadata);\n+            }\n+\n+            // Move on to next place in list where we can concat if we are done with append based concats.\n+            if (!useAppend) {\n+                targetChunkName = nextChunkName;\n+            }\n+\n+            // Toggle\n+            useAppend = !useAppend;\n+        }\n+\n+        // Make sure no invariants are broken.\n+        segmentMetadata.checkInvariants();\n+    }\n+\n+    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n+        long writeAtOffset = concatArgs[0].getLength();\n+        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n+        for (int i = 1; i < concatArgs.length; i++) {\n+            int readAtOffset = 0;\n+            val arg = concatArgs[i];\n+            int bytesToRead = Math.toIntExact(arg.getLength());\n+\n+            while (bytesToRead > 0) {\n+                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n+                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n+                bytesToRead -= size;\n+                readAtOffset += size;\n+                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Check preconditions\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                segmentMetadata.setActive(false);\n+\n+                // Delete chunks\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    // Delete underlying file.\n+                    chunksToDelete.add(currentChunkName);\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                    txn.delete(currentMetadata.getName());\n+                }\n+\n+                // Commit.\n+                txn.delete(streamSegmentName);\n+                txn.commit();\n+\n+                // Collect garbage.\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index.\n+                readIndexCache.remove(streamSegmentName);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+\n+            String streamSegmentName = handle.getSegmentName();\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Check preconditions\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+                }\n+\n+                if (segmentMetadata.getStartOffset() == offset) {\n+                    // Nothing to do\n+                    return null;\n+                }\n+\n+                String currentChunkName = segmentMetadata.getFirstChunk();\n+                ChunkMetadata currentMetadata;\n+                long oldLength = segmentMetadata.getLength();\n+                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+                ArrayList<String> chunksToDelete = new ArrayList<>();\n+                while (currentChunkName != null) {\n+                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                        break;\n+                    }\n+\n+                    startOffset += currentMetadata.getLength();\n+                    chunksToDelete.add(currentMetadata.getName());\n+                    segmentMetadata.decrementChunkCount();\n+\n+                    // move to next chunk\n+                    currentChunkName = currentMetadata.getNextChunk();\n+                }\n+                segmentMetadata.setFirstChunk(currentChunkName);\n+                segmentMetadata.setStartOffset(offset);\n+                segmentMetadata.setFirstChunkStartOffset(startOffset);\n+                for (String toDelete : chunksToDelete) {\n+                    txn.delete(toDelete);\n+                    // Adjust last chunk if required.\n+                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                        segmentMetadata.setLastChunk(null);\n+                    }\n+                }\n+                txn.update(segmentMetadata);\n+\n+                // Check invariants.\n+                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                segmentMetadata.checkInvariants();\n+\n+                // Commit system logs.\n+                if (isStorageSystemSegment(segmentMetadata)) {\n+                    val finalStartOffset = startOffset;\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecord(\n+                                SystemJournal.TruncationRecord.builder()\n+                                        .segmentName(streamSegmentName)\n+                                        .offset(offset)\n+                                        .firstChunkName(segmentMetadata.getFirstChunk())\n+                                        .startOffset(finalStartOffset)\n+                                        .build());\n+                        return null;\n+                    });\n+                }\n+\n+                // Finally commit.\n+                txn.commit(chunksToDelete.size() == 0); // if layout did not change then commit with lazyWrite.\n+\n+                collectGarbage(chunksToDelete);\n+\n+                // Update the read index by removing all entries below truncate offset.\n+                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyNzgxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446227810", "bodyText": "Segment is made up of multiple consecutive chunks. ChunkManager has an ability to truncate head part of the segment by removing chunks which are completely below truncate offset. When truncation offset falls in the middle of a given chunk, we can't remove or truncate that chunk at front. In such cases we keep the entire chunk but make sure data below requested truncation is not returned after truncate.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:43:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk3MzgxOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446973818", "bodyText": "A couple of questions:\n1- If chunk manager is a layer of abstraction on top of real storage and it is always there, then does checking whether it supports truncation even makes sense? Is this a legacy check that we can remove with this work?\n2- Truncating at the granularity of chunks is not really supporting truncation. One potential undesirable behavior from a user perspective is that after a successful call to truncation, the space used by truncated data might not be entirely reclaimed.\nIs there any chance we can support real truncation (i.e., truncating chunks partially when the offset falls in the middle of chunk) for the underlying storage system that provide such functionality?", "author": "fpj", "createdAt": "2020-06-29T13:32:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA2OTY1OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447069658", "bodyText": "The public boolean supportsTruncation() on Storage interface is used in select few places in the existing code. (SegmentAttributeBTreeIndex, SegmentAggregator). Old code has slightly different abstractions and it is not possible to cleanly remove this method at this time. (Without making bigger change.)\nNone of the underlying storage (FileSystem,ExtendedS3,HDFS)  actually provide ability to truncate at the front of the chunk.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:43:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAwNDU5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448004593", "bodyText": "Created issue to track this as per our conversation #4912.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T22:01:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5NzU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446099229", "bodyText": "It is confusing that ChunkManager implements Storage while ChunkStorage does not implement any interface. Isn't it more natural that ChunkStorage implements Storage rather than ChunkManager?", "author": "fpj", "createdAt": "2020-06-26T10:20:41Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -0,0 +1,194 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import java.io.InputStream;\n+\n+/**\n+ * Defines an abstraction for Permanent Storage.\n+ * Note: not all operations defined here are needed.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are\n+ * provided even though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent\n+ * (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent\n+ * operation performed.</li>\n+ * </ul>\n+ * There is no need to implement any special logic to handle concurrent access to the underlying objects/files.\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end).\n+ * This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending\n+ * on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ * It is recommended that the implementations should extend {@link BaseChunkStorage}.\n+ */\n+public interface ChunkStorage extends AutoCloseable {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIyMzM1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446223357", "bodyText": "Storage is the interface that is already defined and used by rest of the segment store. This change does not aim to change that interface. This interface has an abstraction of segments and operates at that level of segments.\nChunkStorage is a new interface which operates at individual chunks (CRDL). We wanted this to be simple interface. Implementations do not need to worry about fencing or how  these chunks are put together to form a segment. Consequently the methods and their behavior is different. Chunk is therefore not same as segment.\nWe need something that translates calls at segment level (Storage) to calls on individual chunks (ChunkStorage) and also manages all these chunks, fencing, rolling etc etc . This is exactly what ChunkManager does. It is lot more than just adapter or wrapper.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:36:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk5NjAzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446996035", "bodyText": "The problem isn't keeping the Storage interface as is, but instead that the abstractions don't quite match here, as I see it:\n\nChunkManager should be called ChunkStorage instead\nChunkStorage should be called something else that reflects the fact that it operates on chunks individually, e.g., ChunkHandler since it handles operations on chunks.", "author": "fpj", "createdAt": "2020-06-29T14:03:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzMjMyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447032324", "bodyText": "The original names were\n\nChunkStorageManager\nChunkStorageProvider\n\nAlternative names are\n\nChunkStorage\nChunkProvider , ChunkHandler (But Handler has connotation of handling event) ,", "author": "sachin-j-joshi", "createdAt": "2020-06-29T14:52:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjA5OTIyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMDUyNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446100527", "bodyText": "Truncates... to be consistent with the other javadocs.", "author": "fpj", "createdAt": "2020-06-26T10:23:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTI2OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446101268", "bodyText": "Shouldn't findFloor be a read operation with no side effect?", "author": "fpj", "createdAt": "2020-06-26T10:25:35Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMDQzMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446230433", "bodyText": "Assuming that client is reading sequentially, it is likely that the same chunk will be read in subsequent calls. So we need to keep that entry in cache. Because this was a cache hit we need to mark it with current generation.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:48:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTI2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMTg1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446101857", "bodyText": "... up to ...", "author": "fpj", "createdAt": "2020-06-26T10:26:56Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncate the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Evicts segments from the cache.\n+     */\n+    private void evictSegmentsFromOldestGeneration() {\n+        val oldGen = oldestGeneration.get();\n+        currentGeneration.getAndIncrement();\n+\n+        val iterator = segmentsToReadIndexMap.entrySet().iterator();\n+        int total = totalChunkCount.get();\n+        int removed = 0;\n+        while (iterator.hasNext() && (segmentsToReadIndexMap.size() >= maxIndexedSegments || total >= maxIndexedChunks)) {\n+            val entry = iterator.next();\n+            if (entry.getValue().generation.get() <= oldGen) {\n+                val size = entry.getValue().chunkIndex.size();\n+                removed += size;\n+                total -= size;\n+                iterator.remove();\n+            }\n+        }\n+        if (removed > 0) {\n+            oldestGeneration.compareAndSet(oldGen, oldGen + 1);\n+            totalChunkCount.getAndAdd(-1 * removed);\n+        }\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(String streamSegmentName, long toRemoveCount) {\n+        val segmentReadIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        evictChunks(segmentReadIndex, toRemoveCount);\n+    }\n+\n+    /**\n+     * Evicts chunks from the cache.\n+     */\n+    private void evictChunks(SegmentReadIndex segmentReadIndex, long toRemoveCount) {\n+        // Increment generation.\n+        val previousGen = currentGeneration.getAndIncrement();\n+        val oldGen = oldestGeneration.get();\n+\n+        // Step 1 : Go through all entries once to record counts per each generation.\n+        TreeMap<Long, Integer> counts = new TreeMap<Long, Integer>();\n+        val iterator = segmentReadIndex.chunkIndex.entrySet().iterator();\n+        while (iterator.hasNext()) {\n+            val entry = iterator.next();\n+            long generation = entry.getValue().generation.get();\n+            val cnt = counts.get(generation);\n+            if (null == cnt) {\n+                counts.put(generation, 1);\n+            } else {\n+                counts.put(generation, cnt + 1);\n+            }\n+        }\n+\n+        // Step 2 : Determine upto what generation to delete.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446102435", "bodyText": "Do we need metrics for this read index cache?", "author": "fpj", "createdAt": "2020-06-26T10:28:08Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMjY1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446232655", "bodyText": "Yes.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:52:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NTA2NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446275065", "bodyText": "I plan to work bit more on  ReadIndexCache\nI have created #4902 For tracking improvements in cache eviction logic for ReadIndexCache in ChunkManager layer. This will be done as separate PR after first set of major changes.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:06:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwMjQzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446105145", "bodyText": "I does not need to be accurate for correctness, but if miscalculated, then can it have an impact to performance?", "author": "fpj", "createdAt": "2020-06-26T10:34:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNTYyOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446235628", "bodyText": "Yes. But I think intention is to prefer performance over accuracy.\nEg. If it improves average performance by avoiding need to synchronize we would prefer that option, even if every once in a while we evict an entry too early or keep it for bit longer than needed.\nIt really depends on extent of miscalculation and probability of it happening.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk3NzA0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446977046", "bodyText": "I need to understand this better, I'll have a look at this again and get back.", "author": "fpj", "createdAt": "2020-06-29T13:37:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyNjk0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447026948", "bodyText": "Ok, I'm trying to understand what made you say best effort here. Is it the fact that upon eviction, we iterate over the elements of the map concurrently with other operations? If not, what's it?\nI also saw a resolved comment down saying that you'll revisit the implementation of the cache in the next iteration? It is not clear to me what the next iteration is, whether for this PR or a future one, so let me know your plans so that I have the right expectation.", "author": "fpj", "createdAt": "2020-06-29T14:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5ODE5Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447998196", "bodyText": "Resolving as per discussion offline. Tracking this here #4902", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:47:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNTE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNzU3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446107572", "bodyText": "Typo in snalpshot", "author": "fpj", "createdAt": "2020-06-26T10:40:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwNzcwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446107708", "bodyText": "Typo in thier", "author": "fpj", "createdAt": "2020-06-26T10:40:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446108708", "bodyText": "Why are we calling it a file? It depends on the underlying storage we are using for chunks, no?", "author": "fpj", "createdAt": "2020-06-26T10:43:11Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.\n+                    for (SegmentSnapshotRecord segmentSnapshot : systemSnapshot.segmentSnapshotRecords) {\n+                        // Update segment data.\n+                        segmentSnapshot.segmentMetadata.setActive(true)\n+                                .setOwnershipChanged(true)\n+                                .setStorageSystemSegment(true);\n+                        segmentSnapshot.segmentMetadata.setOwnerEpoch(epoch);\n+\n+                        // Add segment data.\n+                        txn.create(segmentSnapshot.segmentMetadata);\n+\n+                        // make sure that the record is marked pinned.\n+                        txn.markPinned(segmentSnapshot.segmentMetadata);\n+\n+                        // Add chunk metadata and keep track of start offsets for each chunk.\n+                        long offset = segmentSnapshot.segmentMetadata.getFirstChunkStartOffset();\n+                        for (ChunkMetadata metadata : segmentSnapshot.chunkMetadataCollection) {\n+                            txn.create(metadata);\n+\n+                            // make sure that the record is marked pinned.\n+                            txn.markPinned(metadata);\n+\n+                            chunkStartOffsets.put(metadata.getName(), offset);\n+                            offset += metadata.getLength();\n+                        }\n+                        found = true;\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+        if (!found) {\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(config.getDefaultRollingPolicy().getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true)\n+                        .setOwnershipChanged(true)\n+                        .setStorageSystemSegment(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+        }\n+        return epochToCheck;\n+    }\n+\n+    /**\n+     * Read contents from file.\n+     */\n+    private byte[] getContents(String snapshotFile) throws ChunkStorageException {\n+        val info = chunkStorage.getInfo(snapshotFile);\n+        val h = chunkStorage.openRead(snapshotFile);\n+        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+        long fromOffset = 0;\n+        int remaining = contents.length;\n+        while (remaining > 0) {\n+            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+            remaining -= bytesRead;\n+            fromOffset += bytesRead;\n+        }\n+        return contents;\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn,\n+                                          long epochToStartScanning,\n+                                          HashMap<String, Long> chunkStartOffsets,\n+                                          HashMap<String, Long> finalTruncateOffsets,\n+                                          HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (long epochToRecover = epochToStartScanning; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 1.\n+            int fileIndexToRecover = 1;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                // Read contents.\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                byte[] contents = getContents(systemLogName);\n+                var input = new ByteArrayInputStream(contents);\n+\n+                // Apply record batches from the file.\n+                // Loop is exited with eventual EOFException.\n+                while (true) {\n+                    try {\n+                        val batch = BATCH_SERIALIZER.deserialize(input);\n+                        if (null != batch.getSystemJournalRecords()) {\n+                            for (var record : batch.getSystemJournalRecords()) {\n+                                log.debug(\"SystemJournal[{}] Processing system log record ={}.\", epoch, record);\n+                                // ChunkAddedRecord.\n+                                if (record instanceof ChunkAddedRecord) {\n+                                    val chunkAddedRecord = (ChunkAddedRecord) record;\n+                                    applyChunkAddition(txn, chunkStartOffsets,\n+                                            chunkAddedRecord.getSegmentName(),\n+                                            nullToEmpty(chunkAddedRecord.getOldChunkName()),\n+                                            chunkAddedRecord.getNewChunkName(),\n+                                            chunkAddedRecord.getOffset());\n+                                }\n+\n+                                // TruncationRecord.\n+                                if (record instanceof TruncationRecord) {\n+                                    val truncationRecord = (TruncationRecord) record;\n+                                    finalTruncateOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getOffset());\n+                                    finalFirstChunkStartsAtOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getStartOffset());\n+                                }\n+                            }\n+                        }\n+                    } catch (EOFException e) {\n+                        log.debug(\"SystemJournal[{}] Done processing file {}.\", containerId, systemLogName);\n+                        break;\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            // Update length of last chunk in metadata to what we actually find on LTS.\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(length);\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     * Apply last effective truncate offsets.\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+                segmentMetadata.decrementChunkCount();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(offset - oldLength);\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+            segmentMetadata.setStartOffset(offset);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.incrementChunkCount();\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNjc3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446236772", "bodyText": "ok changing it to getSystemJournalChunkName.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:58:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3NDA5OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446274098", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwODcwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446109205", "bodyText": "Do these data classes need to be public?", "author": "fpj", "createdAt": "2020-06-26T10:44:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Find and apply latest snapshot.\n+     */\n+    private long applyLatestSnapshot(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets) throws Exception {\n+        long epochToCheck = epoch - 1;\n+        String snapshotFile = null;\n+        boolean found = false;\n+\n+        // Find latest epoch with snapshot.\n+        for (epochToCheck = epoch - 1; epochToCheck >= 0; epochToCheck--) {\n+            snapshotFile = getSystemJournalFileName(containerId, epochToCheck, 0);\n+            if (chunkStorage.exists(snapshotFile)) {\n+                // Read contents.\n+                byte[] contents = getContents(snapshotFile);\n+                SystemSnapshotRecord systemSnapshot = SYSTEM_SNAPSHOT_SERIALIZER.deserialize(contents);\n+                if (null != systemSnapshot) {\n+                    log.debug(\"SystemJournal[{}] Processing system log snalpshot {}.\", containerId, systemSnapshot);\n+                    // Initialize the segments and thier chunks.\n+                    for (SegmentSnapshotRecord segmentSnapshot : systemSnapshot.segmentSnapshotRecords) {\n+                        // Update segment data.\n+                        segmentSnapshot.segmentMetadata.setActive(true)\n+                                .setOwnershipChanged(true)\n+                                .setStorageSystemSegment(true);\n+                        segmentSnapshot.segmentMetadata.setOwnerEpoch(epoch);\n+\n+                        // Add segment data.\n+                        txn.create(segmentSnapshot.segmentMetadata);\n+\n+                        // make sure that the record is marked pinned.\n+                        txn.markPinned(segmentSnapshot.segmentMetadata);\n+\n+                        // Add chunk metadata and keep track of start offsets for each chunk.\n+                        long offset = segmentSnapshot.segmentMetadata.getFirstChunkStartOffset();\n+                        for (ChunkMetadata metadata : segmentSnapshot.chunkMetadataCollection) {\n+                            txn.create(metadata);\n+\n+                            // make sure that the record is marked pinned.\n+                            txn.markPinned(metadata);\n+\n+                            chunkStartOffsets.put(metadata.getName(), offset);\n+                            offset += metadata.getLength();\n+                        }\n+                        found = true;\n+                    }\n+                    break;\n+                }\n+            }\n+        }\n+        if (!found) {\n+            for (String systemSegment : systemSegments) {\n+                SegmentMetadata segmentMetadata = SegmentMetadata.builder()\n+                        .name(systemSegment)\n+                        .ownerEpoch(epoch)\n+                        .maxRollinglength(config.getDefaultRollingPolicy().getMaxLength())\n+                        .build();\n+                segmentMetadata.setActive(true)\n+                        .setOwnershipChanged(true)\n+                        .setStorageSystemSegment(true);\n+                segmentMetadata.checkInvariants();\n+                txn.create(segmentMetadata);\n+                txn.markPinned(segmentMetadata);\n+            }\n+        }\n+        return epochToCheck;\n+    }\n+\n+    /**\n+     * Read contents from file.\n+     */\n+    private byte[] getContents(String snapshotFile) throws ChunkStorageException {\n+        val info = chunkStorage.getInfo(snapshotFile);\n+        val h = chunkStorage.openRead(snapshotFile);\n+        byte[] contents = new byte[Math.toIntExact(info.getLength())];\n+        long fromOffset = 0;\n+        int remaining = contents.length;\n+        while (remaining > 0) {\n+            int bytesRead = chunkStorage.read(h, fromOffset, remaining, contents, Math.toIntExact(fromOffset));\n+            remaining -= bytesRead;\n+            fromOffset += bytesRead;\n+        }\n+        return contents;\n+    }\n+\n+    /**\n+     * Process all systemLog entries to recreate the state of metadata storage system segments.\n+     */\n+    private void applySystemLogOperations(MetadataTransaction txn,\n+                                          long epochToStartScanning,\n+                                          HashMap<String, Long> chunkStartOffsets,\n+                                          HashMap<String, Long> finalTruncateOffsets,\n+                                          HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws ChunkStorageException, IOException, StorageMetadataException {\n+        for (long epochToRecover = epochToStartScanning; epochToRecover < epoch; epochToRecover++) {\n+            // Start scan with file index 1.\n+            int fileIndexToRecover = 1;\n+            while (chunkStorage.exists(getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover))) {\n+                // Read contents.\n+                val systemLogName = getSystemJournalFileName(containerId, epochToRecover, fileIndexToRecover);\n+                byte[] contents = getContents(systemLogName);\n+                var input = new ByteArrayInputStream(contents);\n+\n+                // Apply record batches from the file.\n+                // Loop is exited with eventual EOFException.\n+                while (true) {\n+                    try {\n+                        val batch = BATCH_SERIALIZER.deserialize(input);\n+                        if (null != batch.getSystemJournalRecords()) {\n+                            for (var record : batch.getSystemJournalRecords()) {\n+                                log.debug(\"SystemJournal[{}] Processing system log record ={}.\", epoch, record);\n+                                // ChunkAddedRecord.\n+                                if (record instanceof ChunkAddedRecord) {\n+                                    val chunkAddedRecord = (ChunkAddedRecord) record;\n+                                    applyChunkAddition(txn, chunkStartOffsets,\n+                                            chunkAddedRecord.getSegmentName(),\n+                                            nullToEmpty(chunkAddedRecord.getOldChunkName()),\n+                                            chunkAddedRecord.getNewChunkName(),\n+                                            chunkAddedRecord.getOffset());\n+                                }\n+\n+                                // TruncationRecord.\n+                                if (record instanceof TruncationRecord) {\n+                                    val truncationRecord = (TruncationRecord) record;\n+                                    finalTruncateOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getOffset());\n+                                    finalFirstChunkStartsAtOffsets.put(truncationRecord.getSegmentName(), truncationRecord.getStartOffset());\n+                                }\n+                            }\n+                        }\n+                    } catch (EOFException e) {\n+                        log.debug(\"SystemJournal[{}] Done processing file {}.\", containerId, systemLogName);\n+                        break;\n+                    }\n+                }\n+                // Move to next file.\n+                fileIndexToRecover++;\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Adjusts the lengths of last chunks for each segment.\n+     */\n+    private void adjustLastChunkLengths(MetadataTransaction txn) throws StorageMetadataException, ChunkStorageException {\n+        for (String systemSegment : systemSegments) {\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+            // Update length of last chunk in metadata to what we actually find on LTS.\n+            if (null != segmentMetadata.getLastChunk()) {\n+                val chunkInfo = chunkStorage.getInfo(segmentMetadata.getLastChunk());\n+                long length = chunkInfo.getLength();\n+\n+                ChunkMetadata lastChunk = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                Preconditions.checkState(null != lastChunk);\n+                lastChunk.setLength(length);\n+                txn.update(lastChunk);\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + length);\n+            }\n+            Preconditions.checkState(segmentMetadata.isOwnershipChanged());\n+            segmentMetadata.checkInvariants();\n+            txn.update(segmentMetadata);\n+        }\n+    }\n+\n+    /**\n+     * Apply last effective truncate offsets.\n+     */\n+    private void applyFinalTruncateOffsets(MetadataTransaction txn, HashMap<String, Long> finalTruncateOffsets, HashMap<String, Long> finalFirstChunkStartsAtOffsets) throws StorageMetadataException {\n+        for (String systemSegment : systemSegments) {\n+            if (finalTruncateOffsets.containsKey(systemSegment)) {\n+                val truncateAt = finalTruncateOffsets.get(systemSegment);\n+                val firstChunkStartsAt = finalFirstChunkStartsAtOffsets.get(systemSegment);\n+                applyTruncate(txn, systemSegment, truncateAt, firstChunkStartsAt);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Apply chunk addition.\n+     */\n+    private void applyChunkAddition(MetadataTransaction txn, HashMap<String, Long> chunkStartOffsets, String segmentName, String oldChunkName, String newChunkName, long offset) throws StorageMetadataException {\n+        Preconditions.checkState(null != oldChunkName);\n+        Preconditions.checkState(null != newChunkName && !newChunkName.isEmpty());\n+\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+\n+        // set length.\n+        segmentMetadata.setLength(offset);\n+\n+        val newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        txn.create(newChunkMetadata);\n+        txn.markPinned(newChunkMetadata);\n+\n+        chunkStartOffsets.put(newChunkName, offset);\n+        // Set first and last pointers.\n+        if (!oldChunkName.isEmpty()) {\n+            ChunkMetadata oldChunk = (ChunkMetadata) txn.get(oldChunkName);\n+            Preconditions.checkState(null != oldChunk);\n+\n+            // In case the old segment store was still writing some zombie chunks when ownership changed\n+            // then new offset may invalidate tail part of chunk list.\n+            // Note that chunk with oldChunkName is still valid, it is the chunks after this that become invalid.\n+            String toDelete = oldChunk.getNextChunk();\n+            while (toDelete != null) {\n+                ChunkMetadata chunkToDelete = (ChunkMetadata) txn.get(toDelete);\n+                txn.delete(toDelete);\n+                toDelete = chunkToDelete.getNextChunk();\n+                segmentMetadata.decrementChunkCount();\n+            }\n+\n+            // Set next chunk\n+            oldChunk.setNextChunk(newChunkName);\n+\n+            // Set length\n+            long oldLength = chunkStartOffsets.get(oldChunkName);\n+            oldChunk.setLength(offset - oldLength);\n+\n+            txn.update(oldChunk);\n+        } else {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+            segmentMetadata.setStartOffset(offset);\n+        }\n+        segmentMetadata.setLastChunk(newChunkName);\n+        segmentMetadata.setLastChunkStartOffset(offset);\n+        segmentMetadata.incrementChunkCount();\n+        segmentMetadata.checkInvariants();\n+        // Save the segment metadata.\n+        txn.update(segmentMetadata);\n+    }\n+\n+    private String getSystemJournalFileName() {\n+        return getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private String getSystemJournalFileName(int containerId, long epoch, long currentFileIndex) {\n+        return NameUtils.getSystemJournalFileName(containerId, epoch, currentFileIndex);\n+    }\n+\n+    private ChunkHandle getChunkHandleForSystemJournal() throws ChunkStorageException {\n+        ChunkHandle h;\n+        val systemLogName = getSystemJournalFileName();\n+        try {\n+            h = chunkStorage.openWrite(systemLogName);\n+        } catch (ChunkNotFoundException e) {\n+            h = chunkStorage.create(systemLogName);\n+        }\n+        return h;\n+    }\n+\n+    /**\n+     * Apply truncate action to the segment metadata.\n+     */\n+    private void applyTruncate(MetadataTransaction txn, String segmentName, long truncateAt, long firstChunkStartsAt) throws StorageMetadataException {\n+        SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(segmentName);\n+        segmentMetadata.checkInvariants();\n+        String currentChunkName = segmentMetadata.getFirstChunk();\n+        ChunkMetadata currentMetadata;\n+        long startOffset = segmentMetadata.getFirstChunkStartOffset();\n+        while (null != currentChunkName) {\n+            currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+            // If for given chunk start <= truncateAt < end  then we have found the chunk that will be the first chunk.\n+            if ((startOffset <= truncateAt) && (startOffset + currentMetadata.getLength() > truncateAt)) {\n+                break;\n+            }\n+\n+            startOffset += currentMetadata.getLength();\n+            // move to next chunk\n+            currentChunkName = currentMetadata.getNextChunk();\n+            txn.delete(currentMetadata.getName());\n+            segmentMetadata.decrementChunkCount();\n+        }\n+        Preconditions.checkState(firstChunkStartsAt == startOffset);\n+        segmentMetadata.setFirstChunk(currentChunkName);\n+        if (null == currentChunkName) {\n+            segmentMetadata.setLastChunk(null);\n+            segmentMetadata.setLastChunkStartOffset(firstChunkStartsAt);\n+        }\n+        segmentMetadata.setStartOffset(truncateAt);\n+        segmentMetadata.setFirstChunkStartOffset(firstChunkStartsAt);\n+        segmentMetadata.checkInvariants();\n+\n+    }\n+\n+    public void validateAndSaveSnapshot(MetadataTransaction txn) throws Exception {\n+        SystemSnapshotRecord systemSnapshot = SystemSnapshotRecord.builder()\n+                .epoch(epoch)\n+                .segmentSnapshotRecords(new ArrayList<>())\n+                .build();\n+\n+        for (String systemSegment : systemSegments) {\n+            // Find segment metadata.\n+            SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(systemSegment);\n+            segmentMetadata.checkInvariants();\n+\n+            SegmentSnapshotRecord segmentSnapshot = SegmentSnapshotRecord.builder()\n+                    .segmentMetadata(segmentMetadata)\n+                    .chunkMetadataCollection(new ArrayList<>())\n+                    .build();\n+\n+            // Enumerate all chunks.\n+            String currentChunkName = segmentMetadata.getFirstChunk();\n+            ChunkMetadata currentMetadata = null;\n+            long dataSize = 0;\n+            long chunkCount = 0;\n+            while (null != currentChunkName) {\n+                currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n+\n+                val chunkInfo = chunkStorage.getInfo(currentChunkName);\n+                dataSize += currentMetadata.getLength();\n+                chunkCount++;\n+                Preconditions.checkState(chunkInfo.getLength() >= currentMetadata.getLength(),\n+                        \"Wrong chunk length chunkInfo=%d, currentMetadata=%d.\", chunkInfo.getLength(), currentMetadata.getLength());\n+\n+                segmentSnapshot.chunkMetadataCollection.add(currentMetadata);\n+                // move to next chunk\n+                currentChunkName = currentMetadata.getNextChunk();\n+            }\n+\n+            // Validate\n+            Preconditions.checkState(chunkCount == segmentMetadata.getChunkCount(), \"Wrong chunk count.\");\n+            Preconditions.checkState(dataSize == segmentMetadata.getLength() - segmentMetadata.getFirstChunkStartOffset(), \"Data size does not match dataSize.\");\n+\n+            // Add to the system snapshot.\n+            systemSnapshot.segmentSnapshotRecords.add(segmentSnapshot);\n+        }\n+\n+        // Write snapshot\n+        val snapshotFile = getSystemJournalFileName(containerId, epoch, 0);\n+        ChunkHandle h = chunkStorage.create(snapshotFile);\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = SYSTEM_SNAPSHOT_SERIALIZER.serialize(systemSnapshot);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+        }\n+        currentFileIndex++;\n+    }\n+\n+    /**\n+     * Indicates whether given segment is a system segment.\n+     *\n+     * @param segmentName Name of the sgement to check.\n+     * @return True if given segment is a system segment.\n+     */\n+    public boolean isStorageSystemSegment(String segmentName) {\n+        if (segmentName.startsWith(systemSegmentsPrefix)) {\n+            for (String systemSegment : systemSegments) {\n+                if (segmentName.equals(systemSegment)) {\n+                    return true;\n+                }\n+            }\n+        }\n+        return false;\n+    }\n+\n+    /**\n+     * Gets the names of the critical storage segments.\n+     *\n+     * @param containerId Container if of the owner container.\n+     * @return Array of names of the critical storage segments.\n+     */\n+    public static String[] getChunkStorageSystemSegments(int containerId) {\n+        return new String[]{\n+                NameUtils.getStorageMetadataSegmentName(containerId),\n+                NameUtils.getAttributeSegmentName(NameUtils.getStorageMetadataSegmentName(containerId)),\n+                NameUtils.getMetadataSegmentName(containerId),\n+                NameUtils.getAttributeSegmentName(NameUtils.getMetadataSegmentName(containerId))\n+        };\n+    }\n+\n+    /**\n+     * Represents a system journal record.\n+     */\n+    @Data\n+    public static class SystemJournalRecord {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzNzI0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446237246", "bodyText": "not really.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3Mzk3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273970", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEwOTIwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446110913", "bodyText": "For readability and organization, should we move this up in the class definition? to be the first after the constructor?", "author": "fpj", "createdAt": "2020-06-26T10:48:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalFileName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI2ODA5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446268092", "bodyText": "It is already immediately after constructor.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T15:54:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Njk4MjUwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446982501", "bodyText": "No, this is after three methods after the constructor: initialize, commitRecord, commitRecords.", "author": "fpj", "createdAt": "2020-06-29T13:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAyNDAxMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447024012", "bodyText": "Ah okay. I was confused about ChunkManager class where boostrap is immediately after constructor.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T14:41:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzIxOTg1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447219850", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-06-29T20:01:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMDkxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446112237", "bodyText": "Do I understand right that this system journal has two primary public calls: commitRecord and commitRecords? The records as declared below in the class can be of four types.\nAlso, is the utilization of the system journal in a follow-up PR?", "author": "fpj", "createdAt": "2020-06-26T10:52:10Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks and truncation of segments.\n+ * This log is replayed when the ChunkManager is booted.\n+ * To avoid data corruption. Each instance writes to its own distinct log file.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkManagerConfig} for the {@link ChunkManager}.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkManagerConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalFileName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalFileName());\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEzNjE0Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446136146", "bodyText": "Also, is the utilization of the system journal in a follow-up PR?\n\nnever mind about this comment, I had ChunkManager marked as viewed and a simple search did not show the system journal being used.", "author": "fpj", "createdAt": "2020-06-26T11:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMTI2OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446231269", "bodyText": "yes bootstrap is called from ChunkManager.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:50:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjIzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjg5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446112894", "bodyText": "Maybe list what it is precisely that is recorded in this journal. The comment says that it records layout changes, and there are four types of records that can be recorded.", "author": "fpj", "createdAt": "2020-06-26T10:53:55Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,940 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3Mzc5MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273790", "bodyText": "updated", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:04:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExMjg5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446115083", "bodyText": "Change it back, please.", "author": "fpj", "createdAt": "2020-06-26T10:59:21Z", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/mocks/InMemoryStorageTests.java", "diffHunk": "@@ -5,7 +5,7 @@\n  * you may not use this file except in compliance with the License.\n  * You may obtain a copy of the License at\n  *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n+ * http://www.apache.org/licenses/LICENSE-2.0", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMTgyMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446231822", "bodyText": "oops..formatting file messed it up.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:50:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjI3MzY1MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446273651", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T16:03:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjExNTA4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyOTU0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446129542", "bodyText": "What pattern have we been using for exceptions? I'm wondering whether we should group the subtypes in the same class definition like in KeeperException:\nhttps://zookeeper.apache.org/doc/r3.6.0/apidocs/zookeeper-server/index.html?org/apache/zookeeper/KeeperException.html\nI don't necessarily want to deviate from how we have been doing it elsewhere in the code.", "author": "fpj", "createdAt": "2020-06-26T11:33:49Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/StorageMetadataException.java", "diffHunk": "@@ -0,0 +1,34 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+/**\n+ * Exception related to storage metadata operations.\n+ */\n+public class StorageMetadataException extends Exception {", "originalCommit": "90c2dd483f8507bb02048504a166fe71ae6a82ae", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjIzMjI5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r446232292", "bodyText": "We are following current convention in the code.", "author": "sachin-j-joshi", "createdAt": "2020-06-26T14:51:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjEyOTU0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447016466", "bodyText": "Can you justify the use of ConcurrentHashMap vs. other data structure implementations, e.g., LinkedHashMap? I'm thinking that it would avoid the whole management of generations and a full iteration over the map upon eviction.", "author": "fpj", "createdAt": "2020-06-29T14:31:12Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();", "originalCommit": "2c3e4b6cbd0a6aa7519b1c7d46c3f288e0a466d0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MDMzOA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447040338", "bodyText": "Conceptually - as entries are found and used, they need to be moved to the end of list to keep LRU order.\nLinkedHashMap keeps order by of insertion order, which is different than what we want.", "author": "sachin-j-joshi", "createdAt": "2020-06-29T15:03:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5ODAxNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447998015", "bodyText": "Resolving as per discussion offline. Tracking this here #4902", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:46:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxNjQ2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAxODY2MQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447018661", "bodyText": "I'm not sure I follow why we need to update the generation upon truncation.", "author": "fpj", "createdAt": "2020-06-29T14:34:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.\n+ * Eviction is performed only when number of segments or chunks exceeds certain given limits.\n+ * Each time items are evicted the generation is incremented.\n+ * For each entry in cache, we keep track of the generation in which it was last accessed.\n+ * During eviction items with oldest generation are evicted first until enough objects are evicted.\n+ * The least accessed segments are removed entirely before removing chunks from more recently used segments.\n+ * This calculation is \"best effort\" and need not be accurate.\n+ */\n+class ReadIndexCache {\n+\n+    /**\n+     * Max number of indexed segments to keep in cache.\n+     */\n+    private final int maxIndexedSegments;\n+\n+    /**\n+     * Max number of indexed chunks to keep in cache.\n+     */\n+    private final int maxIndexedChunks;\n+\n+    /**\n+     * Max number of indexed chunks to keep per segment in cache.\n+     */\n+    private final int maxIndexedChunksPerSegment;\n+\n+    /**\n+     * Current generation of cache entries.\n+     */\n+    private final AtomicLong currentGeneration = new AtomicLong();\n+\n+    /**\n+     * Lowest generation of cache entries.\n+     */\n+    private final AtomicLong oldestGeneration = new AtomicLong();\n+\n+    /**\n+     * Total number of chunks in the cache.\n+     */\n+    private final AtomicInteger totalChunkCount = new AtomicInteger();\n+\n+    /**\n+     * Index of chunks for a segment by their start offsets.\n+     */\n+    private final ConcurrentHashMap<String, SegmentReadIndex> segmentsToReadIndexMap = new ConcurrentHashMap<String, SegmentReadIndex>();\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param maxIndexedSegments\n+     * @param maxIndexedChunksPerSegment\n+     * @param maxIndexedChunks\n+     */\n+    public ReadIndexCache(int maxIndexedSegments, int maxIndexedChunksPerSegment, int maxIndexedChunks) {\n+        this.maxIndexedChunksPerSegment = maxIndexedChunksPerSegment;\n+        this.maxIndexedSegments = maxIndexedSegments;\n+        this.maxIndexedChunks = maxIndexedChunks;\n+    }\n+\n+    /**\n+     * Retrieves the read index for given segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @return Read index corresponding to the given segment. A new empty index is created if it doesn't already exist.\n+     */\n+    private SegmentReadIndex getSegmentReadIndex(String streamSegmentName) {\n+        SegmentReadIndex readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+\n+        if (null == readIndex) {\n+            // Evict segments if required.\n+            if (maxIndexedSegments < segmentsToReadIndexMap.size() + 1 || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictSegmentsFromOldestGeneration();\n+            }\n+            val newReadIndex = SegmentReadIndex.builder()\n+                    .chunkIndex(new ConcurrentSkipListMap<Long, SegmentReadIndexEntry>())\n+                    .generation(new AtomicLong(currentGeneration.get()))\n+                    .build();\n+            val oldReadIndex = segmentsToReadIndexMap.putIfAbsent(streamSegmentName, newReadIndex);\n+            readIndex = null != oldReadIndex ? oldReadIndex : newReadIndex;\n+        }\n+\n+        return readIndex;\n+    }\n+\n+    /**\n+     * Gets total number of chunks in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalChunksCount() {\n+        return totalChunkCount.get();\n+    }\n+\n+    /**\n+     * Gets total number of segments in cache.\n+     *\n+     * @return\n+     */\n+    public int getTotalSegmentCount() {\n+        return segmentsToReadIndexMap.size();\n+    }\n+\n+    /**\n+     * Gets oldest generation in cache.\n+     *\n+     * @return\n+     */\n+    public long getOldestGeneration() {\n+        return oldestGeneration.get();\n+    }\n+\n+    /**\n+     * Gets current generation of cache.\n+     *\n+     * @return\n+     */\n+    public long getCurrentGeneration() {\n+        return currentGeneration.get();\n+    }\n+\n+    /**\n+     * Adds a new index entry for a given chunk in index for the segment.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param chunkName         Name of the chunk.\n+     * @param startOffset       Start offset of the chunk.\n+     */\n+    public void addIndexEntry(String streamSegmentName, String chunkName, long startOffset) {\n+        if (null != chunkName) {\n+            val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+\n+            // Evict chunks if required.\n+            if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + 1\n+                    || maxIndexedChunks < totalChunkCount.get() + 1) {\n+                evictChunks(streamSegmentName, 1);\n+            }\n+\n+            segmentReadIndex.chunkIndex.put(startOffset,\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(chunkName)\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+            totalChunkCount.incrementAndGet();\n+        }\n+    }\n+\n+    /**\n+     * Updates read index for given segment with new entries.\n+     *\n+     * @param streamSegmentName\n+     * @param newReadIndexEntries List of {@link ChunkNameOffsetPair} for new entries.\n+     */\n+    public void addIndexEntries(String streamSegmentName, List<ChunkNameOffsetPair> newReadIndexEntries) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        // Evict chunks if required.\n+        if (maxIndexedChunksPerSegment < segmentReadIndex.chunkIndex.size() + newReadIndexEntries.size()\n+                || maxIndexedChunks < totalChunkCount.get() + newReadIndexEntries.size()) {\n+            evictChunks(streamSegmentName, newReadIndexEntries.size());\n+        }\n+\n+        for (val entry : newReadIndexEntries) {\n+            segmentReadIndex.chunkIndex.put(entry.getOffset(),\n+                    SegmentReadIndexEntry.builder()\n+                            .chunkName(entry.getChunkName())\n+                            .generation(new AtomicLong(currentGeneration.get()))\n+                            .build());\n+            segmentReadIndex.generation.set(currentGeneration.get());\n+\n+        }\n+        totalChunkCount.getAndAdd(newReadIndexEntries.size());\n+    }\n+\n+    /**\n+     * Removes the given segment from cache.\n+     *\n+     * @param streamSegmentName\n+     */\n+    public void remove(String streamSegmentName) {\n+        val readIndex = segmentsToReadIndexMap.get(streamSegmentName);\n+        if (null != readIndex) {\n+            segmentsToReadIndexMap.remove(streamSegmentName);\n+            totalChunkCount.getAndAdd(-1 * readIndex.chunkIndex.size());\n+        }\n+    }\n+\n+    /**\n+     * Finds a chunk that is floor to the given offset.\n+     *\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset for which to search.\n+     * @return\n+     */\n+    public ChunkNameOffsetPair findFloor(String streamSegmentName, long offset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (segmentReadIndex.chunkIndex.size() > 0) {\n+            val floorEntry = segmentReadIndex.chunkIndex.floorEntry(offset);\n+            if (null != floorEntry) {\n+                // mark with current generation\n+                segmentReadIndex.generation.set(currentGeneration.get());\n+                floorEntry.getValue().generation.set(currentGeneration.get());\n+                // return value.\n+                return new ChunkNameOffsetPair(floorEntry.getKey(), floorEntry.getValue().getChunkName());\n+            }\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Truncates the read index for given segment by removing all the chunks that are below given offset.\n+     *\n+     * @param streamSegmentName\n+     * @param startOffset\n+     */\n+    public void truncateReadIndex(String streamSegmentName, long startOffset) {\n+        val segmentReadIndex = getSegmentReadIndex(streamSegmentName);\n+        if (null != segmentReadIndex) {\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                val headMap = segmentReadIndex.chunkIndex.headMap(startOffset);\n+                if (null != headMap) {\n+                    int removed = 0;\n+                    ArrayList<Long> keysToRemove = new ArrayList<Long>();\n+                    keysToRemove.addAll(headMap.keySet());\n+                    for (val keyToRemove : keysToRemove) {\n+                        segmentReadIndex.chunkIndex.remove(keyToRemove);\n+                        removed++;\n+                    }\n+                    if (removed > 0) {\n+                        totalChunkCount.getAndAdd(-1 * removed);\n+                    }\n+                }\n+            }\n+            if (segmentReadIndex.chunkIndex.size() > 0) {\n+                segmentReadIndex.generation.set(currentGeneration.get());", "originalCommit": "2c3e4b6cbd0a6aa7519b1c7d46c3f288e0a466d0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NTc2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447565760", "bodyText": "Nit: are we using these tags in the rest of documentation? If not, it would be great to keep the same format.", "author": "RaulGracia", "createdAt": "2020-06-30T10:02:22Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447567701", "bodyText": "Nit: I don't remember any place in the code in which we describe in the Javadoc the places where are a class is tested. One of the reasons may be that this information can get quickly outdated as there is a new test class, or a rename or rellocation of one of the existing test classes. This forces the developer to be aware of updating the documentation of a class based on a work related to test, which looks error-prone.", "author": "RaulGracia", "createdAt": "2020-06-30T10:05:53Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0ODI0MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447748240", "bodyText": "This is given as a guidance that  derived classes should be tested using this set of tests. This is helpful reminder for anyone implementing this class.\nWe don't have a separate compatibility test suite as of now.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NTU2Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448895566", "bodyText": "I think that any developer is able to locate where a given class is being tested, right? I'm not saying that this information is not useful, what I'm saying is that, as far as I'm aware, nowhere else in the Javadoc we enumerate the test classes related to the class, so this is a comment about consistency. Also, I suspect that this may introduce additional maintenance burden, as we need to keep this part of the Javadoc updated for any changes related to test classes.", "author": "RaulGracia", "createdAt": "2020-07-02T10:09:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTAxMzEwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449013103", "bodyText": "This is intentional.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T13:45:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2NzcwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU2OTEyNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447569125", "bodyText": "Nit: this is not adding much information.", "author": "RaulGracia", "createdAt": "2020-06-30T10:08:21Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/BaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ * <div>\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ * </div>\n+ * <div>\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkManager will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ * </div>\n+ *\n+ * <div>\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkManagerRollingTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ * </div>\n+ */\n+@Slf4j\n+public abstract class BaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Constructor.\n+     */\n+    public BaseChunkStorage() {\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public boolean exists(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+\n+        // Call concrete implementation.\n+        boolean retValue = checkExists(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public ChunkHandle create(String chunkName) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doCreate(chunkName);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+        log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public void delete(ChunkHandle handle) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        doDelete(handle);\n+\n+        // Record metrics.\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+        log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenRead(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openRead\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Opens chunk for Write (or modifications).\n+     *\n+     * @param chunkName String name of the chunk to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkHandle openWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunkName, \"chunkName must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkHandle handle = doOpenWrite(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"openWrite\", traceId, chunkName);\n+\n+        return handle;\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public ChunkInfo getInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkNotNull(chunkName);\n+        long traceId = LoggerHelpers.traceEnter(log, \"getInfo\", chunkName);\n+\n+        // Call concrete implementation.\n+        ChunkInfo info = doGetInfo(chunkName);\n+\n+        LoggerHelpers.traceLeave(log, \"getInfo\", traceId, chunkName);\n+\n+        return info;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying chunk.\n+     *\n+     * @param handle       ChunkHandle of the chunk to read from.\n+     * @param fromOffset   Offset in the chunk from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds or offset is not a valid offset in the underlying file/object.\n+     */\n+    @Override\n+    final public int read(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle\");\n+        Preconditions.checkArgument(null != buffer, \"buffer\");\n+        Preconditions.checkArgument(fromOffset >= 0, \"fromOffset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0 && length <= buffer.length, \"length\");\n+        Preconditions.checkElementIndex(bufferOffset, buffer.length, \"bufferOffset\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"read\", handle.getChunkName(), fromOffset, bufferOffset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesRead = doRead(handle, fromOffset, length, buffer, bufferOffset);\n+\n+        Duration elapsed = timer.getElapsed();\n+        ChunkStorageMetrics.READ_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.READ_BYTES.add(bytesRead);\n+\n+        log.debug(\"Read - chunk={}, offset={}, bytesRead={}, latency={}.\", handle.getChunkName(), fromOffset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesRead);\n+\n+        return bytesRead;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying chunk.\n+     *\n+     * <ul>\n+     * <li>It is expected that in cases where it can not overwrite the existing data at given offset, the implementation should throw IndexOutOfBoundsException.</li>\n+     * For storage where underlying files/objects are immutable once written, the implementation should return false on {@link ChunkStorage#supportsAppend()}.\n+     * <li>In such cases only valid offset is 0.</li>\n+     * <li>For storages where underlying files/objects can only be appended but not overwritten, it must match actual current length of underlying file/object.</li>\n+     * <li>In all cases the offset can not be greater that actual current length of underlying file/object. </li>\n+     * </ul>\n+     * @param handle ChunkHandle of the chunk to write to.\n+     * @param offset Offset in the chunk to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public int write(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(null != data, \"data must not be null\");\n+        Preconditions.checkArgument(offset >= 0, \"offset must be non-negative\");\n+        Preconditions.checkArgument(length >= 0, \"length must be non-negative\");\n+        if (!supportsAppend()) {\n+            Preconditions.checkArgument(offset == 0, \"offset must be 0 because storage does not support appends.\");\n+        }\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"write\", handle.getChunkName(), offset, length);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int bytesWritten = doWrite(handle, offset, length, data);\n+\n+        Duration elapsed = timer.getElapsed();\n+\n+        ChunkStorageMetrics.WRITE_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.WRITE_BYTES.add(bytesWritten);\n+\n+        log.debug(\"Write - chunk={}, offset={}, bytesWritten={}, latency={}.\", handle.getChunkName(), offset, length, elapsed.toMillis());\n+        LoggerHelpers.traceLeave(log, \"read\", traceId, bytesWritten);\n+\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks. The first chunk is concatenated to.\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be concatenated together.\n+     *               The chunks must be concatenated in the same sequence the arguments are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public int concat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        checkConcatArgs(chunks);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"concat\", chunks[0].getName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        int retValue = doConcat(chunks);\n+\n+        Duration elapsed = timer.getElapsed();\n+        log.debug(\"concat - target={}, latency={}.\", chunks[0].getName(), elapsed.toMillis());\n+\n+        ChunkStorageMetrics.CONCAT_LATENCY.reportSuccessEvent(elapsed);\n+        ChunkStorageMetrics.CONCAT_BYTES.add(retValue);\n+        ChunkStorageMetrics.CONCAT_COUNT.inc();\n+        ChunkStorageMetrics.LARGE_CONCAT_COUNT.inc();\n+\n+        LoggerHelpers.traceLeave(log, \"concat\", traceId, chunks[0].getName());\n+\n+        return retValue;\n+    }\n+\n+    private void checkConcatArgs(ConcatArgument[] chunks) {\n+        // Validate parameters\n+        Preconditions.checkArgument(null != chunks, \"chunks must not be null\");\n+        Preconditions.checkArgument(chunks.length >= 2, \"There must be at least two chunks\");\n+\n+        Preconditions.checkArgument(null != chunks[0], \"target chunk must not be null\");\n+        Preconditions.checkArgument(chunks[0].getLength() >= 0, \"target chunk lenth must be non negative.\");\n+\n+        for (int i = 1; i < chunks.length; i++) {\n+            Preconditions.checkArgument(null != chunks[i], \"source chunk must not be null\");\n+            Preconditions.checkArgument(chunks[i].getLength() >= 0, \"source chunk lenth must be non negative.\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[0].getName()), \"source chunk is same as target\");\n+            Preconditions.checkArgument(!chunks[i].getName().equals(chunks[i - 1].getName()), \"duplicate chunk found\");\n+        }\n+    }\n+\n+    /**\n+     * Truncates a given chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to truncate.\n+     * @param offset Offset to truncate to.\n+     * @return True if the object was truncated, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean truncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+        Preconditions.checkArgument(offset > 0, \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doTruncate(handle, offset);\n+\n+        LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Sets readonly attribute for the chunk.\n+     *\n+     * @param handle     ChunkHandle of the chunk.\n+     * @param isReadonly True if chunk is set to be readonly.\n+     * @return True if the operation was successful, false otherwise.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    final public boolean setReadOnly(ChunkHandle handle, boolean isReadonly) throws ChunkStorageException, UnsupportedOperationException {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"setReadOnly\", handle.getChunkName());\n+\n+        // Call concrete implementation.\n+        boolean retValue = doSetReadOnly(handle, isReadonly);\n+\n+        LoggerHelpers.traceLeave(log, \"setReadOnly\", traceId, handle.getChunkName());\n+\n+        return retValue;\n+    }\n+\n+    /**\n+     * Closes.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447572530", "bodyText": "Do you know how long this list can get in the worst case?", "author": "RaulGracia", "createdAt": "2020-06-30T10:14:25Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4ODczMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447588730", "bodyText": "Also, I see that you use this list only under a synchronized block. In this case, maybe you should add the annotation GuardedBy here to make it more explicit and create awareness that this list needs to be safely accessed.", "author": "RaulGracia", "createdAt": "2020-06-30T10:44:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMzQ1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447733457", "bodyText": "There is no theoretical upper bound.\n#4903 We plan to revisit this.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3MjUzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3Nzc3OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447577778", "bodyText": "While I may be missing some deep details here, I have doubt that we need another cache implementation in the code. As this PR is very large and we need to reduce potential sources of issues, I really recommend to evaluate if we can use here a Guava cache as the Controller is already doing: \n  \n    \n      pravega/controller/src/main/java/io/pravega/controller/store/stream/AbstractStreamMetadataStore.java\n    \n    \n         Line 75\n      in\n      de7f435\n    \n    \n    \n    \n\n        \n          \n           private final LoadingCache<String, Scope> scopeCache; \n        \n    \n  \n\n\nThe more code in this PR, the slower the reviews are and the higher the uncertainty, so please consider this option to make this PR more lightweight in case that the Guava cache fulfills the main requirements that led you to write your own cache. If it is not the case, I also recommend to move this class to common, so others can use this data structure if it brings a functionality that does not exist yet in the current code/dependencies.", "author": "RaulGracia", "createdAt": "2020-06-30T10:23:46Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadIndexCache.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.TreeMap;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentSkipListMap;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+/**\n+ * An in-memory implementation of cache for read index that maps chunk start offset to chunk name for recently used segments.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczMDE5Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447730192", "bodyText": "That's the plan #4902 . However this change is getting bigger and it is better to do it as a separate PR.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:31:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU3Nzc3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4MzAwMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447583001", "bodyText": "Can this delay significantly a write operation?", "author": "RaulGracia", "createdAt": "2020-06-30T10:33:16Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyMDExMQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447720111", "bodyText": "This is called when write operation has already failed.\nThe GC will be implemented on background thread in near future #4903", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:19:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4MzAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4NzIzNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447587234", "bodyText": "If this check fails it will throw an IllegalStateException, but I don't see a handling for this in the catch block. What would happen in this case?", "author": "RaulGracia", "createdAt": "2020-06-30T10:41:17Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczNDk1Ng==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447734956", "bodyText": "This is an extra ordinary situation. This exception needs to be bubbled back to the caller.  An error will be logged in SS by calling code.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:37:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4NzIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447587780", "bodyText": "Nit: systemJournal != null seems more readable.", "author": "RaulGracia", "createdAt": "2020-06-30T10:42:24Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyODI3MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447728270", "bodyText": "I'm an old C/C++ programmer , some habits die hard :).\nLittle fun background .\nIn those language any non-zero value is considered true and assignment operator returns value. So if by mistake you wrote if (systemJournal = NULL) ... that is a valid code and you had unexpectedly assigned  null to systemJournal. Doing it if (NULL = systemJournal )  prevents assignment to const and catches syntax errors.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:29:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NzA5Mw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447997093", "bodyText": "skipping...forgot but will fix in general in future", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:44:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU4Nzc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTM2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447591360", "bodyText": "We really need to evaluate the performance of this method. The reason is the following: if by some reason this is slow, given the amount of computations inside this method, it will greatly limit the throughput of the whole system, specially for scenarios with 1 or few segments in which parallelism is limited. So, could you please deploy this branch of Pravega, create a 1-segment Stream and see the throughput when writing large events?", "author": "RaulGracia", "createdAt": "2020-06-30T10:48:52Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzczOTkzNw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447739937", "bodyText": "For \"happy path\" of appending to existing chunk this method is fast. It directly writes to the existing chunk. The metadata is not written to underlying store in this case. In system tests most writes are complete under 3 ms. (haven't run a perf run but don't expect a vastly different result. But will do it shortly.)\nIn case of need to add new chunk there is small extra overhead of creating a new chunk, updating and storing metadata.\nFor storage system segments, it is necessary to journal this change in system journal.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:44:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MTM2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjgwNQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447592805", "bodyText": "Using targetLastChunk != null && shouldDefrag() will execute fewer times shouldDefrag() in the case it contains some more complex logic.", "author": "RaulGracia", "createdAt": "2020-06-30T10:51:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc0NTk1Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447745957", "bodyText": "null == targetLastChunk means it is an empty segment.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:51:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MjgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447593395", "bodyText": "Why this is left after the logging? This is still part of the logic, right? Logging and traceLeave() should be the last part of the execution in most of the cases.", "author": "RaulGracia", "createdAt": "2020-06-30T10:52:39Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxODk0OA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447718948", "bodyText": "good point. will fix.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjgzMg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996832", "bodyText": "fixed 1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5MzM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5Mzg0Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447593842", "bodyText": "Typo: throughputand -> throughput and", "author": "RaulGracia", "createdAt": "2020-06-30T10:53:27Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjY1OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996659", "bodyText": "fixed. 1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5Mzg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NDI1Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447594252", "bodyText": "You can remove -i.e., write-", "author": "RaulGracia", "createdAt": "2020-06-30T10:54:18Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NTk3Mg==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447995972", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:42:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NDI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NTE4Nw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447595187", "bodyText": "While I understand in theory Consequently, a native option for merging is desirable may be true, I'm not so sure that based on our real experience with this option relying on this feature is a great idea. We can end up in a situation in which, if the native concat is not efficient in the system at hand, we can get real problems that we cannot solve because we are relying on logic implemented by third parties.", "author": "RaulGracia", "createdAt": "2020-06-30T10:56:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxNjgzOQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447716839", "bodyText": "In the long term the storage optimizer will be running on the background thread.\nNote that currently defrag is called only with concat.\nThere are two settings  minSizeLimitForConcat and maxSizeLimitForConcat to control when concat is invoked (vs data is copied using appends). We'll measure and tune how often and how much to defrag.", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:14:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5NTE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5ODY1NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447598654", "bodyText": "If I'm not mistaken, you are executing this logic as part of concat() and, in addition to that, you are executing collectGarbage() as part of write(). These are the two most performance critical operations, as they will dictate the write throughput and the transaction throughput to Tier 2, respectively. Please, consider if these 2 tasks should be executed on a regular basis by an independent service instead of adding this burden to the regular write activity of Tier 2.", "author": "RaulGracia", "createdAt": "2020-06-30T11:03:09Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManager.java", "diffHunk": "@@ -0,0 +1,1308 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+public class ChunkManager implements Storage {\n+    /**\n+     * Configuration options for this ChunkManager instance.\n+     */\n+    @Getter\n+    private final ChunkManagerConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkManager#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkManager#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage ChunkStorage instance.\n+     * @param executor     An Executor for async operations.\n+     * @param config       Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Creates a new instance of the ChunkManager class.\n+     *\n+     * @param chunkStorage  ChunkStorage instance.\n+     * @param metadataStore Metadata store.\n+     * @param executor      An Executor for async operations.\n+     * @param config        Configuration options for this ChunkManager instance.\n+     */\n+    public ChunkManager(ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, Executor executor, ChunkManagerConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+        this.readIndexCache = new ReadIndexCache(config.getMaxIndexedSegments(),\n+                config.getMaxIndexedChunksPerSegment(),\n+                config.getMaxIndexedChunks());\n+        this.closed = new AtomicBoolean(false);\n+    }\n+\n+    /**\n+     * Initializes the ChunkManager and bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @param metadataStore Metadata store.\n+     * @param containerId   container id.\n+     * @throws Exception In case of any errors.\n+     */\n+    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+        this.containerId = containerId;\n+        this.logPrefix = String.format(\"ChunkManager[%d]\", containerId);\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.systemJournal = new SystemJournal(containerId,\n+                epoch,\n+                chunkStorage,\n+                metadataStore,\n+                config);\n+\n+        // Now bootstrap\n+        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        this.systemJournal.bootstrap();\n+        log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n+    }\n+\n+    @Override\n+    public void initialize(long containerEpoch) {\n+        this.epoch = containerEpoch;\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                    claimOwnership(txn, segmentMetadata);\n+                }\n+                // If created by newer instance then abort.\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // This instance is the owner, return a handle.\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Checks ownership and adjusts the length of the segment if required.\n+     *\n+     * @param txn             Active {@link MetadataTransaction}.\n+     * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n+     * @throws ChunkStorageException In case of any chunk storage related errors.\n+     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     */\n+    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n+        // Claim ownership.\n+        // This is safe because the previous instance is definitely not an owner anymore. (even if this instance is no more owner)\n+        // If this instance is no more owner, then transaction commit will fail.So it is still safe.\n+        segmentMetadata.setOwnerEpoch(this.epoch);\n+        segmentMetadata.setOwnershipChanged(true);\n+\n+        // Get the last chunk\n+        String lastChunkName = segmentMetadata.getLastChunk();\n+        if (null != lastChunkName) {\n+            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n+            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                    logPrefix,\n+                    segmentMetadata.getName(),\n+                    lastChunk.getName(),\n+                    lastChunk.getLength());\n+            ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n+            Preconditions.checkState(chunkInfo != null);\n+            Preconditions.checkState(lastChunk != null);\n+            // Adjust its length;\n+            if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                lastChunk.setLength(chunkInfo.getLength());\n+                segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                txn.update(lastChunk);\n+                log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                        logPrefix,\n+                        segmentMetadata.getName(),\n+                        lastChunk.getName(),\n+                        chunkInfo.getLength());\n+            }\n+        }\n+        // Update and commit\n+        // If This instance is fenced this update will fail.\n+        txn.update(segmentMetadata);\n+        txn.commit();\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentHandle> create(String streamSegmentName, SegmentRollingPolicy rollingPolicy, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"create\", streamSegmentName, rollingPolicy);\n+            Timer timer = new Timer();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                // Retrieve metadata and make sure it does not exist.\n+                SegmentMetadata oldSegmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                if (null != oldSegmentMetadata) {\n+                    throw new StreamSegmentExistsException(streamSegmentName);\n+                }\n+\n+                // Create a new record.\n+                SegmentMetadata newSegmentMetatadata = SegmentMetadata.builder()\n+                        .name(streamSegmentName)\n+                        .maxRollinglength(rollingPolicy.getMaxLength() == 0 ? SegmentRollingPolicy.NO_ROLLING.getMaxLength() : rollingPolicy.getMaxLength())\n+                        .ownerEpoch(this.epoch)\n+                        .build();\n+\n+                newSegmentMetatadata.setActive(true);\n+                txn.create(newSegmentMetatadata);\n+                // commit.\n+                txn.commit();\n+\n+                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} create - segment={}, rollingPolicy={}, latency={}.\", logPrefix, streamSegmentName, rollingPolicy, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"create\", traceId, retValue);\n+                return retValue;\n+            } catch (StorageMetadataAlreadyExistsException ex) {\n+                throw new StreamSegmentExistsException(streamSegmentName, ex);\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> write(SegmentHandle handle, long offset, InputStream data, int length, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            Timer timer = new Timer();\n+\n+            // Validate preconditions.\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(null != data, \"data\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkArgument(null != streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            Preconditions.checkArgument(length >= 0, \"length\");\n+\n+            ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+            List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<ChunkNameOffsetPair>();\n+            int chunksAddedCount = 0;\n+            boolean isCommited = false;\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                boolean didSegmentLayoutChange = false;\n+\n+                // Retrieve metadata.\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                segmentMetadata.checkInvariants();\n+                checkNotSealed(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // Validate that offset is correct.\n+                if ((segmentMetadata.getLength()) != offset) {\n+                    throw new BadOffsetException(streamSegmentName, segmentMetadata.getLength(), offset);\n+                }\n+\n+                boolean isSystemSegment = isStorageSystemSegment(segmentMetadata);\n+\n+                // Check if this is a first write after ownership changed.\n+                boolean isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                ChunkMetadata lastChunkMetadata = null;\n+                ChunkHandle chunkHandle = null;\n+                int bytesRemaining = length;\n+                long currentOffset = offset;\n+\n+                // Get the last chunk segmentMetadata for the segment.\n+                if (null != segmentMetadata.getLastChunk()) {\n+                    lastChunkMetadata = (ChunkMetadata) txn.get(segmentMetadata.getLastChunk());\n+                }\n+\n+                while (bytesRemaining > 0) {\n+                    // Check if new chunk needs to be added.\n+                    // This could be either because there are no existing chunks or last chunk has reached max rolling length.\n+                    if (null == lastChunkMetadata\n+                            || (lastChunkMetadata.getLength() >= segmentMetadata.getMaxRollinglength())\n+                            || isFirstWriteAfterFailover\n+                            || !shouldAppend()) {\n+\n+                        // Create new chunk\n+                        String newChunkName = getNewChunkName(streamSegmentName,\n+                                segmentMetadata.getLength());\n+                        chunkHandle = chunkStorage.create(newChunkName);\n+\n+                        String previousLastChunkName = lastChunkMetadata == null ? null : lastChunkMetadata.getName();\n+\n+                        // update first and last chunks.\n+                        lastChunkMetadata = updateMetadataForChunkAddition(txn,\n+                                segmentMetadata,\n+                                newChunkName,\n+                                isFirstWriteAfterFailover,\n+                                lastChunkMetadata);\n+\n+                        // Record the creation of new chunk.\n+                        if (isSystemSegment) {\n+                            addSystemLogRecord(systemLogRecords,\n+                                    streamSegmentName,\n+                                    segmentMetadata.getLength(),\n+                                    previousLastChunkName,\n+                                    newChunkName);\n+                            txn.markPinned(lastChunkMetadata);\n+                        }\n+                        // Update read index.\n+                        newReadIndexEntries.add(new ChunkNameOffsetPair(segmentMetadata.getLength(), newChunkName));\n+\n+                        isFirstWriteAfterFailover = false;\n+                        didSegmentLayoutChange = true;\n+                        chunksAddedCount++;\n+\n+                        log.debug(\"{} write - New chunk added - segment={}, chunk={}, offset={}.\",\n+                                logPrefix, streamSegmentName, newChunkName, segmentMetadata.getLength());\n+                    } else {\n+                        // No new chunk needed just write data to existing chunk.\n+                        chunkHandle = chunkStorage.openWrite(lastChunkMetadata.getName());\n+                    }\n+\n+                    // Calculate the data that needs to be written.\n+                    long offsetToWriteAt = currentOffset - segmentMetadata.getLastChunkStartOffset();\n+                    int writeSize = (int) Math.min(bytesRemaining, segmentMetadata.getMaxRollinglength() - offsetToWriteAt);\n+\n+                    // Write data to last chunk.\n+                    int bytesWritten = writeToChunk(txn,\n+                            segmentMetadata,\n+                            offset,\n+                            data,\n+                            chunkHandle,\n+                            lastChunkMetadata,\n+                            offsetToWriteAt,\n+                            writeSize);\n+\n+                    // Update the counts\n+                    bytesRemaining -= bytesWritten;\n+                    currentOffset += bytesWritten;\n+                }\n+\n+                // Check invariants.\n+                segmentMetadata.checkInvariants();\n+\n+                // commit all system log records if required.\n+                if (isSystemSegment && chunksAddedCount > 0) {\n+                    // commit all system log records.\n+                    Preconditions.checkState(chunksAddedCount == systemLogRecords.size());\n+                    txn.setExternalCommitStep(() -> {\n+                        systemJournal.commitRecords(systemLogRecords);\n+                        return null;\n+                    });\n+                }\n+\n+                // if layout did not change then commit with lazyWrite.\n+                txn.commit(!didSegmentLayoutChange);\n+                isCommited = true;\n+\n+                // Post commit actions.\n+                // Update the read index.\n+                readIndexCache.addIndexEntries(streamSegmentName, newReadIndexEntries);\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} write - segment={}, offset={}, length={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, length, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"write\", traceId, handle, offset);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            } finally {\n+                if (!isCommited && chunksAddedCount > 0) {\n+                    // Collect garbage.\n+                    collectGarbage(newReadIndexEntries.stream().map(entry -> entry.getChunkName()).collect(Collectors.toList()));\n+                }\n+            }\n+        });\n+    }\n+\n+    /**\n+     * Updates the segment metadata for the newly added chunk.\n+     */\n+    private ChunkMetadata updateMetadataForChunkAddition(MetadataTransaction txn,\n+                                                         SegmentMetadata segmentMetadata,\n+                                                         String newChunkName,\n+                                                         boolean isFirstWriteAfterFailover,\n+                                                         ChunkMetadata lastChunkMetadata) throws StorageMetadataException {\n+        ChunkMetadata newChunkMetadata = ChunkMetadata.builder()\n+                .name(newChunkName)\n+                .build();\n+        segmentMetadata.setLastChunk(newChunkName);\n+        if (lastChunkMetadata == null) {\n+            segmentMetadata.setFirstChunk(newChunkName);\n+        } else {\n+            lastChunkMetadata.setNextChunk(newChunkName);\n+            txn.update(lastChunkMetadata);\n+        }\n+        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+\n+        // Reset ownershipChanged flag after first write is done.\n+        if (isFirstWriteAfterFailover) {\n+            segmentMetadata.setOwnerEpoch(this.epoch);\n+            segmentMetadata.setOwnershipChanged(false);\n+            log.debug(\"{} write - First write after failover - segment={}.\", logPrefix, segmentMetadata.getName());\n+        }\n+        segmentMetadata.incrementChunkCount();\n+\n+        // Update the transaction.\n+        txn.update(newChunkMetadata);\n+        txn.update(segmentMetadata);\n+        return newChunkMetadata;\n+    }\n+\n+    /**\n+     * Write to chunk.\n+     */\n+    private int writeToChunk(MetadataTransaction txn,\n+                             SegmentMetadata segmentMetadata,\n+                             long offset,\n+                             InputStream data,\n+                             ChunkHandle chunkHandle,\n+                             ChunkMetadata chunkWrittenMetadata,\n+                             long offsetToWriteAt,\n+                             int bytesCount) throws IOException, StorageMetadataException, BadOffsetException {\n+        int bytesWritten;\n+        Preconditions.checkState(0 != bytesCount, \"Attempt to write zero bytes\");\n+        try {\n+\n+            // Finally write the data.\n+            try (BoundedInputStream bis = new BoundedInputStream(data, bytesCount)) {\n+                bytesWritten = chunkStorage.write(chunkHandle, offsetToWriteAt, bytesCount, bis);\n+            }\n+\n+            // Update the metadata for segment and chunk.\n+            Preconditions.checkState(bytesWritten >= 0);\n+            segmentMetadata.setLength(segmentMetadata.getLength() + bytesWritten);\n+            chunkWrittenMetadata.setLength(chunkWrittenMetadata.getLength() + bytesWritten);\n+            txn.update(chunkWrittenMetadata);\n+            txn.update(segmentMetadata);\n+        } catch (IndexOutOfBoundsException e) {\n+            try {\n+                throw new BadOffsetException(segmentMetadata.getName(), chunkStorage.getInfo(chunkHandle.getChunkName()).getLength(), offset);\n+            } catch (ChunkStorageException cse) {\n+                log.error(\"{} write - Error while retrieving ChunkInfo for {}.\", logPrefix, chunkHandle.getChunkName());\n+                // The exact expected offset for the  operation does not matter, the StorageWriter will enter reconciliation loop anyway.\n+                throw new BadOffsetException(segmentMetadata.getName(), offset, offset);\n+            }\n+        }\n+        return bytesWritten;\n+    }\n+\n+    /**\n+     * Gets whether given segment is a critical storage system segment.\n+     *\n+     * @param segmentMetadata Meatadata for the segment.\n+     * @return True if this is a storage system segment.\n+     */\n+    private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n+        return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n+    }\n+\n+    /**\n+     * Adds a system log.\n+     *\n+     * @param systemLogRecords\n+     * @param streamSegmentName Name of the segment.\n+     * @param offset            Offset at which new chunk was added.\n+     * @param oldChunkName      Name of the previous last chunk.\n+     * @param newChunkName      Name of the new last chunk.\n+     */\n+    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n+        systemLogRecords.add(\n+                SystemJournal.ChunkAddedRecord.builder()\n+                        .segmentName(streamSegmentName)\n+                        .offset(offset)\n+                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n+                        .newChunkName(newChunkName)\n+                        .build());\n+    }\n+\n+    /**\n+     * Delete the garbage chunks.\n+     *\n+     * @param chunksTodelete List of chunks to delete.\n+     */\n+    private void collectGarbage(Collection<String> chunksTodelete) {\n+        for (val chunkTodelete : chunksTodelete) {\n+            try {\n+                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n+                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n+            } catch (ChunkNotFoundException e) {\n+                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+            } catch (Exception e) {\n+                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                // Add it to garbage chunks.\n+                synchronized (garbageChunks) {\n+                    garbageChunks.add(chunkTodelete);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n+            Preconditions.checkNotNull(handle, \"handle\");\n+            String streamSegmentName = handle.getSegmentName();\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+                // Validate preconditions.\n+                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                // seal if it is not already sealed.\n+                if (!segmentMetadata.isSealed()) {\n+                    segmentMetadata.setSealed(true);\n+                    txn.update(segmentMetadata);\n+                    txn.commit();\n+                }\n+\n+                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                return null;\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            }\n+        });\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n+        checkInitialized();\n+        return execute(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+            Timer timer = new Timer();\n+\n+            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n+            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n+            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+            String targetSegmentName = targetHandle.getSegmentName();\n+\n+            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n+\n+                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n+\n+                // Validate preconditions.\n+                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n+                targetSegmentMetadata.checkInvariants();\n+                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n+\n+                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n+                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n+                sourceSegmentMetadata.checkInvariants();\n+\n+                // This is a critical assumption at this point which should not be broken,\n+                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n+\n+                checkSealed(sourceSegmentMetadata);\n+                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n+\n+                if (sourceSegmentMetadata.getStartOffset() != 0) {\n+                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n+                }\n+\n+                if (offset != targetSegmentMetadata.getLength()) {\n+                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n+                }\n+\n+                // Update list of chunks by appending sources list of chunks.\n+                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n+                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n+\n+                if (targetLastChunk != null) {\n+                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n+                    txn.update(targetLastChunk);\n+                } else {\n+                    if (sourceFirstChunk != null) {\n+                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n+                        txn.update(sourceFirstChunk);\n+                    }\n+                }\n+\n+                // Update segments's last chunk to point to the sources last segment.\n+                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n+\n+                // Update the length of segment.\n+                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n+                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n+\n+                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n+\n+                txn.update(targetSegmentMetadata);\n+                txn.delete(sourceSegment);\n+\n+                // Finally defrag immediately.\n+                if (shouldDefrag() && null != targetLastChunk) {\n+                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null);\n+                }\n+\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                txn.commit();\n+\n+                Duration elapsed = timer.getElapsed();\n+                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n+                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n+\n+                // Update the read index.\n+                readIndexCache.remove(sourceSegment);\n+\n+            } catch (StorageMetadataWritesFencedOutException ex) {\n+                throw new StorageNotPrimaryException(targetSegmentName, ex);\n+            }\n+\n+            return null;\n+        });\n+    }\n+\n+    private boolean shouldAppend() {\n+        return chunkStorage.supportsAppend() && !config.isAppendsDisabled();\n+    }\n+\n+    private boolean shouldDefrag() {\n+        return shouldAppend() || chunkStorage.supportsConcat();\n+    }\n+\n+    /**\n+     * Defragments the list of chunks for a given segment.\n+     * It finds eligible consecutive chunks that can be merged together.\n+     * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+     * Conceptually this is like deleting nodes from middle of the list of chunks.\n+     *\n+     * <Ul>\n+     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+     * is fragmented - this may impact both the read throughputand the performance of the metadata store.\n+     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+     * each write becomes a separate chunk.\n+     * </li>\n+     * <li>\n+     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+     * whereas NFS has no concept of merging natively.\n+     *\n+     * As chunks become larger, append writes (read source completely and append-i.e., write- it back at the end of target)\n+     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+     * when available, and if not available, then we use appends.\n+     * </li>\n+     * <li>\n+     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+     * We can then fine tune that background task to run optimally with low overhead.\n+     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+     * </li>\n+     * <li>\n+     * <li>\n+     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+     * </li>\n+     * </ul>\n+     *\n+     * <div>\n+     * What controls whether we invoke concat or simulate through appends?\n+     * There are a few different capabilities that ChunkStorage needs to provide.\n+     * <ul>\n+     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+     * This is indicated by supportsAppend.</li>\n+     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+     * append or concat.</li>\n+     * <li>Also when ChunkStorage supports both concat and append, ChunkManager will invoke appropriate method\n+     * depending on size of target and source chunks. (Eg. ECS)</li>\n+     * </ul>\n+     * </div>\n+     * <li>\n+     * What controls defrag?\n+     * There are two additional parameters that control when concat\n+     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+     * </li>\n+     * <li>\n+     * What is the defrag algorithm\n+     * <pre>\n+     * While(segment.hasConcatableChunks()){", "originalCommit": "daaae74181cb4974594f142a64d62aa72ebc0bbc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcwODIyNA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447708224", "bodyText": "There is a planned StorageOptimiser component that is planned but not part of this set of PRs. https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2#components\n#4903", "author": "sachin-j-joshi", "createdAt": "2020-06-30T14:03:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzU5ODY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447771060", "bodyText": "typo\nThis is \"max chunk length\", not max segment length. Segments can have a Long.MAX_VALUE length.", "author": "andreipaduroiu", "createdAt": "2020-06-30T15:24:15Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -10,22 +10,23 @@\n package io.pravega.segmentstore.storage;\n \n import com.google.common.base.Preconditions;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n import lombok.Getter;\n \n /**\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n     /**\n-     * The max allowed value for 61 bit signed number which is 2,305,843,009,213,693,952.\n+     * The max allowed value for segment size.\n      */\n-    public static final long MAX_62_BIT_SIGNED_NUMBER = 1L << 61;\n+    public static final long MAX_SGEMENT_LENGTH = RevisionDataOutput.COMPACT_LONG_MAX;", "originalCommit": "681f95cc731842023fbbac1281109e01aa25a304", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjA1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996055", "bodyText": "fixed", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:42:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzk5NjQ5NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r447996494", "bodyText": "1fd6faf", "author": "sachin-j-joshi", "createdAt": "2020-06-30T21:43:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MTA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1OTAxMA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448159010", "bodyText": "Don't we need to rename this to ChunkedSegmentStorageConfig?", "author": "fpj", "createdAt": "2020-07-01T07:07:44Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkManagerConfig.java", "diffHunk": "@@ -16,13 +16,13 @@\n import lombok.NonNull;\n \n /**\n- * Configuration for {@link ChunkManager}.\n+ * Configuration for {@link ChunkedSegmentStorage}.\n  */\n @AllArgsConstructor\n @Builder(toBuilder = true)\n public class ChunkManagerConfig {\n     /**\n-     * Default configuration for {@link ChunkManager}.\n+     * Default configuration for {@link ChunkedSegmentStorage}.\n      */\n     public static final ChunkManagerConfig DEFAULT_CONFIG = ChunkManagerConfig.builder()", "originalCommit": "989fc1726ae61852ee5867c384ab5b788704b64b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODQwMTQ2NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448401464", "bodyText": "Sorry missed that.\nFixed.\neda9341", "author": "sachin-j-joshi", "createdAt": "2020-07-01T14:26:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODE1OTAxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg3MDg0NA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448870844", "bodyText": "Please do not leave TODOs in the code without an associated issue:\nhttps://github.com/pravega/pravega/wiki/Contributing#todos", "author": "fpj", "createdAt": "2020-07-02T09:26:34Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/mocks/InMemoryChunkStorage.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.mocks;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.Builder;\n+import lombok.Getter;\n+import lombok.Setter;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * In-Memory mock for ChunkStorage. Contents is destroyed when object is garbage collected.\n+ */\n+@Slf4j\n+public class InMemoryChunkStorage extends AbstractInMemoryChunkStorage {\n+    private final ConcurrentHashMap<String, InMemoryChunk> chunks = new ConcurrentHashMap<String, InMemoryChunk>();\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        InMemoryChunk chunk = chunks.get(chunkName);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doGetInfo\");\n+        }\n+        return ChunkInfo.builder()\n+                .length(chunk.getLength())\n+                .name(chunkName)\n+                .build();\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Preconditions.checkNotNull(chunkName);\n+        if (null != chunks.putIfAbsent(chunkName, new InMemoryChunk(chunkName))) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"InMemoryChunkStorage::doCreate\");\n+        }\n+        return new ChunkHandle(chunkName, false);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return chunks.containsKey(chunkName);\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (null == chunk) {\n+            throw new ChunkNotFoundException(handle.getChunkName(), \"InMemoryChunkStorage::doDelete\");\n+        }\n+        if (chunk.isReadOnly) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"chunk is readonly\");\n+        }\n+        chunks.remove(handle.getChunkName());\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, true);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenRead\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (chunks.containsKey(chunkName)) {\n+            return new ChunkHandle(chunkName, false);\n+        }\n+        throw new ChunkNotFoundException(chunkName, \"InMemoryChunkStorage::doOpenWrite\");\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        InMemoryChunk chunk = getInMemoryChunk(handle);\n+        if (fromOffset >= chunk.getLength()) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read at wrong offset\");\n+        }\n+        if (length == 0) {\n+            throw new ChunkStorageException(handle.getChunkName(), \"Attempt to read 0 bytes\");\n+        }\n+\n+        // This is implemented this way to simulate possibility of partial read.\n+        InMemoryChunkData matchingData = null;\n+\n+        // TODO : This is OK for now. This is just test code, but need binary search here.", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3ODU5OQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449078599", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:20:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg3MDg0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NzU1MA==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448897550", "bodyText": "Some classes in the Javadoc are mentioned in plain text, whereas others are using {@link xxx} . I think that the latter is the right one.", "author": "RaulGracia", "createdAt": "2020-07-02T10:13:30Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -0,0 +1,1310 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.io.BoundedInputStream;\n+import io.pravega.common.util.ImmutableDate;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentSealedException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataAlreadyExistsException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Getter;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Implements storage for segments using {@link ChunkStorage} and {@link ChunkMetadataStore}.\n+ * The metadata about the segments is stored in metadataStore using two types of records {@link SegmentMetadata} and {@link ChunkMetadata}.\n+ * Any changes to layout must be made inside a {@link MetadataTransaction} which will atomically change the records upon\n+ * {@link MetadataTransaction#commit()}.\n+ * Detailed design is documented here https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n+ */\n+@Slf4j\n+@Beta\n+public class ChunkedSegmentStorage implements Storage {\n+    /**\n+     * Configuration options for this ChunkedSegmentStorage instance.\n+     */\n+    @Getter\n+    private final ChunkedSegmentStorageConfig config;\n+\n+    /**\n+     * Metadata store containing all storage data.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Underlying {@link ChunkStorage} to use to read and write data.\n+     */\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    private final Executor executor;\n+\n+    /**\n+     * Tracks whether this instance is closed or not.\n+     */\n+    private final AtomicBoolean closed;\n+\n+    /**\n+     * Current epoch of the {@link Storage} instance.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#initialize(long)}.\n+     */\n+    @Getter\n+    private long epoch;\n+\n+    /**\n+     * Id of the current Container.\n+     * Initialized by segment container via {@link ChunkedSegmentStorage#bootstrap(int, ChunkMetadataStore)}.\n+     */\n+    @Getter\n+    private int containerId;\n+\n+    /**\n+     * {@link SystemJournal} that logs all changes to system segment layout so that they can be are used during system bootstrap.\n+     */\n+    @Getter\n+    private SystemJournal systemJournal;\n+\n+    /**\n+     * {@link ReadIndexCache} that has index of chunks by start offset\n+     */\n+    private final ReadIndexCache readIndexCache;\n+\n+    /**\n+     * List of garbage chunks.\n+     */\n+    private final List<String> garbageChunks = new ArrayList<String>();\n+\n+    /**\n+     * Prefix string to use for logging.\n+     */\n+    private String logPrefix;\n+\n+    /**\n+     * Creates a new instance of the ChunkedSegmentStorage class.", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NjYwMw==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r449076603", "bodyText": "fixed.", "author": "sachin-j-joshi", "createdAt": "2020-07-02T15:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODg5NzU1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODkxMDY1NQ==", "url": "https://github.com/pravega/pravega/pull/4686#discussion_r448910655", "bodyText": "Nit: batch = extra space", "author": "RaulGracia", "createdAt": "2020-07-02T10:38:58Z", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -0,0 +1,945 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.ObjectBuilder;\n+import io.pravega.common.io.serialization.RevisionDataInput;\n+import io.pravega.common.io.serialization.RevisionDataOutput;\n+import io.pravega.common.io.serialization.VersionedSerializer;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataException;\n+import io.pravega.shared.NameUtils;\n+import lombok.Builder;\n+import lombok.Data;\n+import lombok.EqualsAndHashCode;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import lombok.var;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import static com.google.common.base.Strings.emptyToNull;\n+import static com.google.common.base.Strings.nullToEmpty;\n+\n+/**\n+ * This class implements system journaling functionality for critical storage system segments which is useful for bootstrap after failover.\n+ * It records any layout changes to storage system segments.\n+ * Storage system segments are the segments that the storage subsystem uses to store all metadata.\n+ * This creates a circular dependency while reading or writing the data about these segments from the metadata segments.\n+ * System journal is a mechanism to break this circular dependency by having independent log of all layout changes to system segments.\n+ * During bootstrap all the system journal files are read and processed to re-create the state of the storage system segments.\n+ * Currently only two actions are considered viz. Addition of new chunks {@link ChunkAddedRecord} and truncation of segments {@link TruncationRecord}.\n+ * In addition to these two records, log also contains system snapshot records {@link SystemSnapshotRecord} which contains the state\n+ * of each storage system segments ({@link SegmentSnapshotRecord}) after replaying all available logs at the time of snapshots.\n+ * These snapshot records help avoid replaying entire log evey time. Each container instance records snapshot immediately after bootstrap.\n+ * To avoid data corruption, each instance writes to its own distinct log file/object.\n+ * The bootstrap algorithm also correctly ignores invalid log entries written by running instance which is no longer owner of the given container.\n+ * To prevent applying partial changes resulting from unexpected crash, the log records are written as {@link SystemJournalRecordBatch}.\n+ * In such cases either a full batch is read and applied completely or no records in the batch are applied.\n+ */\n+@Slf4j\n+public class SystemJournal {\n+    /**\n+     * Serializer for {@link SystemJournalRecordBatch}.\n+     */\n+    private static final SystemJournalRecordBatch.SystemJournalRecordBatchSerializer BATCH_SERIALIZER = new SystemJournalRecordBatch.SystemJournalRecordBatchSerializer();\n+\n+    /**\n+     * Serializer for {@link SystemSnapshotRecord}.\n+     */\n+    private static final SystemSnapshotRecord.Serializer SYSTEM_SNAPSHOT_SERIALIZER = new SystemSnapshotRecord.Serializer();\n+\n+    private final Object lock = new Object();\n+\n+    @Getter\n+    private final ChunkStorage chunkStorage;\n+\n+    @Getter\n+    private final ChunkMetadataStore metadataStore;\n+\n+    /**\n+     * Epoch of the current instance.\n+     */\n+    @Getter\n+    private final long epoch;\n+\n+    /**\n+     * Container id of the owner container.\n+     */\n+    @Getter\n+    private final int containerId;\n+\n+    /**\n+     * Index of current journal file.\n+     */\n+    @Getter\n+    private int currentFileIndex;\n+\n+    /**\n+     * String prefix for all system segments.\n+     */\n+    @Getter\n+    private final String systemSegmentsPrefix;\n+\n+    /**\n+     * System segments to track.\n+     */\n+    @Getter\n+    private final String[] systemSegments;\n+\n+    /**\n+     * Offset at which next log will be written.\n+     */\n+    private long systemJournalOffset;\n+\n+    /**\n+     * Configuration {@link ChunkedSegmentStorageConfig} for the {@link ChunkedSegmentStorage}.\n+     */\n+    @Getter\n+    private final ChunkedSegmentStorageConfig config;\n+\n+    private final AtomicBoolean reentryGuard = new AtomicBoolean();\n+\n+    /**\n+     * Constructs an instance of {@link SystemJournal}.\n+     *\n+     * @param containerId   Container id of the owner container.\n+     * @param epoch         Epoch of the current container instance.\n+     * @param chunkStorage  ChunkStorage instance to use for writing all logs.\n+     * @param metadataStore ChunkMetadataStore for owner container.\n+     * @param config        Configuration options for this ChunkedSegmentStorage instance.\n+     * @throws Exception In case of any errors.\n+     */\n+    public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, ChunkMetadataStore metadataStore, ChunkedSegmentStorageConfig config) throws Exception {\n+        this.chunkStorage = Preconditions.checkNotNull(chunkStorage, \"chunkStorage\");\n+        this.metadataStore = Preconditions.checkNotNull(metadataStore, \"metadataStore\");\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.containerId = containerId;\n+        this.epoch = epoch;\n+        this.systemSegments = getChunkStorageSystemSegments(containerId);\n+        this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n+\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()));\n+    }\n+\n+    /**\n+     * Initializes this instance.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    public void initialize() throws Exception {\n+        chunkStorage.create(getSystemJournalChunkName());\n+    }\n+\n+    /**\n+     * Bootstrap the metadata about storage metadata segments by reading and processing the journal.\n+     *\n+     * @throws Exception Exception in case of any error.\n+     */\n+    public void bootstrap() throws Exception {\n+        Preconditions.checkState(!reentryGuard.getAndSet(true), \"bootstrap called multiple times.\");\n+        try (val txn = metadataStore.beginTransaction()) {\n+            // Keep track of offsets at which chunks were added to the system segments.\n+            val chunkStartOffsets = new HashMap<String, Long>();\n+\n+            // Keep track of offsets at which system segments were truncated.\n+            // We don't need to apply each truncate operation, only need to apply the final truncate offset.\n+            val finalTruncateOffsets = new HashMap<String, Long>();\n+            val finalFirstChunkStartsAtOffsets = new HashMap<String, Long>();\n+\n+            // Step 1: Create metadata records for system segments from latest snapshot.\n+            val epochToStart = applyLatestSnapshot(txn, chunkStartOffsets);\n+\n+            // Step 2: For each epoch, find the corresponding system journal files, process them and apply operations recorded.\n+            applySystemLogOperations(txn, epochToStart, chunkStartOffsets, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 3: Adjust the length of the last chunk.\n+            adjustLastChunkLengths(txn);\n+\n+            // Step 4: Apply the truncate offsets.\n+            applyFinalTruncateOffsets(txn, finalTruncateOffsets, finalFirstChunkStartsAtOffsets);\n+\n+            // Step 5: Validate and save a snapshot.\n+            validateAndSaveSnapshot(txn);\n+\n+            // Step 5: Finally commit all data.\n+            txn.commit(true, true);\n+        }\n+    }\n+\n+    /**\n+     * Commits a given system log record to the underlying log chunk.\n+     *\n+     * @param record Record to persist.\n+     * @throws ChunkStorageException Exception if any.\n+     */\n+    public void commitRecord(SystemJournalRecord record) throws ChunkStorageException {\n+        commitRecords(Collections.singletonList(record));\n+    }\n+\n+    /**\n+     * Commits a given list of system log records to the underlying log chunk.\n+     *\n+     * @param records List of records to log to.\n+     * @throws ChunkStorageException Exception in case of any error.\n+     */\n+    public void commitRecords(Collection<SystemJournalRecord> records) throws ChunkStorageException {\n+        Preconditions.checkState(null != records);\n+        Preconditions.checkState(records.size() > 0);\n+\n+        // Open the underlying chunk to write.\n+        ChunkHandle h = getChunkHandleForSystemJournal();\n+\n+        SystemJournalRecordBatch batch = SystemJournalRecordBatch.builder().systemJournalRecords(records).build();\n+        ByteArraySegment bytes;\n+        try {\n+            bytes = BATCH_SERIALIZER.serialize(batch);\n+        } catch (IOException e) {\n+            throw new ChunkStorageException(getSystemJournalChunkName(), \"Unable to serialize\", e);\n+        }\n+        // Persist\n+        synchronized (lock) {\n+            val bytesWritten = chunkStorage.write(h, systemJournalOffset, bytes.getLength(),\n+                    new ByteArrayInputStream(bytes.array(), bytes.arrayOffset(), bytes.getLength()));\n+            Preconditions.checkState(bytesWritten == bytes.getLength());\n+            systemJournalOffset += bytesWritten;\n+            // Add a new log file if required.\n+            if (!chunkStorage.supportsAppend() || config.isAppendsDisabled()) {\n+                currentFileIndex++;\n+                systemJournalOffset = 0;\n+            }\n+        }\n+        log.debug(\"SystemJournal[{}] Logging system log records - file={}, batch ={}.\", containerId, h.getChunkName(), batch);", "originalCommit": "18e63a2df449f4e4249d5ed2fc71557351d3842f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "504d605ea10dec901777041ef5964e2fe145b391", "url": "https://github.com/pravega/pravega/commit/504d605ea10dec901777041ef5964e2fe145b391", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Core functionality.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "3cdf67f5e1665461649746cbefe730247c2904ea", "url": "https://github.com/pravega/pravega/commit/3cdf67f5e1665461649746cbefe730247c2904ea", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Timing chunk lookup for reads, using debug friendly chunk names.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "5d58a22f8812915bebe13a03928ac518f6c9f8de", "url": "https://github.com/pravega/pravega/commit/5d58a22f8812915bebe13a03928ac518f6c9f8de", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Serialize with VersionedSerializer.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "3a310cef434d43a6fbbbeaf7604957ae04b386d3", "url": "https://github.com/pravega/pravega/commit/3a310cef434d43a6fbbbeaf7604957ae04b386d3", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1aa25b5a5ddd007e35f5761db478b314538cc848", "url": "https://github.com/pravega/pravega/commit/1aa25b5a5ddd007e35f5761db478b314538cc848", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Use VersionedSerializer and add snapshot records in SystemJournalLog.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "be55302044c89300557e8b67d0b7716fe4d5251a", "url": "https://github.com/pravega/pravega/commit/be55302044c89300557e8b67d0b7716fe4d5251a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Formatting.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "21804f7bb90e6d46e545f670ed6a4231bcc2ee34", "url": "https://github.com/pravega/pravega/commit/21804f7bb90e6d46e545f670ed6a4231bcc2ee34", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Renaming and other review changes.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "c1247972baba6bc197a9e2f80178917603fadac8", "url": "https://github.com/pravega/pravega/commit/c1247972baba6bc197a9e2f80178917603fadac8", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Missing comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "587c483043b5b5ba7c3e317438277d83e4e2f420", "url": "https://github.com/pravega/pravega/commit/587c483043b5b5ba7c3e317438277d83e4e2f420", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1e29944475db6539459206028f14130fa390f65a", "url": "https://github.com/pravega/pravega/commit/1e29944475db6539459206028f14130fa390f65a", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "43357b2125ffdebe6f01b8f36a692b5b1f33df6d", "url": "https://github.com/pravega/pravega/commit/43357b2125ffdebe6f01b8f36a692b5b1f33df6d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - More cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "d86cf60326c95782afa1847c345fe4962802a776", "url": "https://github.com/pravega/pravega/commit/d86cf60326c95782afa1847c345fe4962802a776", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Remove unnecessary asserts for pinned records.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "c5f833b56a03f14cdb789a92b6a19f6d57fd587e", "url": "https://github.com/pravega/pravega/commit/c5f833b56a03f14cdb789a92b6a19f6d57fd587e", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "8d82ca4d4a6ee29e28485f30cd34c0a9ec0c2bb7", "url": "https://github.com/pravega/pravega/commit/8d82ca4d4a6ee29e28485f30cd34c0a9ec0c2bb7", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nUse RevisionDataOutput.COMPACT_LONG_MAX as max segment length.\nMake fields in SystemJournal final.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "5a21fd216be6897d4f98c3ef25675bf1df6b4f72", "url": "https://github.com/pravega/pravega/commit/5a21fd216be6897d4f98c3ef25675bf1df6b4f72", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nFix typos.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "433231dce580163cff6d56989fd9fa63617c52e4", "url": "https://github.com/pravega/pravega/commit/433231dce580163cff6d56989fd9fa63617c52e4", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nRename ChunkManager to ChunkedSegmentStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "1231efcf568485bfe2e15168ce9866b98e964f39", "url": "https://github.com/pravega/pravega/commit/1231efcf568485bfe2e15168ce9866b98e964f39", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Address review comments.\nRename ChunkManagerConfig to ChunkedSegmentStorageConfig.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "d622132b086f09b86028dba5cbf4980ce070b9d3", "url": "https://github.com/pravega/pravega/commit/d622132b086f09b86028dba5cbf4980ce070b9d3", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Add @Beta annotation.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "152739c667387c371d71a6bbd833b87d70eafd7d", "url": "https://github.com/pravega/pravega/commit/152739c667387c371d71a6bbd833b87d70eafd7d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "commit"}, {"oid": "152739c667387c371d71a6bbd833b87d70eafd7d", "url": "https://github.com/pravega/pravega/commit/152739c667387c371d71a6bbd833b87d70eafd7d", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>", "committedDate": "2020-07-13T17:08:20Z", "type": "forcePushed"}]}