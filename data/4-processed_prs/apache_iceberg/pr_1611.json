{"pr_number": 1611, "pr_title": "DOCS: describe type compatibility between Spark and Iceberg", "pr_createdAt": "2020-10-14T06:27:44Z", "pr_url": "https://github.com/apache/iceberg/pull/1611", "timeline": [{"oid": "d9714fbff562957386de8ea723ec492c732e9fc1", "url": "https://github.com/apache/iceberg/commit/d9714fbff562957386de8ea723ec492c732e9fc1", "message": "DOCS: describe type compatibility between Spark and Iceberg", "committedDate": "2020-10-14T06:15:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMjg0OQ==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504432849", "bodyText": "While I simply put uuid -> string based on the implementation of TypeToSparkType, I failed to verify this as it looks to be really tricky to write UUID column.\nIt doesn't look possible to write UUID column from Spark. Even adding UUID type to SUPPORTED_PRIMITIVES in DataTest leads multiple tests failing.\nIs there any known way to write UUID column, or it'd be better to simply remove uuid here?", "author": "HeartSaVioR", "createdAt": "2020-10-14T06:33:12Z", "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+The type conversion is asymmetric: this table doesn't represent the types of Iceberg Spark can \"read\" from, or \"write\" to.\n+The following sections describe the feasibility on read/write for Iceberg type from Spark.\n+\n+### Iceberg to Spark on reading from Iceberg table\n+\n+| Iceberg                    | Spark                   | Note  |\n+|----------------------------|-------------------------|-------|\n+| boolean                    | boolean                 |       |\n+| integer                    | integer                 |       |\n+| long                       | long                    |       |\n+| float                      | float                   |       |\n+| double                     | double                  |       |\n+| date                       | date                    |       |\n+| time                       | <N/A>                   |       |\n+| timestamp with timezone    | timestamp               |       |\n+| timestamp without timezone | <N/A>                   |       |\n+| string                     | string                  |       |\n+| uuid                       | string                  |       |", "originalCommit": "d9714fbff562957386de8ea723ec492c732e9fc1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5NDE5Ng==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504794196", "bodyText": "We should ignore UUID for now. No engines support it and we are considering whether we should remove it from the spec.", "author": "rdblue", "createdAt": "2020-10-14T15:59:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMjg0OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDY3ODIwMTM5OQ==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r678201399", "bodyText": "Followed up at trinodb/trino#6663 and on the mailing list.", "author": "findepi", "createdAt": "2021-07-28T11:08:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDQzMjg0OQ=="}], "type": "inlineReview"}, {"oid": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "url": "https://github.com/apache/iceberg/commit/e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "message": "refine", "committedDate": "2020-10-14T06:37:20Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504793706", "bodyText": "Binary can be written to a fixed column. It will be validated at write time.", "author": "rdblue", "createdAt": "2020-10-14T15:58:37Z", "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |", "originalCommit": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5NTE4MA==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504795180", "bodyText": "I see, you've created two separate tables. I think I would probably combine them with notes about create vs write instead of having two. That would be more confusing because readers would need to choose which one they need.", "author": "rdblue", "createdAt": "2020-10-14T16:00:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzMzA0Mw==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r505133043", "bodyText": "Yeah I had to split read and write because of UUID, but if we don't mind about UUID then it'll be closely symmetric. I'll try to consolidate twos into one.\nI still think we'd better having two different matrixes for create table and read/write (consolidated, Iceberg types to (->) Spark types), because in many cases the type to pivot on create table in Spark is Spark type (as these types are what end users need to type in), while the type to pivot on read/write table in any engines is Iceberg type. I thought the type to pivot on write to table is engine's column type but I changed my mind as the types of columns are Iceberg types hence end users need to think based on these types.\nIt might be arguable that end users need to think about the final type of the column (Iceberg type) when creating table, which might end up with pivoting Iceberg type on create table. I don't have a strong opinion, as it's also a valid opinion, but there might be also someone who wants to see the Iceberg table as Spark's world of view (restrict the usage to Spark only) and don't want to concern about Iceberg types.\nEither of the direction would be fine for me. WDYT?", "author": "HeartSaVioR", "createdAt": "2020-10-15T02:49:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk1NzIwNA==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r505957204", "bodyText": "I think that having multiple tables causes users to need to look carefully for what is different between them, and increases what they need to pay attention to (\"which table do I need?\"). I'd like to have as few of these as possible. So I'd remove UUID and add notes for any differences between create and write.", "author": "rdblue", "createdAt": "2020-10-16T01:14:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTk5ODQ2NQ==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r505998465", "bodyText": "OK thanks for the opinion. I consolidated tables on create and write. Actually I tried to consolidate all of three tables, but it seemed a bit confusing as directions on conversion are opposite. Please take a look again. Thanks!", "author": "HeartSaVioR", "createdAt": "2020-10-16T02:35:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5MzcwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5Mzk0MA==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r504793940", "bodyText": "Should we convert timestamp without zone to timestamp in Spark when reading?", "author": "rdblue", "createdAt": "2020-10-14T15:58:56Z", "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,82 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type on creating table\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on creating Iceberg table via Spark without using Iceberg core API.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| integer         | integer                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| string          | string                  |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+The type conversion is asymmetric: this table doesn't represent the types of Iceberg Spark can \"read\" from, or \"write\" to.\n+The following sections describe the feasibility on read/write for Iceberg type from Spark.\n+\n+### Iceberg to Spark on reading from Iceberg table\n+\n+| Iceberg                    | Spark                   | Note  |\n+|----------------------------|-------------------------|-------|\n+| boolean                    | boolean                 |       |\n+| integer                    | integer                 |       |\n+| long                       | long                    |       |\n+| float                      | float                   |       |\n+| double                     | double                  |       |\n+| date                       | date                    |       |\n+| time                       | N/A                     |       |\n+| timestamp with timezone    | timestamp               |       |\n+| timestamp without timezone | N/A                     |       |", "originalCommit": "e276e08ec3e0ff76cec2598b5f27410bbc654f5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTEzNDIyNA==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r505134224", "bodyText": "Honestly I wasn't too concerned about timestamp (I tried my best to consider the type as epoch based on UTC) so no strong opinion here. If we'd like to also support this (probably adjusting to the TZ in Spark?) it might be also good to do vice versa, so that read and write are symmetric.", "author": "HeartSaVioR", "createdAt": "2020-10-15T02:54:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDc5Mzk0MA=="}], "type": "inlineReview"}, {"oid": "bdc7cfabcfe9db39aec9e742ec795dce1508108c", "url": "https://github.com/apache/iceberg/commit/bdc7cfabcfe9db39aec9e742ec795dce1508108c", "message": "Applied review comment", "committedDate": "2020-10-16T02:31:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE3MTY2Mg==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r511171662", "bodyText": "I think this could be merged with the table above by adding lines for time, timestamp without time zone, and fixed. What do you think about that? It would be a lot less documentation and the \"notes\" column above would be used. Right now, it's blank.", "author": "rdblue", "createdAt": "2020-10-23T21:53:04Z", "path": "site/docs/spark.md", "diffHunk": "@@ -728,3 +730,62 @@ spark.read.format(\"iceberg\").load(\"db.table.files\").show(truncate = false)\n // Hadoop path table\n spark.read.format(\"iceberg\").load(\"hdfs://nn:8020/path/to/table#files\").show(truncate = false)\n ```\n+\n+## Type compatibility\n+\n+Spark and Iceberg support different set of types. Iceberg does the type conversion automatically, but not for all combinations,\n+so you may want to understand the type conversion in Iceberg in prior to design the types of columns in your tables.\n+\n+### Spark type to Iceberg type\n+\n+This type conversion table describes how Spark types are converted to the Iceberg types. The conversion applies on both creating Iceberg table and writing to Iceberg table via Spark.\n+\n+| Spark           | Iceberg                 | Notes |\n+|-----------------|-------------------------|-------|\n+| boolean         | boolean                 |       |\n+| short           | integer                 |       |\n+| byte            | integer                 |       |\n+| integer         | integer                 |       |\n+| long            | long                    |       |\n+| float           | float                   |       |\n+| double          | double                  |       |\n+| date            | date                    |       |\n+| timestamp       | timestamp with timezone |       |\n+| char            | string                  |       |\n+| varchar         | string                  |       |\n+| string          | string                  |       |\n+| binary          | binary                  |       |\n+| decimal         | decimal                 |       |\n+| struct          | struct                  |       |\n+| array           | list                    |       |\n+| map             | map                     |       |\n+\n+!!! Note\n+    The table is based on representing conversion during creating table. In fact, broader supports are applied on write. Here're some points on write:\n+    \n+    * Iceberg numeric types (`integer`, `long`, `float`, `double`, `decimal`) support promotion during writes. e.g. You can write Spark types `short`, `byte`, `integer`, `long` to Iceberg type `long`.\n+    * You can write to Iceberg `fixed` type using Spark `binary` type. Note that assertion on the length will be performed.\n+\n+### Iceberg type to Spark type\n+\n+This type conversion table describes how Iceberg types are converted to the Spark types. The conversion applies on reading from Iceberg table via Spark.\n+\n+| Iceberg                    | Spark                   | Note          |\n+|----------------------------|-------------------------|---------------|\n+| boolean                    | boolean                 |               |\n+| integer                    | integer                 |               |\n+| long                       | long                    |               |\n+| float                      | float                   |               |\n+| double                     | double                  |               |\n+| date                       | date                    |               |\n+| time                       |                         | Not supported |\n+| timestamp with timezone    | timestamp               |               |\n+| timestamp without timezone |                         | Not supported |", "originalCommit": "bdc7cfabcfe9db39aec9e742ec795dce1508108c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMjQ1NjAzNQ==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r512456035", "bodyText": "Personally I feel more confusing if I have to look into table via -> direction in some cases and <- direction in some other cases, with considering notes to determine asymmetric behavior.\ne.g. If we consider conversion from Spark to Iceberg, multiple Spark types are converted with one Iceberg type, but for sure it's not true for opposite direction. That said, we need to leave special mark representing the opposite case, which Spark type is the result of conversion for one Iceberg type.\nThe more we consolidate, the less intuitive the table would be. The read case doesn't require any explanation on the table, and once we consolidate this into create/write, explanations for create/write would make confusions even on read case.", "author": "HeartSaVioR", "createdAt": "2020-10-27T07:05:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE3MTY2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTcyNzMyMQ==", "url": "https://github.com/apache/iceberg/pull/1611#discussion_r515727321", "bodyText": "@rdblue Kindly reminder to see your thought about my comment. If you are still not persuaded, I can probably raise another PR to go with your suggestion, and we could probably compare twos and pick one. WDYT?", "author": "HeartSaVioR", "createdAt": "2020-11-02T03:47:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE3MTY2Mg=="}], "type": "inlineReview"}]}