{"pr_number": 1477, "pr_title": "Flink: maintain the complete data files into manifest before checkpoint finished.", "pr_createdAt": "2020-09-18T10:24:54Z", "pr_url": "https://github.com/apache/iceberg/pull/1477", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2NzA3Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493167076", "bodyText": "Do we need this stack? Why not print the message?", "author": "chenjunjiedada", "createdAt": "2020-09-23T02:54:59Z", "path": "core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java", "diffHunk": "@@ -83,6 +83,7 @@ private ReadBuilder(ClassLoader loader) {\n         return ValueReaders.record(fields, record);\n \n       } catch (ClassNotFoundException e) {\n+        e.printStackTrace();", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDEwNzkwNw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r494107907", "bodyText": "OK,  I used this message to debug before, could be removed now.", "author": "openinx", "createdAt": "2020-09-24T07:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2NzA3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODExMw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493168113", "bodyText": "Why use format v2? Can we use the version from the table as constructor parameter? That could keep consistency, I think.", "author": "chenjunjiedada", "createdAt": "2020-09-23T02:59:09Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTg1OQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496319859", "bodyText": "v2 is a superset of metadata, so it should be okay.", "author": "rdblue", "createdAt": "2020-09-29T01:17:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE2ODExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjEzNQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493172135", "bodyText": "I see the createFactory definition is createFactory(Table table, String flinkJobId, int partitionId, long taskId) . Do we need to make parameters as same as here?", "author": "chenjunjiedada", "createdAt": "2020-09-23T03:15:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -117,6 +120,10 @@ public void initializeState(StateInitializationContext context) throws Exception\n     // Open the table loader and load the table.\n     this.tableLoader.open(hadoopConf.get());\n     this.table = tableLoader.loadTable();\n+\n+    int subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    int attemptId = getRuntimeContext().getAttemptNumber();\n+    this.flinkManifestFactory = FlinkManifest.createFactory(table, flinkJobId, subTaskId, attemptId);", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MTE5NA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495671194", "bodyText": "+1 on the comment. I also asked a question regarding partitionId in the definition place, as it is not a very intuitive name in the context. Now looking at the usage here. I am wondering if the arg order is correct. In the definition, taskId is the last/4th arg, while here subtaskid is supplied in the 3rd arg.", "author": "stevenzwu", "createdAt": "2020-09-28T03:26:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjQ2MTE2Mw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502461163", "bodyText": "It's true. we should use subTaskId and attemptId for those arguments.", "author": "openinx", "createdAt": "2020-10-09T14:20:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjYxMg==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r493172612", "bodyText": "Since we are now writing manifest to state, do we need to rename datafiles related variables to manifest context?", "author": "chenjunjiedada", "createdAt": "2020-09-23T03:17:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -149,7 +156,7 @@ public void snapshotState(StateSnapshotContext context) throws Exception {\n     LOG.info(\"Start to flush snapshot state to state backend, table: {}, checkpointId: {}\", table, checkpointId);\n \n     // Update the checkpoint state.\n-    dataFilesPerCheckpoint.put(checkpointId, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    dataFilesPerCheckpoint.put(checkpointId, writeToManifest(checkpointId));", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjQ1OTgwOA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502459808", "bodyText": "Make sense !", "author": "openinx", "createdAt": "2020-10-09T14:18:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzE3MjYxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY2NzIwNA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495667204", "bodyText": "hmm. I am not sure if we should tie Flink state serialization to Iceberg serialization", "author": "stevenzwu", "createdAt": "2020-09-28T03:05:41Z", "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {\n+    return AvroSchemaUtil.convert(ManifestFile.schema(), ImmutableMap.of(\n+        ManifestFile.schema().asStruct(), GenericManifestFile.class.getName(),\n+        ManifestFile.PARTITION_SUMMARY_TYPE, GenericPartitionFieldSummary.class.getName()\n+    ));\n+  }\n+\n+  public static byte[] encode(ManifestFile manifestFile) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<GenericManifestFile> writer = new GenericAvroWriter<>(getManifestAvroSchema());\n+      writer.write((GenericManifestFile) manifestFile, encoder);", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzQzOA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317438", "bodyText": "I agree with this, but I'm okay making some utils available for this.", "author": "rdblue", "createdAt": "2020-09-29T01:08:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY2NzIwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY2Nzc2Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495667766", "bodyText": "what is partitionId here?", "author": "stevenzwu", "createdAt": "2020-09-28T03:08:56Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MDU4OQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495670589", "bodyText": "instead of a Byte[] value type, should we just define the a Flink ManifestFile state type? maybe an Avro record or use SimpleVersionedSerializer that Jingsong suggested before? In addition, we can have the state type include the metadata (like jobId). '\n{\n    \"type\": \"record\",\n    \"name\": \"ManifestFileState\",\n    \"namespace\": \"org.apache.iceberg....\",\n    \"fields\": [\n        {\"name\":\"path\", \"type\":\"string\"},\n        {\"name\":\"length\", \"type\":\"long\"},\n        {\"name\":\"specId\", \"type\":\"int\"},\n        { \"name\":\"dataFileCount\", \"type\": \"long\"},\n        { \"name\":\"recordCount\", \"type\": \"long\"},\n////////////// Flink specific fields\n        {\"name\":\"jobId\", \"type\":\"string\"},\n        { \"name\":\"checkpointId\", \"type\": \"long\"},\n        { \"name\":\"checkpointTimestamp\", \"type\": \"long\"},\n        { \"name\":\"byteCount\", \"type\": \"long\"}\n    ]\n}\n\nThen we also have FlinkManifestFile interface extends from ManifestFile interface and can convert from/to ManifestFileState\npublic interface FlinkManifestFile extends ManifestFile {\n  String jobId();\n  long checkpointId();\n  long checkpointTimestamp();\n  long byteCount();\n  ManifestFileState toState();\n}", "author": "stevenzwu", "createdAt": "2020-09-28T03:23:26Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -100,8 +103,8 @@\n       \"iceberg-flink-job-id\", BasicTypeInfo.STRING_TYPE_INFO);\n   private transient ListState<String> jobIdState;\n   // All pending checkpoints states for this function.\n-  private static final ListStateDescriptor<SortedMap<Long, List<DataFile>>> STATE_DESCRIPTOR = buildStateDescriptor();\n-  private transient ListState<SortedMap<Long, List<DataFile>>> checkpointsState;\n+  private static final ListStateDescriptor<SortedMap<Long, Byte[]>> STATE_DESCRIPTOR = buildStateDescriptor();", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MjEyMw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r495672123", "bodyText": "just a note for future improvement, we probably need to allow users to supply the manifest file base path. E.g. we run the streaming Flink jobs in 3 different regions, while Iceberg table's base path is only in one region. We don't want to write the manifiest file to the table home/base path. Instead, we want to write it to a local region (S3 bucket). I haven't thought about too much on how we can extend from the open source implementation yet. We probably can discuss it in a more systematic/thorough way maybe when we integrating Iceberg sink with the new FLIP-143 sink interface.", "author": "stevenzwu", "createdAt": "2020-09-28T03:31:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {\n+    return new FlinkManifestFactory(table.location(), table.spec(), FileFormat.AVRO, table.locationProvider(),\n+        table.io(), table.encryption(), flinkJobId, partitionId, taskId);\n+  }\n+\n+  static class FlinkManifestFactory {\n+    private final String tableLocation;\n+    private final PartitionSpec spec;\n+    private final FileFormat format;\n+    private final OutputFileFactory outputFileFactory;\n+    private final int partitionId;\n+    private final long taskId;\n+    private final String flinkJobId;\n+    private final AtomicInteger fileCount = new AtomicInteger(0);\n+\n+    FlinkManifestFactory(String tableLocation, PartitionSpec spec, FileFormat format, LocationProvider locations,\n+                         FileIO io, EncryptionManager encryptionManager, String flinkJobId, int partitionId,\n+                         long taskId) {\n+      this.tableLocation = tableLocation;\n+      this.spec = spec;\n+      this.format = format;\n+      this.flinkJobId = flinkJobId;\n+      this.partitionId = partitionId;\n+      this.taskId = taskId;\n+      this.outputFileFactory = new OutputFileFactory(spec, format, locations, io,\n+          encryptionManager, partitionId, taskId);\n+    }\n+\n+    private String generateRelativeFilePath(long checkpointId) {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMDEwMw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496320103", "bodyText": "+1 to this.", "author": "rdblue", "createdAt": "2020-09-29T01:18:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3MjEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzIwMQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317201", "bodyText": "In Avro, the write schema -- that was used to encode a record -- is required in order to correctly read the record. Because this doesn't encode anything about the write schema that was used, the bytes will no longer be readable if the schema changes.\nThe checkpoint state will need to store the ManifestFile Avro schema string and this would need a way to pass the Avro schema string into decode.", "author": "rdblue", "createdAt": "2020-09-29T01:07:52Z", "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {\n+    return AvroSchemaUtil.convert(ManifestFile.schema(), ImmutableMap.of(\n+        ManifestFile.schema().asStruct(), GenericManifestFile.class.getName(),\n+        ManifestFile.PARTITION_SUMMARY_TYPE, GenericPartitionFieldSummary.class.getName()\n+    ));\n+  }\n+\n+  public static byte[] encode(ManifestFile manifestFile) throws IOException {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3MjA5MA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502772090", "bodyText": "I agree that it's better to attach the AVRO write schema in the serialized byte[].  In this case,   the serialized byte[] only contains one manifest entry,  so we will need to attach the schema string for every ManifestFile, seems like a waste. But I  can not think of a better way.", "author": "openinx", "createdAt": "2020-10-10T09:54:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzIwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317366", "bodyText": "I would rather not leak an Avro class from this API. Can we put these methods in a utility in the Avro package?", "author": "rdblue", "createdAt": "2020-09-29T01:08:32Z", "path": "core/src/main/java/org/apache/iceberg/ManifestFiles.java", "diffHunk": "@@ -149,6 +160,33 @@ private ManifestFiles() {\n     throw new UnsupportedOperationException(\"Cannot write manifest for table version: \" + formatVersion);\n   }\n \n+  private static org.apache.avro.Schema getManifestAvroSchema() {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjQ2NDg1Mw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502464853", "bodyText": "Yeah,  in theory we should not expose any avro classes, GenericAvroReader, GenericAvroWriter (or any other internal classes ) to public API. It's a draft patch before,  so I tried to implement this as soon as possible and ignored those points.  Let me make it more reasonable.", "author": "openinx", "createdAt": "2020-10-09T14:25:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc1NzcyMQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502757721", "bodyText": "We had discussed this before and I also submitted #1219 that start to use Iceberg model instead of Avro's IndexedRecord for GenericManifestFile.", "author": "chenjunjiedada", "createdAt": "2020-10-10T07:15:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc3MDMwMQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502770301", "bodyText": "Skimmed that pr,  looks like we are changing the GenericManifestFile from an IndexRecord to an Iceberg Record, and use the DataReader and DataWriter to accomplish IO.   What's the purpose that we turn to Iceberg record ?  For unifying the IndexRecord and Record and keeping the Record  only in future ?", "author": "openinx", "createdAt": "2020-10-10T09:36:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzEzMTA1OA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r503131058", "bodyText": "Yes, the purpose is to make internal classes not depend on Avro data model. After we replace the Avro data mode with Iceberg data model, we could refactor two Avro readers and writers as mentioned in #1152.", "author": "chenjunjiedada", "createdAt": "2020-10-12T08:38:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE1NDYwOA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r503154608", "bodyText": "OK, thanks for the reminding,  it's worth to finish that.", "author": "openinx", "createdAt": "2020-10-12T09:16:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzM2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxNzYzMw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496317633", "bodyText": "Adding the utility methods to the Avro package would avoid needing to expose this class (and the corresponding writer) publicly. I think that would be good.", "author": "rdblue", "createdAt": "2020-09-29T01:09:37Z", "path": "core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java", "diffHunk": "@@ -30,14 +30,14 @@\n import org.apache.iceberg.common.DynClasses;\n import org.apache.iceberg.data.avro.DecoderResolver;\n \n-class GenericAvroReader<T> implements DatumReader<T> {\n+public class GenericAvroReader<T> implements DatumReader<T> {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTAxMA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496319010", "bodyText": "This should use TableOperations.metadataFileLocation(filename) to create the full location, and should use FileIO directly to create the output file.\nThe output file factory is used for data files and it creates paths using the location provider. The location provider is only used for data files. For metadata files, TableOperations.metadataFileLocation is used.\nAlso, there is no key metadata tracking for metadata files, so using the encryption manager would produce a file that is either plaintext or unreadable because the key metadata is lost.", "author": "rdblue", "createdAt": "2020-09-29T01:14:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);\n+    try (ManifestWriter<DataFile> closeableWriter = writer) {\n+      closeableWriter.addAll(dataFiles);\n+    }\n+    return writer.toManifestFile();\n+  }\n+\n+  static List<DataFile> read(ManifestFile manifestFile, FileIO io) throws IOException {\n+    try (CloseableIterable<DataFile> dataFiles = ManifestFiles.read(manifestFile, io).project(ManifestFile.schema())) {\n+      return Lists.newArrayList(dataFiles);\n+    }\n+  }\n+\n+  static FlinkManifestFactory createFactory(Table table, String flinkJobId, int partitionId, long taskId) {\n+    return new FlinkManifestFactory(table.location(), table.spec(), FileFormat.AVRO, table.locationProvider(),\n+        table.io(), table.encryption(), flinkJobId, partitionId, taskId);\n+  }\n+\n+  static class FlinkManifestFactory {\n+    private final String tableLocation;\n+    private final PartitionSpec spec;\n+    private final FileFormat format;\n+    private final OutputFileFactory outputFileFactory;\n+    private final int partitionId;\n+    private final long taskId;\n+    private final String flinkJobId;\n+    private final AtomicInteger fileCount = new AtomicInteger(0);\n+\n+    FlinkManifestFactory(String tableLocation, PartitionSpec spec, FileFormat format, LocationProvider locations,\n+                         FileIO io, EncryptionManager encryptionManager, String flinkJobId, int partitionId,\n+                         long taskId) {\n+      this.tableLocation = tableLocation;\n+      this.spec = spec;\n+      this.format = format;\n+      this.flinkJobId = flinkJobId;\n+      this.partitionId = partitionId;\n+      this.taskId = taskId;\n+      this.outputFileFactory = new OutputFileFactory(spec, format, locations, io,\n+          encryptionManager, partitionId, taskId);\n+    }\n+\n+    private String generateRelativeFilePath(long checkpointId) {\n+      return format.addExtension(\n+          String.format(\"%s/flink-manifest/%s-%05d-%d-%d-%05d\", tableLocation, flinkJobId, partitionId, taskId,\n+              checkpointId, fileCount.incrementAndGet()));\n+    }\n+\n+    FlinkManifest create(long checkpointId) {\n+      String relativeFilePath = generateRelativeFilePath(checkpointId);\n+      return new FlinkManifest(outputFileFactory.newOutputFile(relativeFilePath).encryptingOutputFile(), spec);", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMxOTczMg==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496319732", "bodyText": "This class needs to create full paths. It should not create an output file for a relative path like this. That's why the location provider is used in the other cases.\nAlso, as I noted below, metadata files should not use the encryption manager and should also use TableOperations.metadataFileLocation to create a full path from a manifest file name. So I don't think we will need to add a method here.\nThat is, unless you wanted to rename this to newMetadataOutputFile, call ops.metadataFileLocation to create the full path, and bypass the encryption manager.", "author": "rdblue", "createdAt": "2020-09-29T01:17:13Z", "path": "core/src/main/java/org/apache/iceberg/io/OutputFileFactory.java", "diffHunk": "@@ -57,20 +57,26 @@ private String generateFilename() {\n         String.format(\"%05d-%d-%s-%05d\", partitionId, taskId, uuid, fileCount.incrementAndGet()));\n   }\n \n+  /**\n+   * Generates EncryptedOutputFile with relative path under iceberg table location.\n+   */\n+  public EncryptedOutputFile newOutputFile(String relativePath) {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496321160", "bodyText": "I think it is okay to do this, but the append operation can append whole manifest files and either copy the manifest contents into an appropriate path for you, or take ownership of the manifest and manage it so it doesn't need to be rewritten. You might consider using AppendFiles.appendManifest instead.\nIf you do choose to append manifests, then we would want to know whether the manifests are written into the table's metadata space (path came from TableOperations.metadataFilePath) or whether the manifest was written to a temp location, like @stevenzwu's suggestion. In the latter case, we would want to request that Iceberg rewrite the manifests in the commit operation.", "author": "rdblue", "createdAt": "2020-09-29T01:22:22Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -178,14 +185,20 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, List<DataFile>> dataFilesMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, Byte[]> manifestsMap,\n                                     String newFlinkJobId,\n-                                    long checkpointId) {\n-    NavigableMap<Long, List<DataFile>> pendingFileMap = dataFilesMap.headMap(checkpointId, true);\n+                                    long checkpointId) throws IOException {\n+    NavigableMap<Long, Byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+\n+    List<ManifestFile> manifestFiles = Lists.newArrayList();\n+    for (Byte[] manifestData : pendingManifestMap.values()) {\n+      ManifestFile manifestFile = ManifestFiles.decode(ArrayUtils.toPrimitive(manifestData));\n+      manifestFiles.add(manifestFile);\n+    }\n \n     List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (List<DataFile> dataFiles : pendingFileMap.values()) {\n-      pendingDataFiles.addAll(dataFiles);\n+    for (ManifestFile manifestFile : manifestFiles) {\n+      pendingDataFiles.addAll(FlinkManifest.read(manifestFile, table.io()));", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjQ1ODcxMA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502458710", "bodyText": "I read all data files from the written manifests, because there may be one of the operations:  ReplacePartitions or AppendFiles.  For AppendFiles , it's true that we could just use appendManifest if it's in the metadata location, while for ReplacePartitions, it does not have an appendManifest so we still have to read all DataFiles and call ReplacePartitions#addFile one by one.  So here I use the unified method to handle both cases.", "author": "openinx", "createdAt": "2020-10-09T14:16:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDg2MjUxNg==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504862516", "bodyText": "should we add the appendManifest to ReplacePartitions too?", "author": "stevenzwu", "createdAt": "2020-10-14T17:49:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTI1MDYxNQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r505250615", "bodyText": "I think you concern is :  the manifests content is so large in an extreme case  that it will take much resources to copy those entries ?  If we really want to save the copy, we will need :\n\nSet the compatibility.snapshot-id-inheritance.enabled to be true,  its default value is false and in that case it will still copy the contents to assign the correct snapshot id.  \n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/FastAppend.java\n    \n    \n         Line 105\n      in\n      dc661d4\n    \n    \n    \n    \n\n        \n          \n           if (snapshotIdInheritanceEnabled && manifest.snapshotId() == null) { \n        \n    \n  \n\n .\nAdd the necessary appendManifest interface and implementation for ReplacePartition.\n\nI agree that it's nice to have that improvement, but as the release time point is coming, I'd rather not to block this the next release 0.10.0, this should not break the user's experience or compatibility.", "author": "openinx", "createdAt": "2020-10-15T07:17:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NjMwNQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511196305", "bodyText": "We would need to scan the manifest to recover the partitions that need to be replaced. We can definitely add that, but I agree that we don't need to block here.", "author": "rdblue", "createdAt": "2020-10-23T23:29:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTM0NQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r496321345", "bodyText": "Why is this Byte[] instead of byte[]?", "author": "rdblue", "createdAt": "2020-09-29T01:23:12Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -240,12 +256,22 @@ public void processElement(StreamRecord<DataFile> element) {\n   }\n \n   @Override\n-  public void endInput() {\n+  public void endInput() throws IOException {\n     // Flush the buffered data files into 'dataFilesPerCheckpoint' firstly.\n-    dataFilesPerCheckpoint.put(Long.MAX_VALUE, ImmutableList.copyOf(dataFilesOfCurrentCheckpoint));\n+    long currentCheckpointId = Long.MAX_VALUE;\n+    dataFilesPerCheckpoint.put(currentCheckpointId, writeToManifest(currentCheckpointId));\n     dataFilesOfCurrentCheckpoint.clear();\n \n-    commitUpToCheckpoint(dataFilesPerCheckpoint, flinkJobId, Long.MAX_VALUE);\n+    commitUpToCheckpoint(dataFilesPerCheckpoint, flinkJobId, currentCheckpointId);\n+  }\n+\n+  /**\n+   * Write all the complete data files to a newly created manifest file and return the manifest's avro serialized bytes.\n+   */\n+  private Byte[] writeToManifest(long checkpointId) throws IOException {", "originalCommit": "ac9a036b07d9404010e337d5c030959df920dc71", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjQ1MjY0Mw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r502452643", "bodyText": "I thought that flink state backend could only support Byte[]  type,  but after skimmed the flink code again, I found that we have PrimitiveArrayTypeInfo.BYTE_PRIMITIVE_ARRAY_TYPE_INFO  to support byte[].  So here we should use the byte[] directly.", "author": "openinx", "createdAt": "2020-10-09T14:07:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMyMTM0NQ=="}], "type": "inlineReview"}, {"oid": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "url": "https://github.com/apache/iceberg/commit/3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "message": "Add more unit tests and javadoc.", "committedDate": "2020-10-13T08:22:15Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5MDY4Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504090686", "bodyText": "curious why don't we use DataFileWriter provided by Avro?", "author": "stevenzwu", "createdAt": "2020-10-13T16:25:18Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {", "originalCommit": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDM2MTI3MA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504361270", "bodyText": "The DataFileWriter would write those avro record into a  file ?  We want to encode the ManifestFile into a binary and then store it in the flink state backend.  I see there's a method to accomplish the memory encoding here, but I'd rather don't use it because that class is mainly used for avro file encoding.  Besides,  the DataFileReader is also not good enough for memory decoding.", "author": "openinx", "createdAt": "2020-10-14T02:22:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5MDY4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDg1MTU5NQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504851595", "bodyText": "yeah. that was a silly question", "author": "stevenzwu", "createdAt": "2020-10-14T17:30:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDA5MDY4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDg1NDA2MQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r504854061", "bodyText": "nit: I thought assertEquals will produce an error msg like coded here. maybe redundant.", "author": "stevenzwu", "createdAt": "2020-10-14T17:34:43Z", "path": "flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergFilesCommitter.java", "diffHunk": "@@ -473,12 +535,65 @@ public void testBoundedStream() throws Exception {\n       harness.processElement(dataFile, 1);\n       ((BoundedOneInput) harness.getOneInputOperator()).endInput();\n \n+      assertFlinkManifests(0);\n       SimpleDataUtil.assertTableRows(tablePath, tableRows);\n       assertSnapshotSize(1);\n       assertMaxCommittedCheckpointId(jobId, Long.MAX_VALUE);\n     }\n   }\n \n+  @Test\n+  public void testFlinkManifests() throws Exception {\n+    long timestamp = 0;\n+    final long checkpoint = 10;\n+\n+    JobID jobId = new JobID();\n+    try (OneInputStreamOperatorTestHarness<DataFile, Void> harness = createStreamSink(jobId)) {\n+      harness.setup();\n+      harness.open();\n+\n+      assertMaxCommittedCheckpointId(jobId, -1L);\n+\n+      RowData row1 = SimpleDataUtil.createRowData(1, \"hello\");\n+      DataFile dataFile1 = writeDataFile(\"data-1\", ImmutableList.of(row1));\n+\n+      harness.processElement(dataFile1, ++timestamp);\n+      assertMaxCommittedCheckpointId(jobId, -1L);\n+\n+      // 1. snapshotState for checkpoint#1\n+      harness.snapshot(checkpoint, ++timestamp);\n+      List<Path> manifestPaths = assertFlinkManifests(1);\n+      Path manifestPath = manifestPaths.get(0);\n+      Assert.assertEquals(\"File name should have the expected pattern.\",\n+          String.format(\"%s-%05d-%d-%d-%05d.avro\", jobId, 0, 0, checkpoint, 1), manifestPath.getFileName().toString());\n+\n+      // 2. Read the data files from manifests and assert.\n+      List<DataFile> dataFiles = FlinkManifest.read(createTestingManifestFile(manifestPath), table.io());\n+      Assert.assertEquals(1, dataFiles.size());\n+      TestFlinkManifest.checkDataFile(dataFile1, dataFiles.get(0));\n+\n+      // 3. notifyCheckpointComplete for checkpoint#1\n+      harness.notifyOfCompletedCheckpoint(checkpoint);\n+      SimpleDataUtil.assertTableRows(tablePath, ImmutableList.of(row1));\n+      assertMaxCommittedCheckpointId(jobId, checkpoint);\n+      assertFlinkManifests(0);\n+    }\n+  }\n+\n+  private ManifestFile createTestingManifestFile(Path manifestPath) {\n+    return new GenericManifestFile(manifestPath.toAbsolutePath().toString(), manifestPath.toFile().length(), 0,\n+        ManifestContent.DATA, 0, 0, 0L, 0, 0, 0, 0, 0, 0, null);\n+  }\n+\n+  private List<Path> assertFlinkManifests(int expectedCount) throws IOException {\n+    List<Path> manifests = Files.list(flinkManifestFolder.toPath())\n+        .filter(p -> !p.toString().endsWith(\".crc\"))\n+        .collect(Collectors.toList());\n+    Assert.assertEquals(String.format(\"Expected %s flink manifests, but the list is: %s\", expectedCount, manifests),", "originalCommit": "3b7043d00879585ee8bb633a71f8267a6e9ba1f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNTIzNDg0MQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r505234841", "bodyText": "This message was designed to print the whole manifests lists so that we could locate the reason why the expectedCount and manifests.size()  is mismatched.  I think we could keep this message.", "author": "openinx", "createdAt": "2020-10-15T07:02:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDg1NDA2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NDk5MA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511194990", "bodyText": "If the snapshot is not known, then this should pass null.", "author": "rdblue", "createdAt": "2020-10-23T23:22:42Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgxMzI2OA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511813268", "bodyText": "The write path is OK if pass null for snapshot. while the read path will throw an exception:\nCannot read from ManifestFile with null (unassigned) snapshot ID\njava.lang.IllegalArgumentException: Cannot read from ManifestFile with null (unassigned) snapshot ID\n\tat org.apache.iceberg.relocated.com.google.common.base.Preconditions.checkArgument(Preconditions.java:142)\n\tat org.apache.iceberg.InheritableMetadataFactory.fromManifest(InheritableMetadataFactory.java:36)\n\tat org.apache.iceberg.ManifestFiles.read(ManifestFiles.java:86)\n\tat org.apache.iceberg.ManifestFiles.read(ManifestFiles.java:71)\n\tat org.apache.iceberg.flink.sink.FlinkManifest.read(FlinkManifest.java:61)\n\tat org.apache.iceberg.flink.sink.TestFlinkManifest.testIO(TestFlinkManifest.java:81)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)\n\tat org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)\n\tat org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)\n\tat org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)\n\tat org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)\n\tat com.sun.proxy.$Proxy2.processTestClass(Unknown Source)\n\tat org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)\n\tat org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:175)\n\tat org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:157)\n\tat org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:404)\n\tat org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:63)\n\tat org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:46)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:748)\nSo I used the dummy snapshot id to get rid of that.", "author": "openinx", "createdAt": "2020-10-26T09:15:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NDk5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTE2Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511195166", "bodyText": "Why not just use ManifestFiles.write(spec, outputFile)? That is intended for cases like this.", "author": "rdblue", "createdAt": "2020-10-23T23:23:41Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {\n+  private static final int ICEBERG_FORMAT_VERSION = 2;\n+  private static final Long DUMMY_SNAPSHOT_ID = 0L;\n+\n+  private final OutputFile outputFile;\n+  private final PartitionSpec spec;\n+\n+  private FlinkManifest(OutputFile outputFile, PartitionSpec spec) {\n+    this.outputFile = outputFile;\n+    this.spec = spec;\n+  }\n+\n+  ManifestFile write(List<DataFile> dataFiles) throws IOException {\n+    ManifestWriter<DataFile> writer = ManifestFiles.write(ICEBERG_FORMAT_VERSION, spec, outputFile, DUMMY_SNAPSHOT_ID);", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgxNTU3NQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511815575", "bodyText": "The above comment answered this issue.", "author": "openinx", "createdAt": "2020-10-26T09:19:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTk2MA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511195960", "bodyText": "Nit: we avoid using get (it doesn't add context or value) and typically use the field name instead.", "author": "rdblue", "createdAt": "2020-10-23T23:27:42Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc1MDE2Mw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511750163", "bodyText": "This getVersion is extending from the flink's SimpleVersionedSerializer interface, so we could not remove the get prefix.", "author": "openinx", "createdAt": "2020-10-26T07:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5NTk2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511197961", "bodyText": "Can we move this so that we don't need to make it public? It works, but it basically defines its own format for a single message. I'd prefer to use Avro's single-message encoding if we can but this is good for this PR.\nThe more important thing is that we should not expose utility methods that we may need to support that serialize single objects with Avro. I think we should move this and the ManifestFiles methods to package-private to support the Flink ManifestFileSerializer.", "author": "rdblue", "createdAt": "2020-10-23T23:38:19Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwMjQ5OA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511802498", "bodyText": "Yeah, in theory the internal avro encoding / decoding should not be exposed to users, but as we've discussed before that the GenericAvroWriter and GenerciAvroReader should not be exposed to users.  So we have to access the writer & reader package-only, and then expose another methods to the package outside.  Moving this to the package where ManifestFiles located does not work.", "author": "openinx", "createdAt": "2020-10-26T08:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTU1Ng==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511805556", "bodyText": "About the native avro's single-message encoding issue, I read the code in IcebergEncoder and IcebergDecoder,   seems it don't encode the writeSchema into the binary, that seems validate the discussion here.  Besides,  it need a user-provided writeSchema to read the avro binary while in our flink states we only have the binary . Unless we decode the binary to get the schema, we could not call the IcebergDecoder.  Finally,  the code would be similar to this PR.", "author": "openinx", "createdAt": "2020-10-26T09:02:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzEwMzgxMQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r513103811", "bodyText": "Thanks for reminding me about the issue with the generic reader/writer. I agree that requires us to make these public for now.\n\nIcebergEncoder and IcebergDecoder, seems it don't encode the writeSchema into the binary\n\nThese encode a schema fingerprint that is used to load the correct schema. What we would need to do is keep track of all of the schemas that have been used for ManifestFile and build a lookup table from fingerprint to schema. That keeps the size of the encoded record lower.\nI'm fine moving forward with what you have here. I think it is something that we can support for Flink jobs. And it is nice to encode the schema with the data. Then we don't have to supply the lookup by fingerprint.\n\nBesides, it need a user-provided writeSchema to read the avro binary while in our flink states we only have the binary\n\nWe will still need to pass in a read schema in the future. Avro reads using the index of a column in the read schema to set the value. So a schema with columns (a, b, c) will turn into something like this:\nrecord = newRecord()\nrecord.set(0, binaryDecoder.readLong())\nrecord.set(1, binaryDecoder.readString())\nrecord.set(2, binaryDecoder.readDouble())\n\nIf the column order in the current ManifestFile class doesn't match, then basing the indexes passed to set on the write schema will cause failures. A read schema allows us to build a resolver that translates to the correct positions. (That's done in the StructReader).", "author": "rdblue", "createdAt": "2020-10-28T00:08:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzE0MzI3OQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r513143279", "bodyText": "That keeps the size of the encoded record lower.\n\nOK , I saw there's a SchemaStore in IcebergDecoder, which keeps the fingerprint-schema mapping. In theory, for all manifest files,  we only need to keep only one schema binary data.  If we really need to save those schema bytes,  we may need another extra states to track the SchemaStore,  and for each manifestFiles encoding or decoding, we need to maintain the SchemaStore ( For example, if use the a new manifests file schema to encode, then we will need to put <fingerprint, schema> to the SchemaStore and persist to state backend,  If plan to decode, then we will fetch the schema by the manifests binary data's fingerprint -- here we may need to encode the fingerpint into the manifests file's binary data -- and use that schema to accomplish the decode).  For me, that sounds a bit complex optimization for making state more smaller, it's not worth to do that in my mind.", "author": "openinx", "createdAt": "2020-10-28T02:32:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE5Nzk2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4Njk0OA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511686948", "bodyText": "You should remove version things in this AvroEncoderUtil since SimpleVersionedSerializer already handle versions.", "author": "JingsongLi", "createdAt": "2020-10-26T02:08:06Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwMDUwNw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511800507", "bodyText": "We introduced a AvroEncoderUtil here because the IndexedRecord avro serialization is depending on the GenericAvroWriter and GenericAvroReader ( which are package-access and should not be exposed to users).", "author": "openinx", "createdAt": "2020-10-26T08:53:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4Njk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4NzMxNw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511687317", "bodyText": "Why write VERSION_NUM again? Should this be a bug?", "author": "JingsongLi", "createdAt": "2020-10-26T02:10:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION_NUM;\n+  }\n+\n+  @Override\n+  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n+    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+\n+    DataOutputSerializer out = new DataOutputSerializer(256);\n+    out.writeInt(VERSION_NUM);", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTc1MDk5NA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511750994", "bodyText": "OK, i read the javadoc again. It usually only encode this object without the version field, and the users would use the  SimpleVersionedSerialization to include its version.  That make sense.", "author": "openinx", "createdAt": "2020-10-26T07:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4NzMxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4NzczNQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511687735", "bodyText": "You should use SimpleVersionedSerialization.readVersionAndDeSerialize", "author": "JingsongLi", "createdAt": "2020-10-26T02:12:08Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestSerializer.java", "diffHunk": "@@ -0,0 +1,71 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import org.apache.flink.core.memory.DataInputDeserializer;\n+import org.apache.flink.core.memory.DataOutputSerializer;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n+  private static final int VERSION_NUM = 1;\n+  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+\n+  @Override\n+  public int getVersion() {\n+    return VERSION_NUM;\n+  }\n+\n+  @Override\n+  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n+    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+\n+    DataOutputSerializer out = new DataOutputSerializer(256);\n+    out.writeInt(VERSION_NUM);\n+\n+    byte[] serialized = ManifestFiles.encode(manifestFile);\n+    out.writeInt(serialized.length);\n+    out.write(serialized);\n+\n+    return out.getCopyOfBuffer();\n+  }\n+\n+  @Override\n+  public ManifestFile deserialize(int version, byte[] serialized) throws IOException {\n+    if (version == VERSION_NUM) {\n+      return ManifestFiles.decode(serialized);\n+    } else {\n+      throw new IOException(\"Unrecognized version or corrupt state: \" + version);\n+    }\n+  }\n+\n+  static ManifestFile readVersionAndDeserialize(byte[] versionedSerialized) throws IOException {", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4ODk1OA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511688958", "bodyText": "Can we use DataInputStream and DataOutputStream to avoid implementing these parser logic?", "author": "JingsongLi", "createdAt": "2020-10-26T02:18:41Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      // Write the magic bytes\n+      out.write(MAGIC_BYTES);\n+\n+      // Write the length of avro schema string.\n+      byte[] avroSchemaBytes = avroSchema.toString().getBytes(StandardCharsets.UTF_8);\n+      out.write(encodeInt(avroSchemaBytes.length));\n+\n+      // Write the avro schema string.\n+      out.write(avroSchemaBytes);\n+\n+      // Encode the datum with avro schema.\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<T> writer = new GenericAvroWriter<>(avroSchema);\n+      writer.write(datum, encoder);\n+      encoder.flush();\n+      return out.toByteArray();\n+    }\n+  }\n+\n+  public static <T> T decode(byte[] data) throws IOException {\n+    byte[] buffer4 = new byte[4];\n+    try (ByteArrayInputStream in = new ByteArrayInputStream(data, 0, data.length)) {", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNTc2Mw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511805763", "bodyText": "Sounds like a great idea, it helps a lot.", "author": "openinx", "createdAt": "2020-10-26T09:02:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4ODk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4OTQxNw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511689417", "bodyText": "Can you extract two methods encodeSchema(Schema schema, OutputStream out) and decodeSchema(InputStream in)?", "author": "JingsongLi", "createdAt": "2020-10-26T02:20:58Z", "path": "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.avro;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Arrays;\n+import org.apache.avro.LogicalTypes;\n+import org.apache.avro.Schema;\n+import org.apache.avro.io.BinaryDecoder;\n+import org.apache.avro.io.BinaryEncoder;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.avro.io.DatumWriter;\n+import org.apache.avro.io.DecoderFactory;\n+import org.apache.avro.io.EncoderFactory;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class AvroEncoderUtil {\n+\n+  private AvroEncoderUtil() {\n+  }\n+\n+  static {\n+    LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+  }\n+\n+  private static final int VERSION = 1;\n+  private static final byte[] MAGIC_BYTES = new byte[] {'a', 'V', 'R', VERSION};\n+\n+  private static byte[] encodeInt(int value) {\n+    return ByteBuffer.allocate(4).putInt(value).array();\n+  }\n+\n+  private static int decodeInt(byte[] value) {\n+    return ByteBuffer.wrap(value).getInt();\n+  }\n+\n+  public static <T> byte[] encode(T datum, Schema avroSchema) throws IOException {\n+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) {\n+      // Write the magic bytes\n+      out.write(MAGIC_BYTES);\n+\n+      // Write the length of avro schema string.\n+      byte[] avroSchemaBytes = avroSchema.toString().getBytes(StandardCharsets.UTF_8);\n+      out.write(encodeInt(avroSchemaBytes.length));\n+\n+      // Write the avro schema string.\n+      out.write(avroSchemaBytes);\n+\n+      // Encode the datum with avro schema.\n+      BinaryEncoder encoder = EncoderFactory.get().binaryEncoder(out, null);\n+      DatumWriter<T> writer = new GenericAvroWriter<>(avroSchema);\n+      writer.write(datum, encoder);\n+      encoder.flush();\n+      return out.toByteArray();\n+    }\n+  }\n+\n+  public static <T> T decode(byte[] data) throws IOException {\n+    byte[] buffer4 = new byte[4];\n+    try (ByteArrayInputStream in = new ByteArrayInputStream(data, 0, data.length)) {\n+      // Read the magic bytes\n+      Preconditions.checkState(in.read(buffer4) == 4, \"Size of magic bytes isn't 4.\");\n+      Preconditions.checkState(Arrays.equals(MAGIC_BYTES, buffer4), \"Magic bytes mismatched.\");\n+\n+      // Read the length of avro schema string.\n+      Preconditions.checkState(in.read(buffer4) == 4, \"Could not read an integer from input stream.\");\n+      int avroSchemaLength = decodeInt(buffer4);\n+      Preconditions.checkState(avroSchemaLength > 0, \"Length of avro schema string should be positive\");\n+\n+      // Read the avro schema string.\n+      byte[] avroSchemaBytes = new byte[avroSchemaLength];", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNjUxNw==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511806517", "bodyText": "After using the DataInputStream and DataOutputStream,  the schema encoding and decoding will be much simpler, so we don't need the extra small method any more.\n      // Write avro schema\n      dataOut.writeUTF(avroSchema.toString());\n\n      // Read avro schema\n      Schema avroSchema = new Schema.Parser().parse(dataInput.readUTF());", "author": "openinx", "createdAt": "2020-10-26T09:03:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY4OTQxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5MDI0NA==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511690244", "bodyText": "I don't quite understand this class. It looks like a manifest writer and reader?", "author": "JingsongLi", "createdAt": "2020-10-26T02:25:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifest.java", "diffHunk": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+class FlinkManifest {", "originalCommit": "caf9803d96cef23ef97ddcfc4605517781631541", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTgwNzM2OQ==", "url": "https://github.com/apache/iceberg/pull/1477#discussion_r511807369", "bodyText": "Em,  its function is not quite clear now. I will make it more clear.", "author": "openinx", "createdAt": "2020-10-26T09:05:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTY5MDI0NA=="}], "type": "inlineReview"}, {"oid": "b1c905dc63937d8aa5a60fe7e6ba3b39b499f386", "url": "https://github.com/apache/iceberg/commit/b1c905dc63937d8aa5a60fe7e6ba3b39b499f386", "message": "Flink: maintain the complete data files into manifest before checkpoint finished.", "committedDate": "2020-10-26T02:58:28Z", "type": "commit"}, {"oid": "42d1435b9708f09e962e3ea14e34375d2ff4c38d", "url": "https://github.com/apache/iceberg/commit/42d1435b9708f09e962e3ea14e34375d2ff4c38d", "message": "Update according to the comments from PR", "committedDate": "2020-10-26T02:58:28Z", "type": "commit"}, {"oid": "ad241e12eda9680f96393e21dad2160d7196f746", "url": "https://github.com/apache/iceberg/commit/ad241e12eda9680f96393e21dad2160d7196f746", "message": "Change from Byte[] to byte[] in flink state", "committedDate": "2020-10-26T02:58:28Z", "type": "commit"}, {"oid": "923ff1bcd44c4f6fb0fcc47c98acc11285dab45c", "url": "https://github.com/apache/iceberg/commit/923ff1bcd44c4f6fb0fcc47c98acc11285dab45c", "message": "Fix the broken unit tests", "committedDate": "2020-10-26T02:58:28Z", "type": "commit"}, {"oid": "e6fc423adf5b670ddf7af65975b3240c1137eb23", "url": "https://github.com/apache/iceberg/commit/e6fc423adf5b670ddf7af65975b3240c1137eb23", "message": "Add basic unit tests for AvroEncoderUtil", "committedDate": "2020-10-26T02:58:28Z", "type": "commit"}, {"oid": "151a19a88f747d857e41412e079e4bb85bf7f71d", "url": "https://github.com/apache/iceberg/commit/151a19a88f747d857e41412e079e4bb85bf7f71d", "message": "More unit tests.", "committedDate": "2020-10-26T02:58:29Z", "type": "commit"}, {"oid": "07761e2ff879fe36edecfd695a1d70b68bfdce50", "url": "https://github.com/apache/iceberg/commit/07761e2ff879fe36edecfd695a1d70b68bfdce50", "message": "Add more unit tests and javadoc.", "committedDate": "2020-10-26T02:58:29Z", "type": "commit"}, {"oid": "cfcce849a054f006f824910248a837791fbc0384", "url": "https://github.com/apache/iceberg/commit/cfcce849a054f006f824910248a837791fbc0384", "message": "Use SimpleVersionedSerializer to serilize/deserilize the flink state", "committedDate": "2020-10-26T02:58:29Z", "type": "commit"}, {"oid": "0da62b8b95fcbb735d45b23dd901811721d30c65", "url": "https://github.com/apache/iceberg/commit/0da62b8b95fcbb735d45b23dd901811721d30c65", "message": "Use SimpleVersionedSerialization to serialize & deserialize the flink manifest.", "committedDate": "2020-10-26T08:26:30Z", "type": "commit"}, {"oid": "96f31d1a058e3dcdcd5e0d9846c3b97422356033", "url": "https://github.com/apache/iceberg/commit/96f31d1a058e3dcdcd5e0d9846c3b97422356033", "message": "Extract the encodeSchema & decodeSchema.", "committedDate": "2020-10-26T08:51:18Z", "type": "commit"}, {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "url": "https://github.com/apache/iceberg/commit/4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "message": "Refactor the FlinkManifest to make it more clear", "committedDate": "2020-10-26T09:49:19Z", "type": "commit"}, {"oid": "4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "url": "https://github.com/apache/iceberg/commit/4b6cae3b5c2d4021f8bbcb0c88196bb398d8c9af", "message": "Refactor the FlinkManifest to make it more clear", "committedDate": "2020-10-26T09:49:19Z", "type": "forcePushed"}]}