{"pr_number": 1888, "pr_title": "Core: Add BaseDeltaWriter", "pr_createdAt": "2020-12-08T10:08:23Z", "pr_url": "https://github.com/apache/iceberg/pull/1888", "timeline": [{"oid": "afe2a59eaeed5e6c98853fa6e9e648ae52c69a8b", "url": "https://github.com/apache/iceberg/commit/afe2a59eaeed5e6c98853fa6e9e648ae52c69a8b", "message": "Core: Add BaseDeltaWriter.", "committedDate": "2020-12-08T06:48:18Z", "type": "commit"}, {"oid": "49028eb7c3432b72d920aaa070a4ddf22047abae", "url": "https://github.com/apache/iceberg/commit/49028eb7c3432b72d920aaa070a4ddf22047abae", "message": "More unit tests.", "committedDate": "2020-12-08T09:59:07Z", "type": "commit"}, {"oid": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "url": "https://github.com/apache/iceberg/commit/c1946455b880e3112bd43aaaae41113ead7ee2ed", "message": "Minor fixes", "committedDate": "2020-12-08T11:20:35Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NTU1Nw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538795557", "bodyText": "Nit: the error message is slightly misleading because it uses \"could\", which implies that there is some case where it could be null. How about \"Equality delete schema cannot be null\"?", "author": "rdblue", "createdAt": "2020-12-08T20:49:32Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc5NjUzMg==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538796532", "bodyText": "How about BaseEqualityDeltaWriter?\nI think Spark MERGE INTO will likely use a delta writer that doesn't create the equality writer or use the SortedPosDeleteWriter because it will request that the rows are already ordered.", "author": "rdblue", "createdAt": "2020-12-08T20:51:11Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538802022", "bodyText": "Why pass a key here instead of a row?\nI think it would be easier to assume that this is a row, so that the write and delete methods accept the same data. That also provides a way to write the row to the delete file, or just the key based on configuration. The way it is here, there is no way to write the whole row in the delete.", "author": "rdblue", "createdAt": "2020-12-08T20:57:15Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(eqDeleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    protected abstract StructLike asCopiedKey(T row);\n+\n+    public void write(T row) throws IOException {\n+      PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n+\n+      StructLike copiedKey = asCopiedKey(row);\n+      // Adding a pos-delete to replace the old filePos.\n+      PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);\n+      if (previous != null) {\n+        // TODO attach the previous row if has a positional-delete row schema in appender factory.\n+        posDeleteWriter.delete(previous.path, previous.rowOffset, null);\n+      }\n+\n+      dataWriter.write(row);\n+    }\n+\n+    /**\n+     * Delete the rows with the given key.\n+     *\n+     * @param key is the projected values which could be write to eq-delete file directly.\n+     */\n+    public void delete(T key) throws IOException {", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxNDkxMw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538814913", "bodyText": "From the tests, I see that this can be used when the deleted row won't be passed. But I don't think this leaves an option for writing an entire row to the delete file and using a subset of its columns for the delete. This assumes that whatever struct is passed here is the entire delete key.\nTo support both use cases (writing an already projected key and writing a full row and projecting), I think we should have two methods: deleteKey and delete. That way both are supported. We would also need a test for passing the full row to the delete file, but deleting by key.", "author": "rdblue", "createdAt": "2020-12-08T21:19:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA1MjU0NQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539052545", "bodyText": "I agreed that it's better to provide a delete(T row) method which can accept an entire row to write eq-delete file. Because it's possible that have a table with (a,b,c,d) columns,  the equality fields are (a,c) columns, while someone want to write the (a,b,c,d) values into eq-delete file.\nThe problem is:  how could we project the generic data type T to match the expected eqDeleteRowSchema (so that the eqDeleteWriter could write the correct column values) ?   Sounds like we will need to provide an extra abstracted method to accomplish that generic projection ?", "author": "openinx", "createdAt": "2020-12-09T06:52:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA4NDgwNg==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539084806", "bodyText": "Thought it again,  if the table has the (a,b,c,d) columns, and the equality fields are (b,c),  then I think it's enough to provide two ways to write the equality-delete file:\n\n\nOnly write the (b,c)  column values into equality-delete file.   That is enough to maintain the correct deletion semantics if people don't expect to do any real incremental pulling.\n\n\nWrite the (a,b,c,d) column values into equality-delete file.  That's suitable if people want to consume the incremental inserts and deletes for future streaming analysis or data pipeline  etc.\n\n\nExcept the above cases,  I can't think of other scenarios that require a custom equality schema.", "author": "openinx", "createdAt": "2020-12-09T07:59:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc1Mzk4Ng==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539753986", "bodyText": "Yes, I think those two cases are correct: the caller may want to pass the key to delete, or an entire row to delete.", "author": "rdblue", "createdAt": "2020-12-10T00:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwMjAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNTE1Mw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538805153", "bodyText": "commitTransaction(result)?", "author": "rdblue", "createdAt": "2020-12-08T21:02:13Z", "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA1Mjc4Mw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539052783", "bodyText": "Yeah,  nice finding !", "author": "openinx", "createdAt": "2020-12-09T06:53:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwNTE1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODEyMQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538808121", "bodyText": "Is it necessary to encode the record twice? What about detecting already deleted keys and omitting the second delete? That might be over-complicating this for a very small optimization though.", "author": "rdblue", "createdAt": "2020-12-08T21:07:29Z", "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testInsertDuplicatedKey() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(idFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"id\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(equalityFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ddd\"));\n+    deltaWriter.write(createRecord(4, \"eee\"));\n+    deltaWriter.write(createRecord(3, \"fff\"));\n+    deltaWriter.write(createRecord(2, \"ggg\"));\n+    deltaWriter.write(createRecord(1, \"hhh\"));\n+\n+    WriteResult result = deltaWriter.complete();\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file\", 1, result.deleteFiles().length);\n+    DeleteFile posDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(\"Should be a pos-delete file\", FileContent.POSITION_DELETES, posDeleteFile.content());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(\n+        createRecord(1, \"aaa\"),\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"ccc\"),\n+        createRecord(4, \"ddd\"),\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    ), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    Schema posDeleteSchema = DeleteSchemaUtil.pathPosSchema();\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 2L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 3L)\n+    ), readRecordsAsList(posDeleteSchema, posDeleteFile.path()));\n+  }\n+\n+  @Test\n+  public void testUpsertSameRow() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+\n+    Record record = createRecord(1, \"aaa\");\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file and an eq-delete file\", 2, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have an expected record\", expectedRowSet(ImmutableList.of(record)), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record, record), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the eq-delete file.\n+    DeleteFile eqDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record), readRecordsAsList(eqDeleteSchema, eqDeleteFile.path()));", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA1OTE0NQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539059145", "bodyText": "Em,  agreed that we don't have to upsert the same key twice.  The first upsert is enough.", "author": "openinx", "createdAt": "2020-12-09T07:07:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgwODkzMQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538808931", "bodyText": "Nit: it would normally be \"2nd\"", "author": "rdblue", "createdAt": "2020-12-08T21:08:58Z", "path": "data/src/test/java/org/apache/iceberg/io/TestTaskDeltaWriter.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.io;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableTestBase;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestTaskDeltaWriter extends TableTestBase {\n+  private static final int FORMAT_V2 = 2;\n+  private static final long TARGET_FILE_SIZE = 128 * 1024 * 1024L;\n+\n+  private final FileFormat format;\n+  private final GenericRecord gRecord = GenericRecord.create(SCHEMA);\n+  private final GenericRecord posRecord = GenericRecord.create(DeleteSchemaUtil.pathPosSchema());\n+\n+  private OutputFileFactory fileFactory = null;\n+  private int idFieldId;\n+  private int dataFieldId;\n+\n+  @Parameterized.Parameters(name = \"FileFormat = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        {\"avro\"},\n+        {\"parquet\"}\n+    };\n+  }\n+\n+  public TestTaskDeltaWriter(String fileFormat) {\n+    super(FORMAT_V2);\n+    this.format = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void setupTable() throws IOException {\n+    this.tableDir = temp.newFolder();\n+    Assert.assertTrue(tableDir.delete()); // created by table create\n+\n+    this.metadataDir = new File(tableDir, \"metadata\");\n+\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = new OutputFileFactory(table.spec(), format, table.locationProvider(), table.io(),\n+        table.encryption(), 1, 1);\n+\n+    this.idFieldId = table.schema().findField(\"id\").fieldId();\n+    this.dataFieldId = table.schema().findField(\"data\").fieldId();\n+\n+    table.updateProperties()\n+        .defaultFormat(format)\n+        .commit();\n+  }\n+\n+  private Record createRecord(Integer id, String data) {\n+    return gRecord.copy(\"id\", id, \"data\", data);\n+  }\n+\n+  @Test\n+  public void testPureInsert() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < 20; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    for (int i = 20; i < 30; i++) {\n+      Record record = createRecord(i, String.format(\"val-%d\", i));\n+      expected.add(record);\n+\n+      deltaWriter.write(record);\n+    }\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should only have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have no delete file\", 0, result.deleteFiles().length);\n+    commitTransaction(deltaWriter.complete());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(expected), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testInsertDuplicatedKey() throws IOException {\n+    List<Integer> equalityFieldIds = Lists.newArrayList(idFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"id\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(equalityFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ddd\"));\n+    deltaWriter.write(createRecord(4, \"eee\"));\n+    deltaWriter.write(createRecord(3, \"fff\"));\n+    deltaWriter.write(createRecord(2, \"ggg\"));\n+    deltaWriter.write(createRecord(1, \"hhh\"));\n+\n+    WriteResult result = deltaWriter.complete();\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file\", 1, result.deleteFiles().length);\n+    DeleteFile posDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(\"Should be a pos-delete file\", FileContent.POSITION_DELETES, posDeleteFile.content());\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(\n+        createRecord(1, \"aaa\"),\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"ccc\"),\n+        createRecord(4, \"ddd\"),\n+        createRecord(4, \"eee\"),\n+        createRecord(3, \"fff\"),\n+        createRecord(2, \"ggg\"),\n+        createRecord(1, \"hhh\")\n+    ), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    Schema posDeleteSchema = DeleteSchemaUtil.pathPosSchema();\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 2L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 3L)\n+    ), readRecordsAsList(posDeleteSchema, posDeleteFile.path()));\n+  }\n+\n+  @Test\n+  public void testUpsertSameRow() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(idFieldId, dataFieldId);\n+    Schema eqDeleteSchema = table.schema();\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+\n+    Record record = createRecord(1, \"aaa\");\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+    deltaWriter.delete(record);\n+    deltaWriter.write(record);\n+\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file.\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file and an eq-delete file\", 2, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have an expected record\", expectedRowSet(ImmutableList.of(record)), actualRowSet(\"*\"));\n+\n+    // Check records in the data file.\n+    DataFile dataFile = result.dataFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record, record), readRecordsAsList(table.schema(), dataFile.path()));\n+\n+    // Check records in the eq-delete file.\n+    DeleteFile eqDeleteFile = result.deleteFiles()[0];\n+    Assert.assertEquals(ImmutableList.of(record, record), readRecordsAsList(eqDeleteSchema, eqDeleteFile.path()));\n+\n+    // Check records in the pos-delete file.\n+    DeleteFile posDeleteFile = result.deleteFiles()[1];\n+    Assert.assertEquals(ImmutableList.of(\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 0L),\n+        posRecord.copy(\"file_path\", dataFile.path(), \"pos\", 1L)\n+    ), readRecordsAsList(DeleteSchemaUtil.pathPosSchema(), posDeleteFile.path()));\n+\n+    deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    deltaWriter.delete(record);\n+    result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have 0 data file.\", 0, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have 1 eq-delete file\", 1, result.deleteFiles().length);\n+    commitTransaction(result);\n+    Assert.assertEquals(\"Should have no record\", expectedRowSet(ImmutableList.of()), actualRowSet(\"*\"));\n+  }\n+\n+  @Test\n+  public void testUpsertByDataField() throws IOException {\n+    List<Integer> eqDeleteFieldIds = Lists.newArrayList(dataFieldId);\n+    Schema eqDeleteSchema = table.schema().select(\"data\");\n+\n+    GenericTaskDeltaWriter deltaWriter = createTaskWriter(eqDeleteFieldIds, eqDeleteSchema);\n+    deltaWriter.write(createRecord(1, \"aaa\"));\n+    deltaWriter.write(createRecord(2, \"bbb\"));\n+    deltaWriter.write(createRecord(3, \"aaa\"));\n+    deltaWriter.write(createRecord(3, \"ccc\"));\n+    deltaWriter.write(createRecord(4, \"ccc\"));\n+\n+    // Commit the 1th transaction.\n+    WriteResult result = deltaWriter.complete();\n+    Assert.assertEquals(\"Should have a data file\", 1, result.dataFiles().length);\n+    Assert.assertEquals(\"Should have a pos-delete file for deduplication purpose\", 1, result.deleteFiles().length);\n+    Assert.assertEquals(\"Should be pos-delete file\", FileContent.POSITION_DELETES, result.deleteFiles()[0].content());\n+    commitTransaction(result);\n+\n+    Assert.assertEquals(\"Should have expected records\", expectedRowSet(ImmutableList.of(\n+        createRecord(2, \"bbb\"),\n+        createRecord(3, \"aaa\"),\n+        createRecord(4, \"ccc\")\n+    )), actualRowSet(\"*\"));\n+\n+    // Start the 2th transaction.", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMzAyMg==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r538813022", "bodyText": "I think this could be removed from the API because this already has asStructLike. Rather than relying on the implementation to create the projection and copy it, this could be implemented here like this:\n  protected StructLike asCopiedKey(T row) {\n    return structProjection.copy().wrap(asStructLike(row));\n  }\nWe would need to create StructProjection in this class, but it would be fairly easy.", "author": "rdblue", "createdAt": "2020-12-08T21:15:56Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +79,113 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseDeltaWriter implements Closeable {\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private StructLikeMap<PathOffset> insertedRowMap;\n+\n+    public BaseDeltaWriter(PartitionKey partition, Schema eqDeleteSchema) {\n+      Preconditions.checkNotNull(eqDeleteSchema, \"equality-delete schema could not be null.\");\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(eqDeleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    protected abstract StructLike asCopiedKey(T row);", "originalCommit": "c1946455b880e3112bd43aaaae41113ead7ee2ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA0NzcwMw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539047703", "bodyText": "I think the above implementation of asCopiedKey may has problems, because in some compute engine --- for example flink,  it will use the singleton RowDataWrapper instance to implement the asStructLike,  then for both the old key and copied key, they are sharing the same RowDataWrapper which has wrapped the new RowData.   That is messing up the keys in insertedRowMap.\nConsidered this issue,  I finally decided to abstract a separate asCopiedKey to clone a totally different key which won't share object values with the old one.", "author": "openinx", "createdAt": "2020-12-09T06:40:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMzAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUxNTU1NA==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539515554", "bodyText": "Good catch. I like the idea of copying the data into a new struct.", "author": "rdblue", "createdAt": "2020-12-09T17:44:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgxMzAyMg=="}], "type": "inlineReview"}, {"oid": "4a6c0321802d7e236a9b8c4ed31bc1b7fbc5eb39", "url": "https://github.com/apache/iceberg/commit/4a6c0321802d7e236a9b8c4ed31bc1b7fbc5eb39", "message": "Address comments.", "committedDate": "2020-12-09T08:36:04Z", "type": "commit"}, {"oid": "46212bc8af5e3f8b6ca7d824210d8cc8649c5936", "url": "https://github.com/apache/iceberg/commit/46212bc8af5e3f8b6ca7d824210d8cc8649c5936", "message": "Minor changes.", "committedDate": "2020-12-09T08:57:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539639585", "bodyText": "Doesn't this have the problem you were talking about with using only wrappers? If asStructLike uses a wrapper that is reused, then even if the key is a new instance, the underlying data in the StructLike wrapper returned will change on the next call to write.\nI think instead of copying the projection, this needs to copy the result of the projection into a new struct and add that to the map.", "author": "rdblue", "createdAt": "2020-12-09T20:53:27Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -75,6 +81,135 @@ public WriteResult complete() throws IOException {\n         .build();\n   }\n \n+  /**\n+   * Base equality delta writer to write both insert records and equality-deletes.\n+   */\n+  protected abstract class BaseEqualityDeltaWriter implements Closeable {\n+    private final StructProjection structProjection;\n+    private RollingFileWriter dataWriter;\n+    private RollingEqDeleteWriter eqDeleteWriter;\n+    private SortedPosDeleteWriter<T> posDeleteWriter;\n+    private Map<StructLike, PathOffset> insertedRowMap;\n+\n+    public BaseEqualityDeltaWriter(PartitionKey partition, Schema schema, Schema deleteSchema) {\n+      Preconditions.checkNotNull(schema, \"Iceberg table schema cannot be null.\");\n+      Preconditions.checkNotNull(deleteSchema, \"Equality-delete schema cannot be null.\");\n+      this.structProjection = StructProjection.create(schema, deleteSchema);\n+\n+      this.dataWriter = new RollingFileWriter(partition);\n+\n+      this.eqDeleteWriter = new RollingEqDeleteWriter(partition);\n+      this.insertedRowMap = StructLikeMap.create(deleteSchema.asStruct());\n+\n+      this.posDeleteWriter = new SortedPosDeleteWriter<>(appenderFactory, fileFactory, format, partition);\n+    }\n+\n+    /**\n+     * Make the generic data could be read as a {@link StructLike}.\n+     */\n+    protected abstract StructLike asStructLike(T data);\n+\n+    public void write(T row) throws IOException {\n+      PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n+\n+      StructLike copiedKey = structProjection.copy().wrap(asStructLike(row));\n+      // Adding a pos-delete to replace the old path-offset.\n+      PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);", "originalCommit": "46212bc8af5e3f8b6ca7d824210d8cc8649c5936", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTc5MjAyNw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539792027", "bodyText": "I'm a little hesitant about the asCopiedStructLike,  because it introduces complexity to the compute engine developer, people need to consider in which case they need to use asStructLike other cases they need to use asCopiedStructLike.  Implementing the two abstracted method will need carefully coding.   So I'm thinking how about always copy the StructLike  ?   see the flink implementation here.\nThe copy is light-weight because only few references copy ( from RowDataWrapper ).  That would simplify the abstracted methods design.", "author": "openinx", "createdAt": "2020-12-10T02:15:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgwMDA5MQ==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r539800091", "bodyText": "Finally, I decided to introduce the asCopiedStructLike.  People don't have to care how to use it inside the BaseEqualityDeltaWriter,  just need to know we need to implement it as a totally new StructLike which won't effect the old one.", "author": "openinx", "createdAt": "2020-12-10T02:37:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5Njk2Ng==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r540596966", "bodyText": "Why did you decide to delegate to the implementation instead of creating a copy here? I think it would be fairly easy to copy with a StructLike class:\n  public static class StructCopy implements StructLike {\n    public static StructLike copy(StructLike struct) {\n      return new StructCopy(struct);\n    }\n\n    private final Object[] values;\n\n    private StructCopy(StructLike toCopy) {\n      this.values = new Object[toCopy.size()];\n      for (int i = 0; i < values.length; i += 1) {\n        Object value = toCopy.get(i, Object.class);\n        if (value instanceof StructLike) {\n          values[i] = copy((StructLike) value);\n        } else {\n          values[i] = value;\n        }\n      }\n    }\n\n    @Override\n    public int size() {\n      return values.length;\n    }\n\n    @Override\n    public <T> T get(int pos, Class<T> javaClass) {\n      return javaClass.cast(values[pos]);\n    }\n\n    @Override\n    public <T> void set(int pos, T value) {\n      throw new UnsupportedOperationException(\"Struct copy cannot be modified\");\n    }\n  }\nThat doesn't handle lists or maps, but the projection doesn't handle lists or maps either, so it would be okay to use that in the StructLikeMap for all of the cases we currently support.", "author": "rdblue", "createdAt": "2020-12-11T00:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDcxMzcwNA==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r540713704", "bodyText": "Okay, I misunderstood your point. Sounds good to create a StructCopy to copy the values, I like the idea. Thanks.", "author": "openinx", "createdAt": "2020-12-11T06:10:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTYzOTU4NQ=="}], "type": "inlineReview"}, {"oid": "8e5cb38e25189b544eca614d5031ab6396f3e006", "url": "https://github.com/apache/iceberg/commit/8e5cb38e25189b544eca614d5031ab6396f3e006", "message": "Introduce asCopiedStructLike", "committedDate": "2020-12-10T02:32:39Z", "type": "commit"}, {"oid": "dfe0401b1f7fc637129f25631e3b87030d3e214f", "url": "https://github.com/apache/iceberg/commit/dfe0401b1f7fc637129f25631e3b87030d3e214f", "message": "Validate the referenced data files.", "committedDate": "2020-12-10T08:52:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDU5OTQxMw==", "url": "https://github.com/apache/iceberg/pull/1888#discussion_r540599413", "bodyText": "Good catch. Should this set be part of the WriteResult instead of separate? I think that tasks are going to need to pass the set back to the commit for validation, so adding it to the WriteResult seems like the right way to handle it.", "author": "rdblue", "createdAt": "2020-12-11T00:24:46Z", "path": "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java", "diffHunk": "@@ -54,6 +64,10 @@ protected BaseTaskWriter(PartitionSpec spec, FileFormat format, FileAppenderFact\n     this.targetFileSize = targetFileSize;\n   }\n \n+  public Set<CharSequence> referencedDataFiles() {\n+    return referencedDataFiles;", "originalCommit": "dfe0401b1f7fc637129f25631e3b87030d3e214f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dfeed41fd0ef72c28a414689a08a4b10617bdf4d", "url": "https://github.com/apache/iceberg/commit/dfeed41fd0ef72c28a414689a08a4b10617bdf4d", "message": "Addressing the comments.", "committedDate": "2020-12-11T06:39:53Z", "type": "commit"}, {"oid": "b9bc7557f9426a348d0e9e070aabcfe5e0b822ce", "url": "https://github.com/apache/iceberg/commit/b9bc7557f9426a348d0e9e070aabcfe5e0b822ce", "message": "Minor changes.", "committedDate": "2020-12-11T07:21:02Z", "type": "commit"}]}