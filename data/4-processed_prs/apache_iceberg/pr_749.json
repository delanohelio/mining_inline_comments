{"pr_number": 749, "pr_title": "Convert Spark In filter to iceberg IN Expression", "pr_createdAt": "2020-01-26T07:27:47Z", "pr_url": "https://github.com/apache/iceberg/pull/749", "timeline": [{"oid": "32abfe21da27ae5bd39c780511675375107f5928", "url": "https://github.com/apache/iceberg/commit/32abfe21da27ae5bd39c780511675375107f5928", "message": "Convert Spark In filter to iceberg IN Expression", "committedDate": "2020-01-26T07:26:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTQ3OQ==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371261479", "bodyText": "Will it be hard to add tests for a couple of other data types?", "author": "aokolnychyi", "createdAt": "2020-01-27T14:13:41Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java", "diffHunk": "@@ -425,6 +426,21 @@ public void testFilterByNonProjectedColumn() {\n     }\n   }\n \n+  @Test\n+  public void testInFilter() {\n+    File location = buildPartitionedTable(\"partitioned_by_data\", PARTITION_BY_DATA, \"data_ident\", \"data\");\n+\n+    DataSourceOptions options = new DataSourceOptions(ImmutableMap.of(\n+        \"path\", location.toString())\n+    );\n+\n+    IcebergSource source = new IcebergSource();\n+    DataSourceReader reader = source.createReader(options);\n+    pushFilters(reader, new In(\"data\", new String[]{\"foo\", \"junction\", \"brush\"}));", "originalCommit": "32abfe21da27ae5bd39c780511675375107f5928", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4ODc3OQ==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371388779", "bodyText": "I agree. Let's add a test for a DateType at least to validate that convertLiteral is called.", "author": "rdblue", "createdAt": "2020-01-27T17:49:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTQ3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mjg0MDA1Mg==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r372840052", "bodyText": "\ud83d\udc4c", "author": "jun-he", "createdAt": "2020-01-30T09:30:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371261529", "bodyText": "What if values contain null? Can we add a test for this?", "author": "aokolnychyi", "createdAt": "2020-01-27T14:13:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkFilters.java", "diffHunk": "@@ -122,11 +122,7 @@ public static Expression convert(Filter filter) {\n \n         case IN:\n           In inFilter = (In) filter;\n-          Expression in = alwaysFalse();\n-          for (Object value : inFilter.values()) {\n-            in = or(in, equal(inFilter.attribute(), convertLiteral(value)));\n-          }\n-          return in;\n+          return in(inFilter.attribute(), inFilter.values());", "originalCommit": "32abfe21da27ae5bd39c780511675375107f5928", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4MjkxNA==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371382914", "bodyText": "The values cannot contain null. It is not allowed when you create the in expression. There's a secondary check when creating a literal as well.", "author": "rdblue", "createdAt": "2020-01-27T17:37:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4MzU0Nw==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371383547", "bodyText": "Yes, but what if Spark passes a list of values that includes null?", "author": "aokolnychyi", "createdAt": "2020-01-27T17:39:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4NTM3MA==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371385370", "bodyText": "Now that I read this a second time, I think you may be talking about when Spark passes a null value here?\nI agree that's a problem. This should detect and filter out null values because Iceberg's in expression will throw an exception if it encounters them. If there is a null, then this can return or(isNull(attr), in(attr, nonNullValues))", "author": "rdblue", "createdAt": "2020-01-27T17:42:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTYxNzM1MQ==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371617351", "bodyText": "Thanks @aokolnychyi for the comments. I will add additional tests for those cases.\n@rdblue I am thinking if we can add this transformation (in -> or(isNull, in)) into iceberg's Expressions so each caller does not need to repeatedly implement this logic.", "author": "jun-he", "createdAt": "2020-01-28T05:37:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTY0MTIwMg==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371641202", "bodyText": "Actually, null is never equal to null.\nnull IN (1, 2, 3) -> null\nnull IN (1, 2, 3, null) -> null\n1 IN (1, 2, 3) -> true\n1 IN (1, 2, 3, null) -> true\n-1 IN (1, 2, 3) -> false\n-1 IN (1, 2, 3, null) -> null\n\nI wonder if we can simply filter out null values to avoid the exception.", "author": "aokolnychyi", "createdAt": "2020-01-28T07:25:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk1NzU2Ng==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371957566", "bodyText": "@aokolnychyi is right. null is never equal to null.\n@jun-he, that's the reason why we shouldn't automatically transform in Expressions. By not allowing callers to create predicates like in(1, null, 2) we avoid the problem in most Iceberg.\nWe still need to fix the case where Spark passes in null, though. I think that Anton is right and we can simply filter null out of the list. A null in the list will never cause a value to be accepted. It will only cause the filter to return null instead of false, which is handled like false when filtering: if the filter evaluates to null then the row is not selected.", "author": "rdblue", "createdAt": "2020-01-28T17:45:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTI2MTUyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4ODQzOQ==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371388439", "bodyText": "This dropped the call to convertLiteral for each value. That converts values that Spark uses into Iceberg values. I think we still need to call it:\n          In inFilter = (In) filter;\n          List<Object> nonNullLiterals = Stream.of(inFilter.values())\n              .filter(Objects::nonNull)\n              .map(SparkFilters::convertLiteral)\n              .collect(Collectors.toList());\n          return Expressions.in(inFilter.attribute(), nonNullLiterals);", "author": "rdblue", "createdAt": "2020-01-27T17:48:59Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkFilters.java", "diffHunk": "@@ -122,11 +122,7 @@ public static Expression convert(Filter filter) {\n \n         case IN:\n           In inFilter = (In) filter;\n-          Expression in = alwaysFalse();\n-          for (Object value : inFilter.values()) {\n-            in = or(in, equal(inFilter.attribute(), convertLiteral(value)));\n-          }\n-          return in;\n+          return in(inFilter.attribute(), inFilter.values());", "originalCommit": "32abfe21da27ae5bd39c780511675375107f5928", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTk1Nzc4Nw==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r371957787", "bodyText": "I'm updating the code above with better null handling.", "author": "rdblue", "createdAt": "2020-01-28T17:46:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MTM4ODQzOQ=="}], "type": "inlineReview"}, {"oid": "842e57b66f940c5885ec344274ed7df068a5e349", "url": "https://github.com/apache/iceberg/commit/842e57b66f940c5885ec344274ed7df068a5e349", "message": "address the comments and add additional unit tests", "committedDate": "2020-01-31T05:27:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NjAzMw==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r373746033", "bodyText": "Why was it necessary to change these values? This doesn't change the hour partitions the values are stored in.", "author": "rdblue", "createdAt": "2020-02-01T01:19:02Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java", "diffHunk": "@@ -543,11 +579,11 @@ private File buildPartitionedTable(String desc, PartitionSpec spec, String udf,\n \n   private List<Record> testRecords(org.apache.avro.Schema avroSchema) {\n     return Lists.newArrayList(\n-        record(avroSchema, 0L, timestamp(\"2017-12-22T09:20:44.294658+00:00\"), \"junction\"),\n+        record(avroSchema, 0L, timestamp(\"2017-12-22T09:20:44.294+00:00\"), \"junction\"),", "originalCommit": "842e57b66f940c5885ec344274ed7df068a5e349", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzg4MDQ2Mg==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r373880462", "bodyText": "@rdblue It is because java.sql.Timestamp constructor uses a milliseconds time value.\nThere is a deprecated java.sql.Timestamp constructor to use year, month, date, hour, minute, second, and nano. But we also need take care of timezone issue (java timestamp is always UTC).\nSo to avoid using deprecated method and make the test straightforward, I just update two records to be millisecond scale.", "author": "jun-he", "createdAt": "2020-02-02T22:24:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NjAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzg4MDc5NA==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r373880794", "bodyText": "These don't use the constructor that only supports milliseconds, so these should be precise to microseconds. But the test only uses Timestamp to create a filter that gets converted to a partition filter, so the timestamps used to create the Spark filter and these timestamps shouldn't need to match. Doesn't the test pass if these are unchanged?", "author": "rdblue", "createdAt": "2020-02-02T22:29:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NjAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3MzkwMDk1NA==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r373900954", "bodyText": "The test fails because those partitions picked in the tests have only one value (equals the lower and higher bound) so the Timestamp must exactly match.\nTo avoid changing those values, I will update the test to use the partition of 2017-12-21T15, which contains two records. So any Timestamp between them will match.", "author": "jun-he", "createdAt": "2020-02-03T02:25:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NjAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NDIyNTQxNw==", "url": "https://github.com/apache/iceberg/pull/749#discussion_r374225417", "bodyText": "Thanks, @jun-he! I think that's a better solution to the problem.", "author": "rdblue", "createdAt": "2020-02-03T17:06:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3Mzc0NjAzMw=="}], "type": "inlineReview"}, {"oid": "8aacecbb18d0754aa8fcd7b05aab19885b9a40fc", "url": "https://github.com/apache/iceberg/commit/8aacecbb18d0754aa8fcd7b05aab19885b9a40fc", "message": "update the unit test", "committedDate": "2020-02-03T08:07:41Z", "type": "commit"}]}