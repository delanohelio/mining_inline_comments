{"pr_number": 1822, "pr_title": "Flink: add limit pushdown for IcebergTableSource", "pr_createdAt": "2020-11-25T05:01:30Z", "pr_url": "https://github.com/apache/iceberg/pull/1822", "timeline": [{"oid": "bd3efd35a40b5c0ab35c4418f4d1a574d1bdd2bd", "url": "https://github.com/apache/iceberg/commit/bd3efd35a40b5c0ab35c4418f4d1a574d1bdd2bd", "message": "add limit pushdown", "committedDate": "2020-11-25T10:27:11Z", "type": "forcePushed"}, {"oid": "06ae6164f5ba5d967c3802d830e5914df32314c8", "url": "https://github.com/apache/iceberg/commit/06ae6164f5ba5d967c3802d830e5914df32314c8", "message": "add limit pushdown", "committedDate": "2020-11-26T01:40:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE2MjUyMw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533162523", "bodyText": "Why disable the  limit push down for flink table source ?", "author": "openinx", "createdAt": "2020-12-01T08:46:18Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -36,25 +36,30 @@\n \n /**\n  * Flink Iceberg table source.\n- * TODO: Implement {@link FilterableTableSource} and {@link LimitableTableSource}.\n+ * TODO: Implement {@link FilterableTableSource}\n  */\n-public class IcebergTableSource implements StreamTableSource<RowData>, ProjectableTableSource<RowData> {\n+public class IcebergTableSource\n+    implements StreamTableSource<RowData>, ProjectableTableSource<RowData>, LimitableTableSource<RowData> {\n \n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n   private final int[] projectedFields;\n+  private boolean isLimitPushDown = false;\n+  private long limit = -1L;\n \n   public IcebergTableSource(TableLoader loader, TableSchema schema, Map<String, String> properties) {\n-    this(loader, schema, properties, null);\n+    this(loader, schema, properties, null, false, -1);", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MjgxNw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r535792817", "bodyText": "I think it may be related to the design of the LimitableTableSource interface in flink 1.11. I looked up some implement classes of LimitableTableSource in flink, such as HiveTableSource. By default, the limit pushdown is disabled", "author": "zhangjun0x01", "createdAt": "2020-12-04T02:38:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE2MjUyMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgyNzY3Mw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r539827673", "bodyText": "Oh, the table won't have a limit cause by default, so we should set it disabled by default.  It's OK here.", "author": "openinx", "createdAt": "2020-12-10T04:00:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE2MjUyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE2NDI5Ng==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533164296", "bodyText": "Could we add the final modifier ?  Also no need to initialize it with a default false because the constructor will always assign a given value to it.", "author": "openinx", "createdAt": "2020-12-01T08:48:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -36,25 +36,30 @@\n \n /**\n  * Flink Iceberg table source.\n- * TODO: Implement {@link FilterableTableSource} and {@link LimitableTableSource}.\n+ * TODO: Implement {@link FilterableTableSource}\n  */\n-public class IcebergTableSource implements StreamTableSource<RowData>, ProjectableTableSource<RowData> {\n+public class IcebergTableSource\n+    implements StreamTableSource<RowData>, ProjectableTableSource<RowData>, LimitableTableSource<RowData> {\n \n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n   private final int[] projectedFields;\n+  private boolean isLimitPushDown = false;", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE4MDc0Nw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533180747", "bodyText": "if limit is -1L, then means we've disabled the limit push down , right ?  If so, why do we need two fields ?", "author": "openinx", "createdAt": "2020-12-01T09:01:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -36,25 +36,30 @@\n \n /**\n  * Flink Iceberg table source.\n- * TODO: Implement {@link FilterableTableSource} and {@link LimitableTableSource}.\n+ * TODO: Implement {@link FilterableTableSource}\n  */\n-public class IcebergTableSource implements StreamTableSource<RowData>, ProjectableTableSource<RowData> {\n+public class IcebergTableSource\n+    implements StreamTableSource<RowData>, ProjectableTableSource<RowData>, LimitableTableSource<RowData> {\n \n   private final TableLoader loader;\n   private final TableSchema schema;\n   private final Map<String, String> properties;\n   private final int[] projectedFields;\n+  private boolean isLimitPushDown = false;\n+  private long limit = -1L;", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5MTAzNg==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r535791036", "bodyText": "I also think we can judge whether to pushdown by the value of limit, but the  LimitableTableSource interface provides two methods, isLimitPushedDown and applyLimit. From the method comments, I think the author wants to judge whether to pushdown by the isLimitPushedDown method.\nIn versions after flink 1.12, a new interface SupportsLimitPushDown is provided. This interface only provides one method. I think we can judge pushdown by the value of limit", "author": "zhangjun0x01", "createdAt": "2020-12-04T02:32:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE4MDc0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE4NjEyMw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533186123", "bodyText": "Here the isLimitPushDown is always true, how about use the string String.format(\", LimitPushDown: %d\", limit) ?", "author": "openinx", "createdAt": "2020-12-01T09:06:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -102,6 +107,21 @@ public String explainSource() {\n     if (projectedFields != null) {\n       explain += \", ProjectedFields: \" + Arrays.toString(projectedFields);\n     }\n+\n+    if (isLimitPushDown) {\n+      explain += String.format(\", LimitPushDown %s, Limit %d\", isLimitPushDown, limit);", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MzI0NQ==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533193245", "bodyText": "Do we need this ?  we don't parse the limit from properties, right ?", "author": "openinx", "createdAt": "2020-12-01T09:12:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -61,6 +61,8 @@\n   private static final ConfigOption<Long> SPLIT_FILE_OPEN_COST =\n       ConfigOptions.key(\"split-file-open-cost\").longType().defaultValue(null);\n \n+  private static final ConfigOption<Long> LIMIT = ConfigOptions.key(\"limit\").longType().defaultValue(-1L);", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NTgxNg==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r535795816", "bodyText": "I removed it", "author": "zhangjun0x01", "createdAt": "2020-12-04T02:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5MzI0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5NzMxNA==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533197314", "bodyText": "How about just using the context.limit()  in reachedEnd, rather than introducing another new transient field limit ?", "author": "openinx", "createdAt": "2020-12-01T09:15:29Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -48,6 +48,8 @@\n   private final ScanContext context;\n \n   private transient RowDataIterator iterator;\n+  private transient long currentReadCount = 0L;\n+  private transient long limit;", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NjAwOA==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r535796008", "bodyText": "I deleted the field limit", "author": "zhangjun0x01", "createdAt": "2020-12-04T02:48:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzE5NzMxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzIwNDQ2MA==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533204460", "bodyText": "nit: with -> WITH", "author": "openinx", "createdAt": "2020-12-01T09:21:11Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import java.util.List;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSource extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+\n+  private final FileFormat format;\n+\n+  @Parameterized.Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+        String catalogName = (String) catalogParams[0];\n+        String[] baseNamespace = (String[]) catalogParams[1];\n+        parameters.add(new Object[] {catalogName, baseNamespace, format});\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSource(String catalogName, String[] baseNamespace, FileFormat format) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+  }\n+\n+  @Before\n+  public void before() {\n+    super.before();\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+    sql(\"CREATE TABLE %s (id INT, data VARCHAR) with ('write.format.default'='%s')\", TABLE_NAME, format.name());", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzIwOTM1NA==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r533209354", "bodyText": "Do we need to add those test cases:\nCase.1 :  SELECT * FROM test LIMIT -1\nCase.2 :  SELECT * FROM test LIMIT 0 \nCase.3:   SELECT * FROM test LIMIT 3 ,  means the limit  exceeds the total rows in table .\nCase.4:   SELECT * FROM test WHERE a = 1 AND LIMIT 2 ,  query data with both limit and filters\netc.", "author": "openinx", "createdAt": "2020-12-01T09:24:39Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSource.java", "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+\n+import java.util.List;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSource extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+\n+  private final FileFormat format;\n+\n+  @Parameterized.Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+        String catalogName = (String) catalogParams[0];\n+        String[] baseNamespace = (String[]) catalogParams[1];\n+        parameters.add(new Object[] {catalogName, baseNamespace, format});\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSource(String catalogName, String[] baseNamespace, FileFormat format) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+  }\n+\n+  @Before\n+  public void before() {\n+    super.before();\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+    sql(\"CREATE TABLE %s (id INT, data VARCHAR) with ('write.format.default'='%s')\", TABLE_NAME, format.name());\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+    super.clean();\n+  }\n+\n+  @Test\n+  public void testLimitPushDown() {\n+    sql(\"INSERT INTO %s  VALUES (1,'a'),(2,'b')\", TABLE_NAME);\n+\n+    String querySql = String.format(\"SELECT * FROM %s LIMIT 1\", TABLE_NAME);", "originalCommit": "06ae6164f5ba5d967c3802d830e5914df32314c8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc5NTA3Mw==", "url": "https://github.com/apache/iceberg/pull/1822#discussion_r535795073", "bodyText": "I added the test case", "author": "zhangjun0x01", "createdAt": "2020-12-04T02:45:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzIwOTM1NA=="}], "type": "inlineReview"}, {"oid": "51088aea6a408dc495ceddceb23677003a53807b", "url": "https://github.com/apache/iceberg/commit/51088aea6a408dc495ceddceb23677003a53807b", "message": "add limit pushdown", "committedDate": "2020-12-04T02:20:44Z", "type": "commit"}, {"oid": "c526a49a27c7822b4ef1d965faa8404289ff9d49", "url": "https://github.com/apache/iceberg/commit/c526a49a27c7822b4ef1d965faa8404289ff9d49", "message": "add test case\nfix some bugs", "committedDate": "2020-12-04T02:20:44Z", "type": "forcePushed"}, {"oid": "3975631f6970dd7c521e62e6e7c5c4f312bd9a95", "url": "https://github.com/apache/iceberg/commit/3975631f6970dd7c521e62e6e7c5c4f312bd9a95", "message": "add test case\nfix some bugs", "committedDate": "2020-12-04T02:42:16Z", "type": "commit"}, {"oid": "3975631f6970dd7c521e62e6e7c5c4f312bd9a95", "url": "https://github.com/apache/iceberg/commit/3975631f6970dd7c521e62e6e7c5c4f312bd9a95", "message": "add test case\nfix some bugs", "committedDate": "2020-12-04T02:42:16Z", "type": "forcePushed"}]}