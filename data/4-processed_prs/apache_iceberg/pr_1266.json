{"pr_number": 1266, "pr_title": "Flink: update parquet reader with schema visitor", "pr_createdAt": "2020-07-29T12:48:00Z", "pr_url": "https://github.com/apache/iceberg/pull/1266", "timeline": [{"oid": "2728b4fadf3c2c2f241760b506aa80280b8a3840", "url": "https://github.com/apache/iceberg/commit/2728b4fadf3c2c2f241760b506aa80280b8a3840", "message": "Flink: update parquet reader to use schema visitor", "committedDate": "2020-07-29T20:50:48Z", "type": "forcePushed"}, {"oid": "2728b4fadf3c2c2f241760b506aa80280b8a3840", "url": "https://github.com/apache/iceberg/commit/2728b4fadf3c2c2f241760b506aa80280b8a3840", "message": "Flink: update parquet reader to use schema visitor", "committedDate": "2020-07-29T20:50:48Z", "type": "commit"}, {"oid": "e57b6dd17b7e66383a1d2b4e8e31bec87207f6ad", "url": "https://github.com/apache/iceberg/commit/e57b6dd17b7e66383a1d2b4e8e31bec87207f6ad", "message": "remove useless class", "committedDate": "2020-07-30T17:54:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ3NzAzNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463477035", "bodyText": "Here seems we could return the determinate parameter type RowData.   Can change it to ParquetValueReader<RowData>.", "author": "openinx", "createdAt": "2020-07-31T08:29:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNTAzMg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463625032", "bodyText": "You are right. Fixed.", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:54:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ3NzAzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4MTMyNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463481325", "bodyText": "Q: Is there any problem here ?  The FallbackReadBuilder don't need to implement those methods ?\n\npublic T list(Types.ListType iList, GroupType array, T element) ;\npublic T map(Types.MapType iMap, GroupType map, T key, T value) ;\npublic T primitive(org.apache.iceberg.types.Type.PrimitiveType iPrimitive, PrimitiveType primitive)", "author": "openinx", "createdAt": "2020-07-31T08:39:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNjg5NQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463626895", "bodyText": "It is. I forgot that it doesn't extend the ReaderBuilder now but its parent class. This is a bit complex than directly extend the ReaderBuilder and override necessary methods. @rdblue, what do you think?", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:57:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4MTMyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg4NzExNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463887115", "bodyText": "I think this refactor should be done in a separate commit. Let's not over-complicated this one. We can judge the value of the change better when it is isolated to its own PR.", "author": "rdblue", "createdAt": "2020-07-31T23:35:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4MTMyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MTk5NQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464451995", "bodyText": "OK, sound good to me.", "author": "chenjunjiedada", "createdAt": "2020-08-03T14:31:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4MTMyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463487629", "bodyText": "Q:  Is there neccessray to abstract a common ReadBuilder  and make the GenericParquetReader , SparkParquetReader, FlinkParquetReader to share it ?  I mean we could define different Reader(s) for different engine data type, for example we may have Flink's StructReader , and Spark's StructReader,  but the implementation of  TypeWithSchemaVisitor could be shared.   Does it make sense ?   CC @rdblue\nI raise this issue because I saw almost all the codes in ReadBuilder are the same, different copies may need extra resources (both contributor and reviewers) to maintain them.", "author": "openinx", "createdAt": "2020-07-31T08:51:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNDUyNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463624525", "bodyText": "+1 to refactor this. The struct/list/map methods are mostly the same, only primitive methods are different from each other due to differences in data models. This should also apply to Avro and Orc ReadBuilder.", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:53:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzc2ODI2MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463768261", "bodyText": "Yes, I think we should refactor this but I'd prefer to do it separately. I'm okay continuing with the current approach in this PR and refactoring later or in parallel.", "author": "rdblue", "createdAt": "2020-07-31T18:34:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDQ1MzM1Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464453352", "bodyText": "Sound good to me, we can take a look at WriteBuilder as well after all these readers and writers get in.", "author": "chenjunjiedada", "createdAt": "2020-08-03T14:33:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MDIzNw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464870237", "bodyText": "Better to create an issue to address this thing,  once the readers and writers get in,  we could do the refactor in that issue .", "author": "openinx", "createdAt": "2020-08-04T07:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NDA0Nw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465084047", "bodyText": "Created #1294.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:17:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1NzIyMw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465457223", "bodyText": "Hi @chenjunjiedada , I have no idea about Avro, it is simple enough, looks like most of codes are related to Flink data structures. It may not be worth refactoring.", "author": "JingsongLi", "createdAt": "2020-08-05T03:57:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ5ODg0Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465498843", "bodyText": "Thanks @JingsongLi for your input, will take a look again when we start that task.", "author": "chenjunjiedada", "createdAt": "2020-08-05T06:21:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ4NzYyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NDU1NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463494554", "bodyText": "This is timestamp with zone or without zone ?  If it's timestamp with zone,  then the value could be negative, then we could not just  use / and '%',  instead we should use Math.floorDiv and Math.floorMod.  Please see this pull request: #1271 .   ( for Java,  -5/2=-2,  while Math.floorDiv(-5, 2)=-3, actually we need the -3  when considering the epoch second).\nSpark's parquet TimestampMillisReader don't have this issue because it does not depend any div or mod operation.", "author": "openinx", "createdAt": "2020-07-31T09:05:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyNzI4MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463627281", "bodyText": "Thanks for pointing this out. Updated this accordingly.", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:57:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NDU1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NTkzOA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463495938", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-07-31T09:08:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODQxNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463628416", "bodyText": "Looks like Iceberg doesn't have time zone attribute for TimeType.  Do we need to handle timezone logic, here?\nI updated this to use Math.floorDiv anyway.", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:59:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NTkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NzUwMQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463497501", "bodyText": "it could be removed now ?", "author": "openinx", "createdAt": "2020-07-31T09:11:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzYyODQ5MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463628490", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-07-31T13:59:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ5NzUwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzUwMzU1Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r463503553", "bodyText": "Is this right ?    should be old.length << 1  ?    CC @rdblue ,  spark's ReusableArrayData have the same issue ?", "author": "openinx", "createdAt": "2020-07-31T09:24:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,719 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    ReadBuilder builder = new ReadBuilder(fileSchema, idToConstant);\n+    if (ParquetSchemaUtil.hasIds(fileSchema)) {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema, builder);\n+    } else {\n+      return (ParquetValueReader<RowData>)\n+          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+              new FallbackReadBuilder(builder));\n+    }\n+  }\n+\n+  private static class FallbackReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private MessageType type;\n+    private final TypeWithSchemaVisitor<ParquetValueReader<?>> builder;\n+\n+    FallbackReadBuilder(TypeWithSchemaVisitor<ParquetValueReader<?>> builder) {\n+      this.builder = builder;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      // the top level matches by ID, but the remaining IDs are missing\n+      this.type = message;\n+      return builder.struct(expected, message, fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType ignored, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // the expected struct is ignored because nested fields are never found when the IDs are missing\n+      List<ParquetValueReader<?>> newFields = Lists.newArrayListWithExpectedSize(\n+          fieldReaders.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(fieldReaders.size());\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        newFields.add(ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+        types.add(fieldType);\n+      }\n+\n+      return new RowDataReader(types, newFields);\n+    }\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new TimeMillisReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicroReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampMicroReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicroReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(value / 1000_000, (value % 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimeMillisReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    TimeMillisReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Flink only supports millisecond, so we discard microseconds in millisecond\n+      return (int) column.nextLong() / 1000;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n+  }\n+\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n+  }\n+\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n+\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n+    }\n+\n+    @Override\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n+    }\n+\n+    @Override\n+    protected void set(GenericRowData row, int pos, Object value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n       row.setField(pos, value);\n     }\n+\n+    @Override\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n+      row.setField(pos, value);\n+    }\n+  }\n+\n+  private static class ReusableMapData implements MapData {\n+    private final ReusableArrayData keys;\n+    private final ReusableArrayData values;\n+\n+    private int numElements;\n+\n+    private ReusableMapData() {\n+      this.keys = new ReusableArrayData();\n+      this.values = new ReusableArrayData();\n+    }\n+\n+    private void grow() {\n+      keys.grow();\n+      values.grow();\n+    }\n+\n+    private int capacity() {\n+      return keys.capacity();\n+    }\n+\n+    public void setNumElements(int numElements) {\n+      this.numElements = numElements;\n+      keys.setNumElements(numElements);\n+      values.setNumElements(numElements);\n+    }\n+\n+    @Override\n+    public int size() {\n+      return numElements;\n+    }\n+\n+    @Override\n+    public ReusableArrayData keyArray() {\n+      return keys;\n+    }\n+\n+    @Override\n+    public ReusableArrayData valueArray() {\n+      return values;\n+    }\n+  }\n+\n+  private static class ReusableArrayData implements ArrayData {\n+    private static final Object[] EMPTY = new Object[0];\n+\n+    private Object[] values = EMPTY;\n+    private int numElements = 0;\n+\n+    private void grow() {\n+      if (values.length == 0) {\n+        this.values = new Object[20];\n+      } else {\n+        Object[] old = values;\n+        this.values = new Object[old.length << 2];", "originalCommit": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg5ODM1Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464898356", "bodyText": "I still think we don't need to grow four times space,  actually double array space is enough.", "author": "openinx", "createdAt": "2020-08-04T08:47:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzUwMzU1Mw=="}], "type": "inlineReview"}, {"oid": "4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "url": "https://github.com/apache/iceberg/commit/4bf79e1f7ac6fa6e272721b38c8f03f9f3fcc0f5", "message": "address comments", "committedDate": "2020-07-31T11:16:10Z", "type": "commit"}, {"oid": "df701a87aa1667c325c5d132d7e2ed958d7d36e9", "url": "https://github.com/apache/iceberg/commit/df701a87aa1667c325c5d132d7e2ed958d7d36e9", "message": "address comments from openinx", "committedDate": "2020-07-31T13:45:04Z", "type": "commit"}, {"oid": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "url": "https://github.com/apache/iceberg/commit/2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "message": "Remove fallback reader and update names", "committedDate": "2020-08-03T22:52:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MTQ3Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464871472", "bodyText": "nit: could code format it, seems we missing the indent.", "author": "openinx", "createdAt": "2020-08-04T08:01:17Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg3MTY2Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464871663", "bodyText": "nit: indent ?", "author": "openinx", "createdAt": "2020-08-04T08:01:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                         List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                        List<ParquetValueReader<?>> fieldReaders) {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg4ODc2OA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464888768", "bodyText": "For this PR ( flink parquet reader) , seems we don't need this writer visitor .", "author": "openinx", "createdAt": "2020-08-04T08:32:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/ParquetWithFlinkSchemaVisitor.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.util.Deque;\n+import java.util.List;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.logical.RowType.RowField;\n+import org.apache.iceberg.avro.AvroSchemaUtil;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.OriginalType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.apache.parquet.schema.Type;\n+\n+public class ParquetWithFlinkSchemaVisitor<T> {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk4MjI3MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464982270", "bodyText": "Yes, let me remove this.", "author": "chenjunjiedada", "createdAt": "2020-08-04T11:28:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDg4ODc2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzM1Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903356", "bodyText": "How about moving this method into org.apache.iceberg.data.RandomGenericData,  then we don't need to expose the RandomRecordGenerator to public ?\nbtw,  we could  integrate the genericIcebergGenerics method with RandomGenericData#generate .", "author": "openinx", "createdAt": "2020-08-04T08:56:17Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NjQ4NQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465086485", "bodyText": "Make sense to mem moved all Record generators to RandomGenericData.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:20:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzM1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzYwNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903606", "bodyText": "ditto", "author": "openinx", "createdAt": "2020-08-04T08:56:43Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NjU3OQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465086579", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:20:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzYwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzY5OA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903698", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-08-04T08:56:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {\n+    return RandomGenericData.generate(schema, numRecords, seed);\n+  }\n+\n   public static Iterable<Row> generate(Schema schema, int numRecords, long seed) {\n     return generateData(schema, numRecords, () -> new RandomRowGenerator(seed));\n   }\n \n-  public static Iterable<Row> generateFallbackData(Schema schema, int numRecords, long seed, long numDictRows) {\n-    return generateData(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzc3MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464903770", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-08-04T08:56:56Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/RandomData.java", "diffHunk": "@@ -88,20 +93,46 @@ public Row next() {\n     };\n   }\n \n+  private static Iterable<Record> generateIcebergGenerics(Schema schema, int numRecords,\n+                                                   Supplier<RandomGenericData.RandomDataGenerator<Record>> supplier) {\n+    return () -> new Iterator<Record>() {\n+      private final RandomGenericData.RandomDataGenerator<Record> generator = supplier.get();\n+      private int count = 0;\n+\n+      @Override\n+      public boolean hasNext() {\n+        return count < numRecords;\n+      }\n+\n+      @Override\n+      public Record next() {\n+        if (!hasNext()) {\n+          throw new NoSuchElementException();\n+        }\n+        ++count;\n+        return (Record) TypeUtil.visit(schema, generator);\n+      }\n+    };\n+  }\n+\n+\n+  public static Iterable<Record> generateRecords(Schema schema, int numRecords, long seed) {\n+    return RandomGenericData.generate(schema, numRecords, seed);\n+  }\n+\n   public static Iterable<Row> generate(Schema schema, int numRecords, long seed) {\n     return generateData(schema, numRecords, () -> new RandomRowGenerator(seed));\n   }\n \n-  public static Iterable<Row> generateFallbackData(Schema schema, int numRecords, long seed, long numDictRows) {\n-    return generateData(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {\n+    return generateIcebergGenerics(schema, numRecords, () -> new FallbackGenerator(seed, numDictRows));\n   }\n \n-  public static Iterable<Row> generateDictionaryEncodableData(Schema schema, int numRecords, long seed) {\n-    return generateData(schema, numRecords, () -> new DictionaryEncodedGenerator(seed));\n+  public static Iterable<Record> generateDictionaryEncodableRecords(Schema schema, int numRecords, long seed) {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NjY2MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465086661", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:20:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwMzc3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwNTkzNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464905936", "bodyText": "Better to extend the org.apache.iceberg.data.DataTest because it provides more test cases, which COMPLEX_SCHEMA  did not cover.   ( I used COMPLEX_SCHEMA before,  because I don't know there's a better testing method).", "author": "openinx", "createdAt": "2020-08-04T09:00:44Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestFlinkParquetReader.java", "diffHunk": "@@ -35,51 +36,49 @@\n \n import static org.apache.iceberg.flink.data.RandomData.COMPLEX_SCHEMA;\n \n-public class TestFlinkParquetReaderWriter {\n+public class TestFlinkParquetReader {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NzA4Nw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465087087", "bodyText": "Make sense to me and updated.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:20:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkwNTkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTIxMQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464911211", "bodyText": "Seems we share the same primitive value assertion between assertRowData and assertArrayValues, could it be a common method ?", "author": "openinx", "createdAt": "2020-08-04T09:09:58Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, Record expected, RowData actual) {\n+    if (expected == null && actual == null) {\n+      return;\n+    }\n+\n+    List<Type> types = new ArrayList<>();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expected.get(i) == null) {\n+        Assert.assertTrue(actual.isNullAt(i));\n+        continue;\n+      }\n+      Type.TypeID typeId = types.get(i).typeId();\n+      Object value = expected.get(i);\n+      switch (typeId) {\n+        case BOOLEAN:\n+          Assert.assertEquals(\"boolean value should be equal\", value, actual.getBoolean(i));\n+          break;\n+        case INTEGER:\n+          Assert.assertEquals(\"int value should be equal\", value, actual.getInt(i));\n+          break;\n+        case LONG:\n+          Assert.assertEquals(\"long value should be equal\", value, actual.getLong(i));\n+          break;\n+        case FLOAT:\n+          Assert.assertEquals(\"float value should be equal\", value, actual.getFloat(i));\n+          break;\n+        case DOUBLE:\n+          Assert.assertEquals(\"double should be equal\", value, actual.getDouble(i));\n+          break;\n+        case STRING:\n+          Assert.assertTrue(\"Should expect a CharSequence\", value instanceof CharSequence);\n+          Assert.assertEquals(\"string should be equal\", String.valueOf(value), actual.getString(i).toString());\n+          break;\n+        case DATE:\n+          Assert.assertTrue(\"Should expect a Date\", value instanceof LocalDate);\n+          LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, actual.getInt(i));\n+          Assert.assertEquals(\"date should be equal\", value, date);\n+          break;\n+        case TIME:\n+          Assert.assertTrue(\"Should expect a LocalTime\", value instanceof LocalTime);\n+          int milliseconds = (int) (((LocalTime) value).toNanoOfDay() / 1000_000);\n+          Assert.assertEquals(\"time millis should be equal\", milliseconds, actual.getInt(i));\n+          break;\n+        case TIMESTAMP:\n+          if (((Types.TimestampType) type.asPrimitiveType()).shouldAdjustToUTC()) {\n+            Assert.assertTrue(\"Should expect a OffsetDataTime\", value instanceof OffsetDateTime);\n+            OffsetDateTime ts = (OffsetDateTime) value;\n+            Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+                actual.getTimestamp(i, 6).toLocalDateTime());\n+          } else {\n+            Assert.assertTrue(\"Should expect a LocalDataTime\", value instanceof LocalDateTime);\n+            LocalDateTime ts = (LocalDateTime) value;\n+            Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+                actual.getTimestamp(i, 6).toLocalDateTime());\n+          }\n+          break;\n+        case FIXED:\n+          Assert.assertTrue(\"Should expect byte[]\", value instanceof byte[]);\n+          Assert.assertArrayEquals(\"binary should be equal\", (byte[]) value, actual.getBinary(i));\n+          break;\n+        case BINARY:\n+          Assert.assertTrue(\"Should expect a ByteBuffer\", value instanceof ByteBuffer);\n+          Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) value).array(), actual.getBinary(i));\n+          break;\n+        case DECIMAL:", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDk5Nzg0NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464997844", "bodyText": "I tried to extract to one common method with an interface like assertData(Type type, Record expected, Object actual) so that we can determine actual via instanceof, but we still have to call the getter method with casting, such as ((RowData)actual).getInt(i) and ((ArrayData)actual).getInt(i), so the code still looks a bit duplicated as current. Does that make sense to you?", "author": "chenjunjiedada", "createdAt": "2020-08-04T11:59:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTIxMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwNzc3MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466807770", "bodyText": "I tried to reuse those big switch-case,  How about the following way ? ( I did not finish all the code, but seems it should work to reuse that code).\npublic class TestHelpers {\n  private TestHelpers() {\n  }\n\n  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n\n  public static void assertRowData(Type type, RowType rowType, Record expectedRecord, RowData actualRowData) {\n    if (expectedRecord == null && actualRowData == null) {\n      return;\n    }\n\n    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n        expectedRecord != null && actualRowData != null);\n\n    List<Type> types = new ArrayList<>();\n    for (Types.NestedField field : type.asStructType().fields()) {\n      types.add(field.type());\n    }\n\n    for (int i = 0; i < types.size(); i += 1) {\n      if (expectedRecord.get(i) == null) {\n        Assert.assertTrue(actualRowData.isNullAt(i));\n        continue;\n      }\n      Object expected = expectedRecord.get(i);\n      LogicalType logicalType = rowType.getTypeAt(i);\n\n      final int fieldPos = i;\n      assertEquals(types.get(i), rowType.getTypeAt(i), expected,\n          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n    }\n  }\n\n  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n    switch (type.typeId()) {\n      case BOOLEAN:\n        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n        break;\n      case INTEGER:\n        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n        break;\n      case LONG:\n        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n        break;\n      case FLOAT:\n        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n        break;\n      case DOUBLE:\n        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n        break;\n      case STRING:\n        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n        Assert.assertEquals(\"string should be equal\",\n            String.valueOf(expected), supplier.get().toString());\n        break;\n      case DATE:\n        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n        Assert.assertEquals(\"date should be equal\", expected, date);\n        break;\n      case TIME:\n        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n        break;\n      case TIMESTAMP:\n        if (((Types.TimestampType) type.asPrimitiveType()).shouldAdjustToUTC()) {\n          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n          OffsetDateTime ts = (OffsetDateTime) expected;\n          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n              ((TimestampData) supplier.get()).toLocalDateTime());\n        } else {\n          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n          LocalDateTime ts = (LocalDateTime) expected;\n          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n              ((TimestampData) supplier.get()).toLocalDateTime());\n        }\n        break;\n      case BINARY:\n        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());\n        break;\n      case DECIMAL:\n        Assert.assertTrue(\"Should expect a BigDecimal\", expected instanceof BigDecimal);\n        BigDecimal bd = (BigDecimal) expected;\n        Assert.assertEquals(\"decimal value should be equal\", bd,\n            ((DecimalData) supplier.get()).toBigDecimal());\n        break;\n      case LIST: // TODO call assertArrayValues.\n      case MAP:\n      case STRUCT:\n      case UUID:\n      case FIXED:\n        // TODO;\n      default:\n        throw new IllegalArgumentException(\"Not a supported type: \" + type);\n    }\n  }\n\n  private static void assertArrayValues(Type type, LogicalType logicalType, Collection<?> expectedArray,\n                                        ArrayData actualArray) {\n    List<?> expectedElements = Lists.newArrayList(expectedArray);\n    for (int i = 0; i < expectedArray.size(); i += 1) {\n      if (expectedElements.get(i) == null) {\n        Assert.assertTrue(actualArray.isNullAt(i));\n        continue;\n      }\n\n      Object expected = expectedElements.get(i);\n\n      final int pos = i;\n      assertEquals(type, logicalType, expected,\n          () -> ArrayData.createElementGetter(logicalType).getElementOrNull(actualArray, pos));\n    }\n  }\n}", "author": "openinx", "createdAt": "2020-08-07T04:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTIxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTkxMQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r464911911", "bodyText": "This don't have to be public, see comment https://github.com/apache/iceberg/pull/1266/files#r464903356", "author": "openinx", "createdAt": "2020-08-04T09:11:12Z", "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -55,8 +55,8 @@ private RandomGenericData() {}\n     return records;\n   }\n \n-  private static class RandomRecordGenerator extends RandomDataGenerator<Record> {\n-    private RandomRecordGenerator(long seed) {\n+  public static class RandomRecordGenerator extends RandomDataGenerator<Record> {", "originalCommit": "2e463b7d68b2ae0ddcab6dbf00f9eaeaed53316a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTA4NzMzMw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r465087333", "bodyText": "Fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-04T14:21:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDkxMTkxMQ=="}], "type": "inlineReview"}, {"oid": "5a62d44065e45034f8bdbca70923735607c434b3", "url": "https://github.com/apache/iceberg/commit/5a62d44065e45034f8bdbca70923735607c434b3", "message": "refactor RandomGenericData", "committedDate": "2020-08-04T11:47:33Z", "type": "commit"}, {"oid": "5069eaa5860cb52e3d837eb00d61039233b840f5", "url": "https://github.com/apache/iceberg/commit/5069eaa5860cb52e3d837eb00d61039233b840f5", "message": "fix failed UT", "committedDate": "2020-08-04T14:11:04Z", "type": "commit"}, {"oid": "9976af28536fa830db6f6c38c4474c339a9ccb0f", "url": "https://github.com/apache/iceberg/commit/9976af28536fa830db6f6c38c4474c339a9ccb0f", "message": "double the size when reallocate the buffer", "committedDate": "2020-08-04T14:24:36Z", "type": "commit"}, {"oid": "f8e130fc17322975d0e970abfcc25f30abbc622d", "url": "https://github.com/apache/iceberg/commit/f8e130fc17322975d0e970abfcc25f30abbc622d", "message": "minor update for naming", "committedDate": "2020-08-07T01:54:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4Nzk5Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466787996", "bodyText": "I think here we'd better to create the record lazily, for Iterable<Record> result, (similar to spark RandomData),  because if we wanna to generate lots of records, it will be not easy to OOM .", "author": "openinx", "createdAt": "2020-08-07T02:35:58Z", "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -55,6 +59,14 @@ private RandomGenericData() {}\n     return records;\n   }\n \n+  public static Iterable<Record> generateFallbackRecords(Schema schema, int numRecords, long seed, long numDictRows) {\n+    return generateRecords(schema, numRecords, new FallbackGenerator(seed, numDictRows));", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwNjg2Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466906862", "bodyText": "Fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:44:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4Nzk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODQ2Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466788463", "bodyText": "We usually don't expose this method to public because RandomRecordGenerator is a private static class and others could not access this method actually.", "author": "openinx", "createdAt": "2020-08-07T02:37:44Z", "path": "data/src/test/java/org/apache/iceberg/data/RandomGenericData.java", "diffHunk": "@@ -46,7 +46,11 @@\n   private RandomGenericData() {}\n \n   public static List<Record> generate(Schema schema, int numRecords, long seed) {\n-    RandomRecordGenerator generator = new RandomRecordGenerator(seed);\n+    return generateRecords(schema, numRecords, new RandomRecordGenerator(seed));\n+  }\n+\n+  public static List<Record> generateRecords(Schema schema, int numRecords,", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwNjkyNw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466906927", "bodyText": "Fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:44:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5MjY0Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466792646", "bodyText": "The DATE type should always be an integer ?   For me,  seems more reasonable to move the DATE case to the place where INT_64 is, because they are surely to use UnboxedReader .", "author": "openinx", "createdAt": "2020-08-07T02:55:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwNzAzNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466907036", "bodyText": "Agreed, updated.", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:45:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5MjY0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NDcyMg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466794722", "bodyText": "Is it correct ?  For my understanding,   the timestamp with time zone don't need to convert to a LocalDateTime (because it has its own time zone),   while the timestamp without time zone need to.", "author": "openinx", "createdAt": "2020-08-07T03:03:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgyNDAwNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466824006", "bodyText": "I think here is constructing the TimestampData from LocalDataTime, but not converting to LocalDataTime.", "author": "chenjunjiedada", "createdAt": "2020-08-07T05:10:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NDcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyMzA1OQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466923059", "bodyText": "OK. But I think we could construct the TimestampData by a simply way like following:\n      long micros = readLong();\n      return TimestampData.fromEpochMillis(Math.floorDiv(micros, 1_000),\n          (int) Math.floorMod(micros, 1_000) * 1_000);\nBTW,  we don't have two timestamp readers: TimestampTzReader and TimestampReader.  Because for both cases, we just read the timestamp into a TimestampData  and there's no difference.  So we could only keep one timestamp reader,  name it as TimestampMicrosReader.", "author": "openinx", "createdAt": "2020-08-07T09:16:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NDcyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyODY4MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r467028680", "bodyText": "In the previous version, I used this way and change to this because I want to make it symmetrical. It looks not necessary now.   Just changed back.", "author": "chenjunjiedada", "createdAt": "2020-08-07T13:08:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NDcyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466798700", "bodyText": "Here we pass the bigDecimal.precision() to DecimalData.fromBigDecimal,  I think it's incorrect.  Because the precision expected for DecimalData is the precision of data type,  it will use this value to do ROUND  etc. You could see the comments in DecimalData:\n\t// The semantics of the fields are as follows:\n\t//  - `precision` and `scale` represent the precision and scale of SQL decimal type\n\t//  - If `decimalVal` is set, it represents the whole decimal value\n\t//  - Otherwise, the decimal value is longVal/(10^scale).\n\t//\n\t// Note that the (precision, scale) must be correct.\n\t// if precision > MAX_COMPACT_PRECISION,\n\t//   `decimalVal` represents the value. `longVal` is undefined\n\t// otherwise, (longVal, scale) represents the value\n\t//   `decimalVal` may be set and cached\n\n\tfinal int precision;\n\tfinal int scale;\nI think we need to pass the decimal type's precision and scale,   and the use the DecimalData.fromUnscaledBytes(binary.getBytes(), precision, scale);  to construct the DecimalData.", "author": "openinx", "createdAt": "2020-08-07T03:20:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5OTEwOQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466799109", "bodyText": "BTW, is it possible that  we have few unit tests to address this ?", "author": "openinx", "createdAt": "2020-08-07T03:22:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgzNTU3MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466835571", "bodyText": "@openinx is right, it should be same to FlinkValueReaders.DecimalReader.", "author": "JingsongLi", "createdAt": "2020-08-07T05:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwODUyNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466908525", "bodyText": "Fixed.  I think we can add unit tests when write side is ready so that we can write RowData and read out to validate. Does that make sense to you?", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:47:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyMzYzMA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466923630", "bodyText": "Yeah, make sense.  I think we may need a TODO comment here so that we won't forget that.", "author": "openinx", "createdAt": "2020-08-07T09:17:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyODk2NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r467028964", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-07T13:08:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5ODcwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTE4NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466801184", "bodyText": "Q: when the case readPos >= list.capacity(), seems we don't need to increment the readPos ?  For my understanding,  in that case the getElement should return a null, means that we will read the value without a provided reuse element.  Then we also shouldn't move the readPos because we actually don't reuse any object from the list.\nOf course, if we increment readPos here,  the logic is also correct. but seem have less possibility to reuse the object in array.\nHow do you think ?", "author": "openinx", "createdAt": "2020-08-07T03:31:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwNTYyOQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466905629", "bodyText": "The addElement will put new read element to the grown buffer, and that should not be reused.  for example:\nwhen capacity = 8, readPos = 8,  list will grow the capacity to 16, and addElement will fill the list[8] with new read obejct, which should not be overwritten.", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:42:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyNDIwNw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466924207", "bodyText": "OK, you are right.", "author": "openinx", "createdAt": "2020-08-07T09:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTE4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMTM5Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466801392", "bodyText": "Same question .", "author": "openinx", "createdAt": "2020-08-07T03:31:59Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,701 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+          case DATE:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int scale) {\n+      super(desc);\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, bigDecimal.precision(), scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMzE0Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466803146", "bodyText": "nit:  Lists.newArrayList.", "author": "openinx", "createdAt": "2020-08-07T03:39:57Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,269 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = new ArrayList<>();", "originalCommit": "f8e130fc17322975d0e970abfcc25f30abbc622d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkwODU4NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466908584", "bodyText": "fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-07T08:48:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjgwMzE0Ng=="}], "type": "inlineReview"}, {"oid": "08b40f4a79884165dda109247f226293fd6a1ac4", "url": "https://github.com/apache/iceberg/commit/08b40f4a79884165dda109247f226293fd6a1ac4", "message": "address comments", "committedDate": "2020-08-07T08:42:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyOTk4Nw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466929987", "bodyText": "nit:  use ((Types.TimestampType) type).shouldAdjustToUTC().", "author": "openinx", "createdAt": "2020-08-07T09:30:06Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type.asPrimitiveType()).shouldAdjustToUTC()) {", "originalCommit": "08b40f4a79884165dda109247f226293fd6a1ac4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAyOTAzNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r467029035", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-07T13:08:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjkyOTk4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njk0ODA5Nw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r466948097", "bodyText": "For flink GenericRowData,  we don't provide setInteger/setLong/setFloat interfaces etc, so seems we don't have to override those methods from ParquetValueReaders.StructReader, because all of them have the default implementation, for example:\n    protected void setBoolean(I struct, int pos, boolean value) {\n      set(struct, pos, value);\n    }\nI mean override the set(I struct, int pos, Object value) is enough.\nBTW, I saw Setter interface in ParquetValueReaders.StructReader don't have meaningful usage. Is it necessary to create a separate pull request to remove all methods related to Setter  ?  cc @rdblue .", "author": "openinx", "createdAt": "2020-08-07T10:08:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,703 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.ZoneOffset;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            if (((Types.TimestampType) expected).shouldAdjustToUTC()) {\n+              return new TimestampTzReader(desc);\n+            } else {\n+              return new TimestampReader(desc);\n+            }\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      return DecimalData.fromBigDecimal(bigDecimal, precision, scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n+  }\n+\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n+  }\n+\n+  private static class TimestampTzReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampTzReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromLocalDateTime(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000)\n+          .atOffset(ZoneOffset.UTC)\n+          .toLocalDateTime());\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class TimestampReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long value = readLong();\n+      return TimestampData.fromInstant(Instant.ofEpochSecond(Math.floorDiv(value, 1000_000),\n+          Math.floorMod(value, 1000_000) * 1000));\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n+\n+      writePos += 1;\n+    }\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n+  }\n+\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n+\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void set(GenericRowData row, int pos, Object value) {\n       row.setField(pos, value);\n     }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n+    }\n+\n+    @Override\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n+      row.setField(pos, value);\n+    }", "originalCommit": "08b40f4a79884165dda109247f226293fd6a1ac4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "url": "https://github.com/apache/iceberg/commit/0f5bf8317204f59d4bafbefe8530a72a54635cd4", "message": "address comments", "committedDate": "2020-08-07T13:05:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNTI2MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469415261", "bodyText": "If the objects to validate are Record and RowData, then type should be StructType.", "author": "rdblue", "createdAt": "2020-08-12T17:14:48Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDE1Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690153", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:30:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNTI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNjczOQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469416739", "bodyText": "If this uses getFieldOrNull then there is no need to do a null check at the top of this loop. It can be done in assertEquals called here.", "author": "rdblue", "createdAt": "2020-08-12T17:17:08Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDE2OA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690168", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:30:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNjczOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNzY0Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469417642", "bodyText": "Why call supplier.get() in every case when you could pass an object instead of a supplier?", "author": "rdblue", "createdAt": "2020-08-12T17:18:33Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDE4Nw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690187", "bodyText": "Fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:30:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNzY0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQ3Mzk1MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r473473950", "bodyText": "Sorry, I meant to remove the supplier entirely. Why is it necessary to pass Supplier<Object> and not just Object to this method? As far as I can tell, get is called on the supplier immediately in all cases.", "author": "rdblue", "createdAt": "2020-08-20T00:30:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNzY0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxODI4NQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469418285", "bodyText": "Can this use the utility methods to convert instead?", "author": "rdblue", "createdAt": "2020-08-12T17:19:33Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDI0Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690242", "bodyText": "Sure, updated.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:31:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxODI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTI1Ng==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469419256", "bodyText": "It is good that this uses a different conversion to LocalDateTime.", "author": "rdblue", "createdAt": "2020-08-12T17:21:11Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTU5OQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469419599", "bodyText": "Why does this convert to LocalDateTime before validating? Can TimestampData be converted to OffsetDateTime?", "author": "rdblue", "createdAt": "2020-08-12T17:21:44Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDU0Mw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690543", "bodyText": "There is no direct method to convert TimestampData to OffsetDateTime.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:32:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTU5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMDY5MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469420690", "bodyText": "This test is brittle because it assumes:\n\nByteBuffer is a HeapByteBuffer\nThe position and limit of the ByteBuffer are 0 and buffer.array().length\nThe buffer's arrayOffset is 0\n\nIt would be better to convert the byte array to a ByteBuffer and validate equality with buffers.", "author": "rdblue", "createdAt": "2020-08-12T17:23:40Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDYwOA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690608", "bodyText": "Updated to compare ByteBuffer.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:32:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMDY5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNDUyNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469424526", "bodyText": "I think it would be better to keep this already large method a bit smaller and add a method to assert array equality, not just one for values. Same with maps. An assertEquals(MapType, LogicalType, Map, MapData) method would be helpful.", "author": "rdblue", "createdAt": "2020-08-12T17:30:05Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());\n+        break;\n+      case DECIMAL:\n+        Assert.assertTrue(\"Should expect a BigDecimal\", expected instanceof BigDecimal);\n+        BigDecimal bd = (BigDecimal) expected;\n+        Assert.assertEquals(\"decimal value should be equal\", bd,\n+            ((DecimalData) supplier.get()).toBigDecimal());\n+        break;\n+      case LIST:\n+        Assert.assertTrue(\"Should expect a Collection\", expected instanceof Collection);\n+        Collection<?> expectedArrayData = (Collection<?>) expected;\n+        ArrayData actualArrayData = (ArrayData) supplier.get();\n+        LogicalType elementType = ((ArrayType) logicalType).getElementType();\n+        Assert.assertEquals(\"array length should be equal\", expectedArrayData.size(), actualArrayData.size());\n+        assertArrayValues(type.asListType().elementType(), elementType, expectedArrayData, actualArrayData);", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MDY0NA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469690644", "bodyText": "Done.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:32:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNDUyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNjEwMw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469426103", "bodyText": "keySet isn't required to be an ordered collection, so this could easily break if keys are not returned in the same order as they are in MapData. This also allows maps to be equal that actually are not, like Map('b' -> 1, 'a' -> 2) and Map('a' -> 1, 'b' -> 2) if the first map's keys are reordered.", "author": "rdblue", "createdAt": "2020-08-12T17:32:47Z", "path": "flink/src/test/java/org/apache/iceberg/flink/data/TestHelpers.java", "diffHunk": "@@ -0,0 +1,198 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.data;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Supplier;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.flink.table.types.logical.ArrayType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.MapType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+\n+public class TestHelpers {\n+  private TestHelpers() {}\n+\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochMilli(0L).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  public static void assertRowData(Type type, LogicalType rowType, Record expectedRecord, RowData actualRowData) {\n+    if (expectedRecord == null && actualRowData == null) {\n+      return;\n+    }\n+\n+    Assert.assertTrue(\"expected Record and actual RowData should be both null or not null\",\n+        expectedRecord != null && actualRowData != null);\n+\n+    List<Type> types = Lists.newArrayList();\n+    for (Types.NestedField field : type.asStructType().fields()) {\n+      types.add(field.type());\n+    }\n+\n+    for (int i = 0; i < types.size(); i += 1) {\n+      if (expectedRecord.get(i) == null) {\n+        Assert.assertTrue(actualRowData.isNullAt(i));\n+        continue;\n+      }\n+\n+      Object expected = expectedRecord.get(i);\n+      LogicalType logicalType = ((RowType) rowType).getTypeAt(i);\n+\n+      final int fieldPos = i;\n+      assertEquals(types.get(i), logicalType, expected,\n+          () -> RowData.createFieldGetter(logicalType, fieldPos).getFieldOrNull(actualRowData));\n+    }\n+  }\n+\n+  private static void assertEquals(Type type, LogicalType logicalType, Object expected, Supplier<Object> supplier) {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        Assert.assertEquals(\"boolean value should be equal\", expected, supplier.get());\n+        break;\n+      case INTEGER:\n+        Assert.assertEquals(\"int value should be equal\", expected, supplier.get());\n+        break;\n+      case LONG:\n+        Assert.assertEquals(\"long value should be equal\", expected, supplier.get());\n+        break;\n+      case FLOAT:\n+        Assert.assertEquals(\"float value should be equal\", expected, supplier.get());\n+        break;\n+      case DOUBLE:\n+        Assert.assertEquals(\"double value should be equal\", expected, supplier.get());\n+        break;\n+      case STRING:\n+        Assert.assertTrue(\"Should expect a CharSequence\", expected instanceof CharSequence);\n+        Assert.assertEquals(\"string should be equal\",\n+            String.valueOf(expected), supplier.get().toString());\n+        break;\n+      case DATE:\n+        Assert.assertTrue(\"Should expect a Date\", expected instanceof LocalDate);\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (int) supplier.get());\n+        Assert.assertEquals(\"date should be equal\", expected, date);\n+        break;\n+      case TIME:\n+        Assert.assertTrue(\"Should expect a LocalTime\", expected instanceof LocalTime);\n+        int milliseconds = (int) (((LocalTime) expected).toNanoOfDay() / 1000_000);\n+        Assert.assertEquals(\"time millis should be equal\", milliseconds, supplier.get());\n+        break;\n+      case TIMESTAMP:\n+        if (((Types.TimestampType) type).shouldAdjustToUTC()) {\n+          Assert.assertTrue(\"Should expect a OffsetDataTime\", expected instanceof OffsetDateTime);\n+          OffsetDateTime ts = (OffsetDateTime) expected;\n+          Assert.assertEquals(\"OffsetDataTime should be equal\", ts.toLocalDateTime(),\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        } else {\n+          Assert.assertTrue(\"Should expect a LocalDataTime\", expected instanceof LocalDateTime);\n+          LocalDateTime ts = (LocalDateTime) expected;\n+          Assert.assertEquals(\"LocalDataTime should be equal\", ts,\n+              ((TimestampData) supplier.get()).toLocalDateTime());\n+        }\n+        break;\n+      case BINARY:\n+        Assert.assertTrue(\"Should expect a ByteBuffer\", expected instanceof ByteBuffer);\n+        Assert.assertArrayEquals(\"binary should be equal\", ((ByteBuffer) expected).array(), (byte[]) supplier.get());\n+        break;\n+      case DECIMAL:\n+        Assert.assertTrue(\"Should expect a BigDecimal\", expected instanceof BigDecimal);\n+        BigDecimal bd = (BigDecimal) expected;\n+        Assert.assertEquals(\"decimal value should be equal\", bd,\n+            ((DecimalData) supplier.get()).toBigDecimal());\n+        break;\n+      case LIST:\n+        Assert.assertTrue(\"Should expect a Collection\", expected instanceof Collection);\n+        Collection<?> expectedArrayData = (Collection<?>) expected;\n+        ArrayData actualArrayData = (ArrayData) supplier.get();\n+        LogicalType elementType = ((ArrayType) logicalType).getElementType();\n+        Assert.assertEquals(\"array length should be equal\", expectedArrayData.size(), actualArrayData.size());\n+        assertArrayValues(type.asListType().elementType(), elementType, expectedArrayData, actualArrayData);\n+        break;\n+      case MAP:\n+        Assert.assertTrue(\"Should expect a Map\", expected instanceof Map);\n+        MapData actual = (MapData) supplier.get();\n+        Assert.assertEquals(\"map size should be equal\",\n+            ((Map<?, ?>) expected).size(), actual.size());\n+        Collection<?> expectedKeyArrayData = ((Map<?, ?>) expected).keySet();", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTcwMDE0MA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469700140", "bodyText": "Fixed.", "author": "chenjunjiedada", "createdAt": "2020-08-13T05:10:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNjEwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDAyNw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469430027", "bodyText": "Shouldn't this check whether the timestamp is a TIMESTAMP WITH ZONE or TIMESTAMP WITHOUT ZONE and return the correct reader? Spark doesn't do this because it doesn't support timestamp without zone, so we know it is always a timestamptz.", "author": "rdblue", "createdAt": "2020-08-12T17:39:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicrosReader(desc);", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MjQxNg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469692416", "bodyText": "Updated.  @openinx, you might want to have a check here. I changed this back to two readers because the type in the imported parquet file could also have time zone info as well.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:40:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDAyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDkyMA==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469430920", "bodyText": "What about TIME_MILLIS? It won't be written by Iceberg, but it is usually good to be able to read from imported files.", "author": "rdblue", "createdAt": "2020-08-12T17:41:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MjQ0OQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469692449", "bodyText": "Added.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:40:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMDkyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMTAyNQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469431025", "bodyText": "Why not support TIMESTAMP_MILLIS as well?", "author": "rdblue", "createdAt": "2020-08-12T17:41:17Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5MjQ4Mg==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469692482", "bodyText": "Added.", "author": "chenjunjiedada", "createdAt": "2020-08-13T04:40:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzMTAyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzODAyMw==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469438023", "bodyText": "I think we can use the same ReusableMapData class for both Spark and Flink, since they are so similar. Some of the data access methods in the concrete classes would need to be implemented in a concrete class the engine-specific interface, but the main parts could be shared. And I think that would mean we could share significant parts of the reader classes as well.\nI don't think we need to do this refactor now, but we should try to deduplicate this.", "author": "rdblue", "createdAt": "2020-08-12T17:53:26Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkParquetReaders.java", "diffHunk": "@@ -19,64 +19,678 @@\n \n package org.apache.iceberg.flink.data;\n \n+import java.math.BigDecimal;\n+import java.math.BigInteger;\n+import java.nio.ByteBuffer;\n import java.util.List;\n-import org.apache.flink.types.Row;\n+import java.util.Map;\n+import org.apache.commons.lang3.ArrayUtils;\n+import org.apache.flink.table.data.ArrayData;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.GenericRowData;\n+import org.apache.flink.table.data.MapData;\n+import org.apache.flink.table.data.RawValueData;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.data.parquet.BaseParquetReaders;\n import org.apache.iceberg.parquet.ParquetValueReader;\n import org.apache.iceberg.parquet.ParquetValueReaders;\n+import org.apache.iceberg.parquet.TypeWithSchemaVisitor;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.LogicalTypeAnnotation.DecimalLogicalTypeAnnotation;\n import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.Type;\n \n-public class FlinkParquetReaders extends BaseParquetReaders<Row> {\n+class FlinkParquetReaders {\n+  private FlinkParquetReaders() {\n+  }\n \n-  private static final FlinkParquetReaders INSTANCE = new FlinkParquetReaders();\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema, MessageType fileSchema) {\n+    return buildReader(expectedSchema, fileSchema, ImmutableMap.of());\n+  }\n \n-  private FlinkParquetReaders() {\n+  @SuppressWarnings(\"unchecked\")\n+  public static ParquetValueReader<RowData> buildReader(Schema expectedSchema,\n+                                                        MessageType fileSchema,\n+                                                        Map<Integer, ?> idToConstant) {\n+    return (ParquetValueReader<RowData>) TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,\n+        new ReadBuilder(fileSchema, idToConstant)\n+    );\n+  }\n+\n+  private static class ReadBuilder extends TypeWithSchemaVisitor<ParquetValueReader<?>> {\n+    private final MessageType type;\n+    private final Map<Integer, ?> idToConstant;\n+\n+    ReadBuilder(MessageType type, Map<Integer, ?> idToConstant) {\n+      this.type = type;\n+      this.idToConstant = idToConstant;\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> message(Types.StructType expected, MessageType message,\n+                                               List<ParquetValueReader<?>> fieldReaders) {\n+      return struct(expected, message.asGroupType(), fieldReaders);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<RowData> struct(Types.StructType expected, GroupType struct,\n+                                              List<ParquetValueReader<?>> fieldReaders) {\n+      // match the expected struct's order\n+      Map<Integer, ParquetValueReader<?>> readersById = Maps.newHashMap();\n+      Map<Integer, Type> typesById = Maps.newHashMap();\n+      List<Type> fields = struct.getFields();\n+      for (int i = 0; i < fields.size(); i += 1) {\n+        Type fieldType = fields.get(i);\n+        int fieldD = type.getMaxDefinitionLevel(path(fieldType.getName())) - 1;\n+        if (fieldType.getId() != null) {\n+          int id = fieldType.getId().intValue();\n+          readersById.put(id, ParquetValueReaders.option(fieldType, fieldD, fieldReaders.get(i)));\n+          typesById.put(id, fieldType);\n+        }\n+      }\n+\n+      List<Types.NestedField> expectedFields = expected != null ?\n+          expected.fields() : ImmutableList.of();\n+      List<ParquetValueReader<?>> reorderedFields = Lists.newArrayListWithExpectedSize(\n+          expectedFields.size());\n+      List<Type> types = Lists.newArrayListWithExpectedSize(expectedFields.size());\n+      for (Types.NestedField field : expectedFields) {\n+        int id = field.fieldId();\n+        if (idToConstant.containsKey(id)) {\n+          // containsKey is used because the constant may be null\n+          reorderedFields.add(ParquetValueReaders.constant(idToConstant.get(id)));\n+          types.add(null);\n+        } else {\n+          ParquetValueReader<?> reader = readersById.get(id);\n+          if (reader != null) {\n+            reorderedFields.add(reader);\n+            types.add(typesById.get(id));\n+          } else {\n+            reorderedFields.add(ParquetValueReaders.nulls());\n+            types.add(null);\n+          }\n+        }\n+      }\n+\n+      return new RowDataReader(types, reorderedFields);\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> list(Types.ListType expectedList, GroupType array,\n+                                      ParquetValueReader<?> elementReader) {\n+      GroupType repeated = array.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type elementType = repeated.getType(0);\n+      int elementD = type.getMaxDefinitionLevel(path(elementType.getName())) - 1;\n+\n+      return new ArrayReader<>(repeatedD, repeatedR, ParquetValueReaders.option(elementType, elementD, elementReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> map(Types.MapType expectedMap, GroupType map,\n+                                     ParquetValueReader<?> keyReader,\n+                                     ParquetValueReader<?> valueReader) {\n+      GroupType repeatedKeyValue = map.getFields().get(0).asGroupType();\n+      String[] repeatedPath = currentPath();\n+\n+      int repeatedD = type.getMaxDefinitionLevel(repeatedPath) - 1;\n+      int repeatedR = type.getMaxRepetitionLevel(repeatedPath) - 1;\n+\n+      Type keyType = repeatedKeyValue.getType(0);\n+      int keyD = type.getMaxDefinitionLevel(path(keyType.getName())) - 1;\n+      Type valueType = repeatedKeyValue.getType(1);\n+      int valueD = type.getMaxDefinitionLevel(path(valueType.getName())) - 1;\n+\n+      return new MapReader<>(repeatedD, repeatedR,\n+          ParquetValueReaders.option(keyType, keyD, keyReader),\n+          ParquetValueReaders.option(valueType, valueD, valueReader));\n+    }\n+\n+    @Override\n+    public ParquetValueReader<?> primitive(org.apache.iceberg.types.Type.PrimitiveType expected,\n+                                           PrimitiveType primitive) {\n+      ColumnDescriptor desc = type.getColumnDescription(currentPath());\n+\n+      if (primitive.getOriginalType() != null) {\n+        switch (primitive.getOriginalType()) {\n+          case ENUM:\n+          case JSON:\n+          case UTF8:\n+            return new StringReader(desc);\n+          case INT_8:\n+          case INT_16:\n+          case INT_32:\n+            if (expected != null && expected.typeId() == Types.LongType.get().typeId()) {\n+              return new ParquetValueReaders.IntAsLongReader(desc);\n+            } else {\n+              return new ParquetValueReaders.UnboxedReader<>(desc);\n+            }\n+          case TIME_MICROS:\n+            return new LossyMicrosToMillisTimeReader(desc);\n+          case DATE:\n+          case INT_64:\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          case TIMESTAMP_MICROS:\n+            return new TimestampMicrosReader(desc);\n+          case DECIMAL:\n+            DecimalLogicalTypeAnnotation decimal = (DecimalLogicalTypeAnnotation) primitive.getLogicalTypeAnnotation();\n+            switch (primitive.getPrimitiveTypeName()) {\n+              case BINARY:\n+              case FIXED_LEN_BYTE_ARRAY:\n+                return new BinaryDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT64:\n+                return new LongDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              case INT32:\n+                return new IntegerDecimalReader(desc, decimal.getPrecision(), decimal.getScale());\n+              default:\n+                throw new UnsupportedOperationException(\n+                    \"Unsupported base type for decimal: \" + primitive.getPrimitiveTypeName());\n+            }\n+          case BSON:\n+            return new ParquetValueReaders.ByteArrayReader(desc);\n+          default:\n+            throw new UnsupportedOperationException(\n+                \"Unsupported logical type: \" + primitive.getOriginalType());\n+        }\n+      }\n+\n+      switch (primitive.getPrimitiveTypeName()) {\n+        case FIXED_LEN_BYTE_ARRAY:\n+        case BINARY:\n+          return new ParquetValueReaders.ByteArrayReader(desc);\n+        case INT32:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.LONG) {\n+            return new ParquetValueReaders.IntAsLongReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case FLOAT:\n+          if (expected != null && expected.typeId() == org.apache.iceberg.types.Type.TypeID.DOUBLE) {\n+            return new ParquetValueReaders.FloatAsDoubleReader(desc);\n+          } else {\n+            return new ParquetValueReaders.UnboxedReader<>(desc);\n+          }\n+        case BOOLEAN:\n+        case INT64:\n+        case DOUBLE:\n+          return new ParquetValueReaders.UnboxedReader<>(desc);\n+        default:\n+          throw new UnsupportedOperationException(\"Unsupported type: \" + primitive);\n+      }\n+    }\n+  }\n+\n+  private static class BinaryDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    BinaryDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      Binary binary = column.nextBinary();\n+      BigDecimal bigDecimal = new BigDecimal(new BigInteger(binary.getBytes()), scale);\n+      // TODO: need a unit test to write-read-validate decimal via FlinkParquetWrite/Reader\n+      return DecimalData.fromBigDecimal(bigDecimal, precision, scale);\n+    }\n+  }\n+\n+  private static class IntegerDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    IntegerDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextInteger(), precision, scale);\n+    }\n   }\n \n-  public static ParquetValueReader<Row> buildReader(Schema expectedSchema, MessageType fileSchema) {\n-    return INSTANCE.createReader(expectedSchema, fileSchema);\n+  private static class LongDecimalReader extends ParquetValueReaders.PrimitiveReader<DecimalData> {\n+    private final int precision;\n+    private final int scale;\n+\n+    LongDecimalReader(ColumnDescriptor desc, int precision, int scale) {\n+      super(desc);\n+      this.precision = precision;\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public DecimalData read(DecimalData ignored) {\n+      return DecimalData.fromUnscaledLong(column.nextLong(), precision, scale);\n+    }\n   }\n \n-  @Override\n-  protected ParquetValueReader<Row> createStructReader(List<Type> types,\n-                                                       List<ParquetValueReader<?>> fieldReaders,\n-                                                       Types.StructType structType) {\n-    return new RowReader(types, fieldReaders, structType);\n+  private static class TimestampMicrosReader extends ParquetValueReaders.UnboxedReader<TimestampData> {\n+    TimestampMicrosReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public TimestampData read(TimestampData ignored) {\n+      long micros = readLong();\n+      return TimestampData.fromEpochMillis(Math.floorDiv(micros, 1_000),\n+          (int) Math.floorMod(micros, 1_000) * 1_000);\n+    }\n+\n+    @Override\n+    public long readLong() {\n+      return column.nextLong();\n+    }\n+  }\n+\n+  private static class StringReader extends ParquetValueReaders.PrimitiveReader<StringData> {\n+    StringReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public StringData read(StringData ignored) {\n+      Binary binary = column.nextBinary();\n+      ByteBuffer buffer = binary.toByteBuffer();\n+      if (buffer.hasArray()) {\n+        return StringData.fromBytes(\n+            buffer.array(), buffer.arrayOffset() + buffer.position(), buffer.remaining());\n+      } else {\n+        return StringData.fromBytes(binary.getBytes());\n+      }\n+    }\n+  }\n+\n+  private static class LossyMicrosToMillisTimeReader extends ParquetValueReaders.PrimitiveReader<Integer> {\n+    LossyMicrosToMillisTimeReader(ColumnDescriptor desc) {\n+      super(desc);\n+    }\n+\n+    @Override\n+    public Integer read(Integer reuse) {\n+      // Discard microseconds since Flink uses millisecond unit for TIME type.\n+      return (int) Math.floorDiv(column.nextLong(), 1000);\n+    }\n+  }\n+\n+  private static class ArrayReader<E> extends ParquetValueReaders.RepeatedReader<ArrayData, ReusableArrayData, E> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    ArrayReader(int definitionLevel, int repetitionLevel, ParquetValueReader<E> reader) {\n+      super(definitionLevel, repetitionLevel, reader);\n+    }\n+\n+    @Override\n+    protected ReusableArrayData newListData(ArrayData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableArrayData) {\n+        return (ReusableArrayData) reuse;\n+      } else {\n+        return new ReusableArrayData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected E getElement(ReusableArrayData list) {\n+      E value = null;\n+      if (readPos < list.capacity()) {\n+        value = (E) list.values[readPos];\n+      }\n+\n+      readPos += 1;\n+\n+      return value;\n+    }\n+\n+    @Override\n+    protected void addElement(ReusableArrayData reused, E element) {\n+      if (writePos >= reused.capacity()) {\n+        reused.grow();\n+      }\n+\n+      reused.values[writePos] = element;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected ArrayData buildList(ReusableArrayData list) {\n+      list.setNumElements(writePos);\n+      return list;\n+    }\n+  }\n+\n+  private static class MapReader<K, V> extends\n+      ParquetValueReaders.RepeatedKeyValueReader<MapData, ReusableMapData, K, V> {\n+    private int readPos = 0;\n+    private int writePos = 0;\n+\n+    private final ParquetValueReaders.ReusableEntry<K, V> entry = new ParquetValueReaders.ReusableEntry<>();\n+    private final ParquetValueReaders.ReusableEntry<K, V> nullEntry = new ParquetValueReaders.ReusableEntry<>();\n+\n+    MapReader(int definitionLevel, int repetitionLevel,\n+              ParquetValueReader<K> keyReader, ParquetValueReader<V> valueReader) {\n+      super(definitionLevel, repetitionLevel, keyReader, valueReader);\n+    }\n+\n+    @Override\n+    protected ReusableMapData newMapData(MapData reuse) {\n+      this.readPos = 0;\n+      this.writePos = 0;\n+\n+      if (reuse instanceof ReusableMapData) {\n+        return (ReusableMapData) reuse;\n+      } else {\n+        return new ReusableMapData();\n+      }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected Map.Entry<K, V> getPair(ReusableMapData map) {\n+      Map.Entry<K, V> kv = nullEntry;\n+      if (readPos < map.capacity()) {\n+        entry.set((K) map.keys.values[readPos], (V) map.values.values[readPos]);\n+        kv = entry;\n+      }\n+\n+      readPos += 1;\n+\n+      return kv;\n+    }\n+\n+    @Override\n+    protected void addPair(ReusableMapData map, K key, V value) {\n+      if (writePos >= map.capacity()) {\n+        map.grow();\n+      }\n+\n+      map.keys.values[writePos] = key;\n+      map.values.values[writePos] = value;\n+\n+      writePos += 1;\n+    }\n+\n+    @Override\n+    protected MapData buildMap(ReusableMapData map) {\n+      map.setNumElements(writePos);\n+      return map;\n+    }\n   }\n \n-  private static class RowReader extends ParquetValueReaders.StructReader<Row, Row> {\n-    private final Types.StructType structType;\n+  private static class RowDataReader extends ParquetValueReaders.StructReader<RowData, GenericRowData> {\n+    private final int numFields;\n \n-    RowReader(List<Type> types, List<ParquetValueReader<?>> readers, Types.StructType struct) {\n+    RowDataReader(List<Type> types, List<ParquetValueReader<?>> readers) {\n       super(types, readers);\n-      this.structType = struct;\n+      this.numFields = readers.size();\n     }\n \n     @Override\n-    protected Row newStructData(Row reuse) {\n-      if (reuse != null) {\n-        return reuse;\n+    protected GenericRowData newStructData(RowData reuse) {\n+      if (reuse instanceof GenericRowData) {\n+        return (GenericRowData) reuse;\n       } else {\n-        return new Row(structType.fields().size());\n+        return new GenericRowData(numFields);\n       }\n     }\n \n     @Override\n-    protected Object getField(Row row, int pos) {\n-      return row.getField(pos);\n+    protected Object getField(GenericRowData intermediate, int pos) {\n+      return intermediate.getField(pos);\n+    }\n+\n+    @Override\n+    protected RowData buildStruct(GenericRowData struct) {\n+      return struct;\n+    }\n+\n+    @Override\n+    protected void set(GenericRowData row, int pos, Object value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setNull(GenericRowData row, int pos) {\n+      row.setField(pos, null);\n+    }\n+\n+    @Override\n+    protected void setBoolean(GenericRowData row, int pos, boolean value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setInteger(GenericRowData row, int pos, int value) {\n+      row.setField(pos, value);\n+    }\n+\n+    @Override\n+    protected void setLong(GenericRowData row, int pos, long value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected Row buildStruct(Row row) {\n-      return row;\n+    protected void setFloat(GenericRowData row, int pos, float value) {\n+      row.setField(pos, value);\n     }\n \n     @Override\n-    protected void set(Row row, int pos, Object value) {\n+    protected void setDouble(GenericRowData row, int pos, double value) {\n       row.setField(pos, value);\n     }\n   }\n+\n+  private static class ReusableMapData implements MapData {", "originalCommit": "0f5bf8317204f59d4bafbefe8530a72a54635cd4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTY5OTg1MQ==", "url": "https://github.com/apache/iceberg/pull/1266#discussion_r469699851", "bodyText": "Opened #1331.", "author": "chenjunjiedada", "createdAt": "2020-08-13T05:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQzODAyMw=="}], "type": "inlineReview"}, {"oid": "e1407d4deec232f8f3e9bcd213b28c7dfde48967", "url": "https://github.com/apache/iceberg/commit/e1407d4deec232f8f3e9bcd213b28c7dfde48967", "message": "address comments", "committedDate": "2020-08-13T04:30:03Z", "type": "commit"}]}