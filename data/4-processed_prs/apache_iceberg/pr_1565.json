{"pr_number": 1565, "pr_title": "Flink: move hadoop configuration to Loaders from Source/Sink API", "pr_createdAt": "2020-10-09T03:53:42Z", "pr_url": "https://github.com/apache/iceberg/pull/1565", "timeline": [{"oid": "64be1f964afdbf2fcbb67f821c1ff4070c8923b6", "url": "https://github.com/apache/iceberg/commit/64be1f964afdbf2fcbb67f821c1ff4070c8923b6", "message": "Flink: move hadoop configuration to Loaders from Source/Sink API", "committedDate": "2020-10-09T03:52:23Z", "type": "commit"}, {"oid": "2a7dc1f3d3fdaa7e5aad21820342c0d671032465", "url": "https://github.com/apache/iceberg/commit/2a7dc1f3d3fdaa7e5aad21820342c0d671032465", "message": "Fix ser conf", "committedDate": "2020-10-09T05:30:06Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502257571", "bodyText": "Nice catch, since the CatalogLoader need to be serializable, so all members need to be serializable. Do we need to add several unit tests to address the serializable issue so that we won't break it in future ?   similar to the unit test TestScanTaskSerialization ?", "author": "openinx", "createdAt": "2020-10-09T08:05:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/CatalogLoader.java", "diffHunk": "@@ -43,18 +44,18 @@ static CatalogLoader hive(String name, Configuration hadoopConf, String uri, int\n \n   class HadoopCatalogLoader implements CatalogLoader {\n     private final String catalogName;\n-    private final Configuration hadoopConf;\n+    private final SerializableConfiguration hadoopConf;", "originalCommit": "2a7dc1f3d3fdaa7e5aad21820342c0d671032465", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1ODI4Mw==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502258283", "bodyText": "TableLoader  need the similar unit test.", "author": "openinx", "createdAt": "2020-10-09T08:07:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI3NTQxMg==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502275412", "bodyText": "This is already tested by ITCases. It is the only way which must be passed.\nBut we can add more unit tests too.", "author": "JingsongLi", "createdAt": "2020-10-09T08:37:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2MzY4OA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502763688", "bodyText": "What do you mean ITCases?", "author": "chenjunjiedada", "createdAt": "2020-10-10T08:22:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc2NDgxNg==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502764816", "bodyText": "TestFlinkScanSql, TestFlinkTableSink and TestFlinkIcebergSink will cover this too.", "author": "JingsongLi", "createdAt": "2020-10-10T08:35:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMjIwMQ==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502832201", "bodyText": "I'm not an expert on how Flink will run in tests, but I think it would be safer not to rely on end-to-end or integration tests to check serialization because there may not be a guarantee that tasks are serialized for those runners. (And this was a problem for Hive testing in the past.)", "author": "rdblue", "createdAt": "2020-10-10T21:05:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyMjE0NQ==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503022145", "bodyText": "Flink StreamExecutionEnvironment.execute will launch a local cluster, the client will connect to this cluster by RPC through binary message.\nI think you are right, it is better to add unit test too.", "author": "JingsongLi", "createdAt": "2020-10-12T03:37:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI1NzU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI2NjcxNw==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502266717", "bodyText": "Is this only visible for testing ?  Moving to test package ?", "author": "openinx", "createdAt": "2020-10-09T08:21:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/TableLoader.java", "diffHunk": "@@ -45,23 +46,30 @@ static TableLoader fromCatalog(CatalogLoader catalogLoader, TableIdentifier iden\n   }\n \n   static TableLoader fromHadoopTable(String location) {", "originalCommit": "2a7dc1f3d3fdaa7e5aad21820342c0d671032465", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI3NTkxMg==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502275912", "bodyText": "I think it is useful for users, since Flink has default hadoop configuration.", "author": "JingsongLi", "createdAt": "2020-10-09T08:38:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjI2NjcxNw=="}], "type": "inlineReview"}, {"oid": "04aca2eb41224441903456e79c8afa9a53edb092", "url": "https://github.com/apache/iceberg/commit/04aca2eb41224441903456e79c8afa9a53edb092", "message": "Fix comments", "committedDate": "2020-10-09T08:36:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMTYxOA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502831618", "bodyText": "It's a little odd that the arguments here are the same as createCatalog, but that they are in a different order and use \"options\" rather than \"properties\". Should we at least use the same argument order?", "author": "rdblue", "createdAt": "2020-10-10T20:58:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java", "diffHunk": "@@ -62,21 +62,22 @@\n   /**\n    * Create an Iceberg {@link org.apache.iceberg.catalog.Catalog} loader to be used by this Flink catalog adapter.\n    *\n-   * @param name    Flink's catalog name\n-   * @param options Flink's catalog options\n+   * @param name       Flink's catalog name\n+   * @param hadoopConf Hadoop configuration for catalog\n+   * @param options    Flink's catalog options\n    * @return an Iceberg catalog loader\n    */\n-  protected CatalogLoader createCatalogLoader(String name, Map<String, String> options) {\n+  protected CatalogLoader createCatalogLoader(String name, Configuration hadoopConf, Map<String, String> options) {", "originalCommit": "04aca2eb41224441903456e79c8afa9a53edb092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAyMjcyMw==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503022723", "bodyText": "I will use same order and properties.", "author": "JingsongLi", "createdAt": "2020-10-12T03:40:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMTYxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMTk0MA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502831940", "bodyText": "I don't think it is sufficient to test that the loader's toString representation is the same. This would still pass if the underlying Configuration were just set to null and not serialized at all. I think it would make sense to do something that will fail if the Configuration is not serialized and deserialized correctly, like load the table. In that case, you would just need to create a Hadoop table on the local FS and load it through the copied loader.", "author": "rdblue", "createdAt": "2020-10-10T21:01:40Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestTableLoader.java", "diffHunk": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestTableLoader {\n+\n+  @Test\n+  public void testJavaSerialization() throws IOException, ClassNotFoundException {\n+    TableLoader loader = TableLoader.fromHadoopTable(\"/my_path/my_table\");\n+    TableLoader copied = TestCatalogLoader.javaSerAndDeSer(loader);\n+    Assert.assertEquals(\"HadoopTableLoader{location=/my_path/my_table}\", copied.toString());", "originalCommit": "04aca2eb41224441903456e79c8afa9a53edb092", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzAzMTEzMw==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503031133", "bodyText": "Agree, I think we can check FileIO too.", "author": "JingsongLi", "createdAt": "2020-10-12T04:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMTk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMjAwNQ==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r502832005", "bodyText": "Same here. I think these tests should do a little more than validate the toString method.", "author": "rdblue", "createdAt": "2020-10-10T21:02:22Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestCatalogLoader.java", "diffHunk": "@@ -0,0 +1,58 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import org.apache.hadoop.conf.Configuration;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestCatalogLoader {\n+\n+  @Test\n+  public void testHadoopJavaSerialization() throws IOException, ClassNotFoundException {", "originalCommit": "04aca2eb41224441903456e79c8afa9a53edb092", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4d6e2bf8da9d5004be4a8340f6746440eb9b617b", "url": "https://github.com/apache/iceberg/commit/4d6e2bf8da9d5004be4a8340f6746440eb9b617b", "message": "Address comments", "committedDate": "2020-10-12T04:24:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE2ODgwMg==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503168802", "bodyText": "nit:   I see somewhere call it options and other place call it properties, is it possible to unify them ?", "author": "openinx", "createdAt": "2020-10-12T09:38:40Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSource.java", "diffHunk": "@@ -68,13 +64,13 @@ public boolean isBounded() {\n \n   @Override\n   public TableSource<RowData> projectFields(int[] fields) {\n-    return new IcebergTableSource(loader, hadoopConf, schema, options, fields);\n+    return new IcebergTableSource(loader, schema, options, fields);\n   }\n \n   @Override\n   public DataStream<RowData> getDataStream(StreamExecutionEnvironment execEnv) {\n-    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).hadoopConf(hadoopConf)\n-        .project(getProjectedSchema()).properties(options).build();\n+    return FlinkSource.forRowData().env(execEnv).tableLoader(loader).project(getProjectedSchema())\n+        .properties(options).build();", "originalCommit": "4d6e2bf8da9d5004be4a8340f6746440eb9b617b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3NTMwNw==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503175307", "bodyText": "OK I'll do it.", "author": "JingsongLi", "createdAt": "2020-10-12T09:48:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE2ODgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3MzMxNA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503173314", "bodyText": "Missed a test for HiveTableLoader ?  I think we don't have to open the table actually,  just need to ensure the members inside the instance are the same, maybe adding an equals inside the CatalogLoader or TableLoader.", "author": "openinx", "createdAt": "2020-10-12T09:45:47Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestCatalogTableLoader.java", "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.types.Types;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+/**\n+ * Test for {@link CatalogLoader} and {@link TableLoader}.\n+ */\n+public class TestCatalogTableLoader extends FlinkTestBase {\n+\n+  private static File warehouse = null;\n+  private static final TableIdentifier IDENTIFIER = TableIdentifier.of(\"default\", \"my_table\");\n+  private static final Schema SCHEMA = new Schema(Types.NestedField.required(1, \"f1\", Types.StringType.get()));\n+\n+  @BeforeClass\n+  public static void createWarehouse() throws IOException {\n+    warehouse = File.createTempFile(\"warehouse\", null);\n+    Assert.assertTrue(warehouse.delete());\n+    hiveConf.set(\"my_key\", \"my_value\");\n+  }\n+\n+  @AfterClass\n+  public static void dropWarehouse() {\n+    if (warehouse != null && warehouse.exists()) {\n+      warehouse.delete();\n+    }\n+  }\n+\n+  @Test\n+  public void testHadoopCatalogLoader() throws IOException, ClassNotFoundException {\n+    CatalogLoader loader = CatalogLoader.hadoop(\"my_catalog\", hiveConf, \"file:\" + warehouse);\n+    validateHadoopConf(javaSerAndDeSer(loader).loadCatalog().createTable(IDENTIFIER, SCHEMA));\n+  }\n+\n+  @Test\n+  public void testHiveCatalogLoader() throws IOException, ClassNotFoundException {\n+    CatalogLoader loader = CatalogLoader.hive(\"my_catalog\", hiveConf, null, 2);\n+    validateHadoopConf(javaSerAndDeSer(loader).loadCatalog().createTable(IDENTIFIER, SCHEMA));\n+  }\n+\n+  @Test\n+  public void testHadoopTableLoader() throws IOException, ClassNotFoundException {\n+    String location = \"file:\" + warehouse + \"/my_table\";\n+    new HadoopTables(hiveConf).create(SCHEMA, location);\n+    TableLoader loader = TableLoader.fromHadoopTable(location, hiveConf);\n+    TableLoader copied = javaSerAndDeSer(loader);\n+    copied.open();\n+    try {\n+      validateHadoopConf(copied.loadTable());\n+    } finally {\n+      copied.close();\n+    }\n+  }\n+", "originalCommit": "4d6e2bf8da9d5004be4a8340f6746440eb9b617b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3NzM5MQ==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503177391", "bodyText": "You mean Tableloader.CatalogTableloader class? I think we don't need to add case for it since we have case for Catalog Loaders.\nOpening the table can cover more since they are loaders, we can try to load something and ensure that is correct.", "author": "JingsongLi", "createdAt": "2020-10-12T09:52:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3MzMxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzYzMTI2NA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503631264", "bodyText": "OK, it's 'CatalogTableLoader'.  it has two members: CatalogLoader & String identifier (the other one is transient member). I agree that in this case if we could assert that CatalogLoader is serializing & deserializing correctly, then it imply that CatalogTableLoader should be OK. But providing those unit tests is to ensure that we won't break this point in future.  What if others introduce another non-serializable member in CatalogTableLoader class?  that's why I said it seems to be more reasonable to write a ser/der unit test for CatalogTableLoader.", "author": "openinx", "createdAt": "2020-10-13T02:29:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3MzMxNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY1MDc2MA==", "url": "https://github.com/apache/iceberg/pull/1565#discussion_r503650760", "bodyText": "make sense, I'll add test.", "author": "JingsongLi", "createdAt": "2020-10-13T03:48:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE3MzMxNA=="}], "type": "inlineReview"}, {"oid": "30c2cf7280d5ea07b98ab44d3928d08414500bbd", "url": "https://github.com/apache/iceberg/commit/30c2cf7280d5ea07b98ab44d3928d08414500bbd", "message": "more options to properties", "committedDate": "2020-10-12T09:48:18Z", "type": "commit"}, {"oid": "0a452321f68965cfb2de96e9d9ce748f0c9612d0", "url": "https://github.com/apache/iceberg/commit/0a452321f68965cfb2de96e9d9ce748f0c9612d0", "message": "Add test for TableLoader.fromCatalog", "committedDate": "2020-10-13T05:21:25Z", "type": "commit"}, {"oid": "0a452321f68965cfb2de96e9d9ce748f0c9612d0", "url": "https://github.com/apache/iceberg/commit/0a452321f68965cfb2de96e9d9ce748f0c9612d0", "message": "Add test for TableLoader.fromCatalog", "committedDate": "2020-10-13T05:21:25Z", "type": "forcePushed"}]}