{"pr_number": 1238, "pr_title": "Refactor the SparkOrcWriter by using OrcSchemaWithTypeVisitor#visit", "pr_createdAt": "2020-07-24T07:54:35Z", "pr_url": "https://github.com/apache/iceberg/pull/1238", "timeline": [{"oid": "1953a387d717acde1c3efafb95eb1a648e3ff105", "url": "https://github.com/apache/iceberg/commit/1953a387d717acde1c3efafb95eb1a648e3ff105", "message": "Refactor the SparkOrcWriter by using OrcSchemaWithTypeVisitor#visit", "committedDate": "2020-07-30T09:56:30Z", "type": "commit"}, {"oid": "1953a387d717acde1c3efafb95eb1a648e3ff105", "url": "https://github.com/apache/iceberg/commit/1953a387d717acde1c3efafb95eb1a648e3ff105", "message": "Refactor the SparkOrcWriter by using OrcSchemaWithTypeVisitor#visit", "committedDate": "2020-07-30T09:56:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzg2NzczNw==", "url": "https://github.com/apache/iceberg/pull/1238#discussion_r463867737", "bodyText": "Nit: this is the only one that isn't plural, byteArrays.", "author": "rdblue", "createdAt": "2020-07-31T22:10:35Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueWriters.java", "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.data;\n+\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+import org.apache.spark.sql.catalyst.expressions.SpecializedGetters;\n+import org.apache.spark.sql.catalyst.util.ArrayData;\n+import org.apache.spark.sql.catalyst.util.MapData;\n+\n+class SparkOrcValueWriters {\n+  private SparkOrcValueWriters() {\n+  }\n+\n+  public static SparkOrcValueWriter booleans() {\n+    return BooleanWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter bytes() {\n+    return ByteWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter shorts() {\n+    return ShortWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter ints() {\n+    return IntWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter longs() {\n+    return LongWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter floats() {\n+    return FloatWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter doubles() {\n+    return DoubleWriter.INSTANCE;\n+  }\n+\n+  public static SparkOrcValueWriter byteArray() {", "originalCommit": "1953a387d717acde1c3efafb95eb1a648e3ff105", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "379accd30acc1dd4e404c5eb38351ec5a95e6611", "url": "https://github.com/apache/iceberg/commit/379accd30acc1dd4e404c5eb38351ec5a95e6611", "message": "Addressing the comments from Ryan.", "committedDate": "2020-08-03T13:29:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwNzM4Mg==", "url": "https://github.com/apache/iceberg/pull/1238#discussion_r465907382", "bodyText": "I think we need to take into account if we need to adjust for UTC. This information is present in iPrimitive.  I think we should dispatch on the iprimitive type instead of the ORC primitive type, no?", "author": "rdsr", "createdAt": "2020-08-05T18:02:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -20,399 +20,121 @@\n package org.apache.iceberg.spark.data;\n \n import java.util.List;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.orc.OrcRowWriter;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.catalyst.expressions.SpecializedGetters;\n-import org.apache.spark.sql.catalyst.util.ArrayData;\n-import org.apache.spark.sql.catalyst.util.MapData;\n \n /**\n  * This class acts as an adaptor from an OrcFileAppender to a\n  * FileAppender&lt;InternalRow&gt;.\n  */\n public class SparkOrcWriter implements OrcRowWriter<InternalRow> {\n \n-  private final Converter[] converters;\n+  private final SparkOrcValueWriter writer;\n \n-  public SparkOrcWriter(TypeDescription schema) {\n-    converters = buildConverters(schema);\n+  public SparkOrcWriter(Schema iSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    writer = OrcSchemaWithTypeVisitor.visit(iSchema, orcSchema, new WriteBuilder());\n   }\n \n   @Override\n   public void write(InternalRow value, VectorizedRowBatch output) {\n-    int row = output.size++;\n-    for (int c = 0; c < converters.length; ++c) {\n-      converters[c].addValue(row, c, value, output.cols[c]);\n-    }\n-  }\n-\n-  /**\n-   * The interface for the conversion from Spark's SpecializedGetters to\n-   * ORC's ColumnVectors.\n-   */\n-  interface Converter {\n-    /**\n-     * Take a value from the Spark data value and add it to the ORC output.\n-     * @param rowId the row in the ColumnVector\n-     * @param column either the column number or element number\n-     * @param data either an InternalRow or ArrayData\n-     * @param output the ColumnVector to put the value into\n-     */\n-    void addValue(int rowId, int column, SpecializedGetters data,\n-                  ColumnVector output);\n-  }\n-\n-  static class BooleanConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getBoolean(column) ? 1 : 0;\n-      }\n-    }\n-  }\n-\n-  static class ByteConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getByte(column);\n-      }\n-    }\n-  }\n-\n-  static class ShortConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getShort(column);\n-      }\n-    }\n-  }\n-\n-  static class IntConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getInt(column);\n-      }\n-    }\n-  }\n-\n-  static class LongConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.getLong(column);\n-      }\n-    }\n-  }\n-\n-  static class FloatConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getFloat(column);\n-      }\n-    }\n-  }\n-\n-  static class DoubleConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data.getDouble(column);\n-      }\n-    }\n-  }\n-\n-  static class StringConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        byte[] value = data.getUTF8String(column).getBytes();\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n+    Preconditions.checkArgument(writer instanceof StructWriter, \"writer must be StructWriter\");\n \n-  static class BytesConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        // getBinary always makes a copy, so we don't need to worry about it\n-        // being changed behind our back.\n-        byte[] value = data.getBinary(column);\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n+    int row = output.size;\n+    output.size += 1;\n+    List<SparkOrcValueWriter> writers = ((StructWriter) writer).writers();\n+    for (int c = 0; c < writers.size(); c++) {\n+      SparkOrcValueWriter child = writers.get(c);\n+      child.write(row, c, value, output.cols[c]);\n     }\n   }\n \n-  static class TimestampTzConverter implements Converter {\n-    @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        TimestampColumnVector cv = (TimestampColumnVector) output;\n-        long micros = data.getLong(column);\n-        cv.time[rowId] = micros / 1_000; // millis\n-        cv.nanos[rowId] = (int) (micros % 1_000_000) * 1_000; // nanos\n-      }\n-    }\n-  }\n-\n-  static class Decimal18Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal18Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<SparkOrcValueWriter> {\n+    private WriteBuilder() {\n     }\n \n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].setFromLongAndScale(\n-            data.getDecimal(column, precision, scale).toUnscaledLong(), scale);\n-      }\n-    }\n-  }\n-\n-  static class Decimal38Converter implements Converter {\n-    private final int precision;\n-    private final int scale;\n-\n-    Decimal38Converter(TypeDescription schema) {\n-      precision = schema.getPrecision();\n-      scale = schema.getScale();\n+    public SparkOrcValueWriter record(Types.StructType iStruct, TypeDescription record,\n+                                      List<String> names, List<SparkOrcValueWriter> fields) {\n+      return new StructWriter(fields);\n     }\n \n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].set(\n-            HiveDecimal.create(data.getDecimal(column, precision, scale)\n-                .toJavaBigDecimal()));\n-      }\n-    }\n-  }\n-\n-  static class StructConverter implements Converter {\n-    private final Converter[] children;\n-\n-    StructConverter(TypeDescription schema) {\n-      children = new Converter[schema.getChildren().size()];\n-      for (int c = 0; c < children.length; ++c) {\n-        children[c] = buildConverter(schema.getChildren().get(c));\n-      }\n+    public SparkOrcValueWriter list(Types.ListType iList, TypeDescription array,\n+                                    SparkOrcValueWriter element) {\n+      return SparkOrcValueWriters.list(element);\n     }\n \n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        InternalRow value = data.getStruct(column, children.length);\n-        StructColumnVector cv = (StructColumnVector) output;\n-        for (int c = 0; c < children.length; ++c) {\n-          children[c].addValue(rowId, c, value, cv.fields[c]);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class ListConverter implements Converter {\n-    private final Converter children;\n-\n-    ListConverter(TypeDescription schema) {\n-      children = buildConverter(schema.getChildren().get(0));\n+    public SparkOrcValueWriter map(Types.MapType iMap, TypeDescription map,\n+                                   SparkOrcValueWriter key, SparkOrcValueWriter value) {\n+      return SparkOrcValueWriters.map(key, value);\n     }\n \n     @Override\n-    public void addValue(int rowId, int column, SpecializedGetters data,\n-                         ColumnVector output) {\n-      if (data.isNullAt(column)) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ArrayData value = data.getArray(column);\n-        ListColumnVector cv = (ListColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.numElements();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.child.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          children.addValue((int) (e + cv.offsets[rowId]), e, value, cv.child);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class MapConverter implements Converter {\n-    private final Converter keyConverter;\n-    private final Converter valueConverter;\n-\n-    MapConverter(TypeDescription schema) {\n-      keyConverter = buildConverter(schema.getChildren().get(0));\n-      valueConverter = buildConverter(schema.getChildren().get(1));\n+    public SparkOrcValueWriter primitive(Type.PrimitiveType iPrimitive, TypeDescription primitive) {\n+      switch (primitive.getCategory()) {\n+        case BOOLEAN:\n+          return SparkOrcValueWriters.booleans();\n+        case BYTE:\n+          return SparkOrcValueWriters.bytes();\n+        case SHORT:\n+          return SparkOrcValueWriters.shorts();\n+        case DATE:\n+        case INT:\n+          return SparkOrcValueWriters.ints();\n+        case LONG:\n+          return SparkOrcValueWriters.longs();\n+        case FLOAT:\n+          return SparkOrcValueWriters.floats();\n+        case DOUBLE:\n+          return SparkOrcValueWriters.doubles();\n+        case BINARY:\n+          return SparkOrcValueWriters.byteArrays();\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return SparkOrcValueWriters.strings();\n+        case DECIMAL:\n+          return SparkOrcValueWriters.decimal(primitive.getPrecision(), primitive.getScale());\n+        case TIMESTAMP_INSTANT:", "originalCommit": "379accd30acc1dd4e404c5eb38351ec5a95e6611", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5ODU4Mg==", "url": "https://github.com/apache/iceberg/pull/1238#discussion_r465998582", "bodyText": "I'm not quite sure what you mean here.\nIceberg must never modify a value that is passed in, other than to convert to a different representation. Adjusting for time zones would produce corrupt values.\nIt sounds like you're talking about the adjust-to-utc property that we set for Avro and Parquet, which tells us whether a timestamp is with time zone (adjust=true) or without time zone (adjust=false). But those only determine the type. Values are never modified because of that type. In this case, we don't need to do that because TIMESTAMP_INSTANT is used for timestamp with time zone, and TIMESTAMP is used for timestamp without time zone. So in this branch of the switch, the type is definitely timestamptz.", "author": "rdblue", "createdAt": "2020-08-05T20:56:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwNzM4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE0MTEwMA==", "url": "https://github.com/apache/iceberg/pull/1238#discussion_r466141100", "bodyText": "So in this branch of the switch, the type is definitely timestamptz.\n\nYes,  in this TIMESAMP_INSTANT case, it must be a timestamp with time zone. see the Category in TypeDescription:\npublic enum Category {\n    BOOLEAN(\"boolean\", true),\n    BYTE(\"tinyint\", true),\n    SHORT(\"smallint\", true),\n    INT(\"int\", true),\n    LONG(\"bigint\", true),\n    FLOAT(\"float\", true),\n    DOUBLE(\"double\", true),\n    STRING(\"string\", true),\n    DATE(\"date\", true),\n    TIMESTAMP(\"timestamp\", true),\n    BINARY(\"binary\", true),\n    DECIMAL(\"decimal\", true),\n    VARCHAR(\"varchar\", true),\n    CHAR(\"char\", true),\n    LIST(\"array\", false),\n    MAP(\"map\", false),\n    STRUCT(\"struct\", false),\n    UNION(\"uniontype\", false),\n    TIMESTAMP_INSTANT(\"timestamp with local time zone\", false);\nbtw, Is there any reason that we spark don't support a timestamp without time zone ?   I saw the spark test here did not include the timestamp without zone data type.  Maybe we could file another issue to address this timestamp without zone issue, because this is a refactor PR and we could get this merged firstly.", "author": "openinx", "createdAt": "2020-08-06T04:38:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwNzM4Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjE1MjIyNA==", "url": "https://github.com/apache/iceberg/pull/1238#discussion_r466152224", "bodyText": "I was thinking that we should also handle TIMESTAMP and not just TIMESTAMP_INSTANT . But I checked with @shardulm94  and the reason is that Spark does not support timestamp without timezone.\nMy other point was to dispatch on primitive so that it is consistent with other writers. e.g GenericOrcWriter.  But here it doesn't make a lot of difference.\nThanks for the explanation!", "author": "rdsr", "createdAt": "2020-08-06T05:20:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkwNzM4Mg=="}], "type": "inlineReview"}]}