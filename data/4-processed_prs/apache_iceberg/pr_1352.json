{"pr_number": 1352, "pr_title": "Support row-level deletes with IcebergGenerics", "pr_createdAt": "2020-08-17T22:33:27Z", "pr_url": "https://github.com/apache/iceberg/pull/1352", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNzkxMA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472217910", "bodyText": "Shouldn't this be 2?", "author": "RussellSpitzer", "createdAt": "2020-08-18T13:58:19Z", "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIyMDE4OQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472220189", "bodyText": "I looked into the code for this, it's actually interesting that 1 and 2 follow a relatively fast path while 3 will require doing some power of 2 estimation.\nhttps://github.com/google/guava/blob/064fac35d71231aba35062d1965983ecd36b6873/guava/src/com/google/common/collect/Maps.java#L324-L328", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:01:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNzkxMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM1NTM0MA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472355340", "bodyText": "Yeah, I'll update this.", "author": "rdblue", "createdAt": "2020-08-18T17:15:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxNzkxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIxODAxMg==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472218012", "bodyText": "And this 3?", "author": "RussellSpitzer", "createdAt": "2020-08-18T13:58:27Z", "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field1, value1);\n+    overwriteValues.put(field2, value2);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2, String field3, Object value3) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzMzMw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472237333", "bodyText": "Should we have a Precondition that sortedDeletesByPartition is not null in the constructor?", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:25:00Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -78,6 +82,10 @@\n     this.sortedDeletesByPartition = sortedDeletesByPartition;\n   }\n \n+  public boolean isEmpty() {\n+    return (globalDeletes == null || globalDeletes.length == 0) && sortedDeletesByPartition.isEmpty();", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzODc1Mw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472238753", "bodyText": "And possibly for specsByID? I'm not sure the project policy on null guarding", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:26:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzMzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE0MTMzMA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475141330", "bodyText": "The reason why I didn't is that this constructor is not public. It will only be called by tests and the builder, so I think we can be more relaxed about validation. We know that this is never null because the builder never calls it that way.", "author": "rdblue", "createdAt": "2020-08-22T21:42:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjIzNzMzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI0ODgxOQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472248819", "bodyText": "Is this different than just lexicographically comparing the bytes of the path with the byte buffer? Just wondering if converting the path to bytes may be cheaper than than buffer to string. Not a big deal though and probably has no real perf impacts at this location.", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:40:09Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM1NjMxNA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472356314", "bodyText": "We don't actually convert to String. I think what gets returned by Conversions is actually a CharBuffer, which implements CharSequence. That avoids the copy like (I think) you're suggesting.", "author": "rdblue", "createdAt": "2020-08-18T17:16:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI0ODgxOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM2MjE2Nw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472362167", "bodyText": "\ud83d\udc4d", "author": "RussellSpitzer", "createdAt": "2020-08-18T17:26:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI0ODgxOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1NzczOQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472257739", "bodyText": "This is a rather complicated function and It would be great help if we had a few comments about the kind of checks we are doing.", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:51:35Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI1OTUxMQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472259511", "bodyText": "Just checking but in this map we will never have a 0 right? Just making sure we aren't ignore nullValueCount.get(X) == 0", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:53:52Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI2MTU4NQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472261585", "bodyText": "Ah I see we do a 0 check here as well", "author": "RussellSpitzer", "createdAt": "2020-08-18T14:56:25Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {\n+      return false;\n+    }\n+\n+    return nullValueCount <= 0;", "originalCommit": "b48d9043704978653c138a42ac65fd400b321275", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "url": "https://github.com/apache/iceberg/commit/fe86cc83cbe46ad95a925a2e486f9292ccc9b1b6", "message": "Apply row-level deletes when reading with IcebergGenerics.", "committedDate": "2020-08-18T21:57:36Z", "type": "forcePushed"}, {"oid": "aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "url": "https://github.com/apache/iceberg/commit/aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "message": "Apply row-level deletes when reading with IcebergGenerics.", "committedDate": "2020-08-18T22:34:45Z", "type": "commit"}, {"oid": "aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "url": "https://github.com/apache/iceberg/commit/aaa13ae2a69ae3653e8774b7cf9c520488eb0154", "message": "Apply row-level deletes when reading with IcebergGenerics.", "committedDate": "2020-08-18T22:34:45Z", "type": "forcePushed"}, {"oid": "c91af88e6083129be9eca4687c6cdf658be43f8a", "url": "https://github.com/apache/iceberg/commit/c91af88e6083129be9eca4687c6cdf658be43f8a", "message": "Fix checkstyle problems.", "committedDate": "2020-08-18T23:28:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1MDc1MA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472850750", "bodyText": "For equality deletes, wouldn't we need the file projection to also include equality field ids if not already present?", "author": "shardulm94", "createdAt": "2020-08-19T08:21:14Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {", "originalCommit": "c91af88e6083129be9eca4687c6cdf658be43f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE5NTg0OQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r473195849", "bodyText": "Yes, very good point. We should probably have two methods, one for position deletes and one for equality.", "author": "rdblue", "createdAt": "2020-08-19T17:14:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1MDc1MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE1Mzg1MA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475153850", "bodyText": "I updated this to create the correct projection when columns are top-level.\nCreating the right projection when columns are nested is going to require a visitor, so I think it should be done in a different commit and have tests. I've added a TODO note to fix this. Until it is done, if a column is not top level this will throw an exception.", "author": "rdblue", "createdAt": "2020-08-23T00:46:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1MDc1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1ODk2Ng==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472858966", "bodyText": "Does this mean equality deletes can only currently support deletes on top level fields?", "author": "shardulm94", "createdAt": "2020-08-19T08:34:47Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {\n+    if (hasPosDeletes) {\n+      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+      columns.add(MetadataColumns.ROW_POSITION);\n+      return new Schema(columns);\n+    }\n+\n+    return projection;\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+      int[] orderedIds = deleteSchema.columns().stream().mapToInt(Types.NestedField::fieldId).toArray();", "originalCommit": "c91af88e6083129be9eca4687c6cdf658be43f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzIwMjE3Mg==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r473202172", "bodyText": "I intended to support nested fields, but I think there is still a problem here.\nFor example, schema 1: id bigint, 2: location struct<3: city, 4: postcode> and delete postcode='HG1 2AD'. The delete id set is {4}, but the delete schema from the previous line is 2: location struct<4: postcode>. The projection created from orderedIds should use ID 2 because that's the top-level field that needs to be pulled from a full record.\nThe problem is that the nested struct also needs a projection, which I missed. I think we probably need to build the projection using a visitor and correctly wrap nested structs.", "author": "rdblue", "createdAt": "2020-08-19T17:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE1Mzk2Nw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475153967", "bodyText": "I replaced ProjectStructLike with StructProjection that handles nested fields. It works as a wrapper like the previous version to avoid copying data.\nAs a consequence, it cannot handle projections within map or list fields. But that's okay because equality predicates can't be applied to map or list fields anyway. It will throw an exception if it is used to project types that are not supported, so it should be safe.", "author": "rdblue", "createdAt": "2020-08-23T00:48:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1ODk2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE1Njc2MQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475156761", "bodyText": "One minor thing, the variable name orderedIds seems a little bit confused to me, If it means IDs are aligned with IDs ordered defined in the schema, would it better to call it alignedIds?", "author": "chenjunjiedada", "createdAt": "2020-08-23T01:30:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg1ODk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472864293", "bodyText": "The schema returned Record here will also include metadata columns and/or columns for equality deletes on top of the schema requested by the user. Do we want to apply a projection on top to match the schema requested?", "author": "shardulm94", "createdAt": "2020-08-19T08:43:24Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;", "originalCommit": "c91af88e6083129be9eca4687c6cdf658be43f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE5NTY4NA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r473195684", "bodyText": "I would like to avoid adding a projection, but I agree that it is a little strange to return a row that reports more columns. I think this will come down to the object model. For example, it is perfectly fine to return a row in Spark that has an additional field, and it is much more efficient not to copy the data into a different row.\nFor generics, I think it is okay to return a row with an extra value, but we might want to make it possible to truncate GenericRecord to make the extra fields unavailable.", "author": "rdblue", "createdAt": "2020-08-19T17:14:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk4ODgwNQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r474988805", "bodyText": "Yeah, I think this should be fine for now. We can add a projection for Generics later.\nFor Spark, I guess we can only return additional data if the we declare the additional fields in the schema. Is that correct? If so, we will have to go through the all file scan tasks to determine what the schema should be. Today we are able to derive schema based solely off of input information for the table scan. It can be potentially sub optimal too, since we may need to project the extra columns for all the file scans even if a single file scan requires the extra column.", "author": "shardulm94", "createdAt": "2020-08-21T22:02:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk4OTk3OA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r474989978", "bodyText": "Actually the above is probably relevant for Generics too. The extra columns reported can change across multiple records within the same scan since each file can have different equality field ids.", "author": "shardulm94", "createdAt": "2020-08-21T22:06:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTE1NDE1Mg==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475154152", "bodyText": "I think this is okay for now, but we should do the truncation for Spark. All we should need to do is truncate the reported size, since the read schema we produce will put additional columns at the end.", "author": "rdblue", "createdAt": "2020-08-23T00:51:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NDI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NTE1Nw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r472865157", "bodyText": "Was this removed to ignore the extra columns coming from the file projection?", "author": "shardulm94", "createdAt": "2020-08-19T08:44:47Z", "path": "core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java", "diffHunk": "@@ -73,11 +73,6 @@ public boolean equals(Object other) {\n       return false;\n     }\n \n-    int len = struct.size();\n-    if (len != that.struct.size()) {\n-      return false;\n-    }\n-", "originalCommit": "c91af88e6083129be9eca4687c6cdf658be43f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzE5MTkwNw==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r473191907", "bodyText": "Yes, it was. Since the wrapper's equality is based on the type, I think it doesn't make sense to check the size here.", "author": "rdblue", "createdAt": "2020-08-19T17:07:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjg2NTE1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0MTY5OA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r474541698", "bodyText": "Can we make this configurable? For example, let users choose how much memory they want to use for this.", "author": "chenjunjiedada", "createdAt": "2020-08-21T08:52:08Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.ProjectStructLike;\n+import org.apache.iceberg.util.StructLikeSet;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(!posDeletes.isEmpty());\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(boolean hasPosDeletes) {\n+    if (hasPosDeletes) {\n+      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+      columns.add(MetadataColumns.ROW_POSITION);\n+      return new Schema(columns);\n+    }\n+\n+    return projection;\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+      int[] orderedIds = deleteSchema.columns().stream().mapToInt(Types.NestedField::fieldId).toArray();\n+\n+      // a wrapper to translate from generic objects to internal representations\n+      InternalRecordWrapper asStructLike = new InternalRecordWrapper(recordSchema.asStruct());\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      ProjectStructLike projectRow = ProjectStructLike.of(recordSchema, orderedIds);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, dataFile, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike.wrap(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<Record> applyPosDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                    CharSequence file, List<DeleteFile> posDeletes, DataFile dataFile) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Accessor<StructLike> posAccessor = recordSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+    Function<Record, Long> posGetter = record -> (Long) posAccessor.get(record);\n+    List<CloseableIterable<StructLike>> deletes = Lists.transform(posDeletes,\n+        delete -> openPosDeletes(delete, dataFile));\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < 100_000L) {", "originalCommit": "c91af88e6083129be9eca4687c6cdf658be43f8a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDgwOTk3NA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r474809974", "bodyText": "Yeah, we will add more features and config as we go. This is mainly just to demonstrate right now, so we can start updating the read paths in parallel.", "author": "rdblue", "createdAt": "2020-08-21T16:44:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU0MTY5OA=="}], "type": "inlineReview"}, {"oid": "ddbecd0eac02e786631af043e565a8aec83e843d", "url": "https://github.com/apache/iceberg/commit/ddbecd0eac02e786631af043e565a8aec83e843d", "message": "Fix problems from review.", "committedDate": "2020-08-23T00:44:18Z", "type": "commit"}, {"oid": "fc1f3452f74d4cb9a6f7f9d344a91bfc16afdf65", "url": "https://github.com/apache/iceberg/commit/fc1f3452f74d4cb9a6f7f9d344a91bfc16afdf65", "message": "Remove accidental additions.", "committedDate": "2020-08-23T00:55:34Z", "type": "commit"}, {"oid": "9607b5958aac84e8d89f17cbabbe2c82124410bd", "url": "https://github.com/apache/iceberg/commit/9607b5958aac84e8d89f17cbabbe2c82124410bd", "message": "Fix license header.", "committedDate": "2020-08-23T00:56:17Z", "type": "commit"}, {"oid": "c4b3a9b79025cf68789acea82b454125e10442fe", "url": "https://github.com/apache/iceberg/commit/c4b3a9b79025cf68789acea82b454125e10442fe", "message": "Fix checkstyle.", "committedDate": "2020-08-23T00:56:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyNTcyNA==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475325724", "bodyText": "Interesting constraint here that a column cannot be deleted if there are live equality delete files depending on the column. Maybe we should be checking this while deleting columns?", "author": "shardulm94", "createdAt": "2020-08-24T03:41:43Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -114,14 +117,40 @@\n     return records;\n   }\n \n-  private Schema fileProjection(boolean hasPosDeletes) {\n-    if (hasPosDeletes) {\n-      List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+  private Schema fileProjection(List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(Sets.difference(requiredIds, TypeUtil.getProjectedIds(projection)));\n+\n+    if (missingIds.isEmpty()) {\n+      return projection;\n+    }\n+\n+    // TODO: support adding nested columns. this will currently fail when finding nested columns to add\n+    List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+    for (int fieldId : missingIds) {\n+      if (fieldId == MetadataColumns.ROW_POSITION.fieldId()) {\n+        continue; // add _pos at the end\n+      }\n+\n+      Types.NestedField field = tableSchema.asStruct().field(fieldId);\n+      Preconditions.checkArgument(field != null, \"Cannot find required field for ID %s\", fieldId);", "originalCommit": "ddbecd0eac02e786631af043e565a8aec83e843d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTc0OTI1NQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475749255", "bodyText": "Is it a necessary constraint? As long as old data files have the deleted columns, we can still apply the deletes. Maybe we just need to change how we build this projection schema. We could base it on the data file's schema instead of the table schema. If the column is in the data file schema, it would work fine.", "author": "rdblue", "createdAt": "2020-08-24T16:40:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyNTcyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTgwMzc0NQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475803745", "bodyText": "Okay yeah, that is correct. We should just be building the projection differently.", "author": "shardulm94", "createdAt": "2020-08-24T18:13:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMyNTcyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4NTM1NQ==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475385355", "bodyText": "I saw the upper layer will need the CloseableIterable<StructLike> and CloseableIteratable<Record>,   How about marking the parameterized T as T extends StructLike here", "author": "openinx", "createdAt": "2020-08-24T07:11:09Z", "path": "data/src/main/java/org/apache/iceberg/data/GenericReader.java", "diffHunk": "@@ -0,0 +1,320 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data;\n+\n+import java.io.Serializable;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.Accessor;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.orc.GenericOrcReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.deletes.Deletes;\n+import org.apache.iceberg.expressions.Evaluator;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableGroup;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PartitionUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructProjection;\n+import org.apache.parquet.Preconditions;\n+\n+class GenericReader implements Serializable {\n+  private static final Schema POS_DELETE_SCHEMA = new Schema(\n+      MetadataColumns.DELETE_FILE_PATH,\n+      MetadataColumns.DELETE_FILE_POS);\n+\n+  private final FileIO io;\n+  private final Schema tableSchema;\n+  private final Schema projection;\n+  private final boolean caseSensitive;\n+  private final boolean reuseContainers;\n+\n+  GenericReader(TableScan scan, boolean reuseContainers) {\n+    this.io = scan.table().io();\n+    this.tableSchema = scan.table().schema();\n+    this.projection = scan.schema();\n+    this.caseSensitive = scan.isCaseSensitive();\n+    this.reuseContainers = reuseContainers;\n+  }\n+\n+  CloseableIterator<Record> open(CloseableIterable<CombinedScanTask> tasks) {\n+    Iterable<FileScanTask> fileTasks = Iterables.concat(Iterables.transform(tasks, CombinedScanTask::files));\n+    return CloseableIterable.concat(Iterables.transform(fileTasks, this::open)).iterator();\n+  }\n+\n+  public CloseableIterable<Record> open(CombinedScanTask task) {\n+    return new CombinedTaskIterable(task);\n+  }\n+\n+  public CloseableIterable<Record> open(FileScanTask task) {\n+    List<DeleteFile> posDeletes = Lists.newArrayList();\n+    List<DeleteFile> eqDeletes = Lists.newArrayList();\n+    for (DeleteFile delete : task.deletes()) {\n+      switch (delete.content()) {\n+        case POSITION_DELETES:\n+          posDeletes.add(delete);\n+          break;\n+        case EQUALITY_DELETES:\n+          eqDeletes.add(delete);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\"Unknown delete file content: \" + delete.content());\n+      }\n+    }\n+\n+    Schema fileProjection = fileProjection(posDeletes, eqDeletes);\n+\n+    CloseableIterable<Record> records = openFile(task, fileProjection);\n+    records = applyPosDeletes(records, fileProjection, task.file().path(), posDeletes, task.file());\n+    records = applyEqDeletes(records, fileProjection, eqDeletes, task.file());\n+    records = applyResidual(records, fileProjection, task.residual());\n+\n+    return records;\n+  }\n+\n+  private Schema fileProjection(List<DeleteFile> posDeletes, List<DeleteFile> eqDeletes) {\n+    Set<Integer> requiredIds = Sets.newLinkedHashSet();\n+    if (!posDeletes.isEmpty()) {\n+      requiredIds.add(MetadataColumns.ROW_POSITION.fieldId());\n+    }\n+\n+    for (DeleteFile eqDelete : eqDeletes) {\n+      requiredIds.addAll(eqDelete.equalityFieldIds());\n+    }\n+\n+    Set<Integer> missingIds = Sets.newLinkedHashSet(Sets.difference(requiredIds, TypeUtil.getProjectedIds(projection)));\n+\n+    if (missingIds.isEmpty()) {\n+      return projection;\n+    }\n+\n+    // TODO: support adding nested columns. this will currently fail when finding nested columns to add\n+    List<Types.NestedField> columns = Lists.newArrayList(projection.columns());\n+    for (int fieldId : missingIds) {\n+      if (fieldId == MetadataColumns.ROW_POSITION.fieldId()) {\n+        continue; // add _pos at the end\n+      }\n+\n+      Types.NestedField field = tableSchema.asStruct().field(fieldId);\n+      Preconditions.checkArgument(field != null, \"Cannot find required field for ID %s\", fieldId);\n+\n+      columns.add(field);\n+    }\n+\n+    if (requiredIds.contains(MetadataColumns.ROW_POSITION.fieldId())) {\n+      columns.add(MetadataColumns.ROW_POSITION);\n+    }\n+\n+    return new Schema(columns);\n+  }\n+\n+  private CloseableIterable<Record> applyResidual(CloseableIterable<Record> records, Schema recordSchema,\n+                                                  Expression residual) {\n+    if (residual != null && residual != Expressions.alwaysTrue()) {\n+      InternalRecordWrapper wrapper = new InternalRecordWrapper(recordSchema.asStruct());\n+      Evaluator filter = new Evaluator(recordSchema.asStruct(), residual, caseSensitive);\n+      return CloseableIterable.filter(records, record -> filter.eval(wrapper.wrap(record)));\n+    }\n+\n+    return records;\n+  }\n+\n+  private CloseableIterable<Record> applyEqDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                   List<DeleteFile> eqDeletes, DataFile dataFile) {\n+    if (eqDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);\n+    for (DeleteFile delete : eqDeletes) {\n+      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);\n+    }\n+\n+    CloseableIterable<Record> filteredRecords = records;\n+    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {\n+      Set<Integer> ids = entry.getKey();\n+      Iterable<DeleteFile> deletes = entry.getValue();\n+\n+      Schema deleteSchema = TypeUtil.select(recordSchema, ids);\n+\n+      // a wrapper to translate from generic objects to internal representations\n+      InternalRecordWrapper asStructLike = new InternalRecordWrapper(recordSchema.asStruct());\n+\n+      // a projection to select and reorder fields of the file schema to match the delete rows\n+      StructProjection projectRow = StructProjection.create(recordSchema, deleteSchema);\n+\n+      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,\n+          delete -> openDeletes(delete, dataFile, deleteSchema));\n+      StructLikeSet deleteSet = Deletes.toEqualitySet(\n+          // copy the delete records because they will be held in a set\n+          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),\n+          deleteSchema.asStruct());\n+\n+      filteredRecords = Deletes.filter(filteredRecords,\n+          record -> projectRow.wrap(asStructLike.wrap(record)), deleteSet);\n+    }\n+\n+    return filteredRecords;\n+  }\n+\n+  private CloseableIterable<Record> applyPosDeletes(CloseableIterable<Record> records, Schema recordSchema,\n+                                                    CharSequence file, List<DeleteFile> posDeletes, DataFile dataFile) {\n+    if (posDeletes.isEmpty()) {\n+      return records;\n+    }\n+\n+    Accessor<StructLike> posAccessor = recordSchema.accessorForField(MetadataColumns.ROW_POSITION.fieldId());\n+    Function<Record, Long> posGetter = record -> (Long) posAccessor.get(record);\n+    List<CloseableIterable<StructLike>> deletes = Lists.transform(posDeletes,\n+        delete -> openPosDeletes(delete, dataFile));\n+\n+    // if there are fewer deletes than a reasonable number to keep in memory, use a set\n+    if (posDeletes.stream().mapToLong(DeleteFile::recordCount).sum() < 100_000L) {\n+      return Deletes.filter(records, posGetter, Deletes.toPositionSet(file, CloseableIterable.concat(deletes)));\n+    }\n+\n+    return Deletes.streamingFilter(records, posGetter, Deletes.deletePositions(file, deletes));\n+  }\n+\n+  private CloseableIterable<StructLike> openPosDeletes(DeleteFile file, DataFile dataFile) {\n+    return openDeletes(file, dataFile, POS_DELETE_SCHEMA);\n+  }\n+\n+  private <T> CloseableIterable<T> openDeletes(DeleteFile deleteFile, DataFile dataFile, Schema deleteSchema) {", "originalCommit": "c4b3a9b79025cf68789acea82b454125e10442fe", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTc1NDA2Ng==", "url": "https://github.com/apache/iceberg/pull/1352#discussion_r475754066", "bodyText": "This should actually be Record. I'll fix it. Thanks for pointing this out.", "author": "rdblue", "createdAt": "2020-08-24T16:48:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTM4NTM1NQ=="}], "type": "inlineReview"}, {"oid": "49f4bf5d729e24bed221b626711081fff6965916", "url": "https://github.com/apache/iceberg/commit/49f4bf5d729e24bed221b626711081fff6965916", "message": "Fix checkstyle failures in tests.", "committedDate": "2020-08-24T16:30:46Z", "type": "commit"}, {"oid": "4d3731a3d20faf7998614b80dcbc812d8412e8dd", "url": "https://github.com/apache/iceberg/commit/4d3731a3d20faf7998614b80dcbc812d8412e8dd", "message": "Fix types in delete helper methods.", "committedDate": "2020-08-24T16:48:24Z", "type": "commit"}]}