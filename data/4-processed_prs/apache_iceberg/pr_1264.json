{"pr_number": 1264, "pr_title": "Expire snapshots action", "pr_createdAt": "2020-07-28T19:30:43Z", "pr_url": "https://github.com/apache/iceberg/pull/1264", "timeline": [{"oid": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "url": "https://github.com/apache/iceberg/commit/896c6cf4ad34fa273287d093cebe189fdeb4c27b", "message": "Expire Snapshots Spark Action\n\nPreviously ther only way to expire snapshots was through a single machine table\noperation with RemoveSnapshots. In this patch we add a new Spark Action which\ndoes the same work, but does so in a scalable way. Instead of using the old\nlogic for analyzing files to remove, we use the Metadata Table representations\nof the table both before and after Snapshot Expiration to determine un-needed\nfiles.", "committedDate": "2020-08-05T16:29:49Z", "type": "commit"}, {"oid": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "url": "https://github.com/apache/iceberg/commit/896c6cf4ad34fa273287d093cebe189fdeb4c27b", "message": "Expire Snapshots Spark Action\n\nPreviously ther only way to expire snapshots was through a single machine table\noperation with RemoveSnapshots. In this patch we add a new Spark Action which\ndoes the same work, but does so in a scalable way. Instead of using the old\nlogic for analyzing files to remove, we use the Metadata Table representations\nof the table both before and after Snapshot Expiration to determine un-needed\nfiles.", "committedDate": "2020-08-05T16:29:49Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkxNTEzOQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465915139", "bodyText": "nit: do we need this extra variable? My IDE always highlights such cases.", "author": "aokolnychyi", "createdAt": "2020-08-05T18:17:18Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,59 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwMTI5Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466001296", "bodyText": "Nope, I just have some strange habits, I'll clean it all up", "author": "RussellSpitzer", "createdAt": "2020-08-05T21:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkxNTEzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkzODA4Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465938082", "bodyText": "nit: same here", "author": "aokolnychyi", "createdAt": "2020-08-05T18:59:04Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,59 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+    return manifestDF;\n+  }\n+\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n+    List<String> manifestLists = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        manifestLists.add(manifestListLocation);\n+      }\n+    }\n+\n+    Dataset<Row> manifestListDF = spark", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTkzODE5Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465938192", "bodyText": "nit: same here", "author": "aokolnychyi", "createdAt": "2020-08-05T18:59:13Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,59 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+    return manifestDF;\n+  }\n+\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n+    List<String> manifestLists = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        manifestLists.add(manifestListLocation);\n+      }\n+    }\n+\n+    Dataset<Row> manifestListDF = spark\n+        .createDataset(manifestLists, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestListDF;\n+  }\n+\n+  protected Dataset<Row> buildOtherMetadataFileDF(SparkSession spark, TableOperations ops) {\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));\n+    TableMetadata metadata = ops.current();\n+    otherMetadataFiles.add(metadata.metadataFileLocation());\n+    for (TableMetadata.MetadataLogEntry previousMetadataFile : metadata.previousFiles()) {\n+      otherMetadataFiles.add(previousMetadataFile.file());\n+    }\n+\n+    Dataset<Row> otherMetadataFileDF = spark\n+        .createDataset(otherMetadataFiles, Encoders.STRING())\n+        .toDF(\"file_path\");\n+    return otherMetadataFileDF;", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTI3OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465941279", "bodyText": "Looks like we don't need this var anymore.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:04:55Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0MTUzMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465941533", "bodyText": "Looks like we can remove this one as well.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:05:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0NTAxMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465945012", "bodyText": "I think it should be Snapshots instead of Snapshot to be consistent with the action name.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:12:07Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotActionResult.java", "diffHunk": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+public class ExpireSnapshotActionResult {", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0NjIxMA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465946210", "bodyText": "Let's move static constants above instance variables.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:14:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0NzIzNw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465947237", "bodyText": "Let's add one empty line before and after adding version hint to have some logical grouping.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:16:15Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,59 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\")\n+        .load(allDataFilesMetadataTable)\n+        .select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    Dataset<Row> manifestDF = spark.read().format(\"iceberg\")\n+        .load(allManifestsMetadataTable)\n+        .selectExpr(\"path as file_path\");\n+    return manifestDF;\n+  }\n+\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n+    List<String> manifestLists = Lists.newArrayList();\n+\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        manifestLists.add(manifestListLocation);\n+      }\n+    }\n+\n+    Dataset<Row> manifestListDF = spark\n+        .createDataset(manifestLists, Encoders.STRING())\n+        .toDF(\"file_path\");\n+\n+    return manifestListDF;\n+  }\n+\n+  protected Dataset<Row> buildOtherMetadataFileDF(SparkSession spark, TableOperations ops) {\n+    List<String> otherMetadataFiles = Lists.newArrayList();\n+    otherMetadataFiles.add(ops.metadataFileLocation(\"version-hint.text\"));", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0ODI0OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465948249", "bodyText": "nit: let's keep only 1 empty line, no need for 2.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:18:08Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0ODUyMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465948523", "bodyText": "nit: extra line here", "author": "aokolnychyi", "createdAt": "2020-08-05T19:18:41Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0OTI2MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465949261", "bodyText": "Why DataFile and not file_type?", "author": "aokolnychyi", "createdAt": "2020-08-05T19:20:07Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"DataFile\"));", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNDY0NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466004645", "bodyText": "Leftover from a prior version, I never call out the column by name again so I didn't change the column name when I changed the design", "author": "RussellSpitzer", "createdAt": "2020-08-05T21:07:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0OTI2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk0OTc2NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465949765", "bodyText": "It would also more natural to have underscores to separate words.\nFor example, DATA_FILE and MANIFEST_LIST.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:21:07Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk1NjI2MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465956260", "bodyText": "nit: all other actions define overridden methods after the constructor, then public methods to configure the action, then public execute, then private methods. I think it makes to follow that here too.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:34:00Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+\n+  @Override\n+  protected Table table() {", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MjU2MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465962561", "bodyText": "I think we have to match the behavior in ExpireSnapshots where the executor service is optional. I am not sure on the best name for the method, though.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:46:04Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"DataFile\"));\n+  }\n+\n+  private Dataset<Row> getValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATAFILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFESTLIST))\n+        .union(appendTypeString(buildOtherMetadataFileDF(spark, ops), OTHER));\n+  }\n+\n+  private Set<String> getFilesOfType(List<Row> files, String type) {\n+    return files.stream()\n+        .filter(row -> row.getString(1).equals(type))\n+        .map(row -> row.getString(0))\n+        .collect(Collectors.toSet());\n+  }\n+\n+  @Override\n+  public ExpireSnapshotActionResult execute() {\n+\n+    Dataset<Row> originalFiles = getValidFileDF().persist();\n+    originalFiles.count(); // Trigger Persist\n+\n+    localExpireSnapshots.commit();\n+\n+    Dataset<Row> validFiles = getValidFileDF();\n+\n+    List<Row> filesToDelete = originalFiles.except(validFiles).collectAsList();\n+\n+    LOG.warn(\"Deleting {} files\", filesToDelete.size());\n+    return new ExpireSnapshotActionResult(\n+        deleteFiles(getFilesOfType(filesToDelete, DATAFILE), DATAFILE),\n+        deleteFiles(getFilesOfType(filesToDelete, MANIFEST), MANIFEST),\n+        deleteFiles(getFilesOfType(filesToDelete, MANIFESTLIST), MANIFESTLIST),\n+        deleteFiles(getFilesOfType(filesToDelete, OTHER), OTHER));\n+  }\n+\n+  private Long deleteFiles(Set<String> paths, String fileType) {\n+    LOG.warn(\"{}s to delete: {}\", fileType, Joiner.on(\", \").join(paths));\n+    AtomicReference<Long> deleteCount = new AtomicReference<>(0L);\n+\n+    Tasks.foreach(paths)\n+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()\n+        .executeWith(ThreadPools.getWorkerPool())", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNzAzMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466007033", "bodyText": "going for executeDeleteWith", "author": "RussellSpitzer", "createdAt": "2020-08-05T21:12:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MjU2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MzMyNA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465963324", "bodyText": "It may be okay to log this for manifests and manifest lists but there may be 10-100k data files to remove. Logging all of them may not be a good idea.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:47:32Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"DataFile\"));\n+  }\n+\n+  private Dataset<Row> getValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATAFILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFESTLIST))\n+        .union(appendTypeString(buildOtherMetadataFileDF(spark, ops), OTHER));\n+  }\n+\n+  private Set<String> getFilesOfType(List<Row> files, String type) {\n+    return files.stream()\n+        .filter(row -> row.getString(1).equals(type))\n+        .map(row -> row.getString(0))\n+        .collect(Collectors.toSet());\n+  }\n+\n+  @Override\n+  public ExpireSnapshotActionResult execute() {\n+\n+    Dataset<Row> originalFiles = getValidFileDF().persist();\n+    originalFiles.count(); // Trigger Persist\n+\n+    localExpireSnapshots.commit();\n+\n+    Dataset<Row> validFiles = getValidFileDF();\n+\n+    List<Row> filesToDelete = originalFiles.except(validFiles).collectAsList();\n+\n+    LOG.warn(\"Deleting {} files\", filesToDelete.size());\n+    return new ExpireSnapshotActionResult(\n+        deleteFiles(getFilesOfType(filesToDelete, DATAFILE), DATAFILE),\n+        deleteFiles(getFilesOfType(filesToDelete, MANIFEST), MANIFEST),\n+        deleteFiles(getFilesOfType(filesToDelete, MANIFESTLIST), MANIFESTLIST),\n+        deleteFiles(getFilesOfType(filesToDelete, OTHER), OTHER));\n+  }\n+\n+  private Long deleteFiles(Set<String> paths, String fileType) {\n+    LOG.warn(\"{}s to delete: {}\", fileType, Joiner.on(\", \").join(paths));", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MzUxNA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r465963514", "bodyText": "I think build will be a better name than get.", "author": "aokolnychyi", "createdAt": "2020-08-05T19:47:54Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Consumer;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.iceberg.util.ThreadPools;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final ExpireSnapshots localExpireSnapshots;\n+  private final TableMetadata base;\n+  private static final String DATAFILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFESTLIST = \"Manifest List\";\n+  private static final String OTHER = \"Other\";\n+\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+  private Consumer<String> deleteFunc = defaultDelete;\n+\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+    this.base = ops.current();\n+    this.localExpireSnapshots = table.expireSnapshots().cleanExpiredFiles(false);\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    localExpireSnapshots.expireSnapshotId(expireSnapshotId);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    localExpireSnapshots.expireOlderThan(timestampMillis);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    localExpireSnapshots.retainLast(numSnapshots);\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"DataFile\"));\n+  }\n+\n+  private Dataset<Row> getValidFileDF() {", "originalCommit": "896c6cf4ad34fa273287d093cebe189fdeb4c27b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxMjk5MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466012991", "bodyText": "sgtm", "author": "RussellSpitzer", "createdAt": "2020-08-05T21:24:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk2MzUxNA=="}], "type": "inlineReview"}, {"oid": "2e0af0a1121d923978344676d593e46d89686962", "url": "https://github.com/apache/iceberg/commit/2e0af0a1121d923978344676d593e46d89686962", "message": "Reviewer Comments\n\nLazy construction of Expire Snapshots Action.\nProcessing of deletes using Local Iterator\nIgnoring Versioning Files\nAdding ExecutorService Option Like RemoveSnapshots", "committedDate": "2020-08-05T22:00:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MDkzOA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466490938", "bodyText": "Can we add Javadoc to the class and methods as we have in RewriteManifestsAction?", "author": "aokolnychyi", "createdAt": "2020-08-06T15:20:58Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYxOTE1OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466619159", "bodyText": "Of course, I've set up most of the docs to link back to the ExpireSnapshots interface", "author": "RussellSpitzer", "createdAt": "2020-08-06T18:53:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MDkzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MTgxMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466491812", "bodyText": "nit: I am not sure whether it is a convention but we tend to add a space between // and the start of the comment.", "author": "aokolnychyi", "createdAt": "2020-08-06T15:22:12Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MTkzMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466491932", "bodyText": "nit: same here", "author": "aokolnychyi", "createdAt": "2020-08-06T15:22:22Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MjcyOA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466492728", "bodyText": "Can we add tests for validating all params? I know we simply delegate but I think it would be better to check.", "author": "aokolnychyi", "createdAt": "2020-08-06T15:23:32Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MzMwNQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466493305", "bodyText": "I think this has to be wrapped into try-finally. We will have to always unpersist if a commit fails, for example.", "author": "aokolnychyi", "createdAt": "2020-08-06T15:24:21Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());\n+    originalFiles.unpersist();", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyMzk5MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466523990", "bodyText": "I was thinking about that, but also thought that any exception here would probably kill the driver :/ I'll add in the try finally though", "author": "RussellSpitzer", "createdAt": "2020-08-06T16:09:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MzMwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2ODY0MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466568641", "bodyText": "The commit in ExpireSnapshots can be unsuccessful. In that case, we should remove the cached state as we no longer need it.", "author": "aokolnychyi", "createdAt": "2020-08-06T17:23:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MzMwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5MzU2NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466493565", "bodyText": "nit: extra space after =", "author": "aokolnychyi", "createdAt": "2020-08-06T15:24:44Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NDM0OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466494349", "bodyText": "paths -> expiredFiles? Just an idea.", "author": "aokolnychyi", "createdAt": "2020-08-06T15:25:45Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());\n+    originalFiles.unpersist();\n+    return result;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n+  }\n+\n+  private Dataset<Row> buildValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  }\n+\n+  private ExpireSnapshotsActionResult deleteFiles(Iterator<Row> paths) {", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjYyMTM4MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466621380", "bodyText": "sgtm", "author": "RussellSpitzer", "createdAt": "2020-08-06T18:57:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NDM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466495741", "bodyText": "@rdblue, I know this action follows what we had before and uses warn level. How appropriate is that, though? I think warn indicates that there is an unusual situation or something went wrong. Here, that's expected, no?", "author": "aokolnychyi", "createdAt": "2020-08-06T15:27:39Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());\n+    originalFiles.unpersist();\n+    return result;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n+  }\n+\n+  private Dataset<Row> buildValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  }\n+\n+  private ExpireSnapshotsActionResult deleteFiles(Iterator<Row> paths) {\n+    AtomicLong dataFileCount = new AtomicLong(0L);\n+    AtomicLong manifestCount = new AtomicLong(0L);\n+    AtomicLong manifestListCount = new AtomicLong(0L);\n+\n+    Tasks.foreach(paths)\n+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()\n+        .executeWith(deleteExecutorService)\n+        .onFailure((fileInfo, exc) ->\n+            LOG.warn(\"Delete failed for {}: {}\", fileInfo.getString(1), fileInfo.getString(0), exc))\n+        .run(fileInfo -> {\n+          String file = fileInfo.getString(0);\n+          String type = fileInfo.getString(1);\n+          deleteFunc.accept(file);\n+          switch (type) {\n+            case DATA_FILE:\n+              dataFileCount.incrementAndGet();\n+              LOG.trace(\"Deleted Data File: {}\", file);\n+              break;\n+            case MANIFEST:\n+              manifestCount.incrementAndGet();\n+              LOG.warn(\"Deleted Manifest: {}\", file);\n+              break;\n+            case MANIFEST_LIST:\n+              manifestListCount.incrementAndGet();\n+              LOG.warn(\"Deleted Manifest List: {}\", file);\n+              break;\n+          }\n+        });\n+    LOG.warn(\"Deleted {} total files\", dataFileCount.get() + manifestCount.get() + manifestListCount.get());", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyNDQ0NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466524445", "bodyText": "I was honestly confused as to why this, and the log manifest and manifest lists lines were all \"warn\" and not \"debug\" or \"info\"", "author": "RussellSpitzer", "createdAt": "2020-08-06T16:09:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2MTUwMQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466561501", "bodyText": "I think \"debug\" is the most appropriate.", "author": "aokolnychyi", "createdAt": "2020-08-06T17:13:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2NTUwMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466565503", "bodyText": "So let's do \"Trace on Data Files\" and \"Debug\" on everything else? I just want to make sure I'm not going against the pattern in RemoveSnapshots", "author": "RussellSpitzer", "createdAt": "2020-08-06T17:19:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU5NjU4MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466596580", "bodyText": "We will need more context from @rdblue on RemoveSnapshots.", "author": "aokolnychyi", "createdAt": "2020-08-06T18:11:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzNTU5OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467335599", "bodyText": "I think it should be info. You're right that it isn't a warning because nothing went wrong. I would also say it isn't debug, because this is an important part of being able to monitor what happened. We don't need to know which files were deleted, but knowing how many were is a good thing to monitor. For example, if a user reports missing data, we'd go look at the logs for the last expiration and see whether this count is crazy high, indicating that we deleted too many files somehow.", "author": "rdblue", "createdAt": "2020-08-08T00:19:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5NTc0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5Njc1MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466496751", "bodyText": "By default, this will use memory and disk. I am OK with this. In the future, we will replace it with reading from an old version file.", "author": "aokolnychyi", "createdAt": "2020-08-06T15:29:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyNDc2Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466524766", "bodyText": "Yeah this is temporary, and I'm hoping that in most use cases it won't actually require a disk spill", "author": "RussellSpitzer", "createdAt": "2020-08-06T16:10:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjQ5Njc1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUxMjc5MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466512790", "bodyText": "I like this name better than executeWith we have in RemoveSnapshots. Shall we update RemoveSnapshots too before we release it? @fbocse @rdblue, what do you think?", "author": "aokolnychyi", "createdAt": "2020-08-06T15:51:56Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzOTU1NA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466539554", "bodyText": "I wonder executeDeleteWith would become very specific to only use thread-pool for delete operation. What if we have some task in the operation, where the same thread pool can be re-used.", "author": "mehtaashish23", "createdAt": "2020-08-06T16:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUxMjc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU1NjA0Mw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466556043", "bodyText": "I find the current behavior in RemoveSnapshots confusing. If I set an executor service with executeWith, I'd expect that to be used for all things since the name is generic. That does not happen because manifests are scanned using ThreadPools.getWorkerPool() and the passed executor service is used only for deletes.", "author": "aokolnychyi", "createdAt": "2020-08-06T17:03:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUxMjc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMzNTgyOA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467335828", "bodyText": "I agree. This is more specific, and since it only applies when deleting the data files being more specific is good. I also think that a pool passed to executeWith would be used to parallelize the other operations.", "author": "rdblue", "createdAt": "2020-08-08T00:20:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUxMjc5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxOTM0OA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468019348", "bodyText": "Sounds good. I know @RussellSpitzer has a separate PR for renaming.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:11:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUxMjc5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyOTc1Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466529752", "bodyText": "Shouldn't this be trace as well?", "author": "mehtaashish23", "createdAt": "2020-08-06T16:18:13Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());\n+    originalFiles.unpersist();\n+    return result;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n+  }\n+\n+  private Dataset<Row> buildValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  }\n+\n+  private ExpireSnapshotsActionResult deleteFiles(Iterator<Row> paths) {\n+    AtomicLong dataFileCount = new AtomicLong(0L);\n+    AtomicLong manifestCount = new AtomicLong(0L);\n+    AtomicLong manifestListCount = new AtomicLong(0L);\n+\n+    Tasks.foreach(paths)\n+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()\n+        .executeWith(deleteExecutorService)\n+        .onFailure((fileInfo, exc) ->\n+            LOG.warn(\"Delete failed for {}: {}\", fileInfo.getString(1), fileInfo.getString(0), exc))\n+        .run(fileInfo -> {\n+          String file = fileInfo.getString(0);\n+          String type = fileInfo.getString(1);\n+          deleteFunc.accept(file);\n+          switch (type) {\n+            case DATA_FILE:\n+              dataFileCount.incrementAndGet();\n+              LOG.trace(\"Deleted Data File: {}\", file);\n+              break;\n+            case MANIFEST:\n+              manifestCount.incrementAndGet();\n+              LOG.warn(\"Deleted Manifest: {}\", file);", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2MjQ5OA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466562498", "bodyText": "This mimics the RemoveSnapshot logic, which uses warn", "author": "RussellSpitzer", "createdAt": "2020-08-06T17:14:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUyOTc1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzMDY1Nw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466530657", "bodyText": "Unnecessary file?", "author": "mehtaashish23", "createdAt": "2020-08-06T16:19:49Z", "path": "spark2/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction24.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+public class TestExpireSnapshotsAction24 extends TestExpireSnapshotsAction{", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2MzAwMA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466563000", "bodyText": "This is the concrete implementation of the Abstract class TestExpireSnapshotsAction, this is how we do Spark 2/3 testing with a separate file for each Spark Version", "author": "RussellSpitzer", "createdAt": "2020-08-06T17:15:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzMDY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzMDg2NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466530865", "bodyText": "same: Unnecessary files?", "author": "mehtaashish23", "createdAt": "2020-08-06T16:20:07Z", "path": "spark3/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction3.java", "diffHunk": "@@ -0,0 +1,25 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.actions;\n+\n+import org.apache.iceberg.actions.TestExpireSnapshotsAction;\n+\n+public class TestExpireSnapshotsAction3 extends TestExpireSnapshotsAction {", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU2MzIxOA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466563218", "bodyText": "Same as above, implementation of the TestExpireSnapshotsAction but within the Spark3 suite", "author": "RussellSpitzer", "createdAt": "2020-08-06T17:16:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjUzMDg2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjU0MjQwMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r466542402", "bodyText": "same here.", "author": "mehtaashish23", "createdAt": "2020-08-06T16:39:59Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    //Metadata before Expiration\n+    Dataset<Row> originalFiles = buildValidFileDF().persist();\n+    originalFiles.count(); // Action to trigger persist\n+\n+    //Perform Expiration\n+    ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+    if (expireSnapshotIdValue != null) {\n+      expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+    }\n+    if (expireOlderThanValue != null) {\n+      expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+    }\n+    if (retainLastValue != null) {\n+      expireSnaps = expireSnaps.retainLast(retainLastValue);\n+    }\n+    expireSnaps.commit();\n+\n+    // Metadata after Expiration\n+    Dataset<Row> validFiles = buildValidFileDF();\n+    Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+    ExpireSnapshotsActionResult result =  deleteFiles(filesToDelete.toLocalIterator());\n+    originalFiles.unpersist();\n+    return result;\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n+  }\n+\n+  private Dataset<Row> buildValidFileDF() {\n+    return appendTypeString(buildValidDataFileDF(spark), DATA_FILE)\n+        .union(appendTypeString(buildManifestFileDF(spark), MANIFEST))\n+        .union(appendTypeString(buildManifestListDF(spark, table), MANIFEST_LIST));\n+  }\n+\n+  private ExpireSnapshotsActionResult deleteFiles(Iterator<Row> paths) {\n+    AtomicLong dataFileCount = new AtomicLong(0L);\n+    AtomicLong manifestCount = new AtomicLong(0L);\n+    AtomicLong manifestListCount = new AtomicLong(0L);\n+\n+    Tasks.foreach(paths)\n+        .retry(3).stopRetryOn(NotFoundException.class).suppressFailureWhenFinished()\n+        .executeWith(deleteExecutorService)\n+        .onFailure((fileInfo, exc) ->\n+            LOG.warn(\"Delete failed for {}: {}\", fileInfo.getString(1), fileInfo.getString(0), exc))\n+        .run(fileInfo -> {\n+          String file = fileInfo.getString(0);\n+          String type = fileInfo.getString(1);\n+          deleteFunc.accept(file);\n+          switch (type) {\n+            case DATA_FILE:\n+              dataFileCount.incrementAndGet();\n+              LOG.trace(\"Deleted Data File: {}\", file);\n+              break;\n+            case MANIFEST:\n+              manifestCount.incrementAndGet();\n+              LOG.warn(\"Deleted Manifest: {}\", file);\n+              break;\n+            case MANIFEST_LIST:\n+              manifestListCount.incrementAndGet();\n+              LOG.warn(\"Deleted Manifest List: {}\", file);", "originalCommit": "2e0af0a1121d923978344676d593e46d89686962", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "url": "https://github.com/apache/iceberg/commit/e73f76971caa117d3fbadf8c70450f46a8ef51a2", "message": "Port over all the tests from TestRemoveSnapshots", "committedDate": "2020-08-07T00:07:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzNTk2OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467035969", "bodyText": "Sounds like creating manifestLists can be placed in core module, so that it can be reused from action for other execution engine, or even without engine-specific action.", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:21:26Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,47 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n+    List<String> manifestLists = Lists.newArrayList();", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5MTA0Nw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467091047", "bodyText": "Sure I can move these over to a new TableUtil class in the core module", "author": "RussellSpitzer", "createdAt": "2020-08-07T14:52:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzNTk2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzAzNjQxMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467036413", "bodyText": "Ditto.", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:22:13Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +51,47 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  protected Dataset<Row> buildValidDataFileDF(SparkSession spark) {\n+    String allDataFilesMetadataTable = metadataTableName(MetadataTableType.ALL_DATA_FILES);\n+    return spark.read().format(\"iceberg\").load(allDataFilesMetadataTable).select(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestFileDF(SparkSession spark) {\n+    String allManifestsMetadataTable = metadataTableName(MetadataTableType.ALL_MANIFESTS);\n+    return spark.read().format(\"iceberg\").load(allManifestsMetadataTable).selectExpr(\"path as file_path\");\n+  }\n+\n+  protected Dataset<Row> buildManifestListDF(SparkSession spark, Table table) {\n+    List<String> manifestLists = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        manifestLists.add(manifestListLocation);\n+      }\n+    }\n+\n+    return spark.createDataset(manifestLists, Encoders.STRING()).toDF(\"file_path\");\n+  }\n+\n+  protected Dataset<Row> buildOtherMetadataFileDF(SparkSession spark, TableOperations ops) {\n+    List<String> otherMetadataFiles = Lists.newArrayList();", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0MTIxMQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467041211", "bodyText": "\"apply that\" seems redundant.", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:30:38Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA5NDE5NA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467094194", "bodyText": "Thanks for all the doc review here!", "author": "RussellSpitzer", "createdAt": "2020-08-07T14:57:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0MTIxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0Mjc2OA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467042768", "bodyText": "nit: MetadataTables -> metadata tables", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:33:22Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.\n+ * <p>\n+ * This implementation uses the MetadataTables for the table being expired to list all Manifest and DataFiles. This", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0Mzc4OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467043789", "bodyText": "nit: Dataframes which are anti-joined", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:35:02Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.\n+ * <p>\n+ * This implementation uses the MetadataTables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which is antiJoined with the same list read after the expiration. This operation will", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NTI2OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467045269", "bodyText": "nit: Shuffle -> shuffle, so -> , and", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:37:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.\n+ * <p>\n+ * This implementation uses the MetadataTables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which is antiJoined with the same list read after the expiration. This operation will\n+ * require a Shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NjMzNA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467046334", "bodyText": "Probably it'd be nice to describe about expiring snapshots and removing obsolete files sequentially, not mixing up.", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:39:22Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.\n+ * <p>\n+ * This implementation uses the MetadataTables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which is antiJoined with the same list read after the expiration. This operation will\n+ * require a Shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA0NjUxMA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467046510", "bodyText": "nit: Spark executors", "author": "HeartSaVioR", "createdAt": "2020-08-07T13:39:40Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions apply that apply to Remove Snapshots.\n+ * <p>\n+ * This implementation uses the MetadataTables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which is antiJoined with the same list read after the expiration. This operation will\n+ * require a Shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. Deletes are still performed locally after retrieving the results\n+ * from the SparkExecutors.", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzA2NzA1Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467067056", "bodyText": "nit: double empty lines", "author": "HeartSaVioR", "createdAt": "2020-08-07T14:13:41Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,781 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+", "originalCommit": "e73f76971caa117d3fbadf8c70450f46a8ef51a2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "url": "https://github.com/apache/iceberg/commit/9cc4e831679d0aab1e4409210f3a7c57094c440c", "message": "Review Comments\n\nMove getManifestLists / getOtherManifestPaths to Core Module\nFixup of doc typos", "committedDate": "2020-08-07T15:38:45Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExOTEwMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467119103", "bodyText": "Partitioning changed from TestTableBase to match the schema I was using here", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:41:52Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExOTU0Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467119542", "bodyText": "Manual metadata rewrite code removed from here since it wasn't really part of the test and I couldn't easily move those functions into Spark module", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:42:43Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExOTY1Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467119652", "bodyText": "New Test - Not Ported", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:42:56Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExOTc3MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467119770", "bodyText": "New test - Not Ported", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:43:11Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzExOTg4MA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467119880", "bodyText": "New test - Not Ported", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:43:21Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDIyOA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120228", "bodyText": "Ported test - everything the same except for the manifest manipulation see comment below", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:44:03Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDU2MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120561", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:44:31Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDY2Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120666", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:44:41Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDcyNg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120726", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:44:48Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDgwOQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120809", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:44:55Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMDkzNg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467120936", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:04Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTA2OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121069", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:16Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTE4OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121189", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:28Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTI0MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121241", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:35Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {\n+    AssertHelpers.assertThrows(\"Should fail retain 0 snapshots \" +\n+            \"because number of snapshots to retain cannot be zero\",\n+        IllegalArgumentException.class,\n+        \"Number of snapshots to retain must be at least 1, cannot be: 0\",\n+        () -> Actions.forTable(table).expireSnapshots().retainLast(0).execute());\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotAppend() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTM2MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121361", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:47Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {\n+    AssertHelpers.assertThrows(\"Should fail retain 0 snapshots \" +\n+            \"because number of snapshots to retain cannot be zero\",\n+        IllegalArgumentException.class,\n+        \"Number of snapshots to retain must be at least 1, cannot be: 0\",\n+        () -> Actions.forTable(table).expireSnapshots().retainLast(0).execute());\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotAppend() {\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotFastAppend() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTQzMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121432", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:45:54Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {\n+    AssertHelpers.assertThrows(\"Should fail retain 0 snapshots \" +\n+            \"because number of snapshots to retain cannot be zero\",\n+        IllegalArgumentException.class,\n+        \"Number of snapshots to retain must be at least 1, cannot be: 0\",\n+        () -> Actions.forTable(table).expireSnapshots().retainLast(0).execute());\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotAppend() {\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotFastAppend() {\n+    table.updateProperties()\n+        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"true\")\n+        .set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"1\")\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+  }\n+\n+  /**\n+   * Test on table below, and expiring the staged commit `B` using `expireOlderThan` API.\n+   * Table: A - C\n+   *          ` B (staged)\n+   */\n+  @Test\n+  public void testWithExpiringDanglingStageCommit() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTQ5Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121492", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:46:02Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {\n+    AssertHelpers.assertThrows(\"Should fail retain 0 snapshots \" +\n+            \"because number of snapshots to retain cannot be zero\",\n+        IllegalArgumentException.class,\n+        \"Number of snapshots to retain must be at least 1, cannot be: 0\",\n+        () -> Actions.forTable(table).expireSnapshots().retainLast(0).execute());\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotAppend() {\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotFastAppend() {\n+    table.updateProperties()\n+        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"true\")\n+        .set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"1\")\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+  }\n+\n+  /**\n+   * Test on table below, and expiring the staged commit `B` using `expireOlderThan` API.\n+   * Table: A - C\n+   *          ` B (staged)\n+   */\n+  @Test\n+  public void testWithExpiringDanglingStageCommit() {\n+    // `A` commit\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    // `B` staged commit\n+    table.newAppend()\n+        .appendFile(FILE_B)\n+        .stageOnly()\n+        .commit();\n+\n+    TableMetadata base = ((BaseTable) table).operations().current();\n+    Snapshot snapshotA = base.snapshots().get(0);\n+    Snapshot snapshotB = base.snapshots().get(1);\n+\n+    // `C` commit\n+    table.newAppend()\n+        .appendFile(FILE_C)\n+        .commit();\n+\n+    Set<String> deletedFiles = new HashSet<>();\n+\n+    // Expire all commits including dangling staged snapshot.\n+    Actions.forTable(table).expireSnapshots()\n+        .deleteWith(deletedFiles::add)\n+        .expireOlderThan(snapshotB.timestampMillis() + 1)\n+        .execute();\n+\n+    Set<String> expectedDeletes = new HashSet<>();\n+    expectedDeletes.add(snapshotA.manifestListLocation());\n+\n+    // Files should be deleted of dangling staged snapshot\n+    snapshotB.addedFiles().forEach(i -> {\n+      expectedDeletes.add(i.path().toString());\n+    });\n+\n+    // ManifestList should be deleted too\n+    expectedDeletes.add(snapshotB.manifestListLocation());\n+    snapshotB.dataManifests().forEach(file -> {\n+      //Only the manifest of B should be deleted.\n+      if (file.snapshotId() == snapshotB.snapshotId()) {\n+        expectedDeletes.add(file.path());\n+      }\n+    });\n+    Assert.assertSame(\"Files deleted count should be expected\", expectedDeletes.size(), deletedFiles.size());\n+    //Take the diff\n+    expectedDeletes.removeAll(deletedFiles);\n+    Assert.assertTrue(\"Exactly same files should be deleted\", expectedDeletes.isEmpty());\n+  }\n+\n+  /**\n+   * Expire cherry-pick the commit as shown below, when `B` is in table's current state\n+   *  Table:\n+   *  A - B - C <--current snapshot\n+   *   `- D (source=B)\n+   */\n+  @Test\n+  public void testWithCherryPickTableSnapshot() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTU2Mw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121563", "bodyText": "Ported Test - Only change is using ExpireSnapshotsAction", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:46:10Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();\n+\n+    Assert.assertEquals(\"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+\n+    for (Path p : expiredDataFiles) {\n+      Assert.assertFalse(String.format(\"File %s still exists but should have been deleted\", p),\n+          Files.exists(p));\n+    }\n+\n+    checkExpirationResults(1L, 2L, 2L, results);\n+  }\n+\n+  @Test\n+  public void dataFilesCleanupWithParallelTasks() throws IOException {\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_B), ImmutableSet.of(FILE_D))\n+        .commit();\n+    long thirdSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    table.newRewrite()\n+        .rewriteFiles(ImmutableSet.of(FILE_A), ImmutableSet.of(FILE_C))\n+        .commit();\n+    long fourthSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+    Set<String> deleteThreads = ConcurrentHashMap.newKeySet();\n+    AtomicInteger deleteThreadsIndex = new AtomicInteger(0);\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .executeDeleteWith(Executors.newFixedThreadPool(4, runnable -> {\n+          Thread thread = new Thread(runnable);\n+          thread.setName(\"remove-snapshot-\" + deleteThreadsIndex.getAndIncrement());\n+          thread.setDaemon(true); // daemon threads will be terminated abruptly when the JVM exits\n+          return thread;\n+        }))\n+        .expireOlderThan(t4)\n+        .deleteWith(s -> {\n+          deleteThreads.add(Thread.currentThread().getName());\n+          deletedFiles.add(s);\n+        })\n+        .execute();\n+\n+    // Verifies that the delete methods ran in the threads created by the provided ExecutorService ThreadFactory\n+    Assert.assertEquals(deleteThreads,\n+        Sets.newHashSet(\"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.path().toString()));\n+  }\n+\n+  @Test\n+  public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().execute();\n+\n+    checkExpirationResults(0L, 0L, 0L, results);\n+  }\n+\n+  @Test\n+  public void testCleanupRepeatedOverwrites() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    for (int i = 0; i < 10; i++) {\n+      writeDF(df, \"overwrite\");\n+    }\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    checkExpirationResults(10L, 19L, 10L, results);\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireOlderThan() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithExpireById() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots, but explicitly remove the first snapshot\n+    Actions.forTable(table).expireSnapshots()\n+        .expireSnapshotId(firstSnapshotId)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots.\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should not present.\",\n+        null, table.snapshot(firstSnapshotId));\n+  }\n+\n+  @Test\n+  public void testRetainLastWithTooFewSnapshots() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+    long firstSnapshotId = table.currentSnapshot().snapshotId();\n+\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 3 snapshots\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t2)\n+        .retainLast(3)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have two snapshots\",\n+        2, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertEquals(\"First snapshot should still present\",\n+        firstSnapshotId, table.snapshot(firstSnapshotId).snapshotId());\n+  }\n+\n+  @Test\n+  public void testRetainLastKeepsExpiringSnapshot() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_D) // data_bucket=3\n+        .commit();\n+\n+    long t4 = System.currentTimeMillis();\n+    while (t4 <= table.currentSnapshot().timestampMillis()) {\n+      t4 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .retainLast(2)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have three snapshots.\",\n+        3, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNotNull(\"Second snapshot should present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testExpireOlderThanMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    Snapshot thirdSnapshot = table.currentSnapshot();\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(secondSnapshot.timestampMillis())\n+        .expireOlderThan(thirdSnapshot.timestampMillis())\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainLastMultipleCalls() {\n+    long t0 = System.currentTimeMillis();\n+    table.newAppend()\n+        .appendFile(FILE_A) // data_bucket=0\n+        .commit();\n+    long t1 = System.currentTimeMillis();\n+    while (t1 <= table.currentSnapshot().timestampMillis()) {\n+      t1 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_B) // data_bucket=1\n+        .commit();\n+\n+    Snapshot secondSnapshot = table.currentSnapshot();\n+    long t2 = System.currentTimeMillis();\n+    while (t2 <= table.currentSnapshot().timestampMillis()) {\n+      t2 = System.currentTimeMillis();\n+    }\n+\n+    table.newAppend()\n+        .appendFile(FILE_C) // data_bucket=2\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    // Retain last 2 snapshots and expire older than t3\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .retainLast(2)\n+        .retainLast(1)\n+        .execute();\n+\n+    Assert.assertEquals(\"Should have one snapshots.\",\n+        1, Lists.newArrayList(table.snapshots()).size());\n+    Assert.assertNull(\"Second snapshot should not present.\",\n+        table.snapshot(secondSnapshot.snapshotId()));\n+  }\n+\n+  @Test\n+  public void testRetainZeroSnapshots() {\n+    AssertHelpers.assertThrows(\"Should fail retain 0 snapshots \" +\n+            \"because number of snapshots to retain cannot be zero\",\n+        IllegalArgumentException.class,\n+        \"Number of snapshots to retain must be at least 1, cannot be: 0\",\n+        () -> Actions.forTable(table).expireSnapshots().retainLast(0).execute());\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotAppend() {\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+\n+  }\n+\n+  @Test\n+  public void testScanExpiredManifestInValidSnapshotFastAppend() {\n+    table.updateProperties()\n+        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"true\")\n+        .set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"1\")\n+        .commit();\n+\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .appendFile(FILE_B)\n+        .commit();\n+\n+    table.newOverwrite()\n+        .addFile(FILE_C)\n+        .deleteFile(FILE_A)\n+        .commit();\n+\n+    table.newFastAppend()\n+        .appendFile(FILE_D)\n+        .commit();\n+\n+    long t3 = System.currentTimeMillis();\n+    while (t3 <= table.currentSnapshot().timestampMillis()) {\n+      t3 = System.currentTimeMillis();\n+    }\n+\n+    Set<String> deletedFiles = Sets.newHashSet();\n+\n+    Actions.forTable(table).expireSnapshots()\n+        .expireOlderThan(t3)\n+        .deleteWith(deletedFiles::add)\n+        .execute();\n+\n+    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.path().toString()));\n+  }\n+\n+  /**\n+   * Test on table below, and expiring the staged commit `B` using `expireOlderThan` API.\n+   * Table: A - C\n+   *          ` B (staged)\n+   */\n+  @Test\n+  public void testWithExpiringDanglingStageCommit() {\n+    // `A` commit\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+\n+    // `B` staged commit\n+    table.newAppend()\n+        .appendFile(FILE_B)\n+        .stageOnly()\n+        .commit();\n+\n+    TableMetadata base = ((BaseTable) table).operations().current();\n+    Snapshot snapshotA = base.snapshots().get(0);\n+    Snapshot snapshotB = base.snapshots().get(1);\n+\n+    // `C` commit\n+    table.newAppend()\n+        .appendFile(FILE_C)\n+        .commit();\n+\n+    Set<String> deletedFiles = new HashSet<>();\n+\n+    // Expire all commits including dangling staged snapshot.\n+    Actions.forTable(table).expireSnapshots()\n+        .deleteWith(deletedFiles::add)\n+        .expireOlderThan(snapshotB.timestampMillis() + 1)\n+        .execute();\n+\n+    Set<String> expectedDeletes = new HashSet<>();\n+    expectedDeletes.add(snapshotA.manifestListLocation());\n+\n+    // Files should be deleted of dangling staged snapshot\n+    snapshotB.addedFiles().forEach(i -> {\n+      expectedDeletes.add(i.path().toString());\n+    });\n+\n+    // ManifestList should be deleted too\n+    expectedDeletes.add(snapshotB.manifestListLocation());\n+    snapshotB.dataManifests().forEach(file -> {\n+      //Only the manifest of B should be deleted.\n+      if (file.snapshotId() == snapshotB.snapshotId()) {\n+        expectedDeletes.add(file.path());\n+      }\n+    });\n+    Assert.assertSame(\"Files deleted count should be expected\", expectedDeletes.size(), deletedFiles.size());\n+    //Take the diff\n+    expectedDeletes.removeAll(deletedFiles);\n+    Assert.assertTrue(\"Exactly same files should be deleted\", expectedDeletes.isEmpty());\n+  }\n+\n+  /**\n+   * Expire cherry-pick the commit as shown below, when `B` is in table's current state\n+   *  Table:\n+   *  A - B - C <--current snapshot\n+   *   `- D (source=B)\n+   */\n+  @Test\n+  public void testWithCherryPickTableSnapshot() {\n+    // `A` commit\n+    table.newAppend()\n+        .appendFile(FILE_A)\n+        .commit();\n+    Snapshot snapshotA = table.currentSnapshot();\n+\n+    // `B` commit\n+    Set<String> deletedAFiles = new HashSet<>();\n+    table.newOverwrite()\n+        .addFile(FILE_B)\n+        .deleteFile(FILE_A)\n+        .deleteWith(deletedAFiles::add)\n+        .commit();\n+    Assert.assertTrue(\"No files should be physically deleted\", deletedAFiles.isEmpty());\n+\n+    // pick the snapshot 'B`\n+    Snapshot snapshotB = table.currentSnapshot();\n+\n+    // `C` commit to let cherry-pick take effect, and avoid fast-forward of `B` with cherry-pick\n+    table.newAppend()\n+        .appendFile(FILE_C)\n+        .commit();\n+    Snapshot snapshotC = table.currentSnapshot();\n+\n+    // Move the table back to `A`\n+    table.manageSnapshots()\n+        .setCurrentSnapshot(snapshotA.snapshotId())\n+        .commit();\n+\n+    // Generate A -> `D (B)`\n+    table.manageSnapshots()\n+        .cherrypick(snapshotB.snapshotId())\n+        .commit();\n+    Snapshot snapshotD = table.currentSnapshot();\n+\n+    // Move the table back to `C`\n+    table.manageSnapshots()\n+        .setCurrentSnapshot(snapshotC.snapshotId())\n+        .commit();\n+    List<String> deletedFiles = new ArrayList<>();\n+\n+    // Expire `C`\n+    Actions.forTable(table).expireSnapshots()\n+        .deleteWith(deletedFiles::add)\n+        .expireOlderThan(snapshotC.timestampMillis() + 1)\n+        .execute();\n+\n+    // Make sure no dataFiles are deleted for the B, C, D snapshot\n+    Lists.newArrayList(snapshotB, snapshotC, snapshotD).forEach(i -> {\n+      i.addedFiles().forEach(item -> {\n+        Assert.assertFalse(deletedFiles.contains(item.path().toString()));\n+      });\n+    });\n+  }\n+\n+  /**\n+   * Test on table below, and expiring `B` which is not in current table state.\n+   *  1) Expire `B`\n+   *  2) All commit\n+   * Table: A - C - D (B)\n+   *          ` B (staged)\n+   */\n+  @Test\n+  public void testWithExpiringStagedThenCherrypick() {", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMTg4Mw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467121883", "bodyText": "These files are directly copied out of the Core Test Base, only modification is in the partitioning path", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:46:48Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzEyMjM5Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r467122396", "bodyText": "The only tests not directly ported were those doing file cleanup (except for the parallel delete one) since we already had new tests for those functions.", "author": "RussellSpitzer", "createdAt": "2020-08-07T15:47:41Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+", "originalCommit": "9cc4e831679d0aab1e4409210f3a7c57094c440c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "url": "https://github.com/apache/iceberg/commit/a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "message": "Change Log Level of  File Delete Count to INFO", "committedDate": "2020-08-10T15:24:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxMTc0Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468011746", "bodyText": "I am OK with the way it is done. An alternative would be to add more methods to TableMetadata directly. It has access to snapshots and previous metadata files. We don't have access to version-hint.text, though. Any thoughts, @rdblue?", "author": "aokolnychyi", "createdAt": "2020-08-10T15:59:19Z", "path": "core/src/main/java/org/apache/iceberg/util/TableUtil.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.util;\n+\n+import java.util.List;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class TableUtil {", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAzODgwMA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468038800", "bodyText": "This is okay, but I don't see much utility for these methods other than in the actions, and methods were already added to BaseAction. I would probably make these private methods in BaseAction instead of adding a utility class.\nI think that would be better because we usually try to have util methods either use internals (TableOperations, TableMetadata) or the public API (Table) and not mix the two. Rather than rewrite one of these methods to use TableMetadata, I'd just move them.", "author": "rdblue", "createdAt": "2020-08-10T16:44:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxMTc0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0MDkwNg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468040906", "bodyText": "@HeartSaVioR Just to check with you, would it be ok if I moved the methods back into BaseAction? I know you wanted these to be back in the core module.", "author": "RussellSpitzer", "createdAt": "2020-08-10T16:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxMTc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxMjQ0OA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468012448", "bodyText": "nit: I think we can skip org.apache.iceberg in the doc as it is already imported.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:00:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxMjU5OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468012599", "bodyText": "nit: to to", "author": "aokolnychyi", "createdAt": "2020-08-10T16:00:40Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNDM0OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468014349", "bodyText": "nit: org.apache.iceberg. can be dropped.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:03:34Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNTQwMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468015403", "bodyText": "Similar -> similar or we need . after Spark action.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:05:14Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxNjU1Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468016552", "bodyText": "nit: org.apache.iceberg. can be dropped in all javadocs.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:07:11Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  /**\n+   * A specific snapshot to expire.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireSnapshotId(long)}", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxODQ0Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468018442", "bodyText": "This must be a list as expireSnapshotId can be called multiple times just like in RemoveSnapshots.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:10:16Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAyMDA3MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468020071", "bodyText": "We will need a test that catches this as well.", "author": "aokolnychyi", "createdAt": "2020-08-10T16:12:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAxODQ0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAyMjA1Nw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468022057", "bodyText": "nit: unnecessary variable, can simply return", "author": "aokolnychyi", "createdAt": "2020-08-10T16:16:03Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  /**\n+   * A specific snapshot to expire.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireSnapshotId(long)}\n+   * @param expireSnapshotId Id of the snapshot to expire\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  /**\n+   * Expire all snapshots older than a given timestamp.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireOlderThan(long)}\n+   * @param timestampMillis all snapshots before this time will be expired\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  /**\n+   * Retain at least x snapshots when expiring\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#retainLast(int)}\n+   * @param numSnapshots number of snapshots to leave\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    Preconditions.checkArgument(1 <= numSnapshots,\n+        \"Number of snapshots to retain must be at least 1, cannot be: %s\", numSnapshots);\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  /**\n+   * The Consumer used on files which have been determined to be expired. By default uses a filesystem delete.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#deleteWith(Consumer)}\n+   * @param newDeleteFunc Consumer which takes a path and deletes it\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    Dataset<Row> originalFiles = null;\n+    try {\n+      // Metadata before Expiration\n+      originalFiles = buildValidFileDF().persist();\n+      // Action to trigger persist\n+      originalFiles.count();\n+\n+      // Perform Expiration\n+      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+      if (expireSnapshotIdValue != null) {\n+        expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+      }\n+      if (expireOlderThanValue != null) {\n+        expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+      }\n+      if (retainLastValue != null) {\n+        expireSnaps = expireSnaps.retainLast(retainLastValue);\n+      }\n+      expireSnaps.commit();\n+\n+      // Metadata after Expiration\n+      Dataset<Row> validFiles = buildValidFileDF();\n+      Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+      ExpireSnapshotsActionResult result = deleteFiles(filesToDelete.toLocalIterator());", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAzMzMwNw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468033307", "bodyText": "Nit: extra newline added.", "author": "rdblue", "createdAt": "2020-08-10T16:34:46Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -19,8 +19,16 @@\n \n package org.apache.iceberg.actions;\n \n+import java.util.List;\n import org.apache.iceberg.MetadataTableType;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.util.TableUtil;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAzNTUwMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468035503", "bodyText": "Nit: formatting here is off. We always add a space between method args and curly braces.", "author": "rdblue", "createdAt": "2020-08-10T16:38:31Z", "path": "core/src/main/java/org/apache/iceberg/util/TableUtil.java", "diffHunk": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.util;\n+\n+import java.util.List;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class TableUtil {\n+\n+  private TableUtil(){}", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE2Mzg5Mg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468163892", "bodyText": "Whole class is now removed", "author": "RussellSpitzer", "createdAt": "2020-08-10T20:21:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODAzNTUwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0MDE3MQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468040171", "bodyText": "Why does this exclude other data files? I would expect this to use valid files and valid metadata files, as returned by the base action.", "author": "rdblue", "createdAt": "2020-08-10T16:46:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  /**\n+   * A specific snapshot to expire.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireSnapshotId(long)}\n+   * @param expireSnapshotId Id of the snapshot to expire\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  /**\n+   * Expire all snapshots older than a given timestamp.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireOlderThan(long)}\n+   * @param timestampMillis all snapshots before this time will be expired\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  /**\n+   * Retain at least x snapshots when expiring\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#retainLast(int)}\n+   * @param numSnapshots number of snapshots to leave\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    Preconditions.checkArgument(1 <= numSnapshots,\n+        \"Number of snapshots to retain must be at least 1, cannot be: %s\", numSnapshots);\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  /**\n+   * The Consumer used on files which have been determined to be expired. By default uses a filesystem delete.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#deleteWith(Consumer)}\n+   * @param newDeleteFunc Consumer which takes a path and deletes it\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    Dataset<Row> originalFiles = null;\n+    try {\n+      // Metadata before Expiration\n+      originalFiles = buildValidFileDF().persist();\n+      // Action to trigger persist\n+      originalFiles.count();\n+\n+      // Perform Expiration\n+      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+      if (expireSnapshotIdValue != null) {\n+        expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+      }\n+      if (expireOlderThanValue != null) {\n+        expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);\n+      }\n+      if (retainLastValue != null) {\n+        expireSnaps = expireSnaps.retainLast(retainLastValue);\n+      }\n+      expireSnaps.commit();\n+\n+      // Metadata after Expiration\n+      Dataset<Row> validFiles = buildValidFileDF();\n+      Dataset<Row> filesToDelete = originalFiles.except(validFiles);\n+\n+      ExpireSnapshotsActionResult result = deleteFiles(filesToDelete.toLocalIterator());\n+      return result;\n+    } finally {\n+      if (originalFiles != null) {\n+        originalFiles.unpersist();\n+      }\n+    }\n+  }\n+\n+  private Dataset<Row> appendTypeString(Dataset<Row> ds, String type) {\n+    return ds.select(new Column(\"file_path\"), functions.lit(type).as(\"file_type\"));\n+  }\n+\n+  private Dataset<Row> buildValidFileDF() {", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0ODA5Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468048096", "bodyText": "From @aokolnychyi 's previous comment\n\nRight now, we don't remove old version files while expiring snapshots. On one hand, we keep old version files for debugging. On the other hand, we have to call RemoveOrphanFilesAction to actually delete them. This action behaves differently compared to RemoveSnapshots and may remove some version files as well. Since expiring snapshots produces a new version, we will delete at most one version file. So, I think ignoring version files from the analysis is the right way to go.", "author": "RussellSpitzer", "createdAt": "2020-08-10T16:59:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0MDE3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0Mjc2OQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468042769", "bodyText": "Nit: we usually add blank lines after if and loop control flow statements.", "author": "rdblue", "createdAt": "2020-08-10T16:51:07Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();\n+\n+  private final SparkSession spark;\n+  private final Table table;\n+  private final TableOperations ops;\n+  private final Consumer<String> defaultDelete = new Consumer<String>() {\n+    @Override\n+    public void accept(String file) {\n+      ops.io().deleteFile(file);\n+    }\n+  };\n+\n+  private Long expireSnapshotIdValue = null;\n+  private Long expireOlderThanValue = null;\n+  private Integer retainLastValue = null;\n+  private Consumer<String> deleteFunc = defaultDelete;\n+  private ExecutorService deleteExecutorService = DEFAULT_DELETE_EXECUTOR_SERVICE;\n+\n+  ExpireSnapshotsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.table = table;\n+    this.ops = ((HasTableOperations) table).operations();\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * An executor service used when deleting files. Only used during the local delete phase of this Spark action\n+   * Similar to {@link org.apache.iceberg.ExpireSnapshots#executeWith(ExecutorService)}\n+   * @param executorService the service to use\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction executeDeleteWith(ExecutorService executorService) {\n+    this.deleteExecutorService = executorService;\n+    return this;\n+  }\n+\n+  /**\n+   * A specific snapshot to expire.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireSnapshotId(long)}\n+   * @param expireSnapshotId Id of the snapshot to expire\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireSnapshotId(long expireSnapshotId) {\n+    this.expireSnapshotIdValue = expireSnapshotId;\n+    return this;\n+  }\n+\n+  /**\n+   * Expire all snapshots older than a given timestamp.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#expireOlderThan(long)}\n+   * @param timestampMillis all snapshots before this time will be expired\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction expireOlderThan(long timestampMillis) {\n+    this.expireOlderThanValue = timestampMillis;\n+    return this;\n+  }\n+\n+  /**\n+   * Retain at least x snapshots when expiring\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#retainLast(int)}\n+   * @param numSnapshots number of snapshots to leave\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction retainLast(int numSnapshots) {\n+    Preconditions.checkArgument(1 <= numSnapshots,\n+        \"Number of snapshots to retain must be at least 1, cannot be: %s\", numSnapshots);\n+    this.retainLastValue = numSnapshots;\n+    return this;\n+  }\n+\n+  /**\n+   * The Consumer used on files which have been determined to be expired. By default uses a filesystem delete.\n+   * Identical to {@link org.apache.iceberg.ExpireSnapshots#deleteWith(Consumer)}\n+   * @param newDeleteFunc Consumer which takes a path and deletes it\n+   * @return this for method chaining\n+   */\n+  public ExpireSnapshotsAction deleteWith(Consumer<String> newDeleteFunc) {\n+    this.deleteFunc = newDeleteFunc;\n+    return this;\n+  }\n+\n+  @Override\n+  public ExpireSnapshotsActionResult execute() {\n+    Dataset<Row> originalFiles = null;\n+    try {\n+      // Metadata before Expiration\n+      originalFiles = buildValidFileDF().persist();\n+      // Action to trigger persist\n+      originalFiles.count();\n+\n+      // Perform Expiration\n+      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);\n+      if (expireSnapshotIdValue != null) {\n+        expireSnaps = expireSnaps.expireSnapshotId(expireSnapshotIdValue);\n+      }\n+      if (expireOlderThanValue != null) {", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0NjgxMw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468046813", "bodyText": "Minor: We don't use get because it doesn't generally add any value. In most cases, the right thing to do is to use a more descriptive verb, like load or create. If there isn't a more descriptive verb because it is just accessing a member field, then we omit get. That's more readable and the convention in most other JVM languages.", "author": "rdblue", "createdAt": "2020-08-10T16:57:44Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsActionResult.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+public class ExpireSnapshotsActionResult {\n+\n+  private final Long dataFilesDeleted;\n+  private final Long manifestFilesDeleted;\n+  private final Long manifestListsDeleted;\n+\n+  public ExpireSnapshotsActionResult(Long dataFilesDeleted, Long manifestFilesDeleted, Long manifestListsDeleted) {\n+    this.dataFilesDeleted = dataFilesDeleted;\n+    this.manifestFilesDeleted = manifestFilesDeleted;\n+    this.manifestListsDeleted = manifestListsDeleted;\n+  }\n+\n+  public Long getDataFilesDeleted() {", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE3MTYxMg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468171612", "bodyText": "sgtm", "author": "RussellSpitzer", "createdAt": "2020-08-10T20:37:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0NjgxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0OTAzMQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468049031", "bodyText": "Should this be a util method?", "author": "rdblue", "createdAt": "2020-08-10T17:01:34Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODE4MTIzOQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468181239", "bodyText": "done, rightAfterCheckpoint()", "author": "RussellSpitzer", "createdAt": "2020-08-10T20:56:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA0OTAzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA1MTg4NQ==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468051885", "bodyText": "A couple of things on these tests:\n\nThe ported tests don't validate the action result in many cases. Can you add those checks?\nThe new tests use Spark to write data, which I don't think is necessary. Using Spark to write data makes the tests harder to maintain because you have to go list files to find the ones you want, rather than constructing commits directly with known file paths (e.g., FILE_A, FILE_B, etc.).\nI find tests that use ThreeColumnRecord and values like AAAA to be a bit hard to read, since there isn't much difference between AAAA and AAAAAA or between c2 and c3. If you remove Spark writes, then you could avoid needing this.", "author": "rdblue", "createdAt": "2020-08-10T17:07:05Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIyMTU2Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468221566", "bodyText": "All fixed up", "author": "RussellSpitzer", "createdAt": "2020-08-10T22:30:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA1MTg4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA1MjEyNg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468052126", "bodyText": "The table instance is used for the expire commit, right? If that's the case, then there should be no need to refresh here.", "author": "rdblue", "createdAt": "2020-08-10T17:07:36Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,780 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public abstract class TestExpireSnapshotsAction extends SparkTestBase {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+\n+  private static final List<ThreeColumnRecord> RECORDS = Lists.newArrayList(new ThreeColumnRecord(1, \"AAAA\", \"AAAA\"));\n+\n+  static final DataFile FILE_A = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-a.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=0\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_B = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-b.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=1\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_C = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-c.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=2\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+  static final DataFile FILE_D = DataFiles.builder(SPEC)\n+      .withPath(\"/path/to/data-d.parquet\")\n+      .withFileSizeInBytes(10)\n+      .withPartitionPath(\"c1=3\") // easy way to set partition data for now\n+      .withRecordCount(1)\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private File tableDir;\n+  private String tableLocation;\n+  private Table table;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    this.tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  }\n+\n+  private Dataset<Row> buildDF(List<ThreeColumnRecord> records) {\n+    return spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+  }\n+\n+  private void writeDF(Dataset<Row> df, String mode) {\n+    df.select(\"c1\", \"c2\", \"c3\")\n+        .write()\n+        .format(\"iceberg\")\n+        .mode(mode)\n+        .save(tableLocation);\n+  }\n+\n+  private void checkExpirationResults(Long expectedDatafiles, Long expectedManifestsDeleted,\n+      Long expectedManifestListsDeleted, ExpireSnapshotsActionResult results) {\n+\n+    Assert.assertEquals(\"Incorrect number of manifest files deleted\",\n+        expectedManifestsDeleted, results.getManifestFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of datafiles deleted\",\n+        expectedDatafiles, results.getDataFilesDeleted());\n+    Assert.assertEquals(\"Incorrect number of manifest lists deleted\",\n+        expectedManifestListsDeleted, results.getManifestListsDeleted());\n+  }\n+\n+  @Test\n+  public void testFilesCleaned() throws Exception {\n+    Dataset<Row> df = buildDF(RECORDS);\n+\n+    writeDF(df, \"append\");\n+\n+    List<Path> expiredDataFiles = Files\n+        .list(tableDir.toPath().resolve(\"data\").resolve(\"c1=1\"))\n+        .collect(Collectors.toList());\n+\n+    Assert.assertEquals(\"There should be a data file to delete but there was none.\",\n+        2, expiredDataFiles.size());\n+\n+    writeDF(df, \"overwrite\");\n+    writeDF(df, \"append\");\n+\n+    long end = System.currentTimeMillis();\n+    while (end <= table.currentSnapshot().timestampMillis()) {\n+      end = System.currentTimeMillis();\n+    }\n+\n+    ExpireSnapshotsActionResult results =\n+        Actions.forTable(table).expireSnapshots().expireOlderThan(end).execute();\n+\n+    table.refresh();", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA2Njg2Ng==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468066866", "bodyText": "One more question: it seems passing any executor service will trigger runParallel in Tasks.\nIn turn, that will call  for (final I item : items) that I assume would load all files on the driver?", "author": "aokolnychyi", "createdAt": "2020-08-10T17:34:19Z", "path": "spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.Iterator;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Consumer;\n+import org.apache.iceberg.ExpireSnapshots;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.functions;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action which performs the same operation as {@link org.apache.iceberg.ExpireSnapshots} but uses Spark\n+ * to to determine the delta in files between the pre and post-expiration table metadata. All of the same\n+ * restrictions of Remove Snapshots also apply to this action.\n+ * <p>\n+ * This implementation uses the metadata tables for the table being expired to list all Manifest and DataFiles. This\n+ * is made into a Dataframe which are anti-joined with the same list read after the expiration. This operation will\n+ * require a shuffle so parallelism can be controlled through spark.sql.shuffle.partitions. The expiration is done\n+ * locally using a direct call to RemoveSnapshots. The snapshot expiration will be fully committed before any deletes\n+ * are issued. Deletes are still performed locally after retrieving the results from the Spark executors.\n+ */\n+public class ExpireSnapshotsAction extends BaseAction<ExpireSnapshotsActionResult> {\n+  private static final Logger LOG = LoggerFactory.getLogger(ExpireSnapshotsAction.class);\n+\n+  private static final String DATA_FILE = \"Data File\";\n+  private static final String MANIFEST = \"Manifest\";\n+  private static final String MANIFEST_LIST = \"Manifest List\";\n+\n+  // Creates an executor service that runs each task in the thread that invokes execute/submit.\n+  private static final ExecutorService DEFAULT_DELETE_EXECUTOR_SERVICE = MoreExecutors.newDirectExecutorService();", "originalCommit": "a50e3e8afe7dd209e0d17404654c7c00a547ceb1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODIzMzk2OA==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468233968", "bodyText": "We decided to shift this to \"null\" so that we will use the single threaded execution path by default and we can figure out a non-eager iterator approach later", "author": "RussellSpitzer", "createdAt": "2020-08-10T23:07:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODA2Njg2Ng=="}], "type": "inlineReview"}, {"oid": "1732bc27804666d2a7dc7907331e2d5000a66a98", "url": "https://github.com/apache/iceberg/commit/1732bc27804666d2a7dc7907331e2d5000a66a98", "message": "Reviewer Comments\n\nRefactoring of Tests, All tests use only table.operations no Spark Writes\nAll tests now check file deletions\nRenaming of class methods to fit style\nRemoval of TableUtils, Functions moved back into BaseAction\nExpireSnapshotsAction defaults to single threaded deleter", "committedDate": "2020-08-10T23:03:59Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI2MTQ5Nw==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468261497", "bodyText": "I think this should be private. We don't want actions using it directly, do we?", "author": "rdblue", "createdAt": "2020-08-11T00:40:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +50,66 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  /**\n+   * Returns all the path locations of all Manifest Lists for a given table\n+   * @param table the table\n+   * @return the paths of the Manifest Lists\n+   */\n+  protected List<String> getManifestListPaths(Table table) {", "originalCommit": "1732bc27804666d2a7dc7907331e2d5000a66a98", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI2MTcyNg==", "url": "https://github.com/apache/iceberg/pull/1264#discussion_r468261726", "bodyText": "Same here. Since we aren't making these common utility methods, we should keep them private so we aren't committing to support them later on.", "author": "rdblue", "createdAt": "2020-08-11T00:40:42Z", "path": "spark/src/main/java/org/apache/iceberg/actions/BaseAction.java", "diffHunk": "@@ -41,4 +50,66 @@ protected String metadataTableName(MetadataTableType type) {\n       return tableName + \".\" + type;\n     }\n   }\n+\n+  /**\n+   * Returns all the path locations of all Manifest Lists for a given table\n+   * @param table the table\n+   * @return the paths of the Manifest Lists\n+   */\n+  protected List<String> getManifestListPaths(Table table) {\n+    List<String> manifestLists = Lists.newArrayList();\n+    for (Snapshot snapshot : table.snapshots()) {\n+      String manifestListLocation = snapshot.manifestListLocation();\n+      if (manifestListLocation != null) {\n+        manifestLists.add(manifestListLocation);\n+      }\n+    }\n+    return manifestLists;\n+  }\n+\n+  /**\n+   * Returns all Metadata file paths which may not be in the current metadata. Specifically\n+   * this includes \"version-hint\" files as well as entries in metadata.previousFiles.\n+   * @param ops TableOperations for the table we will be getting paths from\n+   * @return a list of paths to metadata files\n+   */\n+  protected List<String> getOtherMetadataFilePaths(TableOperations ops) {", "originalCommit": "1732bc27804666d2a7dc7907331e2d5000a66a98", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}