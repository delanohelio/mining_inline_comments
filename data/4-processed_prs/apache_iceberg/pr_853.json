{"pr_number": 853, "pr_title": "Task data reader changes for vectorized reads", "pr_createdAt": "2020-03-18T21:54:31Z", "pr_url": "https://github.com/apache/iceberg/pull/853", "timeline": [{"oid": "b4cc4c598619a6554407b61c3d3076b067cead90", "url": "https://github.com/apache/iceberg/commit/b4cc4c598619a6554407b61c3d3076b067cead90", "message": "Refactor TaskDataReader", "committedDate": "2020-03-19T19:16:30Z", "type": "commit"}, {"oid": "b4cc4c598619a6554407b61c3d3076b067cead90", "url": "https://github.com/apache/iceberg/commit/b4cc4c598619a6554407b61c3d3076b067cead90", "message": "Refactor TaskDataReader", "committedDate": "2020-03-19T19:16:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MDk5Mg==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395740992", "bodyText": "This will only be used by the row reader, right?", "author": "rdblue", "createdAt": "2020-03-20T16:10:11Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MTc0OQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395741749", "bodyText": "This should implement InputPartitionReader<T> and document what T is in javadoc.", "author": "rdblue", "createdAt": "2020-03-20T16:11:23Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MjQ0MA==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395742440", "bodyText": "Also, let's get rid of Task in these class names. It is unnecessary.", "author": "rdblue", "createdAt": "2020-03-20T16:12:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MTc0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MTk4Mg==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395741982", "bodyText": "Needs @Override.", "author": "rdblue", "createdAt": "2020-03-20T16:11:45Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n+      .impl(UnsafeProjection.class, InternalRow.class)\n+      .build();\n+\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;\n+  final int batchSize;\n+\n+  BaseTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive) {\n+    this(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive, -1);\n+  }\n+\n+  BaseTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive, int bSize) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.tableSchema = tableSchema;\n+    this.expectedSchema = expectedSchema;\n+    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(Iterables.transform(\n+        task.files(),\n+        fileScanTask ->\n+            EncryptedFiles.encryptedInput(\n+                this.fileIo.newInputFile(fileScanTask.file().path().toString()),\n+                fileScanTask.file().keyMetadata())));\n+    ImmutableMap.Builder<String, InputFile> inputFileBuilder = ImmutableMap.builder();\n+    decryptedFiles.forEach(decrypted -> inputFileBuilder.put(decrypted.location(), decrypted));\n+    this.inputFiles = inputFileBuilder.build();\n+    this.caseSensitive = caseSensitive;\n+    this.batchSize = bSize;\n+    // open last because the schemas, fileIo and batchSize must be set\n+    this.currentIterator = open(tasks.next());\n+  }\n+\n+  public boolean next() throws IOException {", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0Mjg5OQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395742899", "bodyText": "Why isn't T get() { return current; } implemented in this base class?", "author": "rdblue", "createdAt": "2020-03-20T16:13:14Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseTaskDataReader.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.common.DynMethods;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseTaskDataReader<T> implements Closeable {\n+  // for some reason, the apply method can't be called from Java without reflection\n+  static final DynMethods.UnboundMethod APPLY_PROJECTION = DynMethods.builder(\"apply\")\n+      .impl(UnsafeProjection.class, InternalRow.class)\n+      .build();\n+\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0MzY0NQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395743645", "bodyText": "How about RowDataReader here instead?", "author": "rdblue", "createdAt": "2020-03-20T16:14:30Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class RowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc0NDQyOQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395744429", "bodyText": "Can we move this into a separate file while we are moving classes?", "author": "rdblue", "createdAt": "2020-03-20T16:15:50Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowTaskDataReader.java", "diffHunk": "@@ -0,0 +1,295 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.data.SparkAvroReader;\n+import org.apache.iceberg.spark.data.SparkOrcReader;\n+import org.apache.iceberg.spark.data.SparkParquetReaders;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.Attribute;\n+import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.expressions.JoinedRow;\n+import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+import org.apache.spark.sql.types.BinaryType;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.Decimal;\n+import org.apache.spark.sql.types.DecimalType;\n+import org.apache.spark.sql.types.StringType;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import scala.collection.JavaConverters;\n+\n+class RowTaskDataReader extends BaseTaskDataReader<InternalRow> implements InputPartitionReader<InternalRow> {\n+\n+  RowTaskDataReader(\n+      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n+      EncryptionManager encryptionManager, boolean caseSensitive) {\n+    super(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive);\n+  }\n+\n+  @Override\n+  public InternalRow get() {\n+    return current;\n+  }\n+\n+  @Override\n+  Iterator<InternalRow> open(FileScanTask task) {\n+    DataFile file = task.file();\n+\n+    // update the current file for Spark's filename() function\n+    InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n+\n+    // schema or rows returned by readers\n+    Schema finalSchema = expectedSchema;\n+    PartitionSpec spec = task.spec();\n+    Set<Integer> idColumns = spec.identitySourceIds();\n+\n+    // schema needed for the projection and filtering\n+    StructType sparkType = SparkSchemaUtil.convert(finalSchema);\n+    Schema requiredSchema = SparkSchemaUtil.prune(tableSchema, sparkType, task.residual(), caseSensitive);\n+    boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+    boolean hasExtraFilterColumns = requiredSchema.columns().size() != finalSchema.columns().size();\n+\n+    Schema iterSchema;\n+    Iterator<InternalRow> iter;\n+\n+    if (hasJoinedPartitionColumns) {\n+      // schema used to read data files\n+      Schema readSchema = TypeUtil.selectNot(requiredSchema, idColumns);\n+      Schema partitionSchema = TypeUtil.select(requiredSchema, idColumns);\n+      PartitionRowConverter convertToRow = new PartitionRowConverter(partitionSchema, spec);\n+      JoinedRow joined = new JoinedRow();\n+\n+      InternalRow partition = convertToRow.apply(file.partition());\n+      joined.withRight(partition);\n+\n+      // create joined rows and project from the joined schema to the final schema\n+      iterSchema = TypeUtil.join(readSchema, partitionSchema);\n+      iter = Iterators.transform(open(task, readSchema), joined::withLeft);\n+    } else if (hasExtraFilterColumns) {\n+      // add projection to the final schema\n+      iterSchema = requiredSchema;\n+      iter = open(task, requiredSchema);\n+    } else {\n+      // return the base iterator\n+      iterSchema = finalSchema;\n+      iter = open(task, finalSchema);\n+    }\n+\n+    // TODO: remove the projection by reporting the iterator's schema back to Spark\n+    return Iterators.transform(\n+        iter,\n+        APPLY_PROJECTION.bind(projection(finalSchema, iterSchema))::invoke);\n+  }\n+\n+  private Iterator<InternalRow> open(FileScanTask task, Schema readSchema) {\n+    CloseableIterable<InternalRow> iter;\n+    if (task.isDataTask()) {\n+      iter = newDataIterable(task.asDataTask(), readSchema);\n+    } else {\n+      InputFile location = inputFiles.get(task.file().path().toString());\n+      Preconditions.checkNotNull(location, \"Could not find InputFile associated with FileScanTask\");\n+\n+      switch (task.file().format()) {\n+        case PARQUET:\n+          iter = newParquetIterable(location, task, readSchema);\n+          break;\n+\n+        case AVRO:\n+          iter = newAvroIterable(location, task, readSchema);\n+          break;\n+\n+        case ORC:\n+          iter = newOrcIterable(location, task, readSchema);\n+          break;\n+\n+        default:\n+          throw new UnsupportedOperationException(\n+              \"Cannot read unknown format: \" + task.file().format());\n+      }\n+    }\n+\n+    this.currentCloseable = iter;\n+\n+    return iter.iterator();\n+  }\n+\n+  private CloseableIterable<InternalRow> newAvroIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return Avro.read(location)\n+        .reuseContainers()\n+        .project(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(SparkAvroReader::new)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newParquetIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return Parquet.read(location)\n+        .project(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(fileSchema -> SparkParquetReaders.buildReader(readSchema, fileSchema))\n+        .filter(task.residual())\n+        .caseSensitive(caseSensitive)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newOrcIterable(\n+      InputFile location,\n+      FileScanTask task,\n+      Schema readSchema) {\n+    return ORC.read(location)\n+        .schema(readSchema)\n+        .split(task.start(), task.length())\n+        .createReaderFunc(SparkOrcReader::new)\n+        .caseSensitive(caseSensitive)\n+        .build();\n+  }\n+\n+  private CloseableIterable<InternalRow> newDataIterable(DataTask task, Schema readSchema) {\n+    StructInternalRow row = new StructInternalRow(tableSchema.asStruct());\n+    CloseableIterable<InternalRow> asSparkRows = CloseableIterable.transform(\n+        task.asDataTask().rows(), row::setStruct);\n+    return CloseableIterable.transform(\n+        asSparkRows, APPLY_PROJECTION.bind(projection(readSchema, tableSchema))::invoke);\n+  }\n+\n+  private static UnsafeProjection projection(Schema finalSchema, Schema readSchema) {\n+    StructType struct = SparkSchemaUtil.convert(readSchema);\n+\n+    List<AttributeReference> refs = JavaConverters.seqAsJavaListConverter(struct.toAttributes()).asJava();\n+    List<Attribute> attrs = Lists.newArrayListWithExpectedSize(struct.fields().length);\n+    List<org.apache.spark.sql.catalyst.expressions.Expression> exprs =\n+        Lists.newArrayListWithExpectedSize(struct.fields().length);\n+\n+    for (AttributeReference ref : refs) {\n+      attrs.add(ref.toAttribute());\n+    }\n+\n+    for (Types.NestedField field : finalSchema.columns()) {\n+      int indexInReadSchema = struct.fieldIndex(field.name());\n+      exprs.add(refs.get(indexInReadSchema));\n+    }\n+\n+    return UnsafeProjection.create(\n+        JavaConverters.asScalaBufferConverter(exprs).asScala().toSeq(),\n+        JavaConverters.asScalaBufferConverter(attrs).asScala().toSeq());\n+  }\n+\n+  private static class PartitionRowConverter implements Function<StructLike, InternalRow> {", "originalCommit": "b4cc4c598619a6554407b61c3d3076b067cead90", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "url": "https://github.com/apache/iceberg/commit/dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "message": "Address code review comments", "committedDate": "2020-03-20T18:44:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzMzAyOQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395833029", "bodyText": "T is the Java class returned by this reader that contains one or more rows.", "author": "rdblue", "createdAt": "2020-03-20T18:55:40Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -28,22 +28,18 @@\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Schema;\n-import org.apache.iceberg.common.DynMethods;\n import org.apache.iceberg.encryption.EncryptedFiles;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.spark.rdd.InputFileBlockHolder;\n-import org.apache.spark.sql.catalyst.InternalRow;\n-import org.apache.spark.sql.catalyst.expressions.UnsafeProjection;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n \n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;", "originalCommit": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395836481", "bodyText": "We need to clean up how these instance fields are shared. Some, like tableSchema, expectedSchema, caseSensitive don't need to be held by this class and should be removed. I think we need a protected method to get an InputFile from inputFiles. And I think that tasks isn't shared and can be private.", "author": "rdblue", "createdAt": "2020-03-20T19:02:32Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+\n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;\n+  final int batchSize;", "originalCommit": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjcxMg==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395836712", "bodyText": "I think batchSize can also be removed.", "author": "rdblue", "createdAt": "2020-03-20T19:02:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg5MDIwNQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395890205", "bodyText": "batchSize is meant really for BatchTaskDataReader case which will get added shortly.", "author": "samarthjain", "createdAt": "2020-03-20T21:07:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTg5MTQ1MQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395891451", "bodyText": "Similarly, the other fields are needed for the BatchTaskDataReader. So makes sense to leave them in the base class.", "author": "samarthjain", "createdAt": "2020-03-20T21:11:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTkwNzQ5MQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395907491", "bodyText": "Isn't batchSize specific to the batch reader?\nFor the others, I think the question isn't whether these are used by both classes, but whether it makes sense to have them in the base class. Maybe it does in some cases, but I think if it is just an instance field, there's no need to put it in the base class when it could be a private field in both child classes -- because the cost of maintaining two child fields is small.", "author": "rdblue", "createdAt": "2020-03-20T21:56:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjQ4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTgzNjgwOQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395836809", "bodyText": "These can be private right?", "author": "rdblue", "createdAt": "2020-03-20T19:03:10Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.Iterables;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.Map;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.spark.rdd.InputFileBlockHolder;\n+import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n+\n+/**\n+ * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ */\n+@SuppressWarnings(\"checkstyle:VisibilityModifier\")\n+abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n+  final Iterator<FileScanTask> tasks;\n+  final Schema tableSchema;\n+  final Schema expectedSchema;\n+  final FileIO fileIo;\n+  final Map<String, InputFile> inputFiles;\n+  final boolean caseSensitive;\n+\n+  Iterator<T> currentIterator;\n+  Closeable currentCloseable = null;\n+  T current = null;", "originalCommit": "dfaf7114160cc554c7f830f9d7ae2183508c0eb3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b6c18780cc3ba6c1e9d8723aed7efabef9a915c1", "url": "https://github.com/apache/iceberg/commit/b6c18780cc3ba6c1e9d8723aed7efabef9a915c1", "message": "Some more cleanup", "committedDate": "2020-03-20T22:52:40Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTk1MTY1MQ==", "url": "https://github.com/apache/iceberg/pull/853#discussion_r395951651", "bodyText": "Can this be private?", "author": "rdblue", "createdAt": "2020-03-21T02:09:59Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BaseDataReader.java", "diffHunk": "@@ -23,48 +23,38 @@\n import com.google.common.collect.Iterables;\n import java.io.Closeable;\n import java.io.IOException;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.Map;\n+import org.apache.arrow.util.Preconditions;\n import org.apache.iceberg.CombinedScanTask;\n import org.apache.iceberg.FileScanTask;\n-import org.apache.iceberg.Schema;\n import org.apache.iceberg.encryption.EncryptedFiles;\n import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.InputFile;\n import org.apache.spark.rdd.InputFileBlockHolder;\n import org.apache.spark.sql.sources.v2.reader.InputPartitionReader;\n \n /**\n- * @param <T> Base class of readers to read data as objects of type @param &lt;T&gt;\n+ * Base class of readers of type {@link InputPartitionReader} to read data as objects of type @param &lt;T&gt;\n+ *\n+ * @param <T> is the Java class returned by this reader whose objects contain one or more rows.\n  */\n @SuppressWarnings(\"checkstyle:VisibilityModifier\")\n abstract class BaseDataReader<T> implements InputPartitionReader<T> {\n-  final Iterator<FileScanTask> tasks;\n-  final Schema tableSchema;\n-  final Schema expectedSchema;\n-  final FileIO fileIo;\n-  final Map<String, InputFile> inputFiles;\n-  final boolean caseSensitive;\n-\n-  Iterator<T> currentIterator;\n-  Closeable currentCloseable = null;\n-  T current = null;\n-  final int batchSize;\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final Map<String, InputFile> inputFiles;\n \n-  BaseDataReader(\n-      CombinedScanTask task, Schema tableSchema, Schema expectedSchema, FileIO fileIo,\n-      EncryptionManager encryptionManager, boolean caseSensitive) {\n-    this(task, tableSchema, expectedSchema, fileIo, encryptionManager, caseSensitive, -1);\n-  }\n+  private Iterator<T> currentIterator;\n+  Closeable currentCloseable;", "originalCommit": "b6c18780cc3ba6c1e9d8723aed7efabef9a915c1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}