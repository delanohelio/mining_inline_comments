{"pr_number": 1197, "pr_title": "Refactor the GenericOrcWriter by using OrcSchemaWithTypeVisitor#visit", "pr_createdAt": "2020-07-13T13:09:37Z", "pr_url": "https://github.com/apache/iceberg/pull/1197", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MDEwNg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453860106", "bodyText": "Now that we had the Iceberg primitive type available, we can use the Iceberg type to decide whether this is long or time. That is similar to what we do in GenericOrcReader.", "author": "shardulm94", "createdAt": "2020-07-13T18:50:45Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();\n+        case DATE:\n+          return GenericOrcWriters.dates();\n+        case INT:\n+          return GenericOrcWriters.ints();\n+        case LONG:\n+          String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyMzQ3Ng==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454023476", "bodyText": "+1", "author": "rdsr", "createdAt": "2020-07-14T00:23:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MDEwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTA0Mw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453861043", "bodyText": "Iceberg would not produce a TypeDescription with BYTE or SHORT types, so we should just throw an unsupported exception for these cases.", "author": "shardulm94", "createdAt": "2020-07-13T18:52:23Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2MTkxOQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453861919", "bodyText": "Now that we had the Iceberg primitive type available, we can use the Iceberg type to decide whether this is UUID, FIXED or BINARY. That is similar to what we do in GenericOrcReader.", "author": "shardulm94", "createdAt": "2020-07-13T18:53:59Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();\n+        case DATE:\n+          return GenericOrcWriters.dates();\n+        case INT:\n+          return GenericOrcWriters.ints();\n+        case LONG:\n+          String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n+          switch (longType) {\n+            case TIME:\n+              return GenericOrcWriters.times();\n+            case LONG:\n+              return GenericOrcWriters.longs();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n+          }\n+        case FLOAT:\n+          return GenericOrcWriters.floats();\n+        case DOUBLE:\n+          return GenericOrcWriters.doubles();\n+        case BINARY:\n+          String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n+              ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n+          switch (binaryType) {\n+            case UUID:\n+              return GenericOrcWriters.uuids();\n+            case FIXED:\n+              return GenericOrcWriters.fixed();\n+            case BINARY:\n+              return GenericOrcWriters.binary();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Binary type found in ORC type attribute: \" + binaryType);\n+          }", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NzA5Mg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453867092", "bodyText": "Since we are rewriting this, should we consider putting the if (precision < 18) then THIS else THAT logic in GenericOrcWriters.decimals() and not exposing Decimal18 and Decimal38 separately? This similar to what we did recently for readers in this PR \n  \n    \n      iceberg/spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcValueReaders.java\n    \n    \n         Line 57\n      in\n      7d0cf1c\n    \n    \n    \n    \n\n        \n          \n           public static OrcValueReader<Decimal> decimals(int precision, int scale) {", "author": "shardulm94", "createdAt": "2020-07-13T19:03:04Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();\n+        case DATE:\n+          return GenericOrcWriters.dates();\n+        case INT:\n+          return GenericOrcWriters.ints();\n+        case LONG:\n+          String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n+          switch (longType) {\n+            case TIME:\n+              return GenericOrcWriters.times();\n+            case LONG:\n+              return GenericOrcWriters.longs();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n+          }\n+        case FLOAT:\n+          return GenericOrcWriters.floats();\n+        case DOUBLE:\n+          return GenericOrcWriters.doubles();\n+        case BINARY:\n+          String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n+              ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n+          switch (binaryType) {\n+            case UUID:\n+              return GenericOrcWriters.uuids();\n+            case FIXED:\n+              return GenericOrcWriters.fixed();\n+            case BINARY:\n+              return GenericOrcWriters.binary();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Binary type found in ORC type attribute: \" + binaryType);\n+          }\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return GenericOrcWriters.strings();\n+        case DECIMAL:\n+          return schema.getPrecision() <= 18 ? GenericOrcWriters.decimal18(schema) :\n+              GenericOrcWriters.decimal38(schema);", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1NDA2Mg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454054062", "bodyText": "Well, let me take a look how to handle those issues you said, thanks for your suggession, @shardulm94 .", "author": "openinx", "createdAt": "2020-07-14T02:06:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg2NzA5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3MTE4MQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453871181", "bodyText": "Nit: Can we just pass in the (precision, scale) to the writer instead of the whole schema?", "author": "shardulm94", "createdAt": "2020-07-13T19:10:29Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData\n+     * @param output the ColumnVector to put the value into\n+     */\n+    void addValue(int rowId, T data, ColumnVector output);\n+  }\n+\n+  public static Converter<Boolean> booleans() {\n+    return BooleanConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Byte> bytes() {\n+    return ByteConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Short> shorts() {\n+    return ShortConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Integer> ints() {\n+    return IntConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalTime> times() {\n+    return TimeConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Long> longs() {\n+    return LongConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Float> floats() {\n+    return FloatConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Double> doubles() {\n+    return DoubleConverter.INSTANCE;\n+  }\n+\n+  public static Converter<String> strings() {\n+    return StringConverter.INSTANCE;\n+  }\n+\n+  public static Converter<ByteBuffer> binary() {\n+    return BytesConverter.INSTANCE;\n+  }\n+\n+  public static Converter<UUID> uuids() {\n+    return UUIDConverter.INSTANCE;\n+  }\n+\n+  public static Converter<byte[]> fixed() {\n+    return FixedConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDate> dates() {\n+    return DateConverter.INSTANCE;\n+  }\n+\n+  public static Converter<OffsetDateTime> timestampTz() {\n+    return TimestampTzConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDateTime> timestamp() {\n+    return TimestampConverter.INSTANCE;\n+  }\n+\n+  public static Converter<BigDecimal> decimal18(TypeDescription schema) {\n+    return new Decimal18Converter(schema);", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3MjYyNQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453872625", "bodyText": "This code is repeated for every converter. Consider doing something similar to the read side, where we put this logic in this Converter interface itself so the the implementations only need to think about non-null cases.\n\n  \n    \n      iceberg/orc/src/main/java/org/apache/iceberg/orc/OrcValueReader.java\n    \n    \n        Lines 25 to 36\n      in\n      809697a\n    \n    \n    \n    \n\n        \n          \n           public interface OrcValueReader<T> { \n        \n\n        \n          \n             default T read(ColumnVector vector, int row) { \n        \n\n        \n          \n               int rowIndex = vector.isRepeating ? 0 : row; \n        \n\n        \n          \n               if (!vector.noNulls && vector.isNull[rowIndex]) { \n        \n\n        \n          \n                 return null; \n        \n\n        \n          \n               } else { \n        \n\n        \n          \n                 return nonNullRead(vector, rowIndex); \n        \n\n        \n          \n               } \n        \n\n        \n          \n             } \n        \n\n        \n          \n            \n        \n\n        \n          \n             T nonNullRead(ColumnVector vector, int row); \n        \n\n        \n          \n           }", "author": "shardulm94", "createdAt": "2020-07-13T19:13:13Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData\n+     * @param output the ColumnVector to put the value into\n+     */\n+    void addValue(int rowId, T data, ColumnVector output);\n+  }\n+\n+  public static Converter<Boolean> booleans() {\n+    return BooleanConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Byte> bytes() {\n+    return ByteConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Short> shorts() {\n+    return ShortConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Integer> ints() {\n+    return IntConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalTime> times() {\n+    return TimeConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Long> longs() {\n+    return LongConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Float> floats() {\n+    return FloatConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Double> doubles() {\n+    return DoubleConverter.INSTANCE;\n+  }\n+\n+  public static Converter<String> strings() {\n+    return StringConverter.INSTANCE;\n+  }\n+\n+  public static Converter<ByteBuffer> binary() {\n+    return BytesConverter.INSTANCE;\n+  }\n+\n+  public static Converter<UUID> uuids() {\n+    return UUIDConverter.INSTANCE;\n+  }\n+\n+  public static Converter<byte[]> fixed() {\n+    return FixedConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDate> dates() {\n+    return DateConverter.INSTANCE;\n+  }\n+\n+  public static Converter<OffsetDateTime> timestampTz() {\n+    return TimestampTzConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDateTime> timestamp() {\n+    return TimestampConverter.INSTANCE;\n+  }\n+\n+  public static Converter<BigDecimal> decimal18(TypeDescription schema) {\n+    return new Decimal18Converter(schema);\n+  }\n+\n+  public static Converter<BigDecimal> decimal38(TypeDescription schema) {\n+    return Decimal38Converter.INSTANCE;\n+  }\n+\n+  private static class BooleanConverter implements Converter<Boolean> {\n+    private static final Converter<Boolean> INSTANCE = new BooleanConverter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Boolean data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3NzE0Nw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453877147", "bodyText": "Nit: rename to elementConverter? Similar to what we do for MapConverter", "author": "shardulm94", "createdAt": "2020-07-13T19:21:30Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData\n+     * @param output the ColumnVector to put the value into\n+     */\n+    void addValue(int rowId, T data, ColumnVector output);\n+  }\n+\n+  public static Converter<Boolean> booleans() {\n+    return BooleanConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Byte> bytes() {\n+    return ByteConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Short> shorts() {\n+    return ShortConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Integer> ints() {\n+    return IntConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalTime> times() {\n+    return TimeConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Long> longs() {\n+    return LongConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Float> floats() {\n+    return FloatConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Double> doubles() {\n+    return DoubleConverter.INSTANCE;\n+  }\n+\n+  public static Converter<String> strings() {\n+    return StringConverter.INSTANCE;\n+  }\n+\n+  public static Converter<ByteBuffer> binary() {\n+    return BytesConverter.INSTANCE;\n+  }\n+\n+  public static Converter<UUID> uuids() {\n+    return UUIDConverter.INSTANCE;\n+  }\n+\n+  public static Converter<byte[]> fixed() {\n+    return FixedConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDate> dates() {\n+    return DateConverter.INSTANCE;\n+  }\n+\n+  public static Converter<OffsetDateTime> timestampTz() {\n+    return TimestampTzConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDateTime> timestamp() {\n+    return TimestampConverter.INSTANCE;\n+  }\n+\n+  public static Converter<BigDecimal> decimal18(TypeDescription schema) {\n+    return new Decimal18Converter(schema);\n+  }\n+\n+  public static Converter<BigDecimal> decimal38(TypeDescription schema) {\n+    return Decimal38Converter.INSTANCE;\n+  }\n+\n+  private static class BooleanConverter implements Converter<Boolean> {\n+    private static final Converter<Boolean> INSTANCE = new BooleanConverter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Boolean data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n+      }\n+    }\n+  }\n+\n+  private static class ByteConverter implements Converter<Byte> {\n+    private static final Converter<Byte> INSTANCE = new ByteConverter();\n+\n+    @Override\n+    public Class<Byte> getJavaClass() {\n+      return Byte.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Byte data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class ShortConverter implements Converter<Short> {\n+    private static final Converter<Short> INSTANCE = new ShortConverter();\n+\n+    @Override\n+    public Class<Short> getJavaClass() {\n+      return Short.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Short data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class IntConverter implements Converter<Integer> {\n+    private static final Converter<Integer> INSTANCE = new IntConverter();\n+\n+    @Override\n+    public Class<Integer> getJavaClass() {\n+      return Integer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Integer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class TimeConverter implements Converter<LocalTime> {\n+    private static final Converter<LocalTime> INSTANCE = new TimeConverter();\n+\n+    @Override\n+    public Class<LocalTime> getJavaClass() {\n+      return LocalTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n+      }\n+    }\n+  }\n+\n+  private static class LongConverter implements Converter<Long> {\n+    private static final Converter<Long> INSTANCE = new LongConverter();\n+\n+    @Override\n+    public Class<Long> getJavaClass() {\n+      return Long.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Long data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class FloatConverter implements Converter<Float> {\n+    private static final Converter<Float> INSTANCE = new FloatConverter();\n+\n+    @Override\n+    public Class<Float> getJavaClass() {\n+      return Float.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Float data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class DoubleConverter implements Converter<Double> {\n+    private static final Converter<Double> INSTANCE = new DoubleConverter();\n+\n+    @Override\n+    public Class<Double> getJavaClass() {\n+      return Double.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Double data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class StringConverter implements Converter<String> {\n+    private static final Converter<String> INSTANCE = new StringConverter();\n+\n+    @Override\n+    public Class<String> getJavaClass() {\n+      return String.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, String data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        byte[] value = data.getBytes(StandardCharsets.UTF_8);\n+        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n+      }\n+    }\n+  }\n+\n+  private static class BytesConverter implements Converter<ByteBuffer> {\n+    private static final Converter<ByteBuffer> INSTANCE = new BytesConverter();\n+\n+    @Override\n+    public Class<ByteBuffer> getJavaClass() {\n+      return ByteBuffer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, ByteBuffer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class UUIDConverter implements Converter<UUID> {\n+    private static final Converter<UUID> INSTANCE = new UUIDConverter();\n+\n+    @Override\n+    public Class<UUID> getJavaClass() {\n+      return UUID.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, UUID data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ByteBuffer buffer = ByteBuffer.allocate(16);\n+        buffer.putLong(data.getMostSignificantBits());\n+        buffer.putLong(data.getLeastSignificantBits());\n+        ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class FixedConverter implements Converter<byte[]> {\n+    private static final Converter<byte[]> INSTANCE = new FixedConverter();\n+\n+    @Override\n+    public Class<byte[]> getJavaClass() {\n+      return byte[].class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, byte[] data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n+      }\n+    }\n+  }\n+\n+  private static class DateConverter implements Converter<LocalDate> {\n+    private static final Converter<LocalDate> INSTANCE = new DateConverter();\n+\n+    @Override\n+    public Class<LocalDate> getJavaClass() {\n+      return LocalDate.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDate data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n+      }\n+    }\n+  }\n+\n+  private static class TimestampTzConverter implements Converter<OffsetDateTime> {\n+    private static final Converter<OffsetDateTime> INSTANCE = new TimestampTzConverter();\n+\n+    @Override\n+    public Class<OffsetDateTime> getJavaClass() {\n+      return OffsetDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, OffsetDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class TimestampConverter implements Converter<LocalDateTime> {\n+    private static final Converter<LocalDateTime> INSTANCE = new TimestampConverter();\n+\n+    @Override\n+    public Class<LocalDateTime> getJavaClass() {\n+      return LocalDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.setIsUTC(true);\n+        cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class Decimal18Converter implements Converter<BigDecimal> {\n+    private final int scale;\n+\n+    Decimal18Converter(TypeDescription schema) {\n+      this.scale = schema.getScale();\n+    }\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId]\n+            .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n+      }\n+    }\n+  }\n+\n+  private static class Decimal38Converter implements Converter<BigDecimal> {\n+    private static final Converter<BigDecimal> INSTANCE = new Decimal38Converter();\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n+      }\n+    }\n+  }\n+\n+  public static class RecordConverter implements Converter<Record> {\n+    private final List<Converter> converters;\n+\n+    RecordConverter(List<Converter> converters) {\n+      this.converters = converters;\n+    }\n+\n+    public List<Converter> converters() {\n+      return converters;\n+    }\n+\n+    @Override\n+    public Class<Record> getJavaClass() {\n+      return Record.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, Record data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        StructColumnVector cv = (StructColumnVector) output;\n+        for (int c = 0; c < converters.size(); ++c) {\n+          converters.get(c).addValue(rowId, data.get(c, converters.get(c).getJavaClass()), cv.fields[c]);\n+        }\n+      }\n+    }\n+  }\n+\n+  public static class ListConverter implements Converter<List> {\n+    private final Converter children;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3Nzc5OQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453877799", "bodyText": "Remove references to Spark and SpecializedGetters.", "author": "shardulm94", "createdAt": "2020-07-13T19:22:41Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3Nzg3OA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453877878", "bodyText": "Remove references to Spark.", "author": "shardulm94", "createdAt": "2020-07-13T19:22:49Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzg3ODI3MA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r453878270", "bodyText": "Remove references to Spark's InternalRow and ArrayData classes", "author": "shardulm94", "createdAt": "2020-07-13T19:23:32Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxNzc0NQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454017745", "bodyText": "We should call this  OrcValueWriter and rename existing OrcValueWriter to OrcRowWriter to preserve symmetry between  OrcValueReader and OrcRowReader", "author": "rdsr", "createdAt": "2020-07-14T00:03:18Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1MTQ0NA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454051444", "bodyText": "It's a good idea to preserve symmetry, one question is: the current OrcValueReader is a public interfaces,  do the refactor seems will affect the downstream users if we don't have some deprecated ways.", "author": "openinx", "createdAt": "2020-07-14T01:56:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxNzc0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA2MTc3Nw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454061777", "bodyText": "I don't think OrcValueReader and OrcValueWriter are public interfaces meant to be used by users of Iceberg. They are meant for internal Iceberg usage", "author": "rdsr", "createdAt": "2020-07-14T02:33:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxNzc0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxODAwMQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454018001", "bodyText": "Any benefit of using GenericOrcWriters.Converter<?>?", "author": "rdsr", "createdAt": "2020-07-14T00:04:14Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA1MzEzMQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454053131", "bodyText": "For me, it seems don't have much difference with a <?> or not,  but  I can changed to keep symmetry as we've discussed above.", "author": "openinx", "createdAt": "2020-07-14T02:02:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxODAwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAxOTY4NQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454019685", "bodyText": "we should add the correct type parameter in the return value . E.g for record we should do  OrcValueWriter<Record> and similarly for map and list", "author": "rdsr", "createdAt": "2020-07-14T00:10:01Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyMzQyOQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454023429", "bodyText": "should the switch use iPrimitive ?", "author": "rdsr", "createdAt": "2020-07-14T00:23:04Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTExNg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454025116", "bodyText": "instead of ++ I think the recommendation is break it into two statements\nrow = output.size\noutput.size += 1", "author": "rdsr", "createdAt": "2020-07-14T00:28:30Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();\n+        case DATE:\n+          return GenericOrcWriters.dates();\n+        case INT:\n+          return GenericOrcWriters.ints();\n+        case LONG:\n+          String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n+          switch (longType) {\n+            case TIME:\n+              return GenericOrcWriters.times();\n+            case LONG:\n+              return GenericOrcWriters.longs();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n+          }\n+        case FLOAT:\n+          return GenericOrcWriters.floats();\n+        case DOUBLE:\n+          return GenericOrcWriters.doubles();\n+        case BINARY:\n+          String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n+              ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n+          switch (binaryType) {\n+            case UUID:\n+              return GenericOrcWriters.uuids();\n+            case FIXED:\n+              return GenericOrcWriters.fixed();\n+            case BINARY:\n+              return GenericOrcWriters.binary();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Binary type found in ORC type attribute: \" + binaryType);\n+          }\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return GenericOrcWriters.strings();\n+        case DECIMAL:\n+          return schema.getPrecision() <= 18 ? GenericOrcWriters.decimal18(schema) :\n+              GenericOrcWriters.decimal38(schema);\n+        case TIMESTAMP:\n+          return GenericOrcWriters.timestamp();\n+        case TIMESTAMP_INSTANT:\n+          return GenericOrcWriters.timestampTz();\n+      }\n+      throw new IllegalArgumentException(\"Unhandled type \" + schema);\n+    }\n   }\n \n   @SuppressWarnings(\"unchecked\")\n   @Override\n-  public void write(Record value, VectorizedRowBatch output) throws IOException {\n-    int row = output.size++;\n-    for (int c = 0; c < converters.length; ++c) {\n-      converters[c].addValue(row, value.get(c, converters[c].getJavaClass()), output.cols[c]);\n-    }\n-  }\n-\n-  /**\n-   * The interface for the conversion from Spark's SpecializedGetters to\n-   * ORC's ColumnVectors.\n-   */\n-  interface Converter<T> {\n-\n-    Class<T> getJavaClass();\n-\n-    /**\n-     * Take a value from the Spark data value and add it to the ORC output.\n-     * @param rowId the row in the ColumnVector\n-     * @param data either an InternalRow or ArrayData\n-     * @param output the ColumnVector to put the value into\n-     */\n-    void addValue(int rowId, T data, ColumnVector output);\n-  }\n-\n-  static class BooleanConverter implements Converter<Boolean> {\n-    @Override\n-    public Class<Boolean> getJavaClass() {\n-      return Boolean.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Boolean data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n-      }\n-    }\n-  }\n-\n-  static class ByteConverter implements Converter<Byte> {\n-    @Override\n-    public Class<Byte> getJavaClass() {\n-      return Byte.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Byte data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class ShortConverter implements Converter<Short> {\n-    @Override\n-    public Class<Short> getJavaClass() {\n-      return Short.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Short data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class IntConverter implements Converter<Integer> {\n-    @Override\n-    public Class<Integer> getJavaClass() {\n-      return Integer.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Integer data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class TimeConverter implements Converter<LocalTime> {\n-    @Override\n-    public Class<LocalTime> getJavaClass() {\n-      return LocalTime.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, LocalTime data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n-      }\n-    }\n-  }\n-\n-  static class LongConverter implements Converter<Long> {\n-    @Override\n-    public Class<Long> getJavaClass() {\n-      return Long.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Long data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class FloatConverter implements Converter<Float> {\n-    @Override\n-    public Class<Float> getJavaClass() {\n-      return Float.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Float data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class DoubleConverter implements Converter<Double> {\n-    @Override\n-    public Class<Double> getJavaClass() {\n-      return Double.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, Double data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DoubleColumnVector) output).vector[rowId] = data;\n-      }\n-    }\n-  }\n-\n-  static class StringConverter implements Converter<String> {\n-    @Override\n-    public Class<String> getJavaClass() {\n-      return String.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, String data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        byte[] value = data.getBytes(StandardCharsets.UTF_8);\n-        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n-      }\n-    }\n-  }\n-\n-  static class BytesConverter implements Converter<ByteBuffer> {\n-    @Override\n-    public Class<ByteBuffer> getJavaClass() {\n-      return ByteBuffer.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, ByteBuffer data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n-      }\n-    }\n-  }\n-\n-  static class UUIDConverter implements Converter<UUID> {\n-    @Override\n-    public Class<UUID> getJavaClass() {\n-      return UUID.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, UUID data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ByteBuffer buffer = ByteBuffer.allocate(16);\n-        buffer.putLong(data.getMostSignificantBits());\n-        buffer.putLong(data.getLeastSignificantBits());\n-        ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n-      }\n-    }\n-  }\n+  public void write(Record value, VectorizedRowBatch output) {\n+    Preconditions.checkArgument(converter instanceof GenericOrcWriters.RecordConverter,\n+        \"Converter must be a RecordConverter.\");\n \n-  static class FixedConverter implements Converter<byte[]> {\n-    @Override\n-    public Class<byte[]> getJavaClass() {\n-      return byte[].class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, byte[] data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n-      }\n-    }\n-  }\n-\n-  static class DateConverter implements Converter<LocalDate> {\n-    @Override\n-    public Class<LocalDate> getJavaClass() {\n-      return LocalDate.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, LocalDate data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n-      }\n-    }\n-  }\n-\n-  static class TimestampTzConverter implements Converter<OffsetDateTime> {\n-    @Override\n-    public Class<OffsetDateTime> getJavaClass() {\n-      return OffsetDateTime.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, OffsetDateTime data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        TimestampColumnVector cv = (TimestampColumnVector) output;\n-        cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n-        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n-      }\n-    }\n-  }\n-\n-  static class TimestampConverter implements Converter<LocalDateTime> {\n-\n-    @Override\n-    public Class<LocalDateTime> getJavaClass() {\n-      return LocalDateTime.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, LocalDateTime data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        TimestampColumnVector cv = (TimestampColumnVector) output;\n-        cv.setIsUTC(true);\n-        cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n-        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n-      }\n-    }\n-  }\n-\n-  static class Decimal18Converter implements Converter<BigDecimal> {\n-    private final int scale;\n-\n-    Decimal18Converter(TypeDescription schema) {\n-      this.scale = schema.getScale();\n-    }\n-\n-    @Override\n-    public Class<BigDecimal> getJavaClass() {\n-      return BigDecimal.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId]\n-            .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n-      }\n-    }\n-  }\n-\n-  static class Decimal38Converter implements Converter<BigDecimal> {\n-    Decimal38Converter(TypeDescription schema) {\n-    }\n-\n-    @Override\n-    public Class<BigDecimal> getJavaClass() {\n-      return BigDecimal.class;\n-    }\n-\n-    @Override\n-    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n-      // TODO: validate precision and scale from schema\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n-      }\n-    }\n-  }\n-\n-  static class StructConverter implements Converter<Record> {\n-    private final Converter[] children;\n-\n-    StructConverter(TypeDescription schema) {\n-      this.children = new Converter[schema.getChildren().size()];\n-      for (int c = 0; c < children.length; ++c) {\n-        children[c] = buildConverter(schema.getChildren().get(c));\n-      }\n-    }\n-\n-    @Override\n-    public Class<Record> getJavaClass() {\n-      return Record.class;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, Record data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        StructColumnVector cv = (StructColumnVector) output;\n-        for (int c = 0; c < children.length; ++c) {\n-          children[c].addValue(rowId, data.get(c, children[c].getJavaClass()), cv.fields[c]);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class ListConverter implements Converter<List> {\n-    private final Converter children;\n-\n-    ListConverter(TypeDescription schema) {\n-      this.children = buildConverter(schema.getChildren().get(0));\n-    }\n-\n-    @Override\n-    public Class<List> getJavaClass() {\n-      return List.class;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, List data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        List<Object> value = (List<Object>) data;\n-        ListColumnVector cv = (ListColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.size();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.child.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          children.addValue((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class MapConverter implements Converter<Map> {\n-    private final Converter keyConverter;\n-    private final Converter valueConverter;\n-\n-    MapConverter(TypeDescription schema) {\n-      this.keyConverter = buildConverter(schema.getChildren().get(0));\n-      this.valueConverter = buildConverter(schema.getChildren().get(1));\n-    }\n-\n-    @Override\n-    public Class<Map> getJavaClass() {\n-      return Map.class;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, Map data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        Map<Object, Object> map = (Map<Object, Object>) data;\n-        List<Object> keys = Lists.newArrayListWithExpectedSize(map.size());\n-        List<Object> values = Lists.newArrayListWithExpectedSize(map.size());\n-        for (Map.Entry<?, ?> entry : map.entrySet()) {\n-          keys.add(entry.getKey());\n-          values.add(entry.getValue());\n-        }\n-        MapColumnVector cv = (MapColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = map.size();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.keys.ensureSize(cv.childCount, true);\n-        cv.values.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          int pos = (int) (e + cv.offsets[rowId]);\n-          keyConverter.addValue(pos, keys.get(e), cv.keys);\n-          valueConverter.addValue(pos, values.get(e), cv.values);\n-        }\n-      }\n-    }\n-  }\n-\n-  private static Converter buildConverter(TypeDescription schema) {\n-    switch (schema.getCategory()) {\n-      case BOOLEAN:\n-        return new BooleanConverter();\n-      case BYTE:\n-        return new ByteConverter();\n-      case SHORT:\n-        return new ShortConverter();\n-      case DATE:\n-        return new DateConverter();\n-      case INT:\n-        return new IntConverter();\n-      case LONG:\n-        String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n-        ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n-            ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n-        switch (longType) {\n-          case TIME:\n-            return new TimeConverter();\n-          case LONG:\n-            return new LongConverter();\n-          default:\n-            throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n-        }\n-      case FLOAT:\n-        return new FloatConverter();\n-      case DOUBLE:\n-        return new DoubleConverter();\n-      case BINARY:\n-        String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n-        ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n-            ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n-        switch (binaryType) {\n-          case UUID:\n-            return new UUIDConverter();\n-          case FIXED:\n-            return new FixedConverter();\n-          case BINARY:\n-            return new BytesConverter();\n-          default:\n-            throw new IllegalStateException(\"Unhandled Binary type found in ORC type attribute: \" + binaryType);\n-        }\n-      case STRING:\n-      case CHAR:\n-      case VARCHAR:\n-        return new StringConverter();\n-      case DECIMAL:\n-        return schema.getPrecision() <= 18 ? new Decimal18Converter(schema) : new Decimal38Converter(schema);\n-      case TIMESTAMP:\n-        return new TimestampConverter();\n-      case TIMESTAMP_INSTANT:\n-        return new TimestampTzConverter();\n-      case STRUCT:\n-        return new StructConverter(schema);\n-      case LIST:\n-        return new ListConverter(schema);\n-      case MAP:\n-        return new MapConverter(schema);\n-    }\n-    throw new IllegalArgumentException(\"Unhandled type \" + schema);\n-  }\n-\n-  private static Converter[] buildConverters(TypeDescription schema) {\n-    if (schema.getCategory() != TypeDescription.Category.STRUCT) {\n-      throw new IllegalArgumentException(\"Top level must be a struct \" + schema);\n-    }\n-\n-    List<TypeDescription> children = schema.getChildren();\n-    Converter[] result = new Converter[children.size()];\n-    for (int c = 0; c < children.size(); ++c) {\n-      result[c] = buildConverter(children.get(c));\n+    int row = output.size++;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTY5NA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454025694", "bodyText": "I guess this suppress unchecked can be removed once we paramterize the converter with a wildcard e.g converter -> converter<?>", "author": "rdsr", "createdAt": "2020-07-14T00:30:14Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -19,590 +19,119 @@\n \n package org.apache.iceberg.data.orc;\n \n-import java.io.IOException;\n-import java.math.BigDecimal;\n-import java.nio.ByteBuffer;\n-import java.nio.charset.StandardCharsets;\n-import java.time.Instant;\n-import java.time.LocalDate;\n-import java.time.LocalDateTime;\n-import java.time.LocalTime;\n-import java.time.OffsetDateTime;\n-import java.time.ZoneOffset;\n-import java.time.temporal.ChronoUnit;\n import java.util.List;\n-import java.util.Map;\n-import java.util.UUID;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.orc.ORCSchemaUtil;\n+import org.apache.iceberg.orc.OrcSchemaWithTypeVisitor;\n import org.apache.iceberg.orc.OrcValueWriter;\n-import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n import org.apache.orc.TypeDescription;\n-import org.apache.orc.storage.common.type.HiveDecimal;\n-import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n-import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n \n public class GenericOrcWriter implements OrcValueWriter<Record> {\n-  private final Converter[] converters;\n-  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n-  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n-\n-  private GenericOrcWriter(TypeDescription schema) {\n-    this.converters = buildConverters(schema);\n-  }\n-\n-  public static OrcValueWriter<Record> buildWriter(TypeDescription fileSchema) {\n-    return new GenericOrcWriter(fileSchema);\n+  private final GenericOrcWriters.Converter converter;\n+\n+  private GenericOrcWriter(Schema expectedSchema, TypeDescription orcSchema) {\n+    Preconditions.checkArgument(orcSchema.getCategory() == TypeDescription.Category.STRUCT,\n+        \"Top level must be a struct \" + orcSchema);\n+\n+    converter = OrcSchemaWithTypeVisitor.visit(expectedSchema, orcSchema, new WriteBuilder());\n+  }\n+\n+  public static OrcValueWriter<Record> buildWriter(Schema expectedSchema, TypeDescription fileSchema) {\n+    return new GenericOrcWriter(expectedSchema, fileSchema);\n+  }\n+\n+  private static class WriteBuilder extends OrcSchemaWithTypeVisitor<GenericOrcWriters.Converter> {\n+    private WriteBuilder() {\n+    }\n+\n+    public GenericOrcWriters.Converter record(Types.StructType iStruct, TypeDescription record,\n+                                              List<String> names, List<GenericOrcWriters.Converter> fields) {\n+      return new GenericOrcWriters.RecordConverter(fields);\n+    }\n+\n+    public GenericOrcWriters.Converter list(Types.ListType iList, TypeDescription array,\n+                                            GenericOrcWriters.Converter element) {\n+      return new GenericOrcWriters.ListConverter(element);\n+    }\n+\n+    public GenericOrcWriters.Converter map(Types.MapType iMap, TypeDescription map,\n+                                           GenericOrcWriters.Converter key, GenericOrcWriters.Converter value) {\n+      return new GenericOrcWriters.MapConverter(key, value);\n+    }\n+\n+    public GenericOrcWriters.Converter primitive(Type.PrimitiveType iPrimitive, TypeDescription schema) {\n+      switch (schema.getCategory()) {\n+        case BOOLEAN:\n+          return GenericOrcWriters.booleans();\n+        case BYTE:\n+          return GenericOrcWriters.bytes();\n+        case SHORT:\n+          return GenericOrcWriters.shorts();\n+        case DATE:\n+          return GenericOrcWriters.dates();\n+        case INT:\n+          return GenericOrcWriters.ints();\n+        case LONG:\n+          String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n+              ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n+          switch (longType) {\n+            case TIME:\n+              return GenericOrcWriters.times();\n+            case LONG:\n+              return GenericOrcWriters.longs();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n+          }\n+        case FLOAT:\n+          return GenericOrcWriters.floats();\n+        case DOUBLE:\n+          return GenericOrcWriters.doubles();\n+        case BINARY:\n+          String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n+          ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n+              ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n+          switch (binaryType) {\n+            case UUID:\n+              return GenericOrcWriters.uuids();\n+            case FIXED:\n+              return GenericOrcWriters.fixed();\n+            case BINARY:\n+              return GenericOrcWriters.binary();\n+            default:\n+              throw new IllegalStateException(\"Unhandled Binary type found in ORC type attribute: \" + binaryType);\n+          }\n+        case STRING:\n+        case CHAR:\n+        case VARCHAR:\n+          return GenericOrcWriters.strings();\n+        case DECIMAL:\n+          return schema.getPrecision() <= 18 ? GenericOrcWriters.decimal18(schema) :\n+              GenericOrcWriters.decimal38(schema);\n+        case TIMESTAMP:\n+          return GenericOrcWriters.timestamp();\n+        case TIMESTAMP_INSTANT:\n+          return GenericOrcWriters.timestampTz();\n+      }\n+      throw new IllegalArgumentException(\"Unhandled type \" + schema);\n+    }\n   }\n \n   @SuppressWarnings(\"unchecked\")", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyODM1NA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455328354", "bodyText": "+1\nWe should avoid using types without parameters.", "author": "rdblue", "createdAt": "2020-07-15T20:40:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTY5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgwNDI0Mg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r457804242", "bodyText": "Here I did not change to use OrcValueWriter<?> because  if we do then we have the following to write child:\nfor (int c = 0; c < writers.size(); ++c) {\n      OrcValueWriter<?> child = writers.get(c);\n      child.write(row, value.get(c, child.getJavaClass()), output.cols[c]);\n}\nThe value is a StructLike and the  get in StructLike is <T> T get(int pos, Class<T> javaClass), while child.getJavaClass is a class like OrcValueWriter<?>.class, it will throw the compile error:\nIncompatible types. Required capture of ? but 'get' was inferred to T: no instance(s) of of type variables(s) exist so that capture of ? conforms to capture of ?", "author": "openinx", "createdAt": "2020-07-21T02:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyNTY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAyODk4Nw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454028987", "bodyText": "keeping these parameterize on wild card may help u remove the unchecked annotation", "author": "rdsr", "createdAt": "2020-07-14T00:40:26Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData\n+     * @param output the ColumnVector to put the value into\n+     */\n+    void addValue(int rowId, T data, ColumnVector output);\n+  }\n+\n+  public static Converter<Boolean> booleans() {\n+    return BooleanConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Byte> bytes() {\n+    return ByteConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Short> shorts() {\n+    return ShortConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Integer> ints() {\n+    return IntConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalTime> times() {\n+    return TimeConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Long> longs() {\n+    return LongConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Float> floats() {\n+    return FloatConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Double> doubles() {\n+    return DoubleConverter.INSTANCE;\n+  }\n+\n+  public static Converter<String> strings() {\n+    return StringConverter.INSTANCE;\n+  }\n+\n+  public static Converter<ByteBuffer> binary() {\n+    return BytesConverter.INSTANCE;\n+  }\n+\n+  public static Converter<UUID> uuids() {\n+    return UUIDConverter.INSTANCE;\n+  }\n+\n+  public static Converter<byte[]> fixed() {\n+    return FixedConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDate> dates() {\n+    return DateConverter.INSTANCE;\n+  }\n+\n+  public static Converter<OffsetDateTime> timestampTz() {\n+    return TimestampTzConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDateTime> timestamp() {\n+    return TimestampConverter.INSTANCE;\n+  }\n+\n+  public static Converter<BigDecimal> decimal18(TypeDescription schema) {\n+    return new Decimal18Converter(schema);\n+  }\n+\n+  public static Converter<BigDecimal> decimal38(TypeDescription schema) {\n+    return Decimal38Converter.INSTANCE;\n+  }\n+\n+  private static class BooleanConverter implements Converter<Boolean> {\n+    private static final Converter<Boolean> INSTANCE = new BooleanConverter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Boolean data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n+      }\n+    }\n+  }\n+\n+  private static class ByteConverter implements Converter<Byte> {\n+    private static final Converter<Byte> INSTANCE = new ByteConverter();\n+\n+    @Override\n+    public Class<Byte> getJavaClass() {\n+      return Byte.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Byte data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class ShortConverter implements Converter<Short> {\n+    private static final Converter<Short> INSTANCE = new ShortConverter();\n+\n+    @Override\n+    public Class<Short> getJavaClass() {\n+      return Short.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Short data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class IntConverter implements Converter<Integer> {\n+    private static final Converter<Integer> INSTANCE = new IntConverter();\n+\n+    @Override\n+    public Class<Integer> getJavaClass() {\n+      return Integer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Integer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class TimeConverter implements Converter<LocalTime> {\n+    private static final Converter<LocalTime> INSTANCE = new TimeConverter();\n+\n+    @Override\n+    public Class<LocalTime> getJavaClass() {\n+      return LocalTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n+      }\n+    }\n+  }\n+\n+  private static class LongConverter implements Converter<Long> {\n+    private static final Converter<Long> INSTANCE = new LongConverter();\n+\n+    @Override\n+    public Class<Long> getJavaClass() {\n+      return Long.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Long data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class FloatConverter implements Converter<Float> {\n+    private static final Converter<Float> INSTANCE = new FloatConverter();\n+\n+    @Override\n+    public Class<Float> getJavaClass() {\n+      return Float.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Float data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class DoubleConverter implements Converter<Double> {\n+    private static final Converter<Double> INSTANCE = new DoubleConverter();\n+\n+    @Override\n+    public Class<Double> getJavaClass() {\n+      return Double.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Double data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class StringConverter implements Converter<String> {\n+    private static final Converter<String> INSTANCE = new StringConverter();\n+\n+    @Override\n+    public Class<String> getJavaClass() {\n+      return String.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, String data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        byte[] value = data.getBytes(StandardCharsets.UTF_8);\n+        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n+      }\n+    }\n+  }\n+\n+  private static class BytesConverter implements Converter<ByteBuffer> {\n+    private static final Converter<ByteBuffer> INSTANCE = new BytesConverter();\n+\n+    @Override\n+    public Class<ByteBuffer> getJavaClass() {\n+      return ByteBuffer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, ByteBuffer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class UUIDConverter implements Converter<UUID> {\n+    private static final Converter<UUID> INSTANCE = new UUIDConverter();\n+\n+    @Override\n+    public Class<UUID> getJavaClass() {\n+      return UUID.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, UUID data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ByteBuffer buffer = ByteBuffer.allocate(16);\n+        buffer.putLong(data.getMostSignificantBits());\n+        buffer.putLong(data.getLeastSignificantBits());\n+        ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class FixedConverter implements Converter<byte[]> {\n+    private static final Converter<byte[]> INSTANCE = new FixedConverter();\n+\n+    @Override\n+    public Class<byte[]> getJavaClass() {\n+      return byte[].class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, byte[] data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n+      }\n+    }\n+  }\n+\n+  private static class DateConverter implements Converter<LocalDate> {\n+    private static final Converter<LocalDate> INSTANCE = new DateConverter();\n+\n+    @Override\n+    public Class<LocalDate> getJavaClass() {\n+      return LocalDate.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDate data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n+      }\n+    }\n+  }\n+\n+  private static class TimestampTzConverter implements Converter<OffsetDateTime> {\n+    private static final Converter<OffsetDateTime> INSTANCE = new TimestampTzConverter();\n+\n+    @Override\n+    public Class<OffsetDateTime> getJavaClass() {\n+      return OffsetDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, OffsetDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class TimestampConverter implements Converter<LocalDateTime> {\n+    private static final Converter<LocalDateTime> INSTANCE = new TimestampConverter();\n+\n+    @Override\n+    public Class<LocalDateTime> getJavaClass() {\n+      return LocalDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.setIsUTC(true);\n+        cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class Decimal18Converter implements Converter<BigDecimal> {\n+    private final int scale;\n+\n+    Decimal18Converter(TypeDescription schema) {\n+      this.scale = schema.getScale();\n+    }\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId]\n+            .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n+      }\n+    }\n+  }\n+\n+  private static class Decimal38Converter implements Converter<BigDecimal> {\n+    private static final Converter<BigDecimal> INSTANCE = new Decimal38Converter();\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n+      }\n+    }\n+  }\n+\n+  public static class RecordConverter implements Converter<Record> {\n+    private final List<Converter> converters;\n+\n+    RecordConverter(List<Converter> converters) {\n+      this.converters = converters;\n+    }\n+\n+    public List<Converter> converters() {\n+      return converters;\n+    }\n+\n+    @Override\n+    public Class<Record> getJavaClass() {\n+      return Record.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, Record data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        StructColumnVector cv = (StructColumnVector) output;\n+        for (int c = 0; c < converters.size(); ++c) {\n+          converters.get(c).addValue(rowId, data.get(c, converters.get(c).getJavaClass()), cv.fields[c]);\n+        }\n+      }\n+    }\n+  }\n+\n+  public static class ListConverter implements Converter<List> {\n+    private final Converter children;\n+\n+    ListConverter(Converter children) {\n+      this.children = children;\n+    }\n+\n+    @Override\n+    public Class<List> getJavaClass() {\n+      return List.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, List data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        List<Object> value = (List<Object>) data;\n+        ListColumnVector cv = (ListColumnVector) output;\n+        // record the length and start of the list elements\n+        cv.lengths[rowId] = value.size();\n+        cv.offsets[rowId] = cv.childCount;\n+        cv.childCount += cv.lengths[rowId];\n+        // make sure the child is big enough\n+        cv.child.ensureSize(cv.childCount, true);\n+        // Add each element\n+        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n+          children.addValue((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n+        }\n+      }\n+    }\n+  }\n+\n+  public static class MapConverter implements Converter<Map> {\n+    private final Converter keyConverter;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzMTQ5NA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454031494", "bodyText": "It seems filling a columnVector using a converter is a pattern which can be extracted across map keys, map values and list ?", "author": "rdsr", "createdAt": "2020-07-14T00:48:28Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,612 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.TypeDescription;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.StructColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  /**\n+   * The interface for the conversion from Spark's SpecializedGetters to\n+   * ORC's ColumnVectors.\n+   */\n+  interface Converter<T> {\n+\n+    Class<T> getJavaClass();\n+\n+    /**\n+     * Take a value from the Spark data value and add it to the ORC output.\n+     *\n+     * @param rowId  the row in the ColumnVector\n+     * @param data   either an InternalRow or ArrayData\n+     * @param output the ColumnVector to put the value into\n+     */\n+    void addValue(int rowId, T data, ColumnVector output);\n+  }\n+\n+  public static Converter<Boolean> booleans() {\n+    return BooleanConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Byte> bytes() {\n+    return ByteConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Short> shorts() {\n+    return ShortConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Integer> ints() {\n+    return IntConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalTime> times() {\n+    return TimeConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Long> longs() {\n+    return LongConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Float> floats() {\n+    return FloatConverter.INSTANCE;\n+  }\n+\n+  public static Converter<Double> doubles() {\n+    return DoubleConverter.INSTANCE;\n+  }\n+\n+  public static Converter<String> strings() {\n+    return StringConverter.INSTANCE;\n+  }\n+\n+  public static Converter<ByteBuffer> binary() {\n+    return BytesConverter.INSTANCE;\n+  }\n+\n+  public static Converter<UUID> uuids() {\n+    return UUIDConverter.INSTANCE;\n+  }\n+\n+  public static Converter<byte[]> fixed() {\n+    return FixedConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDate> dates() {\n+    return DateConverter.INSTANCE;\n+  }\n+\n+  public static Converter<OffsetDateTime> timestampTz() {\n+    return TimestampTzConverter.INSTANCE;\n+  }\n+\n+  public static Converter<LocalDateTime> timestamp() {\n+    return TimestampConverter.INSTANCE;\n+  }\n+\n+  public static Converter<BigDecimal> decimal18(TypeDescription schema) {\n+    return new Decimal18Converter(schema);\n+  }\n+\n+  public static Converter<BigDecimal> decimal38(TypeDescription schema) {\n+    return Decimal38Converter.INSTANCE;\n+  }\n+\n+  private static class BooleanConverter implements Converter<Boolean> {\n+    private static final Converter<Boolean> INSTANCE = new BooleanConverter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Boolean data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n+      }\n+    }\n+  }\n+\n+  private static class ByteConverter implements Converter<Byte> {\n+    private static final Converter<Byte> INSTANCE = new ByteConverter();\n+\n+    @Override\n+    public Class<Byte> getJavaClass() {\n+      return Byte.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Byte data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class ShortConverter implements Converter<Short> {\n+    private static final Converter<Short> INSTANCE = new ShortConverter();\n+\n+    @Override\n+    public Class<Short> getJavaClass() {\n+      return Short.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Short data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class IntConverter implements Converter<Integer> {\n+    private static final Converter<Integer> INSTANCE = new IntConverter();\n+\n+    @Override\n+    public Class<Integer> getJavaClass() {\n+      return Integer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Integer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class TimeConverter implements Converter<LocalTime> {\n+    private static final Converter<LocalTime> INSTANCE = new TimeConverter();\n+\n+    @Override\n+    public Class<LocalTime> getJavaClass() {\n+      return LocalTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n+      }\n+    }\n+  }\n+\n+  private static class LongConverter implements Converter<Long> {\n+    private static final Converter<Long> INSTANCE = new LongConverter();\n+\n+    @Override\n+    public Class<Long> getJavaClass() {\n+      return Long.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Long data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class FloatConverter implements Converter<Float> {\n+    private static final Converter<Float> INSTANCE = new FloatConverter();\n+\n+    @Override\n+    public Class<Float> getJavaClass() {\n+      return Float.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Float data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class DoubleConverter implements Converter<Double> {\n+    private static final Converter<Double> INSTANCE = new DoubleConverter();\n+\n+    @Override\n+    public Class<Double> getJavaClass() {\n+      return Double.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, Double data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DoubleColumnVector) output).vector[rowId] = data;\n+      }\n+    }\n+  }\n+\n+  private static class StringConverter implements Converter<String> {\n+    private static final Converter<String> INSTANCE = new StringConverter();\n+\n+    @Override\n+    public Class<String> getJavaClass() {\n+      return String.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, String data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        byte[] value = data.getBytes(StandardCharsets.UTF_8);\n+        ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n+      }\n+    }\n+  }\n+\n+  private static class BytesConverter implements Converter<ByteBuffer> {\n+    private static final Converter<ByteBuffer> INSTANCE = new BytesConverter();\n+\n+    @Override\n+    public Class<ByteBuffer> getJavaClass() {\n+      return ByteBuffer.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, ByteBuffer data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class UUIDConverter implements Converter<UUID> {\n+    private static final Converter<UUID> INSTANCE = new UUIDConverter();\n+\n+    @Override\n+    public Class<UUID> getJavaClass() {\n+      return UUID.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, UUID data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ByteBuffer buffer = ByteBuffer.allocate(16);\n+        buffer.putLong(data.getMostSignificantBits());\n+        buffer.putLong(data.getLeastSignificantBits());\n+        ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n+      }\n+    }\n+  }\n+\n+  private static class FixedConverter implements Converter<byte[]> {\n+    private static final Converter<byte[]> INSTANCE = new FixedConverter();\n+\n+    @Override\n+    public Class<byte[]> getJavaClass() {\n+      return byte[].class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, byte[] data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n+      }\n+    }\n+  }\n+\n+  private static class DateConverter implements Converter<LocalDate> {\n+    private static final Converter<LocalDate> INSTANCE = new DateConverter();\n+\n+    @Override\n+    public Class<LocalDate> getJavaClass() {\n+      return LocalDate.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDate data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n+      }\n+    }\n+  }\n+\n+  private static class TimestampTzConverter implements Converter<OffsetDateTime> {\n+    private static final Converter<OffsetDateTime> INSTANCE = new TimestampTzConverter();\n+\n+    @Override\n+    public Class<OffsetDateTime> getJavaClass() {\n+      return OffsetDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, OffsetDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class TimestampConverter implements Converter<LocalDateTime> {\n+    private static final Converter<LocalDateTime> INSTANCE = new TimestampConverter();\n+\n+    @Override\n+    public Class<LocalDateTime> getJavaClass() {\n+      return LocalDateTime.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, LocalDateTime data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        TimestampColumnVector cv = (TimestampColumnVector) output;\n+        cv.setIsUTC(true);\n+        cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n+        cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+      }\n+    }\n+  }\n+\n+  private static class Decimal18Converter implements Converter<BigDecimal> {\n+    private final int scale;\n+\n+    Decimal18Converter(TypeDescription schema) {\n+      this.scale = schema.getScale();\n+    }\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId]\n+            .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n+      }\n+    }\n+  }\n+\n+  private static class Decimal38Converter implements Converter<BigDecimal> {\n+    private static final Converter<BigDecimal> INSTANCE = new Decimal38Converter();\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n+      }\n+    }\n+  }\n+\n+  public static class RecordConverter implements Converter<Record> {\n+    private final List<Converter> converters;\n+\n+    RecordConverter(List<Converter> converters) {\n+      this.converters = converters;\n+    }\n+\n+    public List<Converter> converters() {\n+      return converters;\n+    }\n+\n+    @Override\n+    public Class<Record> getJavaClass() {\n+      return Record.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, Record data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        StructColumnVector cv = (StructColumnVector) output;\n+        for (int c = 0; c < converters.size(); ++c) {\n+          converters.get(c).addValue(rowId, data.get(c, converters.get(c).getJavaClass()), cv.fields[c]);\n+        }\n+      }\n+    }\n+  }\n+\n+  public static class ListConverter implements Converter<List> {\n+    private final Converter children;\n+\n+    ListConverter(Converter children) {\n+      this.children = children;\n+    }\n+\n+    @Override\n+    public Class<List> getJavaClass() {\n+      return List.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, List data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        List<Object> value = (List<Object>) data;\n+        ListColumnVector cv = (ListColumnVector) output;\n+        // record the length and start of the list elements\n+        cv.lengths[rowId] = value.size();\n+        cv.offsets[rowId] = cv.childCount;\n+        cv.childCount += cv.lengths[rowId];\n+        // make sure the child is big enough\n+        cv.child.ensureSize(cv.childCount, true);\n+        // Add each element\n+        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n+          children.addValue((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n+        }\n+      }\n+    }\n+  }\n+\n+  public static class MapConverter implements Converter<Map> {\n+    private final Converter keyConverter;\n+    private final Converter valueConverter;\n+\n+    MapConverter(Converter keyConverter, Converter valueConverter) {\n+      this.keyConverter = keyConverter;\n+      this.valueConverter = valueConverter;\n+    }\n+\n+    @Override\n+    public Class<Map> getJavaClass() {\n+      return Map.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void addValue(int rowId, Map data, ColumnVector output) {\n+      if (data == null) {\n+        output.noNulls = false;\n+        output.isNull[rowId] = true;\n+      } else {\n+        output.isNull[rowId] = false;\n+        Map<Object, Object> map = (Map<Object, Object>) data;\n+        List<Object> keys = Lists.newArrayListWithExpectedSize(map.size());\n+        List<Object> values = Lists.newArrayListWithExpectedSize(map.size());\n+        for (Map.Entry<?, ?> entry : map.entrySet()) {\n+          keys.add(entry.getKey());\n+          values.add(entry.getValue());\n+        }\n+        MapColumnVector cv = (MapColumnVector) output;", "originalCommit": "a64e0a077840a0d04dd8b655658047f46135aeba", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDA4OTkwMA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454089900", "bodyText": "It's true, but seems hacky to abstract the similar partten into a separate method, I'd prefer to keep the current implementation.", "author": "openinx", "createdAt": "2020-07-14T04:18:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDAzMTQ5NA=="}], "type": "inlineReview"}, {"oid": "2800db49d27ccf656b091e39e0d2707f96aac05f", "url": "https://github.com/apache/iceberg/commit/2800db49d27ccf656b091e39e0d2707f96aac05f", "message": "Fixed broken unit tests.", "committedDate": "2020-07-14T07:00:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDEwNzU3OQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454107579", "bodyText": "Nit: Change method name to reflect class name change", "author": "shardulm94", "createdAt": "2020-07-14T05:19:48Z", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java", "diffHunk": "@@ -146,8 +146,8 @@ private static Writer newOrcWriter(OutputFile file,\n   }\n \n   @SuppressWarnings(\"unchecked\")\n-  private static <D> OrcValueWriter<D> newOrcValueWriter(\n-      TypeDescription schema, Function<TypeDescription, OrcValueWriter<?>> createWriterFunc) {\n-    return (OrcValueWriter<D>) createWriterFunc.apply(schema);\n+  private static <D> OrcRowWriter<D> newOrcValueWriter(", "originalCommit": "1b0772214bbb9d53db1792fa2a7412b719e2d8ac", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4MDExNQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454180115", "bodyText": "Thanks for your reminding, Good point.", "author": "openinx", "createdAt": "2020-07-14T08:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDEwNzU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDI4NTk5Ng==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454285996", "bodyText": "Is there problem here ? I mean the iceberg  FIXED type is mapping to ByteBuffer class and the FixedConverter  will write a byte[] instance to the ColumnVector,  I guess the unit tests did not expose this issue because the RandomDataGenerator are producing a ByteBuffer instance and we did not use the TypeID#javaClass(). The BINARY & TIME have similar issues I think.    @rdsr @shardulm94  FYI.", "author": "openinx", "createdAt": "2020-07-14T11:23:20Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriter.java", "diffHunk": "@@ -434,175 +144,12 @@ public void addValue(int rowId, BigDecimal data, ColumnVector output) {\n \n     @Override\n     @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, Record data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        StructColumnVector cv = (StructColumnVector) output;\n-        for (int c = 0; c < children.length; ++c) {\n-          children[c].addValue(rowId, data.get(c, children[c].getJavaClass()), cv.fields[c]);\n-        }\n-      }\n-    }\n-  }\n-\n-  static class ListConverter implements Converter<List> {\n-    private final Converter children;\n-\n-    ListConverter(TypeDescription schema) {\n-      this.children = buildConverter(schema.getChildren().get(0));\n-    }\n-\n-    @Override\n-    public Class<List> getJavaClass() {\n-      return List.class;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, List data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        List<Object> value = (List<Object>) data;\n-        ListColumnVector cv = (ListColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = value.size();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.child.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          children.addValue((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n-        }\n+    public void nonNullWrite(int rowId, Record data, ColumnVector output) {\n+      StructColumnVector cv = (StructColumnVector) output;\n+      for (int c = 0; c < writers.size(); ++c) {\n+        OrcValueWriter child = writers.get(c);\n+        child.write(rowId, data.get(c, child.getJavaClass()), cv.fields[c]);\n       }\n     }\n   }\n-\n-  static class MapConverter implements Converter<Map> {\n-    private final Converter keyConverter;\n-    private final Converter valueConverter;\n-\n-    MapConverter(TypeDescription schema) {\n-      this.keyConverter = buildConverter(schema.getChildren().get(0));\n-      this.valueConverter = buildConverter(schema.getChildren().get(1));\n-    }\n-\n-    @Override\n-    public Class<Map> getJavaClass() {\n-      return Map.class;\n-    }\n-\n-    @Override\n-    @SuppressWarnings(\"unchecked\")\n-    public void addValue(int rowId, Map data, ColumnVector output) {\n-      if (data == null) {\n-        output.noNulls = false;\n-        output.isNull[rowId] = true;\n-      } else {\n-        output.isNull[rowId] = false;\n-        Map<Object, Object> map = (Map<Object, Object>) data;\n-        List<Object> keys = Lists.newArrayListWithExpectedSize(map.size());\n-        List<Object> values = Lists.newArrayListWithExpectedSize(map.size());\n-        for (Map.Entry<?, ?> entry : map.entrySet()) {\n-          keys.add(entry.getKey());\n-          values.add(entry.getValue());\n-        }\n-        MapColumnVector cv = (MapColumnVector) output;\n-        // record the length and start of the list elements\n-        cv.lengths[rowId] = map.size();\n-        cv.offsets[rowId] = cv.childCount;\n-        cv.childCount += cv.lengths[rowId];\n-        // make sure the child is big enough\n-        cv.keys.ensureSize(cv.childCount, true);\n-        cv.values.ensureSize(cv.childCount, true);\n-        // Add each element\n-        for (int e = 0; e < cv.lengths[rowId]; ++e) {\n-          int pos = (int) (e + cv.offsets[rowId]);\n-          keyConverter.addValue(pos, keys.get(e), cv.keys);\n-          valueConverter.addValue(pos, values.get(e), cv.values);\n-        }\n-      }\n-    }\n-  }\n-\n-  private static Converter buildConverter(TypeDescription schema) {\n-    switch (schema.getCategory()) {\n-      case BOOLEAN:\n-        return new BooleanConverter();\n-      case BYTE:\n-        return new ByteConverter();\n-      case SHORT:\n-        return new ShortConverter();\n-      case DATE:\n-        return new DateConverter();\n-      case INT:\n-        return new IntConverter();\n-      case LONG:\n-        String longAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_LONG_TYPE_ATTRIBUTE);\n-        ORCSchemaUtil.LongType longType = longAttributeValue == null ? ORCSchemaUtil.LongType.LONG :\n-            ORCSchemaUtil.LongType.valueOf(longAttributeValue);\n-        switch (longType) {\n-          case TIME:\n-            return new TimeConverter();\n-          case LONG:\n-            return new LongConverter();\n-          default:\n-            throw new IllegalStateException(\"Unhandled Long type found in ORC type attribute: \" + longType);\n-        }\n-      case FLOAT:\n-        return new FloatConverter();\n-      case DOUBLE:\n-        return new DoubleConverter();\n-      case BINARY:\n-        String binaryAttributeValue = schema.getAttributeValue(ORCSchemaUtil.ICEBERG_BINARY_TYPE_ATTRIBUTE);\n-        ORCSchemaUtil.BinaryType binaryType = binaryAttributeValue == null ? ORCSchemaUtil.BinaryType.BINARY :\n-            ORCSchemaUtil.BinaryType.valueOf(binaryAttributeValue);\n-        switch (binaryType) {\n-          case UUID:\n-            return new UUIDConverter();\n-          case FIXED:\n-            return new FixedConverter();", "originalCommit": "592aea9d9f4fe4db270c885492c97df1955e33c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDU2Mzg4NQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454563885", "bodyText": "TypeID#javaClass() does not define the Java types exposed by the iceberg-data module. They are mainly used internally in iceberg-core when serializing and deserializing to/from manifest files and in Expressions as literals. Iceberg Generics use byte[] for fixed types. So this looks correct and is in parity with Parquet and Avro Generics. You can also look at Timestamp types. Iceberg Generics uses LocalDateTime and OffsetDateTime for Timestamp Without Zone and Timestamp With Zone respectively. However both types map to longs in TypeID#javaClass().", "author": "shardulm94", "createdAt": "2020-07-14T18:37:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDI4NTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDc2NDQ4OQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454764489", "bodyText": "I see.  The TypeID#javaClass defines the class of iceberg desired data types(expressions as you said), the Record's data type is different, we also have a class to transform between them . \n  \n    \n      iceberg/data/src/main/java/org/apache/iceberg/data/InternalRecordWrapper.java\n    \n    \n         Line 45\n      in\n      68e417c\n    \n    \n    \n    \n\n        \n          \n           private static Function<Object, Object> converter(Type type) { \n        \n    \n  \n\n\nBut another question is: why we need to use the different data types in Expression or manifest files serialization & deserialization, which produces this complexity....", "author": "openinx", "createdAt": "2020-07-15T03:12:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDI4NTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDc3NDAxMQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r454774011", "bodyText": "Seems the TypeID#javaClass  defines the literal type, means the type to compare or serialize/deserialize. Actually, I'd prefer to  pass the LocalDateTime object to the comparator and do the LocalDateTime to Long conversion when comparing.  Then  we hidden the literal types between different types, and the upper layer won't to  wrap again and again.", "author": "openinx", "createdAt": "2020-07-15T03:50:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDI4NTk5Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTI4MTA1NA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455281054", "bodyText": "I'd prefer to pass the LocalDateTime object to the comparator and do the LocalDateTime to Long conversion when comparing\n\nIceberg's internal representation does not use higher-level types like LocalDateTime for a few good reasons:\n\nIt is simpler to work with ordinal values\nThe interpretation of an ordinal value is delegated to the object model: Iceberg is agnostic to calendars, time zones, and other concerns that are built into the processing engines\nThe guarantee is simpler: whatever data values are passed into Iceberg will be passed back out, unmodified", "author": "rdblue", "createdAt": "2020-07-15T19:11:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDI4NTk5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyODk1Mg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455328952", "bodyText": "Missing parameter types.", "author": "rdblue", "createdAt": "2020-07-15T20:41:03Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.orc.OrcValueWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  public static OrcValueWriter<Boolean> booleans() {\n+    return BooleanWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Integer> ints() {\n+    return IntWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalTime> times() {\n+    return TimeWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Long> longs() {\n+    return LongWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Float> floats() {\n+    return FloatWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Double> doubles() {\n+    return DoubleWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<String> strings() {\n+    return StringWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<ByteBuffer> byteBuffers() {\n+    return ByteBufferWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<UUID> uuids() {\n+    return UUIDWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<byte[]> fixed() {\n+    return FixedWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDate> dates() {\n+    return DateWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<OffsetDateTime> timestampTz() {\n+    return TimestampTzWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDateTime> timestamp() {\n+    return TimestampWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<BigDecimal> decimal(int scala, int precision) {\n+    if (precision <= 18) {\n+      return new Decimal18Writer(scala);\n+    } else {\n+      return Decimal38Writer.INSTANCE;\n+    }\n+  }\n+\n+  public static OrcValueWriter<List> list(OrcValueWriter element) {\n+    return new ListWriter(element);\n+  }\n+\n+  public static OrcValueWriter<Map> map(OrcValueWriter key, OrcValueWriter value) {\n+    return new MapWriter(key, value);\n+  }\n+\n+  private static class BooleanWriter implements OrcValueWriter<Boolean> {\n+    private static final OrcValueWriter<Boolean> INSTANCE = new BooleanWriter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Boolean data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n+    }\n+  }\n+\n+  private static class IntWriter implements OrcValueWriter<Integer> {\n+    private static final OrcValueWriter<Integer> INSTANCE = new IntWriter();\n+\n+    @Override\n+    public Class<Integer> getJavaClass() {\n+      return Integer.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Integer data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class TimeWriter implements OrcValueWriter<LocalTime> {\n+    private static final OrcValueWriter<LocalTime> INSTANCE = new TimeWriter();\n+\n+    @Override\n+    public Class<LocalTime> getJavaClass() {\n+      return LocalTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalTime data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n+    }\n+  }\n+\n+  private static class LongWriter implements OrcValueWriter<Long> {\n+    private static final OrcValueWriter<Long> INSTANCE = new LongWriter();\n+\n+    @Override\n+    public Class<Long> getJavaClass() {\n+      return Long.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Long data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class FloatWriter implements OrcValueWriter<Float> {\n+    private static final OrcValueWriter<Float> INSTANCE = new FloatWriter();\n+\n+    @Override\n+    public Class<Float> getJavaClass() {\n+      return Float.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Float data, ColumnVector output) {\n+      ((DoubleColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class DoubleWriter implements OrcValueWriter<Double> {\n+    private static final OrcValueWriter<Double> INSTANCE = new DoubleWriter();\n+\n+    @Override\n+    public Class<Double> getJavaClass() {\n+      return Double.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Double data, ColumnVector output) {\n+      ((DoubleColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class StringWriter implements OrcValueWriter<String> {\n+    private static final OrcValueWriter<String> INSTANCE = new StringWriter();\n+\n+    @Override\n+    public Class<String> getJavaClass() {\n+      return String.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, String data, ColumnVector output) {\n+      byte[] value = data.getBytes(StandardCharsets.UTF_8);\n+      ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n+    }\n+  }\n+\n+  private static class ByteBufferWriter implements OrcValueWriter<ByteBuffer> {\n+    private static final OrcValueWriter<ByteBuffer> INSTANCE = new ByteBufferWriter();\n+\n+    @Override\n+    public Class<ByteBuffer> getJavaClass() {\n+      return ByteBuffer.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n+      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+    }\n+  }\n+\n+  private static class UUIDWriter implements OrcValueWriter<UUID> {\n+    private static final OrcValueWriter<UUID> INSTANCE = new UUIDWriter();\n+\n+    @Override\n+    public Class<UUID> getJavaClass() {\n+      return UUID.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, UUID data, ColumnVector output) {\n+      ByteBuffer buffer = ByteBuffer.allocate(16);\n+      buffer.putLong(data.getMostSignificantBits());\n+      buffer.putLong(data.getLeastSignificantBits());\n+      ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n+    }\n+  }\n+\n+  private static class FixedWriter implements OrcValueWriter<byte[]> {\n+    private static final OrcValueWriter<byte[]> INSTANCE = new FixedWriter();\n+\n+    @Override\n+    public Class<byte[]> getJavaClass() {\n+      return byte[].class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, byte[] data, ColumnVector output) {\n+      ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n+    }\n+  }\n+\n+  private static class DateWriter implements OrcValueWriter<LocalDate> {\n+    private static final OrcValueWriter<LocalDate> INSTANCE = new DateWriter();\n+\n+    @Override\n+    public Class<LocalDate> getJavaClass() {\n+      return LocalDate.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n+    }\n+  }\n+\n+  private static class TimestampTzWriter implements OrcValueWriter<OffsetDateTime> {\n+    private static final OrcValueWriter<OffsetDateTime> INSTANCE = new TimestampTzWriter();\n+\n+    @Override\n+    public Class<OffsetDateTime> getJavaClass() {\n+      return OffsetDateTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n+      TimestampColumnVector cv = (TimestampColumnVector) output;\n+      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n+      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+    }\n+  }\n+\n+  private static class TimestampWriter implements OrcValueWriter<LocalDateTime> {\n+    private static final OrcValueWriter<LocalDateTime> INSTANCE = new TimestampWriter();\n+\n+    @Override\n+    public Class<LocalDateTime> getJavaClass() {\n+      return LocalDateTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n+      TimestampColumnVector cv = (TimestampColumnVector) output;\n+      cv.setIsUTC(true);\n+      cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n+      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+    }\n+  }\n+\n+  private static class Decimal18Writer implements OrcValueWriter<BigDecimal> {\n+    private final int scale;\n+\n+    Decimal18Writer(int scale) {\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      ((DecimalColumnVector) output).vector[rowId]\n+          .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n+    }\n+  }\n+\n+  private static class Decimal38Writer implements OrcValueWriter<BigDecimal> {\n+    private static final OrcValueWriter<BigDecimal> INSTANCE = new Decimal38Writer();\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n+    }\n+  }\n+\n+  private static class ListWriter implements OrcValueWriter<List> {\n+    private final OrcValueWriter element;\n+\n+    ListWriter(OrcValueWriter element) {\n+      this.element = element;\n+    }\n+\n+    @Override\n+    public Class<List> getJavaClass() {\n+      return List.class;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    public void nonNullWrite(int rowId, List data, ColumnVector output) {\n+      List<Object> value = (List<Object>) data;\n+      ListColumnVector cv = (ListColumnVector) output;\n+      // record the length and start of the list elements\n+      cv.lengths[rowId] = value.size();\n+      cv.offsets[rowId] = cv.childCount;\n+      cv.childCount += cv.lengths[rowId];\n+      // make sure the child is big enough\n+      cv.child.ensureSize(cv.childCount, true);\n+      // Add each element\n+      for (int e = 0; e < cv.lengths[rowId]; ++e) {\n+        element.write((int) (e + cv.offsets[rowId]), value.get(e), cv.child);\n+      }\n+    }\n+  }\n+\n+  private static class MapWriter implements OrcValueWriter<Map> {", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ3NzM0Nw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455477347", "bodyText": "ditto.", "author": "openinx", "createdAt": "2020-07-16T02:47:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyODk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyOTA4MQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455329081", "bodyText": "Missing parameter type.", "author": "rdblue", "createdAt": "2020-07-15T20:41:18Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,418 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.orc.OrcValueWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  public static OrcValueWriter<Boolean> booleans() {\n+    return BooleanWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Integer> ints() {\n+    return IntWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalTime> times() {\n+    return TimeWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Long> longs() {\n+    return LongWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Float> floats() {\n+    return FloatWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Double> doubles() {\n+    return DoubleWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<String> strings() {\n+    return StringWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<ByteBuffer> byteBuffers() {\n+    return ByteBufferWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<UUID> uuids() {\n+    return UUIDWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<byte[]> fixed() {\n+    return FixedWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDate> dates() {\n+    return DateWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<OffsetDateTime> timestampTz() {\n+    return TimestampTzWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDateTime> timestamp() {\n+    return TimestampWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<BigDecimal> decimal(int scala, int precision) {\n+    if (precision <= 18) {\n+      return new Decimal18Writer(scala);\n+    } else {\n+      return Decimal38Writer.INSTANCE;\n+    }\n+  }\n+\n+  public static OrcValueWriter<List> list(OrcValueWriter element) {\n+    return new ListWriter(element);\n+  }\n+\n+  public static OrcValueWriter<Map> map(OrcValueWriter key, OrcValueWriter value) {\n+    return new MapWriter(key, value);\n+  }\n+\n+  private static class BooleanWriter implements OrcValueWriter<Boolean> {\n+    private static final OrcValueWriter<Boolean> INSTANCE = new BooleanWriter();\n+\n+    @Override\n+    public Class<Boolean> getJavaClass() {\n+      return Boolean.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Boolean data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data ? 1 : 0;\n+    }\n+  }\n+\n+  private static class IntWriter implements OrcValueWriter<Integer> {\n+    private static final OrcValueWriter<Integer> INSTANCE = new IntWriter();\n+\n+    @Override\n+    public Class<Integer> getJavaClass() {\n+      return Integer.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Integer data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class TimeWriter implements OrcValueWriter<LocalTime> {\n+    private static final OrcValueWriter<LocalTime> INSTANCE = new TimeWriter();\n+\n+    @Override\n+    public Class<LocalTime> getJavaClass() {\n+      return LocalTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalTime data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data.toNanoOfDay() / 1_000;\n+    }\n+  }\n+\n+  private static class LongWriter implements OrcValueWriter<Long> {\n+    private static final OrcValueWriter<Long> INSTANCE = new LongWriter();\n+\n+    @Override\n+    public Class<Long> getJavaClass() {\n+      return Long.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Long data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class FloatWriter implements OrcValueWriter<Float> {\n+    private static final OrcValueWriter<Float> INSTANCE = new FloatWriter();\n+\n+    @Override\n+    public Class<Float> getJavaClass() {\n+      return Float.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Float data, ColumnVector output) {\n+      ((DoubleColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class DoubleWriter implements OrcValueWriter<Double> {\n+    private static final OrcValueWriter<Double> INSTANCE = new DoubleWriter();\n+\n+    @Override\n+    public Class<Double> getJavaClass() {\n+      return Double.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, Double data, ColumnVector output) {\n+      ((DoubleColumnVector) output).vector[rowId] = data;\n+    }\n+  }\n+\n+  private static class StringWriter implements OrcValueWriter<String> {\n+    private static final OrcValueWriter<String> INSTANCE = new StringWriter();\n+\n+    @Override\n+    public Class<String> getJavaClass() {\n+      return String.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, String data, ColumnVector output) {\n+      byte[] value = data.getBytes(StandardCharsets.UTF_8);\n+      ((BytesColumnVector) output).setRef(rowId, value, 0, value.length);\n+    }\n+  }\n+\n+  private static class ByteBufferWriter implements OrcValueWriter<ByteBuffer> {\n+    private static final OrcValueWriter<ByteBuffer> INSTANCE = new ByteBufferWriter();\n+\n+    @Override\n+    public Class<ByteBuffer> getJavaClass() {\n+      return ByteBuffer.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n+      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+    }\n+  }\n+\n+  private static class UUIDWriter implements OrcValueWriter<UUID> {\n+    private static final OrcValueWriter<UUID> INSTANCE = new UUIDWriter();\n+\n+    @Override\n+    public Class<UUID> getJavaClass() {\n+      return UUID.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, UUID data, ColumnVector output) {\n+      ByteBuffer buffer = ByteBuffer.allocate(16);\n+      buffer.putLong(data.getMostSignificantBits());\n+      buffer.putLong(data.getLeastSignificantBits());\n+      ((BytesColumnVector) output).setRef(rowId, buffer.array(), 0, buffer.array().length);\n+    }\n+  }\n+\n+  private static class FixedWriter implements OrcValueWriter<byte[]> {\n+    private static final OrcValueWriter<byte[]> INSTANCE = new FixedWriter();\n+\n+    @Override\n+    public Class<byte[]> getJavaClass() {\n+      return byte[].class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, byte[] data, ColumnVector output) {\n+      ((BytesColumnVector) output).setRef(rowId, data, 0, data.length);\n+    }\n+  }\n+\n+  private static class DateWriter implements OrcValueWriter<LocalDate> {\n+    private static final OrcValueWriter<LocalDate> INSTANCE = new DateWriter();\n+\n+    @Override\n+    public Class<LocalDate> getJavaClass() {\n+      return LocalDate.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalDate data, ColumnVector output) {\n+      ((LongColumnVector) output).vector[rowId] = ChronoUnit.DAYS.between(EPOCH_DAY, data);\n+    }\n+  }\n+\n+  private static class TimestampTzWriter implements OrcValueWriter<OffsetDateTime> {\n+    private static final OrcValueWriter<OffsetDateTime> INSTANCE = new TimestampTzWriter();\n+\n+    @Override\n+    public Class<OffsetDateTime> getJavaClass() {\n+      return OffsetDateTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, OffsetDateTime data, ColumnVector output) {\n+      TimestampColumnVector cv = (TimestampColumnVector) output;\n+      cv.time[rowId] = data.toInstant().toEpochMilli(); // millis\n+      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+    }\n+  }\n+\n+  private static class TimestampWriter implements OrcValueWriter<LocalDateTime> {\n+    private static final OrcValueWriter<LocalDateTime> INSTANCE = new TimestampWriter();\n+\n+    @Override\n+    public Class<LocalDateTime> getJavaClass() {\n+      return LocalDateTime.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, LocalDateTime data, ColumnVector output) {\n+      TimestampColumnVector cv = (TimestampColumnVector) output;\n+      cv.setIsUTC(true);\n+      cv.time[rowId] = data.toInstant(ZoneOffset.UTC).toEpochMilli(); // millis\n+      cv.nanos[rowId] = (data.getNano() / 1_000) * 1_000; // truncate nanos to only keep microsecond precision\n+    }\n+  }\n+\n+  private static class Decimal18Writer implements OrcValueWriter<BigDecimal> {\n+    private final int scale;\n+\n+    Decimal18Writer(int scale) {\n+      this.scale = scale;\n+    }\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      ((DecimalColumnVector) output).vector[rowId]\n+          .setFromLongAndScale(data.unscaledValue().longValueExact(), scale);\n+    }\n+  }\n+\n+  private static class Decimal38Writer implements OrcValueWriter<BigDecimal> {\n+    private static final OrcValueWriter<BigDecimal> INSTANCE = new Decimal38Writer();\n+\n+    @Override\n+    public Class<BigDecimal> getJavaClass() {\n+      return BigDecimal.class;\n+    }\n+\n+    @Override\n+    public void nonNullWrite(int rowId, BigDecimal data, ColumnVector output) {\n+      // TODO: validate precision and scale from schema\n+      ((DecimalColumnVector) output).vector[rowId].set(HiveDecimal.create(data, false));\n+    }\n+  }\n+\n+  private static class ListWriter implements OrcValueWriter<List> {", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ3NzMwMw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455477303", "bodyText": "I did not provide the explicit parameter type here, because  getJavaClass()  will need to return a  List class with generic type, while Java don't support this now.   Pls see here", "author": "openinx", "createdAt": "2020-07-16T02:47:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyOTA4MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMDAyMQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458230021", "bodyText": "We still need to add the parameterized types.\nThe FAQ entry you pointed to explains why there is no class literal, like List<String>.class. All variants of List use List.class because there is only one concrete type at runtime. But we still want to use type parameters to be explicit about what is passed around.\nThis class handles lists of some type, T. The class should be parameterized by T so that we can use type-safe operations to pass around T instances. The wrapped value writer should be OrcValueWriter<T> elementWriter. By doing this, the implementation of nonNullWrite will get a List<T> and will be able to pass those values to the elementWriter without casting.", "author": "rdblue", "createdAt": "2020-07-21T16:28:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMyOTA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMzMDIyNA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455330224", "bodyText": "What is the purpose of changing OrcValueWriter to OrcRowWriter? It seems like this isn't really necessary for the purpose of this commit and causes quite a few changes. Is there value in doing this?", "author": "rdblue", "createdAt": "2020-07-15T20:43:28Z", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java", "diffHunk": "@@ -51,12 +51,12 @@\n   private final OutputFile file;\n   private final Writer writer;\n   private final VectorizedRowBatch batch;\n-  private final OrcValueWriter<D> valueWriter;\n+  private final OrcRowWriter<D> valueWriter;", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ2Mzc1Ng==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455463756", "bodyText": "Pls see the discussion here: #1197 (comment)", "author": "openinx", "createdAt": "2020-07-16T01:56:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMzMDIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMzk1Nw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458233957", "bodyText": "Okay, sounds good to me.", "author": "rdblue", "createdAt": "2020-07-21T16:34:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMzMDIyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMzMTM0OA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455331348", "bodyText": "Nit: unnecessary whitespace changes.", "author": "rdblue", "createdAt": "2020-07-15T20:45:36Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -65,9 +65,10 @@ public void write(InternalRow value, VectorizedRowBatch output) {\n   interface Converter {\n     /**\n      * Take a value from the Spark data value and add it to the ORC output.\n-     * @param rowId the row in the ColumnVector\n+     *\n+     * @param rowId  the row in the ColumnVector\n      * @param column either the column number or element number\n-     * @param data either an InternalRow or ArrayData\n+     * @param data   either an InternalRow or ArrayData", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTQ2Mzg1Mw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r455463853", "bodyText": "OK, I guess I did a code format before.", "author": "openinx", "createdAt": "2020-07-16T01:56:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTMzMTM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMzM3MQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458233371", "bodyText": "Since the schema is passed to the write builder, what about adding a createWriterFunc method that accepts BiFunction<Schema, TypeDescription>? Then this wouldn't need to change.\nWe do this in Avro: https://github.com/apache/iceberg/blob/master/core/src/main/java/org/apache/iceberg/avro/Avro.java#L199-L203\nThat would cut down on the number of files that need to change in this PR.", "author": "rdblue", "createdAt": "2020-07-21T16:33:54Z", "path": "data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilterTypes.java", "diffHunk": "@@ -180,7 +180,7 @@ public void createOrcInputFile(List<Record> records) throws IOException {\n     OutputFile outFile = Files.localOutput(ORC_FILE);\n     try (FileAppender<Record> appender = ORC.write(outFile)\n         .schema(FILE_SCHEMA)\n-        .createWriterFunc(GenericOrcWriter::buildWriter)\n+        .createWriterFunc(typeDesc -> GenericOrcWriter.buildWriter(FILE_SCHEMA, typeDesc))", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUxMTQ4MA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458511480", "bodyText": "OK, that sounds good to me.", "author": "openinx", "createdAt": "2020-07-22T03:25:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMzM3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI1MDcxNg==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459250716", "bodyText": "Would it make more sense to add createWriterFunc method that accepts BiFunction<Schema, TypeDescription> instead of replacing the existing one? Replacing the existing createWriterFunc  causes changes in the files?\nTestSparkOrcReadMetadataColumns.java\nTestSparkOrcReader.java\nTestOrcWrite.java\nSparkAppenderFactory.java", "author": "rdsr", "createdAt": "2020-07-23T07:02:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzMzM3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzNDc3Ng==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458234776", "bodyText": "What will this be used for? I don't see anything calling it in this commit.", "author": "rdblue", "createdAt": "2020-07-21T16:36:06Z", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcValueWriter.java", "diffHunk": "@@ -19,20 +19,28 @@\n \n package org.apache.iceberg.orc;\n \n-import java.io.IOException;\n-import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n \n-/**\n- * Write data value of a schema.\n- */\n public interface OrcValueWriter<T> {\n \n+  Class<T> getJavaClass();", "originalCommit": "09beacb25ffe95bd9208f94586553b22348e6e3d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODUxMDcyNQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r458510725", "bodyText": "It's used for reading the field from Record and casting the value to target class , see here: https://github.com/apache/iceberg/pull/1197/files#diff-69c0f1e45966d2eb49a315fe32734cf5R125.", "author": "openinx", "createdAt": "2020-07-22T03:22:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODIzNDc3Ng=="}], "type": "inlineReview"}, {"oid": "b4164d9c3c0f765e6beaaf1c4a5f68e0ac81fcd9", "url": "https://github.com/apache/iceberg/commit/b4164d9c3c0f765e6beaaf1c4a5f68e0ac81fcd9", "message": "Refactor the GenericOrcWriter by using OrcSchemaWithTypeVisitor#visit", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "e21eadeacebeaa0a9744bda186f411b650c31e43", "url": "https://github.com/apache/iceberg/commit/e21eadeacebeaa0a9744bda186f411b650c31e43", "message": "Refactor the OrcValueWriter to OrcRowWriter and refactor converter to OrcValueWriter.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "e88f6786fafd6993d057292b047354e37daf4c78", "url": "https://github.com/apache/iceberg/commit/e88f6786fafd6993d057292b047354e37daf4c78", "message": "Remove the `OrcValue` from all the writer names.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "afe95c7b73d8e5dfc9f82bf59e712d4fc2dc7a36", "url": "https://github.com/apache/iceberg/commit/afe95c7b73d8e5dfc9f82bf59e712d4fc2dc7a36", "message": "Addressing serveral minor issues", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "cd5580b2b62c1f70eef34749a3a6235f1d4bc11f", "url": "https://github.com/apache/iceberg/commit/cd5580b2b62c1f70eef34749a3a6235f1d4bc11f", "message": "Fixed broken unit tests.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "e29467aaa1644ea3ec33bc0fe4455e9a48bffda0", "url": "https://github.com/apache/iceberg/commit/e29467aaa1644ea3ec33bc0fe4455e9a48bffda0", "message": "Minor fix: rename the OrcFileAppender#newOrcValueWriter to OrcFileAppender#newOrcRowWriter.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "4fce2d06b9dce13b5759d0686c9cec5b0ab2c852", "url": "https://github.com/apache/iceberg/commit/4fce2d06b9dce13b5759d0686c9cec5b0ab2c852", "message": "align the class name of writers and use the iceberg types in switch-cases.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "c6db2270aee00e77226c210aa8b45ff97035d153", "url": "https://github.com/apache/iceberg/commit/c6db2270aee00e77226c210aa8b45ff97035d153", "message": "Fix checkstyle", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "a19855fc3f34944eabe5d79741297885573d0383", "url": "https://github.com/apache/iceberg/commit/a19855fc3f34944eabe5d79741297885573d0383", "message": "Addressing the comment.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "c147747e3a8e3193841c7a2a7bdafaf682083824", "url": "https://github.com/apache/iceberg/commit/c147747e3a8e3193841c7a2a7bdafaf682083824", "message": "return the OrcValueWriter with a type.", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "6b6bb4ecac273bfbadf8d3eebdb1fb7759149c30", "url": "https://github.com/apache/iceberg/commit/6b6bb4ecac273bfbadf8d3eebdb1fb7759149c30", "message": "typo", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "7a021e318ed089f5dac9143e97e48ab444e0af02", "url": "https://github.com/apache/iceberg/commit/7a021e318ed089f5dac9143e97e48ab444e0af02", "message": "Provide the parameter type for ListWriter and MapWriter", "committedDate": "2020-07-22T03:59:06Z", "type": "commit"}, {"oid": "c37a207308993219960a7867d26428a42a38f585", "url": "https://github.com/apache/iceberg/commit/c37a207308993219960a7867d26428a42a38f585", "message": "User the BiFunction instead of Function to reduce code changes.", "committedDate": "2020-07-22T03:59:07Z", "type": "commit"}, {"oid": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "url": "https://github.com/apache/iceberg/commit/d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "message": "Rebase and fix the broken unit tests.", "committedDate": "2020-07-22T04:01:51Z", "type": "commit"}, {"oid": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "url": "https://github.com/apache/iceberg/commit/d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "message": "Rebase and fix the broken unit tests.", "committedDate": "2020-07-22T04:01:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0MTMxOQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459241319", "bodyText": "nit: scala -> scale :)", "author": "rdsr", "createdAt": "2020-07-23T06:37:55Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.data.orc;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.charset.StandardCharsets;\n+import java.time.Instant;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.time.temporal.ChronoUnit;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.iceberg.orc.OrcValueWriter;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.orc.storage.common.type.HiveDecimal;\n+import org.apache.orc.storage.ql.exec.vector.BytesColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DecimalColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.DoubleColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.ListColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.LongColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.MapColumnVector;\n+import org.apache.orc.storage.ql.exec.vector.TimestampColumnVector;\n+\n+public class GenericOrcWriters {\n+  private static final OffsetDateTime EPOCH = Instant.ofEpochSecond(0).atOffset(ZoneOffset.UTC);\n+  private static final LocalDate EPOCH_DAY = EPOCH.toLocalDate();\n+\n+  private GenericOrcWriters() {\n+  }\n+\n+  public static OrcValueWriter<Boolean> booleans() {\n+    return BooleanWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Integer> ints() {\n+    return IntWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalTime> times() {\n+    return TimeWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Long> longs() {\n+    return LongWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Float> floats() {\n+    return FloatWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<Double> doubles() {\n+    return DoubleWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<String> strings() {\n+    return StringWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<ByteBuffer> byteBuffers() {\n+    return ByteBufferWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<UUID> uuids() {\n+    return UUIDWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<byte[]> fixed() {\n+    return FixedWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDate> dates() {\n+    return DateWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<OffsetDateTime> timestampTz() {\n+    return TimestampTzWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<LocalDateTime> timestamp() {\n+    return TimestampWriter.INSTANCE;\n+  }\n+\n+  public static OrcValueWriter<BigDecimal> decimal(int scala, int precision) {\n+    if (precision <= 18) {\n+      return new Decimal18Writer(scala);", "originalCommit": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0Mzc2OA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459243768", "bodyText": "nit: non functional change", "author": "rdsr", "createdAt": "2020-07-23T06:44:39Z", "path": "orc/src/main/java/org/apache/iceberg/orc/ORC.java", "diffHunk": "@@ -143,7 +144,7 @@ private ReadBuilder(InputFile file) {\n     /**\n      * Restricts the read to the given range: [start, start + length).\n      *\n-     * @param newStart the start position for this read\n+     * @param newStart  the start position for this read", "originalCommit": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0ODYxMw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459248613", "bodyText": "Does it make sense to file a ticket to refactor SparkOrcWriter to utilize similar approach of using OrchSchemaVisitor. This could help with reuse of common generic writers defined in GenericOrcWriters, similar to how we did in SparkOrcReaders?\nWe can tackle this ticket in future PRs", "author": "rdsr", "createdAt": "2020-07-23T06:57:17Z", "path": "spark/src/main/java/org/apache/iceberg/spark/data/SparkOrcWriter.java", "diffHunk": "@@ -42,7 +42,7 @@\n  * This class acts as an adaptor from an OrcFileAppender to a\n  * FileAppender&lt;InternalRow&gt;.\n  */\n-public class SparkOrcWriter implements OrcValueWriter<InternalRow> {\n+public class SparkOrcWriter implements OrcRowWriter<InternalRow> {\n \n   private final Converter[] converters;", "originalCommit": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI3NDUyNw==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459274527", "bodyText": "Yeah, that's exactly what I plan to do.  I've filed an issue to address this thing. #1236.   Thanks for bringing it up.", "author": "openinx", "createdAt": "2020-07-23T07:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0ODYxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTYyODY4OQ==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459628689", "bodyText": "Thankyou!", "author": "rdsr", "createdAt": "2020-07-23T17:57:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0ODYxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTI0OTI2OA==", "url": "https://github.com/apache/iceberg/pull/1197#discussion_r459249268", "bodyText": "nit: maybe reword as ..\nwrites or appends  a row to Orc's VectorizedRowBatch", "author": "rdsr", "createdAt": "2020-07-23T06:58:54Z", "path": "orc/src/main/java/org/apache/iceberg/orc/OrcRowWriter.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.orc;\n+\n+import java.io.IOException;\n+import org.apache.orc.storage.ql.exec.vector.VectorizedRowBatch;\n+\n+/**\n+ * Write data value of a schema.", "originalCommit": "d5fcd984ba84ae678e3e12060da092b6c6aaf05b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "e820e89183a7b812b3f1899e7cf3128d7228c59f", "url": "https://github.com/apache/iceberg/commit/e820e89183a7b812b3f1899e7cf3128d7228c59f", "message": "Addressing the comments from rdsr", "committedDate": "2020-07-23T07:58:36Z", "type": "commit"}]}