{"pr_number": 1767, "pr_title": "AWS: Add progressive multipart upload to S3FileIO", "pr_createdAt": "2020-11-13T18:23:49Z", "pr_url": "https://github.com/apache/iceberg/pull/1767", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NDgzNA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523154834", "bodyText": "In #1754 I chose to directly store the necessary arguments instead of storing the entire property map. Not sure which way is better, what do you think?", "author": "jackye1995", "createdAt": "2020-11-13T18:43:48Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/BaseS3File.java", "diffHunk": "@@ -29,10 +31,18 @@\n   private final S3Client client;\n   private final S3URI uri;\n   private HeadObjectResponse metadata;\n+  private Map<String, String> properties;\n \n   BaseS3File(S3Client client, S3URI uri) {\n     this.client = client;\n     this.uri = uri;\n+    this.properties = Collections.emptyMap();", "originalCommit": "c805102c1e886633f2e5e8a33a616a1f359090ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE5NjAwMA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523196000", "bodyText": "I actually like what you have with the AwsCatalogProperties, but maybe we could use that as utility for translating to/from the catalog properties.  I feel like there are going to be a lot of these configs so we probably don't want to pull each one out individually.  The catalog properties are also hard to deal with becuase it's just Map<String,String> which makes defaults/casting harder.  I'm thinking of something like awsCatalogProperties = AwsCatalogProperties.from(properties) and then we can just use awsCatalogProperties.multipartSize() (for example).", "author": "danielcweeks", "createdAt": "2020-11-13T20:07:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NDgzNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDA1NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r526294054", "bodyText": "I've converted these over to using AwsProperties", "author": "danielcweeks", "createdAt": "2020-11-18T17:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NDgzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NTcyNQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523155725", "bodyText": "Also in #1754 and #1633 I created a file AwsCatalogProperties to centralize all the properties, we can move all the properties there. I am not sure if . is allowed character in Spark and Flink catalog config property, let me verify that and come back later.", "author": "jackye1995", "createdAt": "2020-11-13T18:45:34Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,83 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadResponse;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.size\";", "originalCommit": "c805102c1e886633f2e5e8a33a616a1f359090ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NDI2Nw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r526294267", "bodyText": "Converted to use AwsProperties", "author": "danielcweeks", "createdAt": "2020-11-18T17:42:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE1NTcyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MDMzNQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523160335", "bodyText": "Looks like multiPartThresholdFactor is only used once in write with multiPartSize * multiPartThresholdFactor, so why not just use a configuration multiPartThresholdSize that defines the actual threshold to start multipart upload?", "author": "jackye1995", "createdAt": "2020-11-13T18:54:28Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,83 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Future;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadResponse;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;", "originalCommit": "c805102c1e886633f2e5e8a33a616a1f359090ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE5MTk5NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523191994", "bodyText": "I think I went with factor because it makes it easier to tune the part size.  If you want smaller (or larger) part sizes, you need to adjust the part size and make sure the threshold size is appropriately sized.  With the factor, you just need to adjust the part size and the default factor will typically work well.  (Though now that you point this out we probably need more validation either way).", "author": "danielcweeks", "createdAt": "2020-11-13T19:58:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MDMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5NjcwMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r526296702", "bodyText": "I updated the code to use a threshold size, but still expose the factor as the user facing property to adjust.", "author": "danielcweeks", "createdAt": "2020-11-18T17:46:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE2MDMzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523182442", "bodyText": "Can we also delete the temp file progressively instead of deleting everything after close() is called?", "author": "jackye1995", "createdAt": "2020-11-13T19:39:03Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,18 +168,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n-      s3.putObject(\n-          PutObjectRequest.builder().bucket(location.bucket()).key(location.key()).build(),\n-          RequestBody.fromFile(stagingFile));\n+      completeUploads();\n     } finally {\n-      if (!stagingFile.delete()) {\n-        LOG.warn(\"Could not delete temporary file: {}\", stagingFile);\n+      stagingFiles.forEach(f -> {", "originalCommit": "c805102c1e886633f2e5e8a33a616a1f359090ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4ODEyMQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523188121", "bodyText": "Good catch! That was the intent and I think at some point I had it in the async upload task.  Should be easy to add back there after the upload is successful.", "author": "danielcweeks", "createdAt": "2020-11-13T19:50:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NzMxNg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523287316", "bodyText": "Also, we can add reliability by using Tasks.foreach. That would allow retries, failure logging, and deleting each file even if another has failed.", "author": "rdblue", "createdAt": "2020-11-13T23:43:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4NjI4NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523786284", "bodyText": "Yeah, let me try switching over to Tasks.  I was having trouble with figuring out how to properly handle the exception cases with CompletableFuture without swallowing the failures.", "author": "danielcweeks", "createdAt": "2020-11-15T17:13:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1MjEyOA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524452128", "bodyText": "@rdblue looking at Tasks.foreach, I'm not sure how well it fits in this case.  It looks like Tasks assumes that you know the full units of work and will submit them in parallel and block.  In this case, we are progressively adding more work and then need to block across all of those at the end to complete the multipart upload.  Now, I could wrap the Tasks.foreach call in future, but that seems like we're just wrapping one async framework with another.\nI think we can handle the concerns of deleting the files, aborting the upload, and logging with CompletableFuture framework and retries should be left to the S3Client (trying to avoid retries on top of retries).\nThoughts?", "author": "danielcweeks", "createdAt": "2020-11-16T17:38:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1ODE4NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524458184", "bodyText": "Are you saying that stagingFiles is changing as this loops through it?\nI think that when we try to revert the uploads, we should know that no more uploads are happening concurrently so that the tasks can run. Maybe I'm misunderstanding your point though?", "author": "rdblue", "createdAt": "2020-11-16T17:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDUxNjU0Mw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524516543", "bodyText": "I think I'm the one who got confused here.  I thought you were suggesting to move a different section to using Tasks.  For the actual cleanup, Tasks will probably work.  I'll take another look.", "author": "danielcweeks", "createdAt": "2020-11-16T19:24:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI5ODY5NQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r526298695", "bodyText": "Ok, so there are actually two areas to resolve here (which was my original confusion).\n\nDelete the staging file when an upload is success full, which I added\nConvert the cleanup during abort to use Tasks framework.", "author": "danielcweeks", "createdAt": "2020-11-18T17:49:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MjQ0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523183195", "bodyText": "If we want to make it async, can we directly leverage S3AsyncClient?", "author": "jackye1995", "createdAt": "2020-11-13T19:40:28Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,18 +168,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n-      s3.putObject(\n-          PutObjectRequest.builder().bucket(location.bucket()).key(location.key()).build(),\n-          RequestBody.fromFile(stagingFile));\n+      completeUploads();\n     } finally {\n-      if (!stagingFile.delete()) {\n-        LOG.warn(\"Could not delete temporary file: {}\", stagingFile);\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(", "originalCommit": "c805102c1e886633f2e5e8a33a616a1f359090ce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4OTAwNg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523189006", "bodyText": "That's a really good question.  Since it's the entire client that changes (right?) this would also require changes in the input stream.  Someone on the original thread commented they had problems with the async implementation, so I had steered away from it.", "author": "danielcweeks", "createdAt": "2020-11-13T19:52:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI3NDgwMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523274802", "bodyText": "Which original thread do you mean? Do you have a link?", "author": "jackye1995", "createdAt": "2020-11-13T22:56:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzMyNQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523707325", "bodyText": "my team uses the StsAssumeRoleCredentialsProvider to keep a fresh session when running on prem. right now this doesn't support StsAsyncClient and only supports StsClient.\nAlso the aws sdk provided nio client doesn't look like it supports proxies with basic auth (needed for us to leave our dc). The CRT one does, but it seems like it's still young?\nhttps://github.com/aws/aws-sdk-java-v2/blob/master/services/sts/src/main/java/software/amazon/awssdk/services/sts/auth/StsAssumeRoleCredentialsProvider.java", "author": "johnclara", "createdAt": "2020-11-15T04:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4NzIwMw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523787203", "bodyText": "@jackye1995 the comment I was referring to about the async client was this: #1573 (comment)\nOverall, I'm not sure we benefit much from the async client in this case because everything around is still synchronous.  I feel like if we move to a vectored-io implementation for read, that might be a better time to explore switching to async.", "author": "danielcweeks", "createdAt": "2020-11-15T17:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU2MjcxOA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524562718", "bodyText": "Yeah I know there are some limitations right now for the NioNettyAsyncHttpClient. In this case let's just use the synchronous client then. Using Tasks suggested by Ryan sounds like a good idea.", "author": "jackye1995", "createdAt": "2020-11-16T20:48:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzE4MzE5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4MTQzMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523281432", "bodyText": "If this check were done outside of synchronized as well, then we wouldn't need to get the lock each time a file is opened. There isn't a huge chance of lock contention, but it is always nice to avoid locking when possible.", "author": "rdblue", "createdAt": "2020-11-13T23:19:54Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,105 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.stream.Collectors;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String UPLOAD_POOL_SIZE  = \"s3fileio.multipart.num-threads\";\n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.part.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int DEFAULT_UPLOAD_WORKER_POOL_SIZE = Runtime.getRuntime().availableProcessors();\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n+  private static ExecutorService executorService;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;\n \n+  private long pos = 0;\n   private boolean closed = false;\n \n-  S3OutputStream(S3Client s3, S3URI location) throws IOException {\n+  S3OutputStream(Map<String, String> properties, S3Client s3, S3URI location) throws IOException {\n+    synchronized (this) {\n+      if (executorService == null) {", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4NzQxNA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523787414", "bodyText": "I'll add the double-check, but like you said, this really isn't a critical section (and I believe the the stack grab we do below is probably much more expensive and likely to cause some sort of locking).", "author": "danielcweeks", "createdAt": "2020-11-15T17:23:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4MTQzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4MTcwMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523281702", "bodyText": "Iceberg has PropertyUtil.propertyAsInt for this situation.", "author": "rdblue", "createdAt": "2020-11-13T23:20:51Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -19,42 +19,105 @@\n \n package org.apache.iceberg.aws.s3;\n \n+import java.io.BufferedInputStream;\n import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n import java.io.File;\n+import java.io.FileInputStream;\n import java.io.FileOutputStream;\n import java.io.IOException;\n-import java.io.OutputStream;\n+import java.io.InputStream;\n+import java.io.SequenceInputStream;\n+import java.io.UncheckedIOException;\n import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadPoolExecutor;\n+import java.util.stream.Collectors;\n import org.apache.iceberg.io.PositionOutputStream;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Predicates;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.io.CountingOutputStream;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.services.s3.S3Client;\n+import software.amazon.awssdk.services.s3.model.AbortMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadRequest;\n+import software.amazon.awssdk.services.s3.model.CompletedMultipartUpload;\n+import software.amazon.awssdk.services.s3.model.CompletedPart;\n+import software.amazon.awssdk.services.s3.model.CreateMultipartUploadRequest;\n import software.amazon.awssdk.services.s3.model.PutObjectRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartRequest;\n+import software.amazon.awssdk.services.s3.model.UploadPartResponse;\n \n class S3OutputStream extends PositionOutputStream {\n   private static final Logger LOG = LoggerFactory.getLogger(S3OutputStream.class);\n \n+  static final String UPLOAD_POOL_SIZE  = \"s3fileio.multipart.num-threads\";\n+  static final String MULTIPART_SIZE = \"s3fileio.multipart.part.size\";\n+  static final String MULTIPART_THRESHOLD_FACTOR = \"s3fileio.multipart.threshold\";\n+\n+  static final int DEFAULT_UPLOAD_WORKER_POOL_SIZE = Runtime.getRuntime().availableProcessors();\n+  static final int MIN_MULTIPART_UPLOAD_SIZE = 5 * 1024 * 1024;\n+  static final int DEFAULT_MULTIPART_SIZE = 32 * 1024 * 1024;\n+  static final double DEFAULT_MULTIPART_THRESHOLD = 1.5;\n+\n+  private static ExecutorService executorService;\n+\n   private final StackTraceElement[] createStack;\n   private final S3Client s3;\n   private final S3URI location;\n \n-  private final OutputStream stream;\n-  private final File stagingFile;\n-  private long pos = 0;\n+  private CountingOutputStream stream;\n+  private final List<File> stagingFiles = Lists.newArrayList();\n+  private File currentStagingFile;\n+  private String multipartUploadId;\n+  private final Map<File, CompletableFuture<CompletedPart>> multiPartMap = Maps.newHashMap();\n+  private final int multiPartSize;\n+  private final double multiPartThresholdFactor;\n \n+  private long pos = 0;\n   private boolean closed = false;\n \n-  S3OutputStream(S3Client s3, S3URI location) throws IOException {\n+  S3OutputStream(Map<String, String> properties, S3Client s3, S3URI location) throws IOException {\n+    synchronized (this) {\n+      if (executorService == null) {\n+        executorService = MoreExecutors.getExitingExecutorService(\n+            (ThreadPoolExecutor) Executors.newFixedThreadPool(\n+                Integer.parseInt(properties.getOrDefault(UPLOAD_POOL_SIZE,", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NTEwNg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523285106", "bodyText": "Nit: Could you move orElseGet to a new line?", "author": "rdblue", "createdAt": "2020-11-13T23:34:16Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(\n+              () -> {\n+                UploadPartResponse response = s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n+                return CompletedPart.builder().eTag(response.eTag()).partNumber(uploadRequest.partNumber()).build();\n+              },\n+              executorService\n+          );\n+\n+          multiPartMap.put(f, future);\n+        });\n+  }\n+\n+  private void completeMultiPartUpload() throws IOException {\n+    try {\n+      List<CompletedPart> completedParts =\n+          multiPartMap.values()\n+              .stream()\n+              .map(CompletableFuture::join)\n+              .sorted(Comparator.comparing(CompletedPart::partNumber))\n+              .collect(Collectors.toList());\n+\n+      s3.completeMultipartUpload(CompleteMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key())\n+          .uploadId(multipartUploadId)\n+          .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()).build());\n+    } catch (CompletionException e) {\n+      abortUpload();\n+      throw new IOException(\"Multipart upload failed for upload id: \" + multipartUploadId, e);\n+    }\n+  }\n+\n+  private void abortUpload() {\n+    if (multipartUploadId != null) {\n+      s3.abortMultipartUpload(AbortMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key()).uploadId(multipartUploadId).build());\n+    }\n+  }\n+\n+  private void completeUploads() throws IOException {\n+    if (multipartUploadId == null) {\n+      long contentLength = stagingFiles.stream().mapToLong(File::length).sum();\n+      InputStream contentStream = new BufferedInputStream(stagingFiles.stream()\n+          .map(S3OutputStream::uncheckedInputStream)\n+          .reduce(SequenceInputStream::new).orElseGet(() -> new ByteArrayInputStream(new byte[0])));", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NzA4Ng==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523287086", "bodyText": "It looks like uploading the last part depends on the line above where currentStagingFile is set to null. Otherwise, the current part is ignored in uploadParts. This leads to slightly confusing behavior for completeUploads because it won't include the last part unless that assumption is met.\nWhat about making completeUploads ensure the current stream is closed and null using a closeCurrent method?", "author": "rdblue", "createdAt": "2020-11-13T23:42:05Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ2NzIyOA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524467228", "bodyText": "So, I changed the logic a bit to address this.  It now does not require the current file to be null, only requires that the stream is closed and I added a check to enforce that.", "author": "danielcweeks", "createdAt": "2020-11-16T18:01:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4NzA4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4ODE2Nw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523288167", "bodyText": "Here's another place where we can add reliability using Tasks. That has a callback for failure that can fail with its own exception, which will be suppressed instead of thrown in place of the CompletionException.", "author": "rdblue", "createdAt": "2020-11-13T23:46:56Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()\n+        .bucket(location.bucket()).key(location.key()).build()).uploadId();\n+  }\n+\n+  private void uploadParts() {\n+    // exit if multipart has not been initiated\n+    if (multipartUploadId == null) {\n+      return;\n+    }\n+\n+    stagingFiles.stream()\n+        // do not upload the file currently being written\n+        .filter(f -> currentStagingFile == null || !currentStagingFile.equals(f))\n+        // do not upload any files that have already been processed\n+        .filter(Predicates.not(multiPartMap::containsKey))\n+        .forEach(f -> {\n+          UploadPartRequest uploadRequest = UploadPartRequest.builder()\n+              .bucket(location.bucket())\n+              .key(location.key())\n+              .uploadId(multipartUploadId)\n+              .partNumber(stagingFiles.indexOf(f) + 1)\n+              .contentLength(f.length())\n+              .build();\n+\n+          CompletableFuture<CompletedPart> future = CompletableFuture.supplyAsync(\n+              () -> {\n+                UploadPartResponse response = s3.uploadPart(uploadRequest, RequestBody.fromFile(f));\n+                return CompletedPart.builder().eTag(response.eTag()).partNumber(uploadRequest.partNumber()).build();\n+              },\n+              executorService\n+          );\n+\n+          multiPartMap.put(f, future);\n+        });\n+  }\n+\n+  private void completeMultiPartUpload() throws IOException {\n+    try {\n+      List<CompletedPart> completedParts =\n+          multiPartMap.values()\n+              .stream()\n+              .map(CompletableFuture::join)\n+              .sorted(Comparator.comparing(CompletedPart::partNumber))\n+              .collect(Collectors.toList());\n+\n+      s3.completeMultipartUpload(CompleteMultipartUploadRequest.builder()\n+          .bucket(location.bucket()).key(location.key())\n+          .uploadId(multipartUploadId)\n+          .multipartUpload(CompletedMultipartUpload.builder().parts(completedParts).build()).build());\n+    } catch (CompletionException e) {\n+      abortUpload();\n+      throw new IOException(\"Multipart upload failed for upload id: \" + multipartUploadId, e);\n+    }", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4ODYxNQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523288615", "bodyText": "When this is used, the class will never switch to progressive upload. Is that intentional?", "author": "rdblue", "createdAt": "2020-11-13T23:48:59Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4ODE1Mw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523788153", "bodyText": "Yeah, that was just an oversight, it should initiate as well.", "author": "danielcweeks", "createdAt": "2020-11-15T17:30:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzI4ODYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQwMDkwMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523400902", "bodyText": "maybe also try to initialize the multipart upload up here so that the individual parts can get kicked off immediately during the loop? or at the beginning of the method?\n(This loop's uploadParts currently returns immediately if this is before initialization right?)", "author": "johnclara", "createdAt": "2020-11-14T09:52:10Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }\n+\n     stream.write(b);\n     pos += 1;\n   }\n \n   @Override\n   public void write(byte[] b, int off, int len) throws IOException {\n-    stream.write(b, off, len);\n+    int remaining = len;\n+    int relativeOffset = off;\n+\n+    // Write the remainder of the part size to the staging file\n+    // and continue to write new staging files if the write is\n+    // larger than the part size.\n+    while (stream.getCount() + remaining > multiPartSize) {\n+      int writeSize = multiPartSize - (int) stream.getCount();", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4ODk2NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523788964", "bodyText": "I feel like this is really unlikely to be an issue.  The loop is mostly for completeness, but for you to hit a second iteration, the write size would have to be 2x the multipart size and given this is internally wrapped in a buffered output stream, the part size would need to be less than 4K (less than half the output buffer size).\nI think your right that it wouldn't start uploading parts until after the loop, but I just don't feel like it's worth the extra complexity.", "author": "danielcweeks", "createdAt": "2020-11-15T17:36:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQwMDkwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgwOTIwOA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523809208", "bodyText": "Makes sense! (Is there a way to mark a comment as resolved?)", "author": "johnclara", "createdAt": "2020-11-15T20:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzQwMDkwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523707940", "bodyText": "s3a supports canned acls for s3 requests, not sure if this should also support them?\nMy team had to use them once for writing from within account A to a bucket owned by bucket B\nWe also had to add it to rdblue/s3committer:\nhttps://github.com/rdblue/s3committer/blob/master/src/main/java/com/netflix/bdp/s3/S3Util.java#L73\n    InitiateMultipartUploadRequest initiateMultipartUploadRequest = new InitiateMultipartUploadRequest(bucket, key)\n          .withCannedACL(CannedAccessControlList.BucketOwnerFullControl);\n    InitiateMultipartUploadResult initiate = client.initiateMultipartUpload(initiateMultipartUploadRequest);", "author": "johnclara", "createdAt": "2020-11-15T04:55:51Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -87,17 +190,105 @@ public void close() throws IOException {\n \n     super.close();\n     closed = true;\n+    currentStagingFile = null;\n \n     try {\n       stream.close();\n \n+      completeUploads();\n+    } finally {\n+      stagingFiles.forEach(f -> {\n+        if (f.exists() && !f.delete()) {\n+          LOG.warn(\"Could not delete temporary file: {}\", f);\n+        }\n+      });\n+    }\n+  }\n+\n+  private void initializeMultiPartUpload() {\n+    multipartUploadId = s3.createMultipartUpload(CreateMultipartUploadRequest.builder()", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzc4OTE4OQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523789189", "bodyText": "Interesting, I didn't even know about canned acls.  Looks like the builder does support it (via .acl()), so that's probably  another thing we can plumb through.", "author": "danielcweeks", "createdAt": "2020-11-15T17:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgwOTc5Nw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523809797", "bodyText": "For sure, it should probably go in another PR.\nShould I file an issue? Or is the issue area mainly for items people are currently working on a PR for? I probably won't work on that any time soon.\n(We only set canned acls to backport iceberg tables from one account to hive tables in a legacy account, so my team doesn't need this in order to move onto S3FileIO)", "author": "johnclara", "createdAt": "2020-11-15T20:32:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1NDkyNg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524454926", "bodyText": "Yes I think canned ACL is a feature for another PR, and I was planning to do the PR after #1754 , let's wait until that is checked in so that we have a stable interface for adding new things.", "author": "jackye1995", "createdAt": "2020-11-16T17:42:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1NTc5NA==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524455794", "bodyText": "I'd say wait until you feel like you need it and then file an issue.  Filing the issue doesn't mean you have to work on it, but this feels like one of the many features of the client that may not be used very much.  It should be trivial to add, so if you find that you do need it, we can easily add it then.", "author": "danielcweeks", "createdAt": "2020-11-16T17:43:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ4NjYwMw==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524486603", "bodyText": "Yes. This is one of the things I planned to port, I am fine either me or @johnclara do it.", "author": "jackye1995", "createdAt": "2020-11-16T18:33:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYyMDEyNg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524620126", "bodyText": "@jackye1995 if you were already planning on it, go ahead! :) my team doesn't need it any time soon but it would be good for people who run into a similar issue", "author": "johnclara", "createdAt": "2020-11-16T21:42:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzcwNzk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgxMTU1NQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r523811555", "bodyText": "Not sure if the staging directory should be configurable at some point (not this PR)?\nFor our clients in docker containers, we've been thinking about moving s3a's staging area to be under a mount optimized for IO.\nWe haven't done any tests to see if this gives us any noticeable performance gains.", "author": "johnclara", "createdAt": "2020-11-15T20:48:07Z", "path": "aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java", "diffHunk": "@@ -69,14 +132,54 @@ public void flush() throws IOException {\n \n   @Override\n   public void write(int b) throws IOException {\n+    if (stream.getCount() >= multiPartSize) {\n+      newStream();\n+      uploadParts();\n+    }\n+\n     stream.write(b);\n     pos += 1;\n   }\n \n   @Override\n   public void write(byte[] b, int off, int len) throws IOException {\n-    stream.write(b, off, len);\n+    int remaining = len;\n+    int relativeOffset = off;\n+\n+    // Write the remainder of the part size to the staging file\n+    // and continue to write new staging files if the write is\n+    // larger than the part size.\n+    while (stream.getCount() + remaining > multiPartSize) {\n+      int writeSize = multiPartSize - (int) stream.getCount();\n+\n+      stream.write(b, relativeOffset, writeSize);\n+      remaining -= writeSize;\n+      relativeOffset += writeSize;\n+\n+      newStream();\n+      uploadParts();\n+    }\n+\n+    stream.write(b, relativeOffset, remaining);\n     pos += len;\n+\n+    // switch to multipart upload\n+    if (multipartUploadId == null && pos >= multiPartSize * multiPartThresholdFactor) {\n+      initializeMultiPartUpload();\n+      uploadParts();\n+    }\n+  }\n+\n+  private void newStream() throws IOException {\n+    if (stream != null) {\n+      stream.close();\n+    }\n+\n+    currentStagingFile = File.createTempFile(\"s3fileio-\", \".tmp\");", "originalCommit": "ba3f8e5e5cdd37f6d78c7240c3b59da3b4fc7ac8", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1NTQ3MQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524455471", "bodyText": "I think you can achieve that by configuring an alternative temp directory when starting the JVM, is this an option for you?", "author": "jackye1995", "createdAt": "2020-11-16T17:43:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgxMTU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1ODEyMg==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524458122", "bodyText": "Yeah, this makes sense and should be easy to add once we have properties pushed all the way down from the catalog/tables (you'll see that mentioned about in #1767 (comment)).  You might get some savings by using st1 volumes for this specifically due to it being entirely sequential read and write, so that might be interesting to experiment with.", "author": "danielcweeks", "createdAt": "2020-11-16T17:47:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgxMTU1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ1ODMzOQ==", "url": "https://github.com/apache/iceberg/pull/1767#discussion_r524458339", "bodyText": "P.S. I would also suggest that as a follow on issue.", "author": "danielcweeks", "createdAt": "2020-11-16T17:47:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzgxMTU1NQ=="}], "type": "inlineReview"}, {"oid": "c6ecb61b33bed32da331f69b762295dc4a2a0db0", "url": "https://github.com/apache/iceberg/commit/c6ecb61b33bed32da331f69b762295dc4a2a0db0", "message": "Refactor setting encryption for requests", "committedDate": "2020-11-17T22:01:31Z", "type": "forcePushed"}, {"oid": "c2d164aea35c26c51099eec6d2bae47af72579ef", "url": "https://github.com/apache/iceberg/commit/c2d164aea35c26c51099eec6d2bae47af72579ef", "message": "AWS: Add progressive multipart upload to S3FileIO", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "1d6aa4ca26b9b3c4dbaa712cac64455b0f5813a6", "url": "https://github.com/apache/iceberg/commit/1d6aa4ca26b9b3c4dbaa712cac64455b0f5813a6", "message": "Fix test after rebase", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "a0f917a2d572f5af90c89b2ab100200514c9c9d1", "url": "https://github.com/apache/iceberg/commit/a0f917a2d572f5af90c89b2ab100200514c9c9d1", "message": "Simply the complete and ensure parts are in order", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "ef308aedabad38737c23e10cedf06557e5667c9b", "url": "https://github.com/apache/iceberg/commit/ef308aedabad38737c23e10cedf06557e5667c9b", "message": "Add sort to stream", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "1ba0d8b404d8b0940202bfd96cbd011aea8bdf05", "url": "https://github.com/apache/iceberg/commit/1ba0d8b404d8b0940202bfd96cbd011aea8bdf05", "message": "Checkstyle", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "f361ee77ea2d3c609ebc704733b2cc8ed18b3df1", "url": "https://github.com/apache/iceberg/commit/f361ee77ea2d3c609ebc704733b2cc8ed18b3df1", "message": "Add abort attempt back to complete", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "a5bc7a45a05148e8798f10ce38806bff4a4f5e89", "url": "https://github.com/apache/iceberg/commit/a5bc7a45a05148e8798f10ce38806bff4a4f5e89", "message": "Initialize only once", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "0a4f9b2af55f23554dd5b8cce75f4c6058536335", "url": "https://github.com/apache/iceberg/commit/0a4f9b2af55f23554dd5b8cce75f4c6058536335", "message": "Add executor service for async tasks per errorprone", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "89cbfb20db81470446af7f47a4fb10fc5b04ebfa", "url": "https://github.com/apache/iceberg/commit/89cbfb20db81470446af7f47a4fb10fc5b04ebfa", "message": "Address comments", "committedDate": "2020-11-20T20:41:05Z", "type": "commit"}, {"oid": "f97ee71c7058dc993789724ee2f40dbb3d76924b", "url": "https://github.com/apache/iceberg/commit/f97ee71c7058dc993789724ee2f40dbb3d76924b", "message": "Refactor setting encryption for requests", "committedDate": "2020-11-20T20:43:41Z", "type": "commit"}, {"oid": "c9b69cc5429f3824dedcf6db3cc1da60ef7e14d7", "url": "https://github.com/apache/iceberg/commit/c9b69cc5429f3824dedcf6db3cc1da60ef7e14d7", "message": "Fix defaults to no-arg AwsProperties", "committedDate": "2020-11-20T20:44:34Z", "type": "commit"}, {"oid": "cf117183a41028f82ebc8da9f4f6391e6b001d9b", "url": "https://github.com/apache/iceberg/commit/cf117183a41028f82ebc8da9f4f6391e6b001d9b", "message": "Address some failure cases and add more testing", "committedDate": "2020-11-20T20:46:14Z", "type": "commit"}, {"oid": "ac1aac22eadbf923c1ebbd486431ab779fe69c03", "url": "https://github.com/apache/iceberg/commit/ac1aac22eadbf923c1ebbd486431ab779fe69c03", "message": "Checkstyle", "committedDate": "2020-11-20T20:46:16Z", "type": "commit"}, {"oid": "ac1aac22eadbf923c1ebbd486431ab779fe69c03", "url": "https://github.com/apache/iceberg/commit/ac1aac22eadbf923c1ebbd486431ab779fe69c03", "message": "Checkstyle", "committedDate": "2020-11-20T20:46:16Z", "type": "forcePushed"}]}