{"pr_number": 1348, "pr_title": "Flink: Support table sink.", "pr_createdAt": "2020-08-17T12:58:06Z", "pr_url": "https://github.com/apache/iceberg/pull/1348", "timeline": [{"oid": "3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "url": "https://github.com/apache/iceberg/commit/3fb0ca0c9237ede8824851e7fd3900f0a33cb0d7", "message": "Introduce the catalog name.", "committedDate": "2020-08-20T13:41:51Z", "type": "forcePushed"}, {"oid": "948ea352e1e828f5f265b9a0ae602b73c62808c2", "url": "https://github.com/apache/iceberg/commit/948ea352e1e828f5f265b9a0ae602b73c62808c2", "message": "Introduce the catalog name.", "committedDate": "2020-08-20T14:24:28Z", "type": "forcePushed"}, {"oid": "4072210b430aad79a91de2103da516c9dc616912", "url": "https://github.com/apache/iceberg/commit/4072210b430aad79a91de2103da516c9dc616912", "message": "Rebase to IcebergFilesCommitter", "committedDate": "2020-08-25T10:35:16Z", "type": "forcePushed"}, {"oid": "0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "url": "https://github.com/apache/iceberg/commit/0345cd4d15679b3aea6afd4f8645fcd0aa12b498", "message": "Rebase to master", "committedDate": "2020-08-26T09:27:46Z", "type": "forcePushed"}, {"oid": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "url": "https://github.com/apache/iceberg/commit/0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "message": "Rebase to flink-committer", "committedDate": "2020-08-27T12:19:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgzMjE4NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r479832185", "bodyText": "Since we are already returning the DataStream, would it make sense to avoid the discarding sink and possibly let people stream the iceberg commit files instead? Like what if I wanted to also feed them into kafka?", "author": "kbendick", "createdAt": "2020-08-30T23:54:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -0,0 +1,192 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.sink;\n+\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeinfo.Types;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.streaming.api.functions.sink.DiscardingSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.flink.table.types.logical.LogicalType;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+public class FlinkSink {\n+\n+  private static final String ICEBERG_STREAM_WRITER_NAME = IcebergStreamWriter.class.getSimpleName();\n+  private static final String ICEBERG_FILES_COMMITTER_NAME = IcebergFilesCommitter.class.getSimpleName();\n+\n+  private FlinkSink() {\n+  }\n+\n+  public static Builder forRow(DataStream<Row> input) {\n+    return new Builder().forRow(input);\n+  }\n+\n+  public static Builder forRowData(DataStream<RowData> input) {\n+    return new Builder().forRowData(input);\n+  }\n+\n+  public static class Builder {\n+    private DataStream<Row> rowInput = null;\n+    private DataStream<RowData> rowDataInput = null;\n+    private TableLoader tableLoader;\n+    private Configuration hadoopConf;\n+    private Table table;\n+    private TableSchema tableSchema;\n+\n+    private Builder forRow(DataStream<Row> newRowInput) {\n+      this.rowInput = newRowInput;\n+      return this;\n+    }\n+\n+    private Builder forRowData(DataStream<RowData> newRowDataInput) {\n+      this.rowDataInput = newRowDataInput;\n+      return this;\n+    }\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder tableLoader(TableLoader newTableLoader) {\n+      this.tableLoader = newTableLoader;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(Configuration newHadoopConf) {\n+      this.hadoopConf = newHadoopConf;\n+      return this;\n+    }\n+\n+    public Builder tableSchema(TableSchema newTableSchema) {\n+      this.tableSchema = newTableSchema;\n+      return this;\n+    }\n+\n+    private DataStream<RowData> convert() {\n+      Preconditions.checkArgument(rowInput != null, \"The DataStream<Row> to convert shouldn't be null\");\n+\n+      RowType rowType;\n+      DataType[] fieldDataTypes;\n+      if (tableSchema != null) {\n+        rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+        fieldDataTypes = tableSchema.getFieldDataTypes();\n+      } else {\n+        rowType = FlinkSchemaUtil.convert(table.schema());\n+        fieldDataTypes = TypeConversions.fromLogicalToDataType(rowType.getChildren().toArray(new LogicalType[0]));\n+      }\n+\n+      DataFormatConverters.RowConverter rowConverter = new DataFormatConverters.RowConverter(fieldDataTypes);\n+\n+      return rowInput.map(rowConverter::toInternal, RowDataTypeInfo.of(rowType));\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public DataStreamSink<RowData> build() {\n+      Preconditions.checkArgument(rowInput != null || rowDataInput != null,\n+          \"Should initialize input DataStream first with DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkArgument(rowInput == null || rowDataInput == null,\n+          \"Could only initialize input DataStream with either DataStream<Row> or DataStream<RowData>\");\n+      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n+      Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n+      Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n+\n+      DataStream<RowData> inputStream = rowInput != null ? convert() : rowDataInput;\n+\n+      IcebergStreamWriter<RowData> streamWriter = createStreamWriter(table, tableSchema);\n+      IcebergFilesCommitter filesCommitter = new IcebergFilesCommitter(tableLoader, hadoopConf);\n+\n+      DataStream<Void> returnStream = inputStream\n+          .transform(ICEBERG_STREAM_WRITER_NAME, TypeInformation.of(DataFile.class), streamWriter)\n+          .setParallelism(inputStream.getParallelism())\n+          .transform(ICEBERG_FILES_COMMITTER_NAME, Types.VOID, filesCommitter)\n+          .setParallelism(1)\n+          .setMaxParallelism(1);\n+\n+      return returnStream.addSink(new DiscardingSink())\n+          .name(String.format(\"IcebergSink %s\", table.toString()))\n+          .setParallelism(1);", "originalCommit": "0caf973eaf4e66dbf9a185cc98dfcb3053fdd870", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTg1OTQ2Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r479859466", "bodyText": "You mean you want to feed the committed data files to kafka ?  Is that meaningful for users ?  It will be better to understand if we have such user cases I guess.\nSome context\nin the first sink version,  I made the IcebergFilesCommitter implemented the SinkFucntion, then we could chain the function by addSink directly,  while we found that it did not work for bounded stream because there was no interface/method to indicate that this stream is a bounded one, then we have no way to commit those data files into iceberg table when the stream has reached its end.  So we have to turn to AbstractStreamOperator  and implemented a BoundedOneInput interface.   Finally, int this version,  we will transform the data stream twice (the first one:  rowdata -> dataFiles, the second one: datafiles -> void), and finally add a discarding sink.", "author": "openinx", "createdAt": "2020-08-31T02:38:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTgzMjE4NQ=="}], "type": "inlineReview"}, {"oid": "afb913e94ca33c3cedea0a1fe0d7dc979748414d", "url": "https://github.com/apache/iceberg/commit/afb913e94ca33c3cedea0a1fe0d7dc979748414d", "message": "Flink: Support table sink.", "committedDate": "2020-08-31T01:58:13Z", "type": "forcePushed"}, {"oid": "4712393d6d4b862cf801875d8f206399c6662ec9", "url": "https://github.com/apache/iceberg/commit/4712393d6d4b862cf801875d8f206399c6662ec9", "message": "Minior fixes", "committedDate": "2020-08-31T11:23:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNDI2NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481624265", "bodyText": "I think it is better to just pass a table loader to sink, source and sink can reuse this loader creation function, just like in:\nhttps://github.com/apache/iceberg/pull/1293/files#diff-0ad7dfff9cfa32fbb760796d976fd650R61\nWhat do you think?", "author": "JingsongLi", "createdAt": "2020-09-02T03:47:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkTableFactory.java", "diffHunk": "@@ -0,0 +1,65 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.ObjectIdentifier;\n+import org.apache.flink.table.catalog.ObjectPath;\n+import org.apache.flink.table.catalog.exceptions.TableNotExistException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.factories.StreamTableSinkFactory;\n+import org.apache.flink.table.sinks.StreamTableSink;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+\n+public class FlinkTableFactory implements StreamTableSinkFactory<RowData> {\n+  private final FlinkCatalog catalog;\n+\n+  public FlinkTableFactory(FlinkCatalog catalog) {\n+    this.catalog = catalog;\n+  }\n+\n+  @Override\n+  public StreamTableSink<RowData> createTableSink(Context context) {\n+    ObjectIdentifier identifier = context.getObjectIdentifier();\n+    ObjectPath objectPath = new ObjectPath(identifier.getDatabaseName(), identifier.getObjectName());\n+    TableIdentifier icebergIdentifier = catalog.toIdentifier(objectPath);\n+    try {\n+      Table table = catalog.loadIcebergTable(objectPath);\n+      return new IcebergTableSink(icebergIdentifier, table,", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTc4Njg3NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481786875", "bodyText": "Make sense to me,  we also don't need to pass the icebergIdentifier  to IcebergTableSink, that makes code more simplier.", "author": "openinx", "createdAt": "2020-09-02T06:32:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNDI2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNTg2OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481625868", "bodyText": "This is a deprecated method, no one will call it, you can just return this.", "author": "JingsongLi", "createdAt": "2020-09-02T03:49:10Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {\n+  private final TableIdentifier tableIdentifier;\n+  private final Table table;\n+  private final CatalogLoader catalogLoader;\n+  private final TableSchema tableSchema;\n+  private final Configuration hadoopConf;\n+\n+  public IcebergTableSink(TableIdentifier tableIdentifier, Table table,\n+                          CatalogLoader catalogLoader, Configuration hadoopConf,\n+                          TableSchema tableSchema) {\n+    this.tableIdentifier = tableIdentifier;\n+    this.table = table;\n+    this.catalogLoader = catalogLoader;\n+    this.hadoopConf = hadoopConf;\n+    this.tableSchema = tableSchema;\n+  }\n+\n+  @Override\n+  public DataStreamSink<?> consumeDataStream(DataStream<RowData> dataStream) {\n+    return FlinkSink.forRowData(dataStream)\n+        .table(table)\n+        .tableLoader(TableLoader.fromCatalog(catalogLoader, tableIdentifier))\n+        .hadoopConf(hadoopConf)\n+        .tableSchema(tableSchema)\n+        .build();\n+  }\n+\n+  @Override\n+  public DataType getConsumedDataType() {\n+    return tableSchema.toRowDataType().bridgedTo(RowData.class);\n+  }\n+\n+  @Override\n+  public TableSchema getTableSchema() {\n+    return this.tableSchema;\n+  }\n+\n+  @Override\n+  public TableSink<RowData> configure(String[] fieldNames, TypeInformation<?>[] fieldTypes) {\n+    if (!Arrays.equals(tableSchema.getFieldNames(), fieldNames)) {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgwMzQ1OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481803458", "bodyText": "OK, I see.  will do .", "author": "openinx", "createdAt": "2020-09-02T06:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyNTg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODA3Nw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628077", "bodyText": "Can we use Parameterized for batch too?", "author": "JingsongLi", "createdAt": "2020-09-02T03:51:29Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgxNTUxMw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481815513", "bodyText": "That's a great idea, we could reuse almost all of the codes then.", "author": "openinx", "createdAt": "2020-09-02T07:10:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODA3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODg2NA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481628864", "bodyText": "Looks like there is no dynamic table options. (Table hints)", "author": "JingsongLi", "createdAt": "2020-09-02T03:52:11Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTg2ODEzMw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481868133", "bodyText": "OK, it could be removed now.", "author": "openinx", "createdAt": "2020-09-02T08:11:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyODg2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYyOTc3NA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481629774", "bodyText": "Can we use TableEnvironment.fromValues?", "author": "JingsongLi", "createdAt": "2020-09-02T03:53:07Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzMzIzOA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481633238", "bodyText": "You can just add a method like:\n  def execInsertSqlAndWaitResult(tEnv: TableEnvironment, insert: String): JobExecutionResult = {\n    tEnv.executeSql(insert).getJobClient.get\n      .getJobExecutionResult(Thread.currentThread.getContextClassLoader)\n      .get\n  }", "author": "JingsongLi", "createdAt": "2020-09-02T03:56:40Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,215 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.streaming.util.FiniteTestSource;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.Pair;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.flink.table.api.Expressions.$;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {\n+  private static final Configuration CONF = new Configuration();\n+  private static final DataFormatConverters.RowConverter CONVERTER = new DataFormatConverters.RowConverter(\n+      SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+\n+  private static final String TABLE_NAME = \"flink_table\";\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+  private String tablePath;\n+  private String warehouse;\n+  private Map<String, String> properties;\n+  private Catalog catalog;\n+  private StreamExecutionEnvironment env;\n+  private StreamTableEnvironment tEnv;\n+\n+  private final FileFormat format;\n+  private final int parallelism;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, parallelism={2}\")\n+  public static Iterable<Object[]> data() {\n+    return Arrays.asList(\n+        new Object[] {\"avro\", 1},\n+        new Object[] {\"avro\", 2},\n+        new Object[] {\"orc\", 1},\n+        new Object[] {\"orc\", 2},\n+        new Object[] {\"parquet\", 1},\n+        new Object[] {\"parquet\", 2}\n+    );\n+  }\n+\n+  public TestFlinkTableSink(String format, int parallelism) {\n+    this.format = FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));\n+    this.parallelism = parallelism;\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File folder = tempFolder.newFolder();\n+    warehouse = folder.getAbsolutePath();\n+\n+    tablePath = warehouse.concat(\"/default/\").concat(TABLE_NAME);\n+    Assert.assertTrue(\"Should create the table path correctly.\", new File(tablePath).mkdirs());\n+\n+    properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    catalog = new HadoopCatalog(CONF, warehouse);\n+\n+    env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+    env.enableCheckpointing(400);\n+    env.setParallelism(parallelism);\n+\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .useBlinkPlanner()\n+        .inStreamingMode()\n+        .build();\n+    tEnv = StreamTableEnvironment.create(env, settings);\n+    tEnv.executeSql(String.format(\"create catalog iceberg_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    tEnv.executeSql(\"use catalog iceberg_catalog\");\n+    tEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    catalog.createTable(TableIdentifier.parse(\"default.\" + TABLE_NAME),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+  }\n+\n+  private DataStream<RowData> generateInputStream(List<Row> rows) {\n+    TypeInformation<Row> typeInformation = new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+    return env.addSource(new FiniteTestSource<>(rows), typeInformation)\n+        .map(CONVERTER::toInternal, RowDataTypeInfo.of(SimpleDataUtil.ROW_TYPE));\n+  }\n+\n+  private Pair<List<Row>, List<Record>> generateData() {\n+    String[] worlds = new String[] {\"hello\", \"world\", \"foo\", \"bar\", \"apache\", \"foundation\"};\n+    List<Row> rows = Lists.newArrayList();\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < worlds.length; i++) {\n+      rows.add(Row.of(i + 1, worlds[i]));\n+      Record record = SimpleDataUtil.createRecord(i + 1, worlds[i]);\n+      expected.add(record);\n+      expected.add(record);\n+    }\n+    return Pair.of(rows, expected);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    Pair<List<Row>, List<Record>> data = generateData();\n+    List<Row> rows = data.first();\n+    List<Record> expected = data.second();\n+    DataStream<RowData> stream = generateInputStream(rows);\n+\n+    // Register the rows into a temporary table named 'sourceTable'.\n+    tEnv.createTemporaryView(\"sourceTable\", tEnv.fromDataStream(stream, $(\"id\"), $(\"data\")));\n+\n+    // Redirect the records from source table to destination table.\n+    String insertSQL = String.format(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+    TableResult result = tEnv.executeSql(insertSQL);\n+    waitComplete(result);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(tablePath, expected);\n+  }\n+\n+  @Test\n+  public void testBatchSQL() throws Exception {\n+    EnvironmentSettings settings = EnvironmentSettings\n+        .newInstance()\n+        .inBatchMode()\n+        .useBlinkPlanner()\n+        .build();\n+    TableEnvironment batchEnv = TableEnvironment.create(settings);\n+    batchEnv.executeSql(String.format(\"create catalog batch_catalog with (\" +\n+        \"'type'='iceberg', 'catalog-type'='hadoop', 'warehouse'='%s')\", warehouse));\n+    batchEnv.executeSql(\"use catalog batch_catalog\");\n+    batchEnv.getConfig().getConfiguration().set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n+\n+    // Create source table.\n+    catalog.createTable(TableIdentifier.parse(\"default.sourceTable\"),\n+        SimpleDataUtil.SCHEMA,\n+        PartitionSpec.unpartitioned(),\n+        properties);\n+\n+    TableResult result;\n+    String[] words = new String[] {\"hello\", \"world\", \"apache\"};\n+    List<Record> expected = Lists.newArrayList();\n+    for (int i = 0; i < words.length; i++) {\n+      expected.add(SimpleDataUtil.createRecord(i, words[i]));\n+      result = batchEnv.executeSql(String.format(\"INSERT INTO sourceTable SELECT %d, '%s'\", i, words[i]));\n+      waitComplete(result);\n+    }\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(warehouse.concat(\"/default/sourceTable\"), expected);\n+  }\n+\n+  private static void waitComplete(TableResult result) {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzNjM4Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481636386", "bodyText": "We can add TODO for these interfaces:\nImplement OverwritableTableSink, so in the Flink SQL, user can write these SQLs:\nINSERT OVERWRITE t ...\nImplement PartitionableTableSink, user can write:\nINSERT OVERWRITE/INTO t PARTITION(...)", "author": "JingsongLi", "createdAt": "2020-09-02T03:59:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.sinks.AppendStreamTableSink;\n+import org.apache.flink.table.sinks.TableSink;\n+import org.apache.flink.table.types.DataType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.flink.sink.FlinkSink;\n+\n+public class IcebergTableSink implements AppendStreamTableSink<RowData> {", "originalCommit": "82f798700f2cb4c0b11ced1f6ec6e4d7fa5af141", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTgwMDk3NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481800975", "bodyText": "Thanks for the remainding, before we add the TODO comment, I will try to implement those two interfaces in the next path.", "author": "openinx", "createdAt": "2020-09-02T06:51:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTYzNjM4Ng=="}], "type": "inlineReview"}, {"oid": "ea63017d5efeb3964192c67903abb502ed53c1d2", "url": "https://github.com/apache/iceberg/commit/ea63017d5efeb3964192c67903abb502ed53c1d2", "message": "Add TODO to implement PartitionedTableSink", "committedDate": "2020-09-02T11:19:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTk5OTMxMA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r481999310", "bodyText": "Think about this unit test again, we'd better to extend the FlinkCatalogTestBase  so that we could cover both hive and hadoop catalog cases.", "author": "openinx", "createdAt": "2020-09-02T11:32:15Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.concurrent.ExecutionException;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends AbstractTestBase {", "originalCommit": "ea63017d5efeb3964192c67903abb502ed53c1d2", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MjY4OA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482082688", "bodyText": "We could use flink DDL to create table here if #1393 get merged.", "author": "openinx", "createdAt": "2020-09-02T13:48:19Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog", "originalCommit": "d6e7c259184f19d6b611ebdfc19910335aa8bf3e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NDMzOQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482594339", "bodyText": "It was merged!", "author": "rdblue", "createdAt": "2020-09-02T23:22:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjA4MjY4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU4OTg4OQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482589889", "bodyText": "Minor: it would be nice to have more context here. Maybe the table loader should define a toString that could be used in the error message here.", "author": "rdblue", "createdAt": "2020-09-02T23:15:35Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java", "diffHunk": "@@ -155,16 +158,29 @@ public Builder tableSchema(TableSchema newTableSchema) {\n       return this;\n     }\n \n+    public Builder overwrite(boolean newOverwrite) {\n+      this.overwrite = newOverwrite;\n+      return this;\n+    }\n+\n     @SuppressWarnings(\"unchecked\")\n     public DataStreamSink<RowData> build() {\n       Preconditions.checkArgument(rowDataInput != null,\n           \"Please use forRowData() to initialize the input DataStream.\");\n-      Preconditions.checkNotNull(table, \"Table shouldn't be null\");\n       Preconditions.checkNotNull(tableLoader, \"Table loader shouldn't be null\");\n       Preconditions.checkNotNull(hadoopConf, \"Hadoop configuration shouldn't be null\");\n \n+      if (table == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          this.table = loader.loadTable();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(\"Failed to load iceberg table.\", e);", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY2MDU1MQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482660551", "bodyText": "Defining the toString sounds good to me.", "author": "openinx", "createdAt": "2020-09-03T02:25:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU4OTg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5MjAxMA==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592010", "bodyText": "I just want to note that we don't encourage the use of ReplacePartitions because the data it deletes is implicit. It is better to specify what data should be overwritten, like in the new API for Spark:\ndf.writeTo(\"iceberg.db.table\").overwrite($\"date\" === \"2020-09-01\")\nIf Flink's semantics are to replace partitions for overwrite, then it should be okay. But I highly recommend being more explicit about data replacement.", "author": "rdblue", "createdAt": "2020-09-02T23:18:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -164,16 +168,51 @@ private void commitUpToCheckpoint(long checkpointId) {\n       pendingDataFiles.addAll(dataFiles);\n     }\n \n-    AppendFiles appendFiles = table.newAppend();\n-    pendingDataFiles.forEach(appendFiles::appendFile);\n-    appendFiles.set(MAX_COMMITTED_CHECKPOINT_ID, Long.toString(checkpointId));\n-    appendFiles.set(FLINK_JOB_ID, flinkJobId);\n-    appendFiles.commit();\n+    if (replacePartitions) {\n+      replacePartitions(pendingDataFiles, checkpointId);\n+    } else {\n+      append(pendingDataFiles, checkpointId);\n+    }\n \n     // Clear the committed data files from dataFilesPerCheckpoint.\n     pendingFileMap.clear();\n   }\n \n+  private void replacePartitions(List<DataFile> dataFiles, long checkpointId) {\n+    ReplacePartitions dynamicOverwrite = table.newReplacePartitions();", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY4ODg5NQ==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482688895", "bodyText": "Yes, Flink's semantics are to replace partitions for overwrite, here is the 1, 2", "author": "openinx", "createdAt": "2020-09-03T03:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5MjAxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5Mjk3Ng==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482592976", "bodyText": "Can this be done automatically when a write completes, or is this a completely separate copy of the table?", "author": "rdblue", "createdAt": "2020-09-02T23:20:09Z", "path": "flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java", "diffHunk": "@@ -126,12 +126,16 @@ public static void assertTableRows(String tablePath, List<RowData> expected) thr\n     assertTableRecords(tablePath, expectedRecords);\n   }\n \n-  public static void assertTableRecords(String tablePath, List<Record> expected) throws IOException {\n-    Preconditions.checkArgument(expected != null, \"expected records shouldn't be null\");\n-    Table newTable = new HadoopTables().load(tablePath);\n-    try (CloseableIterable<Record> iterable = IcebergGenerics.read(newTable).build()) {\n+  public static void assertTableRecords(Table table, List<Record> expected) throws IOException {\n+    table.refresh();", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjY5MzY1Mw==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482693653", "bodyText": "Since we don't support to scan table by flink sql,   so we have to read records from iceberg table by iceberg Java API in unit tests.  In this test,  we get the icebergTable instance firstly, then the following test methods will commit the iceberg table by flink sql,  the icebergTable need a fresh to catch the latest changes.", "author": "openinx", "createdAt": "2020-09-03T04:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5Mjk3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjU5NDE5Mg==", "url": "https://github.com/apache/iceberg/pull/1348#discussion_r482594192", "bodyText": "It would be good to also have a partitioned test.", "author": "rdblue", "createdAt": "2020-09-02T23:21:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java", "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.TimeCharacteristic;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.api.Table;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.After;\n+import org.junit.Assume;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkTableSink extends FlinkCatalogTestBase {\n+  private static final String TABLE_NAME = \"test_table\";\n+  private TableEnvironment tEnv;\n+  private org.apache.iceberg.Table icebergTable;\n+\n+  private final FileFormat format;\n+  private final boolean isStreamingJob;\n+\n+  @Parameterized.Parameters(name = \"{index}: format={0}, isStreaming={1}, catalogName={2}, baseNamespace={3}\")\n+  public static Iterable<Object[]> parameters() {\n+    List<Object[]> parameters = Lists.newArrayList();\n+    for (FileFormat format : new FileFormat[] {FileFormat.ORC, FileFormat.AVRO, FileFormat.PARQUET}) {\n+      for (Boolean isStreaming : new Boolean[] {true, false}) {\n+        for (Object[] catalogParams : FlinkCatalogTestBase.parameters()) {\n+          String catalogName = (String) catalogParams[0];\n+          String[] baseNamespace = (String[]) catalogParams[1];\n+          parameters.add(new Object[] {format, isStreaming, catalogName, baseNamespace});\n+        }\n+      }\n+    }\n+    return parameters;\n+  }\n+\n+  public TestFlinkTableSink(FileFormat format, Boolean isStreamingJob, String catalogName, String[] baseNamespace) {\n+    super(catalogName, baseNamespace);\n+    this.format = format;\n+    this.isStreamingJob = isStreamingJob;\n+  }\n+\n+  @Override\n+  protected TableEnvironment getTableEnv() {\n+    if (tEnv == null) {\n+      synchronized (this) {\n+        EnvironmentSettings.Builder settingsBuilder = EnvironmentSettings\n+            .newInstance()\n+            .useBlinkPlanner();\n+        if (isStreamingJob) {\n+          settingsBuilder.inStreamingMode();\n+          StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+          env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n+          env.enableCheckpointing(400);\n+          tEnv = StreamTableEnvironment.create(env, settingsBuilder.build());\n+        } else {\n+          settingsBuilder.inBatchMode();\n+          tEnv = TableEnvironment.create(settingsBuilder.build());\n+        }\n+      }\n+    }\n+    return tEnv;\n+  }\n+\n+  @Before\n+  public void before() {\n+    sql(\"CREATE DATABASE %s\", flinkDatabase);\n+    sql(\"USE CATALOG %s\", catalogName);\n+    sql(\"USE %s\", DATABASE);\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+    this.icebergTable = validationCatalog\n+        .createTable(TableIdentifier.of(icebergNamespace, TABLE_NAME),\n+            SimpleDataUtil.SCHEMA,\n+            PartitionSpec.unpartitioned(),\n+            properties);\n+  }\n+\n+  @After\n+  public void clean() {\n+    sql(\"DROP TABLE IF EXISTS %s.%s\", flinkDatabase, TABLE_NAME);\n+    sql(\"DROP DATABASE IF EXISTS %s\", flinkDatabase);\n+  }\n+\n+  @Test\n+  public void testStreamSQL() throws Exception {\n+    // Register the rows into a temporary table.\n+    Table sourceTable = getTableEnv().fromValues(SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+        Expressions.row(1, \"hello\"),\n+        Expressions.row(2, \"world\"),\n+        Expressions.row(3, (String) null),\n+        Expressions.row(null, \"bar\")\n+    );\n+    getTableEnv().createTemporaryView(\"sourceTable\", sourceTable);\n+\n+    // Redirect the records from source table to destination table.\n+    sql(\"INSERT INTO %s SELECT id,data from sourceTable\", TABLE_NAME);\n+\n+    // Assert the table records as expected.\n+    SimpleDataUtil.assertTableRecords(icebergTable, Lists.newArrayList(\n+        SimpleDataUtil.createRecord(1, \"hello\"),\n+        SimpleDataUtil.createRecord(2, \"world\"),\n+        SimpleDataUtil.createRecord(3, null),\n+        SimpleDataUtil.createRecord(null, \"bar\")\n+    ));\n+  }\n+\n+  @Test\n+  public void testOverwriteTable() throws Exception {", "originalCommit": "2eec2057a685beab08d98a02efa46ed0eb86dfb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "7a328cf866a622a867201be07e01826b04e8960e", "url": "https://github.com/apache/iceberg/commit/7a328cf866a622a867201be07e01826b04e8960e", "message": "Flink: Support table sink.", "committedDate": "2020-09-03T01:48:32Z", "type": "commit"}, {"oid": "23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "url": "https://github.com/apache/iceberg/commit/23ddefb60ef49b5fbdc0917ed7fe32cd95c1224d", "message": "Minior fixes", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "f733b3e6c578632428d222b8d7a52621230e7459", "url": "https://github.com/apache/iceberg/commit/f733b3e6c578632428d222b8d7a52621230e7459", "message": "Remove the public modifier", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "url": "https://github.com/apache/iceberg/commit/17af5a15b1ae44bbd90005a8bbe86f8e23edeb4f", "message": "Pass the table loader rather than CatalogLoader to IcebergTabelSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "aa3326689f99238dd007603931db5a4e7262d151", "url": "https://github.com/apache/iceberg/commit/aa3326689f99238dd007603931db5a4e7262d151", "message": "Remove the implementation for IcebergTableSink#configure because it is deprecated.", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "f9760c31094f8b1e7f99c4d9220b6116748bb355", "url": "https://github.com/apache/iceberg/commit/f9760c31094f8b1e7f99c4d9220b6116748bb355", "message": "Refactor the unit tests.", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "5393428404f5ab8724381e6f85ad458cb70c9504", "url": "https://github.com/apache/iceberg/commit/5393428404f5ab8724381e6f85ad458cb70c9504", "message": "Create an executeSQLAndWaitResult to execute sql", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "url": "https://github.com/apache/iceberg/commit/9df390bc5d8fb7a344d335eb36a57df7abbaadc2", "message": "Implement the OverwritableTableSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "url": "https://github.com/apache/iceberg/commit/9215ced81fc5f0ba34bf19cc39b64201d740b6fe", "message": "Add TODO to implement PartitionedTableSink", "committedDate": "2020-09-03T01:49:30Z", "type": "commit"}, {"oid": "53a16d957035e970a6416ca6712972625a258a17", "url": "https://github.com/apache/iceberg/commit/53a16d957035e970a6416ca6712972625a258a17", "message": "Make the case extend the FlinkCatalogTestBase", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "01bc54d013dfec407fce9289cfebed2055ee6244", "url": "https://github.com/apache/iceberg/commit/01bc54d013dfec407fce9289cfebed2055ee6244", "message": "Fix the broken unit tests.", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "url": "https://github.com/apache/iceberg/commit/afb94a8e60ad3a91f973a2ba7b69e791e334d30d", "message": "Minor changes", "committedDate": "2020-09-03T01:51:25Z", "type": "commit"}, {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "message": "Addressing comments from Ryan", "committedDate": "2020-09-03T03:56:48Z", "type": "commit"}, {"oid": "7587cb1643d92759751be85517cd1844ff8937d5", "url": "https://github.com/apache/iceberg/commit/7587cb1643d92759751be85517cd1844ff8937d5", "message": "Addressing comments from Ryan", "committedDate": "2020-09-03T03:56:48Z", "type": "forcePushed"}, {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "message": "Minior changes.", "committedDate": "2020-09-03T04:27:33Z", "type": "commit"}, {"oid": "57ec91c154d3dc79daab77945f4b2724e37a7ef9", "url": "https://github.com/apache/iceberg/commit/57ec91c154d3dc79daab77945f4b2724e37a7ef9", "message": "Minior changes.", "committedDate": "2020-09-03T04:27:33Z", "type": "forcePushed"}]}