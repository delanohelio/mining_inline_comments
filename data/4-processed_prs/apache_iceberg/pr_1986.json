{"pr_number": 1986, "pr_title": "Spark: Add a rule to align updates in MERGE operations", "pr_createdAt": "2020-12-26T13:13:04Z", "pr_url": "https://github.com/apache/iceberg/pull/1986", "timeline": [{"oid": "84a4d5e619edabf8baf373a73ee6b00b710681e7", "url": "https://github.com/apache/iceberg/commit/84a4d5e619edabf8baf373a73ee6b00b710681e7", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2020-12-26T13:32:50Z", "type": "forcePushed"}, {"oid": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "url": "https://github.com/apache/iceberg/commit/54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2020-12-28T09:25:04Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjMwNQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549536305", "bodyText": "Can you be more specific about \"inspired by\"? Did you use the code from Delta as a reference? Did you use the same test cases?", "author": "rdblue", "createdAt": "2020-12-29T01:27:02Z", "path": "LICENSE", "diffHunk": "@@ -289,3 +289,12 @@ Copyright: 2011-2018 The Apache Software Foundation\n Home page: https://spark.apache.org/\n License: https://www.apache.org/licenses/LICENSE-2.0\n \n+--------------------------------------------------------------------------------\n+\n+This product includes code from Delta Lake.\n+\n+* AssignmentAlignmentSupport is an independent development but was inspired by an equivalent rule in Delta.", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYyNzQyNA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549627424", "bodyText": "It was used as a reference. I did not simply copy the code as is so I added \"inspired by\" to give credit to the original authors. The rules should be logically equivalent and should be part of Spark, not data sources.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjMwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MjEzNw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549642137", "bodyText": "Updated.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:55:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjMwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjY0NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549536644", "bodyText": "Nit: I would probably add a newline before this case since the last block was so large.", "author": "rdblue", "createdAt": "2020-12-29T01:29:16Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MTc1MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549641751", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:54:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNjY0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzU4NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549537584", "bodyText": "What about getInsertReference or asInsertReference? Seems like this is actually quite specific to the references that we expect in the left side of an INSERT clause.", "author": "rdblue", "createdAt": "2020-12-29T01:35:42Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")\n+        case (from, to) if requiresCast(from, to) => cast(expr, dataType)\n+        case _ => expr\n+      }\n+  }\n+\n+  private def requiresCast(from: DataType, to: DataType): Boolean = {\n+    !DataType.equalsIgnoreCaseAndNullability(from, to)\n+  }\n+\n+  private def isValidCast(from: StructType, to: StructType, resolver: Resolver): Boolean = {\n+    from.length == to.length && from.exists { f => to.exists(t => resolver(f.name, t.name))}\n+  }\n+\n+  protected def getNameParts(expr: Expression): Seq[String] = expr match {", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MDQzMA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549640430", "bodyText": "It is also used for UPDATE SET clauses too.\nWhat about asMultipartIdentifier? We can make it implicit if we want to.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4MjI1NQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549782255", "bodyText": "Or asAssignReference?", "author": "rdblue", "createdAt": "2020-12-29T17:13:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgzMTg4OA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549831888", "bodyText": "I'd go for asAssignmentReference and making the method implicit.", "author": "aokolnychyi", "createdAt": "2020-12-29T20:07:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzg4Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549537883", "bodyText": "Why add [ and ]? Those seem distracting to me. I'd also add a space after , to make it more readable.", "author": "rdblue", "createdAt": "2020-12-29T01:37:17Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYyOTU0OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549629549", "bodyText": "Let me fix it.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:16:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzg4Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MjE5MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549642191", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:55:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzNzg4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODU3OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549538579", "bodyText": "This assumes that the rewrite was needed, but we expect the MergeIntoTable plan to be resolved after this rule runs. As a result, this rule will always produce a new MergeIntoTable instance and fastEquals used by the rule executor will always compare the full tree.\nThis shouldn't affect correctness, but I think it is a best practice to detect whether anything has changed and avoid creating a new node if it is the same.", "author": "rdblue", "createdAt": "2020-12-29T01:42:17Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYyOTM4Nw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549629387", "bodyText": "Spark is going to run AlignMergeIntoTable only once after the resolution is done and fully successful (we inject this rule in post-hoc resolution phase and check that the plan is fully resolved before applying this logic).\nThe purpose of the rule is to align already resolved expressions and do it only once.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzODU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549539207", "bodyText": "This could avoid calling getNameParts by creating a map out of insertCols and assignments:\nval assignmentMap = insertCols.zip(assigments).toMap\n\nThat would be better since this is going to call getNameParts for every assignment until a matching one is found.", "author": "rdblue", "createdAt": "2020-12-29T01:46:19Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignments.find(a => resolver(targetAttr.name, getNameParts(a.key).head))", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzMTAxNQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549631015", "bodyText": "Good point.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:20:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzNTk4MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549635981", "bodyText": "I think a simple zip won't work since the assignments haven't been aligned yet. Plus, we have to use the resolver for equality so cannot have O(1) lookups.\nThat said, we can avoid the main problem of constantly calling getNameParts. Let me do that.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:36:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc5NTQ0MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549795441", "bodyText": "I thought insertCols was derived from assignments like assigments.map(getNameParts(_.key).head), so we know that they are equivalent. You're right about the resolver, though.", "author": "rdblue", "createdAt": "2020-12-29T17:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTgyNjY4OA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549826688", "bodyText": "Yeah, you are right. I can simplify this a bit further.", "author": "aokolnychyi", "createdAt": "2020-12-29T19:48:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3OTQ4NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550279484", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-30T18:02:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTUzOTIwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549540194", "bodyText": "Why not cast directly to the output column's data type?\nSeems like this should cast to the types from MergeIntoTable.output so that all of the assignments produce the same type. Are all of the assignment keys for a column guaranteed to have the same output type?\nAlso, if column resolution adds a projection with a cast to the a table output type, can these expressions be optimized to remove the duplicate cast? If not, we may want to cast directly to the table's type and update the output of the MergeIntoTable node to be identical to the table so column resolution is done here rather than in the normal write resolution rule.", "author": "rdblue", "createdAt": "2020-12-29T01:53:23Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              \"Duplicate column names inside INSERT clause: \" +\n+              duplicateColNames.mkString(\"[\", \",\", \"]\"))\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignments.find(a => resolver(targetAttr.name, getNameParts(a.key).head))\n+      if (assignment.isEmpty) {\n+        throw new AnalysisException(\n+          s\"Cannot find column '${targetAttr.name}' of the target table among \" +\n+          s\"the INSERT columns: ${insertCols.mkString(\", \")}. \" +\n+          \"INSERT clauses must provide values for all columns of the target table.\")\n+      }\n+\n+      val key = assignment.get.key\n+      val value = assignment.get.value\n+      Assignment(key, castIfNeeded(value, key.dataType, resolver))", "originalCommit": "54e5f1530d8b1aa3dd83be96dc812ee435a37e5c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTYzOTQ3OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549639479", "bodyText": "I think you are correct we should cast to targetAttr.dataType. The resolution should be done before this rule so we should be safe with castIfNeeded not adding anything extra if it is not needed.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:47:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MjMyNQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549642325", "bodyText": "Updated.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:56:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc5NjU4Ng==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549796586", "bodyText": "Good context that this is a post-hoc rule, but I think that this might introduce problems.\nWhere does the output of MergeIntoTable come from? Table resolution is only going to add projections, so I don't think we can cast directly to the table type, we need to cast to the expected output type. I think that should be the same?", "author": "rdblue", "createdAt": "2020-12-29T18:00:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3OTY4MA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550279680", "bodyText": "I am not sure I fully understood. Could you check whether the current implementation is similar to what you mean?", "author": "aokolnychyi", "createdAt": "2020-12-30T18:03:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI5MTk0OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550291949", "bodyText": "Will do.", "author": "rdblue", "createdAt": "2020-12-30T18:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU0MDE5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY0MTg3MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549641871", "bodyText": "Here is what I came up with, @rdblue.", "author": "aokolnychyi", "createdAt": "2020-12-29T09:55:04Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val namePartsSeq = assignments.map(_.key).map(getNameParts)\n+          namePartsSeq.foreach { nameParts =>\n+            if (nameParts.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${nameParts.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = namePartsSeq.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, colNames, assignments))\n+\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      insertCols: Seq[String],\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+\n+    // init a map of assignments to avoid calling getNameParts many times\n+    val assignmentMap = assignments.map(a => getNameParts(a.key).head -> a).toMap", "originalCommit": "2f0f1303fb0f96512736a553c1efe8c9e5b13552", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTY5Mzg1Mg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549693852", "bodyText": "This one is redundant, I'll remove it.", "author": "aokolnychyi", "createdAt": "2020-12-29T12:54:07Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,204 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  protected abstract Map<String, String> extraTableProperties();", "originalCommit": "2f0f1303fb0f96512736a553c1efe8c9e5b13552", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4Mzc3Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549783773", "bodyText": "I think would be a bit easier to read this code if the isValidCast check were in a single case for structs. That way you don't have to compare the case clauses to see what's different. It also avoids running requiresCast twice.\n  if (!isValidCast(from, to resolver)) {\n    throw new AnalysisException(...)\n  }\n  ...", "author": "rdblue", "createdAt": "2020-12-29T17:18:00Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")", "originalCommit": "2f0f1303fb0f96512736a553c1efe8c9e5b13552", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI3OTgzNw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550279837", "bodyText": "Resolving this as the code changed.", "author": "aokolnychyi", "createdAt": "2020-12-30T18:03:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4Mzc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4NjE0Mg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549786142", "bodyText": "This inserts a cast regardless of whether it is safe to cast. There is also now an assignment policy that changes which casts should be inserted. I think this should apply the same logic as the table output resolver, just using the assignment names. What do you think of the logic in TableOutputResolver.checkField?", "author": "rdblue", "createdAt": "2020-12-29T17:25:40Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) && isValidCast(from, to, resolver) =>\n+          val fieldExprs = to.flatMap { field =>\n+            val fieldName = Literal(field.name)\n+            val fieldExpr = ExtractValue(expr, fieldName, resolver)\n+            Seq(fieldName, castIfNeeded(fieldExpr, field.dataType, resolver))\n+          }\n+          CreateNamedStruct(fieldExprs)\n+        case (from: StructType, to: StructType) if requiresCast(from, to) =>\n+          throw new AnalysisException(s\"Invalid assignments: cannot cast $from to $to\")\n+        case (from, to) if requiresCast(from, to) => cast(expr, dataType)", "originalCommit": "2f0f1303fb0f96512736a553c1efe8c9e5b13552", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4OTAzOQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r549789039", "bodyText": "This means that assignments within a struct are always done by name and never by position? In table output resolution, it looks like structs must match the expected order and if writing by name then the names must match. I would probably use the same logic as in canWrite.", "author": "rdblue", "createdAt": "2020-12-29T17:35:04Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,193 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AttributeReference, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.types.{DataType, NullType, StructField, StructType}\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(nameParts: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(getNameParts(a.key), a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.nameParts.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(update.expr, col.dataType, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.nameParts).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(nameParts = u.nameParts.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.nameParts match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      expr: Expression,\n+      dataType: DataType,\n+      resolver: Resolver): Expression = expr match {\n+    // some types cannot be casted from NullType (e.g. StructType)\n+    case Literal(value, NullType) => Literal(value, dataType)\n+    case _ =>\n+      (expr.dataType, dataType) match {\n+        // resolve structs by name if they they have the same number of fields and their names match\n+        // e.g., it is ok to set a struct with fields (a, b) as another struct with fields (b, a)\n+        // it is invalid to a set a struct with fields (a, d) as another struct with fields (a, b)", "originalCommit": "2f0f1303fb0f96512736a553c1efe8c9e5b13552", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDE2ODcxMA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550168710", "bodyText": "I gave this a bit of thought.\nWe should discuss whether to resolve nested struct fields by name or by position. In particular, what should happen if one sets a struct with fields (b, a) to a struct column with fields (a, b)? Should we match the fields by name or should we match them by position?\nI think actions in MERGE operations are resolved by position.\nFor example, INSERT (c1, c2) VALUES (s.c2, s.c1) as a branch of MERGE will be resolved as c1 = s.c2, c2 = s.c1. I think we can follow what Spark does for regular SQL INSERTs and resolve struct fields by position, not by name like this PR does now.\nNot resolving nested fields by name may lead to unexpected results from the user perspective but we have this problem inside INSERT statements already. Spark ignores column names inside INSERTs and adds aliases if needed.\nAre we ok ignoring column names inside MERGE too? I guess that would be reasonable and would match the existing behavior for SQL INSERTs. If so, assigning a struct (b, a) as struct (a, b) may be legal but the new value of b will be applied to a column.", "author": "aokolnychyi", "createdAt": "2020-12-30T11:54:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4OTAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI5MTc5OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550291799", "bodyText": "In particular, what should happen if one sets a struct with fields (b, a) to a struct column with fields (a, b)? Should we match the fields by name or should we match them by position?\n\nSQL writes are always by position unless you have (names...) VALUES (values...), but that's not the case for these structs. So I think think the right behavior for SQL is by position.\nBecause dataframe columns don't have an obvious position, the expectation for users is that the writes happen by name. That's why we added byName variants of the logical plans. I think that extends to nested structs as well because nested structs are easy to produce by some conversion from an object. When converting from an object using an Encoder, there is no column order guarantee so it is reasonable to assume that columns will be written by name.\nBecause we don't currently support dataframe merge into, I think that we should move forward with the behavior you've added that matches the insert behavior. We can add the byName flag later when we add a dataframe API.", "author": "rdblue", "createdAt": "2020-12-30T18:46:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4OTAzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQyNjU5Nw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550426597", "bodyText": "Well, we do have (names...) VALUES (values...) in the insert action:\nWHEN NOT MATCHED THEN\n  INSERT (c1, c2) VALUES (v1, v2)\n\nBut I am not sure whether that means nested structs must be resolved by name.", "author": "aokolnychyi", "createdAt": "2020-12-31T08:22:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTc4OTAzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwODY2Nw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550208667", "bodyText": "We build the map a bit earlier now.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:14:45Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val refs = assignments.map(_.key).map(asAssignmentReference)\n+          refs.foreach { ref =>\n+            if (ref.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${ref.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = refs.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          val assignmentMap = colNames.zip(assignments).toMap", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTAzMg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209032", "bodyText": "I've reworked castIfNeeded completely. Now, it is closer to checkField in TableOutputResolver.\nI should probably add this to the licence.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:15:51Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDI4OTM1NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550289354", "bodyText": "Yes, please do.", "author": "rdblue", "createdAt": "2020-12-30T18:38:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTAzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ3OTk1NQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550479955", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-12-31T13:08:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTAzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTIyMQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209221", "bodyText": "Now we should have proper validation for writing nullable values to non-null columns.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:16:31Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTMzMQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209331", "bodyText": "I made this implicit.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:16:49Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+        }\n+\n+        val errors = new mutable.ArrayBuffer[String]()\n+        val canWrite = DataType.canWrite(\n+          expr.dataType, tableAttr.dataType, byName = false, resolver, tableAttr.name,\n+          storeAssignmentPolicy, err => errors += err)\n+\n+        if (!canWrite) {\n+          throw new AnalysisException(s\"Cannot write incompatible data:\\n- ${errors.mkString(\"\\n- \")}\")\n+        }\n+\n+      case _ => // OK\n+    }\n+\n+    storeAssignmentPolicy match {\n+      case _ if tableAttr.dataType.sameType(expr.dataType) =>\n+        expr\n+      case StoreAssignmentPolicy.ANSI =>\n+        AnsiCast(expr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))\n+      case _ =>\n+        Cast(expr, tableAttr.dataType, Option(conf.sessionLocalTimeZone))\n+    }\n+  }\n+\n+  implicit protected def asAssignmentReference(expr: Expression): Seq[String] = expr match {", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIwOTU4Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550209583", "bodyText": "I am going to address the last two TODOs before this PR is merged.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:17:41Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for action conditions\n+  // TODO: tests for writing nullable to not nullable, incompatible arrays, structs, atomic types", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxMDQ3Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550210473", "bodyText": "Important: I am passing byName = false. We can pass byName = true if we wanted to throw an exception when a struct with fields (a, b) is assigned as a struct with fields (b, a). However, that would make it different compared to SQL INSERTs.", "author": "aokolnychyi", "createdAt": "2020-12-30T14:20:41Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))\n+    val updatedFieldExprs = applyUpdates(fieldExprs, newUpdates, resolver, namePrefix :+ col.name)\n+\n+    // construct a new struct with updated field expressions\n+    toNamedStruct(fields, updatedFieldExprs)\n+  }\n+\n+  private def toNamedStruct(fields: Seq[StructField], fieldExprs: Seq[Expression]): Expression = {\n+    val namedStructExprs = fields.zip(fieldExprs).flatMap { case (field, expr) =>\n+      Seq(Literal(field.name), expr)\n+    }\n+    CreateNamedStruct(namedStructExprs)\n+  }\n+\n+  private def hasExactMatch(\n+      updates: Seq[ColumnUpdate],\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    updates.exists(assignment => isExactMatch(assignment, col, resolver))\n+  }\n+\n+  private def isExactMatch(\n+      update: ColumnUpdate,\n+      col: NamedExpression,\n+      resolver: Resolver): Boolean = {\n+\n+    update.ref match {\n+      case Seq(namePart) if resolver(namePart, col.name) => true\n+      case _ => false\n+    }\n+  }\n+\n+  protected def castIfNeeded(\n+      tableAttr: NamedExpression,\n+      expr: Expression,\n+      resolver: Resolver): Expression = {\n+\n+    val storeAssignmentPolicy = conf.storeAssignmentPolicy\n+\n+    // run the type check and catch type errors\n+    storeAssignmentPolicy match {\n+      case StoreAssignmentPolicy.STRICT | StoreAssignmentPolicy.ANSI =>\n+        if (expr.nullable && !tableAttr.nullable) {\n+          throw new AnalysisException(\n+            s\"Cannot write nullable values to non-null column '${tableAttr.name}'\")\n+        }\n+\n+        val errors = new mutable.ArrayBuffer[String]()\n+        val canWrite = DataType.canWrite(", "originalCommit": "35180160909b685a2b3ac070aafda2ddaabc91b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyMzgyOA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552123828", "bodyText": "I'm not sure about this one either. byName = true is more careful, so I would probably go with that to start with.", "author": "rdblue", "createdAt": "2021-01-05T18:46:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDIxMDQ3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550480118", "bodyText": "I think we should set it to null instead of throwing an exception.", "author": "aokolnychyi", "createdAt": "2020-12-31T13:09:22Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d\n+      }\n+\n+      val alignedNotMatchedActions = m.notMatchedActions.map {\n+        case i @ InsertAction(_, assignments) =>\n+          // check no nested columns are present\n+          val refs = assignments.map(_.key).map(asAssignmentReference)\n+          refs.foreach { ref =>\n+            if (ref.size > 1) {\n+              throw new AnalysisException(\n+                \"Nested fields are not supported inside INSERT clauses of MERGE operations: \" +\n+                s\"${ref.mkString(\"`\", \"`.`\", \"`\")}\")\n+            }\n+          }\n+\n+          val colNames = refs.map(_.head)\n+\n+          // check there are no duplicates\n+          val duplicateColNames = colNames.groupBy(identity).collect {\n+            case (name, matchingNames) if matchingNames.size > 1 => name\n+          }\n+\n+          if (duplicateColNames.nonEmpty) {\n+            throw new AnalysisException(\n+              s\"Duplicate column names inside INSERT clause: ${duplicateColNames.mkString(\", \")}\")\n+          }\n+\n+          // reorder assignments by the target table column order\n+          val assignmentMap = colNames.zip(assignments).toMap\n+          i.copy(assignments = alignInsertActionAssignments(m.targetTable, assignmentMap))\n+\n+        case _ =>\n+          throw new AnalysisException(\"Not matched actions can only contain INSERT\")\n+      }\n+\n+      m.copy(matchedActions = alignedMatchedActions, notMatchedActions = alignedNotMatchedActions)\n+  }\n+\n+  private def alignInsertActionAssignments(\n+      targetTable: LogicalPlan,\n+      assignmentMap: Map[String, Assignment]): Seq[Assignment] = {\n+\n+    val resolver = conf.resolver\n+\n+    targetTable.output.map { targetAttr =>\n+      val assignment = assignmentMap\n+        .find { case (name, _) => resolver(name, targetAttr.name) }\n+        .map { case (_, assignment) => assignment }\n+\n+      if (assignment.isEmpty) {\n+        throw new AnalysisException(", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDg2MDEyNg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550860126", "bodyText": "Well, this would work fine only if the assignment mode isn't legacy.\nIn strict and ansi, there will be an exception in castIfNeeded if the column is not nullable. In legacy, this will go through.", "author": "aokolnychyi", "createdAt": "2021-01-02T08:47:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyOTc5MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551629791", "bodyText": "Should this check if the mode is legacy and add null otherwise?", "author": "rdblue", "createdAt": "2021-01-04T23:31:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTgzNTk1Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551835953", "bodyText": "I guess that would be reasonable.\nHm, I wanted to match the behavior in INSERT but did not manage to make it work with an explicit column list.\nINSERT INTO t (col1, col2, ...) VALUES (v1, v2, ...)\n\nShall we leave it as is for now? The current option is the most restrictive.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:11:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyMjg4Ng==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552122886", "bodyText": "Sounds good.", "author": "rdblue", "createdAt": "2021-01-05T18:44:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDExOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4MDIzMA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r550480230", "bodyText": "These TODOs require an actual implementation.", "author": "aokolnychyi", "createdAt": "2020-12-31T13:10:05Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyODc5OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551628799", "bodyText": "The equivalent match expression for notMatchedActions has a default case that throws \"Not matched actions can only contain INSERT\". Should there be a similar case here to reject any INSERT action?", "author": "rdblue", "createdAt": "2021-01-04T23:28:06Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AlignMergeIntoTable.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, DeleteAction, InsertAction, LogicalPlan, MergeIntoTable, UpdateAction}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.internal.SQLConf\n+\n+case class AlignMergeIntoTable(conf: SQLConf) extends Rule[LogicalPlan] with AssignmentAlignmentSupport {\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case m: MergeIntoTable if m.resolved =>\n+      val alignedMatchedActions = m.matchedActions.map {\n+        case u @ UpdateAction(_, assignments) =>\n+          u.copy(assignments = alignAssignments(m.targetTable, assignments))\n+        case d: DeleteAction => d", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2MzE1NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551863154", "bodyText": "I think Spark validates them while parsing but I added for consistency.", "author": "aokolnychyi", "createdAt": "2021-01-05T11:04:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYyODc5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNjc5OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551636799", "bodyText": "If this happened, then two expressions in the left side of the assignment expression would match, right?\nINSERT (a.b, a.b) VALUES (2, 3)\nSince we know that the assignment references on the left are unique from an earlier check, we only have the second case, right?", "author": "rdblue", "createdAt": "2021-01-04T23:48:49Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0MTEzMQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551841131", "bodyText": "We do that check only for INSERT actions. So you are right we can only hit case 2 there cause we know INSERT can only have top-level columns and we validate they are unique upfront. However, this logic is also applied to UPDATE actions where we don't do that check.\nUPDATEs are a bit more tricky as they can have references to nested columns. We could try to do the uniqueness check early but we should be careful.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:21:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNjc5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyMjQ2OA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552122468", "bodyText": "This seems fine to me.", "author": "rdblue", "createdAt": "2021-01-05T18:43:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNjc5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNzQ1NQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551637455", "bodyText": "Minor: I prefer not to add [ and ] unless they add value or form a literal.", "author": "rdblue", "createdAt": "2021-01-04T23:51:13Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2Mjk2OA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551862968", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2021-01-05T11:03:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTYzNzQ1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjA1Mg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551642052", "bodyText": "I find the logic here hard to follow because applyUpdates and applyStructUpdates call one another to recursively traverse the schema. It is also strange that the current column is passed as NamedExpression, which will be created with an alias for each level.\nI think it would be easier to read and a little cleaner if you removed col and just traversed the schema in one method. It should be possible to build an expression equivalent to col from namePrefix and each field name.\nI don't think this is a blocker, though.", "author": "rdblue", "createdAt": "2021-01-05T00:07:00Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0NDk3NQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551844975", "bodyText": "I did this as our Scala checks complained about the method length. Let me see if I can revert this change.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:28:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjA1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0NzQ4NA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551847484", "bodyText": "I am going to increase the allowed method length to 100.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:33:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjA1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjI2Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551642263", "bodyText": "Updates with empty references will be removed by the filter in applyUpdates, right?", "author": "rdblue", "createdAt": "2021-01-05T00:07:46Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/AssignmentAlignmentSupport.scala", "diffHunk": "@@ -0,0 +1,199 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.expressions.{Alias, AnsiCast, AttributeReference, Cast, CreateNamedStruct, Expression, ExtractValue, GetStructField, Literal, NamedExpression}\n+import org.apache.spark.sql.catalyst.plans.logical.{Assignment, LogicalPlan}\n+import org.apache.spark.sql.internal.SQLConf.StoreAssignmentPolicy\n+import org.apache.spark.sql.types.{DataType, StructField, StructType}\n+import scala.collection.mutable\n+\n+trait AssignmentAlignmentSupport extends CastSupport {\n+\n+  private case class ColumnUpdate(ref: Seq[String], expr: Expression)\n+\n+  /**\n+   * Aligns assignments to match table columns.\n+   * <p>\n+   * This method processes and reorders given assignments so that each target column gets\n+   * an expression it should be set to. If a column does not have a matching assignment,\n+   * it will be set to its current value. For example, if one passes a table with columns c1, c2\n+   * and an assignment c2 = 1, this method will return c1 = c1, c2 = 1.\n+   * <p>\n+   * This method also handles updates to nested columns. If there is an assignment to a particular\n+   * nested field, this method will construct a new struct with one field updated\n+   * preserving other fields that have not been modified. For example, if one passes a table with\n+   * columns c1, c2 where c2 is a struct with fields n1 and n2 and an assignment c2.n2 = 1,\n+   * this method will return c1 = c1, c2 = struct(c2.n1, 1).\n+   *\n+   * @param table a target table\n+   * @param assignments assignments to align\n+   * @return aligned assignments that match table columns\n+   */\n+  protected def alignAssignments(\n+      table: LogicalPlan,\n+      assignments: Seq[Assignment]): Seq[Assignment] = {\n+\n+    val columnUpdates = assignments.map(a => ColumnUpdate(a.key, a.value))\n+    val outputExprs = applyUpdates(table.output, columnUpdates)\n+    outputExprs.zip(table.output).map {\n+      case (expr, attr) => Assignment(attr, expr)\n+    }\n+  }\n+\n+  private def applyUpdates(\n+      cols: Seq[NamedExpression],\n+      updates: Seq[ColumnUpdate],\n+      resolver: Resolver = conf.resolver,\n+      namePrefix: Seq[String] = Nil): Seq[Expression] = {\n+\n+    // iterate through columns at the current level and find which column updates match\n+    cols.map { col =>\n+      // find matches for this column or any of its children\n+      val prefixMatchedUpdates = updates.filter(a => resolver(a.ref.head, col.name))\n+      prefixMatchedUpdates match {\n+        // if there is no exact match and no match for children, return the column as is\n+        case updates if updates.isEmpty =>\n+          col\n+\n+        // if there is an exact match, return the assigned expression\n+        case Seq(update) if isExactMatch(update, col, resolver) =>\n+          castIfNeeded(col, update.expr, resolver)\n+\n+        // if there are matches only for children\n+        case updates if !hasExactMatch(updates, col, resolver) =>\n+          col.dataType match {\n+            case StructType(fields) =>\n+              applyStructUpdates(col, prefixMatchedUpdates, fields, resolver, namePrefix)\n+            case otherType =>\n+              val colName = (namePrefix :+ col.name).mkString(\".\")\n+              throw new AnalysisException(\n+                \"Updating nested fields is only supported for StructType \" +\n+                s\"but $colName is of type $otherType\"\n+              )\n+          }\n+\n+        // if there are conflicting updates, throw an exception\n+        // there are two illegal scenarios:\n+        // - multiple updates to the same column\n+        // - updates to a top-level struct and its nested fields (e.g., a.b and a.b.c)\n+        case updates if hasExactMatch(updates, col, resolver) =>\n+          val conflictingCols = updates.map(u => (namePrefix ++ u.ref).mkString(\".\"))\n+          throw new AnalysisException(\n+            \"Updates are in conflict for these columns: \" +\n+            conflictingCols.distinct.mkString(\"[\", \", \", \"]\"))\n+      }\n+    }\n+  }\n+\n+  private def applyStructUpdates(\n+      col: NamedExpression,\n+      updates: Seq[ColumnUpdate],\n+      fields: Seq[StructField],\n+      resolver: Resolver,\n+      namePrefix: Seq[String]): Expression = {\n+\n+    // build field expressions\n+    val fieldExprs = fields.zipWithIndex.map { case (field, ordinal) =>\n+      Alias(GetStructField(col, ordinal, Some(field.name)), field.name)()\n+    }\n+\n+    // recursively apply this method on nested fields\n+    val newUpdates = updates.map(u => u.copy(ref = u.ref.tail))", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg0NjQyNg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551846426", "bodyText": "Yep, when we ensure there is a match by prefix.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:31:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MjI2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551643731", "bodyText": "What is the correct behavior here? Reject it?\nI think what would currently happen is it would fail if any existing field were accessed, but it would succeed if all fields were assigned. Is that correct?", "author": "rdblue", "createdAt": "2021-01-05T00:13:11Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1Mjc4Mw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551852783", "bodyText": "I think rejecting will be too restrictive. That would mean we won't be able to update any nullable structs.\nHere is what Postgres does:\npostgres=# INSERT INTO t_100 (id, c) VALUES (2, null);\nINSERT 0 1\n\npostgres=# SELECT * FROM t_100;\n id |   c   \n----+-------\n  1 | (1,2)\n  2 | \n(2 rows)\n\npostgres=# UPDATE t_100 SET c.n1 = -1;\nUPDATE 2\n\npostgres=# SELECT * FROM t_100;\n id |   c    \n----+--------\n  1 | (-1,2)\n  2 | (-1,)\n(2 rows)", "author": "aokolnychyi", "createdAt": "2021-01-05T10:43:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg1MzExNw==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551853117", "bodyText": "I think the existing logic matches Postgres.", "author": "aokolnychyi", "createdAt": "2021-01-05T10:44:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyMTY3MQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552121671", "bodyText": "Okay, so the struct gets created and unspecified fields get set to null. Sounds good to me.", "author": "rdblue", "createdAt": "2021-01-05T18:42:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQ0NzIwMg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552447202", "bodyText": "Yeah, correct.", "author": "aokolnychyi", "createdAt": "2021-01-06T09:00:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0MzczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0NDc2MA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551644760", "bodyText": "I don't think that this should have a WHEN MATCHED case, to ensure that it is failing for the NOT MATCHED THEN INSERT.", "author": "rdblue", "createdAt": "2021-01-05T00:16:32Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,336 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Arrays;\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct\n+  // TODO: tests for subqueries in conditions\n+\n+  @Test\n+  public void testMergeWithNonExistingColumns() {\n+    createAndInitNestedColumnsTable();\n+    createOrReplaceView(\"source\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid top-level column\",\n+        AnalysisException.class, \"cannot resolve '`t.invalid_col`'\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.invalid_col = s.c2\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid nested column\",\n+        AnalysisException.class, \"No such struct field invalid_col\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.invalid_col = s.c2\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about the invalid top-level column\",\n+        AnalysisException.class, \"cannot resolve '`invalid_col`'\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, invalid_col) VALUES (s.c1, null)\", tableName);\n+        });\n+  }\n+\n+  @Test\n+  public void testMergeWithInvalidColumnsInInsert() {\n+    createAndInitNestedColumnsTable();\n+    createOrReplaceView(\"source\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n+\n+    AssertHelpers.assertThrows(\"Should complain about the nested column\",\n+        AnalysisException.class, \"Nested fields are not supported inside INSERT clauses\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, c.n2) VALUES (s.c1, null)\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about duplicate columns\",\n+        AnalysisException.class, \"Duplicate column names inside INSERT clause\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +\n+              \"WHEN NOT MATCHED THEN \" +\n+              \"  INSERT (id, id) VALUES (s.c1, null)\", tableName);\n+        });\n+\n+    AssertHelpers.assertThrows(\"Should complain about missing columns\",\n+        AnalysisException.class, \"must provide values for all columns of the target table\",\n+        () -> {\n+          sql(\"MERGE INTO %s t USING source s \" +\n+              \"ON t.id == s.c1 \" +\n+              \"WHEN MATCHED THEN \" +\n+              \"  UPDATE SET t.c.n2.dn1 = s.c2 \" +", "originalCommit": "7c14b157748a2146f66ee8101b30413433834328", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2MTU5OQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551861599", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2021-01-05T11:00:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0NDc2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTg2MjgxNg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r551862816", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2021-01-05T11:03:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTY0NDc2MA=="}], "type": "inlineReview"}, {"oid": "12b567796ccb6ef878d2c063d2802df42168907f", "url": "https://github.com/apache/iceberg/commit/12b567796ccb6ef878d2c063d2802df42168907f", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2021-01-05T10:59:17Z", "type": "forcePushed"}, {"oid": "8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "url": "https://github.com/apache/iceberg/commit/8d48b6f6290b1f5a16cf1aea0d840ebb0ea36bcc", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2021-01-05T11:01:44Z", "type": "forcePushed"}, {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a", "url": "https://github.com/apache/iceberg/commit/58c7e78bb673e39fff0a71815073b8c07cef659a", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2021-01-05T11:02:43Z", "type": "commit"}, {"oid": "58c7e78bb673e39fff0a71815073b8c07cef659a", "url": "https://github.com/apache/iceberg/commit/58c7e78bb673e39fff0a71815073b8c07cef659a", "message": "Spark: Add a rule to align updates in MERGE operations", "committedDate": "2021-01-05T11:02:43Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyNjY0NQ==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552126645", "bodyText": "I think it would be much easier to read these tests if instead of using these methods, you just embedded the CREATE TABLE here, like this:\n  sql(\"CREATE TABLE %s (..., complex struct<c1 int, c2 int) ...\", tableName);\n  initTable(tableName);\nThat way, it is easy to see what the available columns are without needing to refer back to the base class. I found it easy to see the structure and data for source, but kept going back to see what the target table's schema was. That was a bit harder considering the method is changed in this test.", "author": "rdblue", "createdAt": "2021-01-05T18:51:41Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java", "diffHunk": "@@ -0,0 +1,329 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.util.Map;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.sql.AnalysisException;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n+\n+  public TestMerge(String catalogName, String implementation, Map<String, String> config,\n+                   String fileFormat, boolean vectorized) {\n+    super(catalogName, implementation, config, fileFormat, vectorized);\n+  }\n+\n+  @BeforeClass\n+  public static void setupSparkConf() {\n+    spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n+  }\n+\n+  @After\n+  public void removeTables() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+    sql(\"DROP TABLE IF EXISTS source\");\n+  }\n+\n+  // TODO: tests for reordering when operations succeed (both insert and update actions)\n+  // TODO: tests for modifying fields in a null struct\n+  // TODO: tests for subqueries in conditions\n+\n+  @Test\n+  public void testMergeWithNonExistingColumns() {\n+    createAndInitNestedColumnsTable();", "originalCommit": "58c7e78bb673e39fff0a71815073b8c07cef659a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQ0MzM4Mg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552443382", "bodyText": "I tried to save some extra lines but I agree it is not readable.\nI'll change this to:\ncreateAndInitTable(\"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\");\n\nOnce we have tests for real cases, we can offer a method that will accept a JSON body.", "author": "aokolnychyi", "createdAt": "2021-01-06T08:51:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyNjY0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjQ0ODcyMA==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552448720", "bodyText": "Updated.", "author": "aokolnychyi", "createdAt": "2021-01-06T09:03:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjEyNjY0NQ=="}], "type": "inlineReview"}, {"oid": "7e58d21b421157de078d96bf92e10f6d352802f5", "url": "https://github.com/apache/iceberg/commit/7e58d21b421157de078d96bf92e10f6d352802f5", "message": "Some updates", "committedDate": "2021-01-06T09:03:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkzMzAyMg==", "url": "https://github.com/apache/iceberg/pull/1986#discussion_r552933022", "bodyText": "Nit: this looks like Runnable with different names. If we are just introducing it to pass a lambda, we could probably use Runnable instead.", "author": "rdblue", "createdAt": "2021-01-06T19:57:07Z", "path": "spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java", "diffHunk": "@@ -132,4 +134,40 @@ protected void assertEquals(String context, List<Object[]> expectedRows, List<Ob\n   protected static String dbPath(String dbName) {\n     return metastore.getDatabasePath(dbName);\n   }\n+\n+  protected void withSQLConf(Map<String, String> conf, Action action) {\n+    SQLConf sqlConf = SQLConf.get();\n+\n+    Map<String, String> currentConfValues = Maps.newHashMap();\n+    conf.keySet().forEach(confKey -> {\n+      if (sqlConf.contains(confKey)) {\n+        String currentConfValue = sqlConf.getConfString(confKey);\n+        currentConfValues.put(confKey, currentConfValue);\n+      }\n+    });\n+\n+    conf.forEach((confKey, confValue) -> {\n+      if (SQLConf.staticConfKeys().contains(confKey)) {\n+        throw new RuntimeException(\"Cannot modify the value of a static config: \" + confKey);\n+      }\n+      sqlConf.setConfString(confKey, confValue);\n+    });\n+\n+    try {\n+      action.invoke();\n+    } finally {\n+      conf.forEach((confKey, confValue) -> {\n+        if (currentConfValues.containsKey(confKey)) {\n+          sqlConf.setConfString(confKey, currentConfValues.get(confKey));\n+        } else {\n+          sqlConf.unsetConf(confKey);\n+        }\n+      });\n+    }\n+  }\n+\n+  @FunctionalInterface\n+  protected interface Action {\n+    void invoke();\n+  }", "originalCommit": "7e58d21b421157de078d96bf92e10f6d352802f5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}