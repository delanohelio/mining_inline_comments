{"pr_number": 839, "pr_title": "Spark: Add tests for identity partition data", "pr_createdAt": "2020-03-12T21:22:13Z", "pr_url": "https://github.com/apache/iceberg/pull/839", "timeline": [{"oid": "4ad5e915bd963baa54638f9b2279ffb037c23648", "url": "https://github.com/apache/iceberg/commit/4ad5e915bd963baa54638f9b2279ffb037c23648", "message": "Add tests for identity partition data.", "committedDate": "2020-03-12T21:17:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4MTE0Nw==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r392081147", "bodyText": "nit: looks like the var is assigned but never used ?", "author": "openinx", "createdAt": "2020-03-13T08:10:42Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }\n+    };\n+  }\n+\n+  private final String format;\n+\n+  public TestIdentityPartitionData(String format) {\n+    this.format = format;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sc = null;", "originalCommit": "4ad5e915bd963baa54638f9b2279ffb037c23648", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIxNjY2Mg==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395216662", "bodyText": "Removed.", "author": "rdblue", "createdAt": "2020-03-19T17:57:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4MTE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4NjUzMQ==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r392086531", "bodyText": "nit: I think the testFullProjection is a special case of testProjections , would merge them together ?", "author": "openinx", "createdAt": "2020-03-13T08:25:07Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }\n+    };\n+  }\n+\n+  private final String format;\n+\n+  public TestIdentityPartitionData(String format) {\n+    this.format = format;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sc = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIdentityPartitionData.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    TestIdentityPartitionData.sc = new JavaSparkContext(spark.sparkContext());\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIdentityPartitionData.spark;\n+    TestIdentityPartitionData.spark = null;\n+    TestIdentityPartitionData.sc = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\"),\n+      LogMessage.info(\"2020-02-02\", \"info event 1\"),\n+      LogMessage.debug(\"2020-02-02\", \"debug event 2\"),\n+      LogMessage.info(\"2020-02-03\", \"info event 2\"),\n+      LogMessage.debug(\"2020-02-03\", \"debug event 3\"),\n+      LogMessage.info(\"2020-02-03\", \"info event 3\"),\n+      LogMessage.error(\"2020-02-03\", \"error event 1\"),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 4\"),\n+      LogMessage.warn(\"2020-02-04\", \"warn event 1\"),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 5\")\n+  );\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private PartitionSpec spec = PartitionSpec.builderFor(LOG_SCHEMA).identity(\"date\").identity(\"level\").build();\n+  private Table table = null;\n+  private Dataset<Row> logs = null;\n+\n+  @Before\n+  public void setupTable() throws Exception {\n+    File location = temp.newFolder(\"logs\");\n+    Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+\n+    Map<String, String> properties = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format);\n+    this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n+    this.logs = spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n+\n+    logs.write().format(\"iceberg\").mode(\"append\").save(location.toString());\n+  }\n+\n+  @Test\n+  public void testFullProjection() {\n+    List<Row> expected = logs.orderBy(\"id\").collectAsList();\n+    List<Row> actual = spark.read().format(\"iceberg\").load(table.location()).orderBy(\"id\").collectAsList();\n+    Assert.assertEquals(\"Rows should match\", expected, actual);\n+  }\n+\n+  @Test\n+  public void testProjections() {\n+    String[][] cases = new String[][] {\n+        // individual fields\n+        new String[] { \"date\" },\n+        new String[] { \"level\" },\n+        new String[] { \"message\" },\n+        // field pairs\n+        new String[] { \"date\", \"message\" },\n+        new String[] { \"level\", \"message\" },\n+        new String[] { \"date\", \"level\" },\n+        // out-of-order pairs\n+        new String[] { \"message\", \"date\" },\n+        new String[] { \"message\", \"level\" },\n+        new String[] { \"level\", \"date\" },\n+        // full projection, different orderings\n+        new String[] { \"date\", \"level\", \"message\" },\n+        new String[] { \"level\", \"date\", \"message\" },\n+        new String[] { \"date\", \"message\", \"level\" },\n+        new String[] { \"level\", \"message\", \"date\" },\n+        new String[] { \"message\", \"date\", \"level\" },\n+        new String[] { \"message\", \"level\", \"date\" }\n+    };\n+\n+    for (String[] ordering : cases) {\n+      List<Row> expected = logs.select(\"id\", ordering).orderBy(\"id\").collectAsList();", "originalCommit": "4ad5e915bd963baa54638f9b2279ffb037c23648", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIxNjYxOA==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395216618", "bodyText": "If this test case fails, it's easier to debug and find what happened than the other case. That's why I included it.", "author": "rdblue", "createdAt": "2020-03-19T17:56:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA4NjUzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NDUwMQ==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395094501", "bodyText": "Do you want to include ORC as well?", "author": "rdsr", "createdAt": "2020-03-19T15:03:05Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIdentityPartitionData  {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\" },\n+        new Object[] { \"avro\" }", "originalCommit": "4ad5e915bd963baa54638f9b2279ffb037c23648", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIxNjYzNA==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395216634", "bodyText": "I tried, but ORC writes aren't currently supported in Spark.", "author": "rdblue", "createdAt": "2020-03-19T17:56:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NDUwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTIzNjg5Mw==", "url": "https://github.com/apache/iceberg/pull/839#discussion_r395236893", "bodyText": "oh ok. I'll file a ticket for that.", "author": "rdsr", "createdAt": "2020-03-19T18:31:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTA5NDUwMQ=="}], "type": "inlineReview"}, {"oid": "159a87921fd41e01de6dc90a5ee1c251e1118a31", "url": "https://github.com/apache/iceberg/commit/159a87921fd41e01de6dc90a5ee1c251e1118a31", "message": "Fix tests.", "committedDate": "2020-03-19T17:53:17Z", "type": "commit"}]}