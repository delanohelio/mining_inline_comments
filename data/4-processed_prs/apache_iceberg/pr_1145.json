{"pr_number": 1145, "pr_title": "Implement the flink stream writer to accept the row data and emit the complete data files event to downstream", "pr_createdAt": "2020-06-30T13:22:02Z", "pr_url": "https://github.com/apache/iceberg/pull/1145", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r448537766", "bodyText": "Why make this modifiable?", "author": "rdblue", "createdAt": "2020-07-01T18:16:59Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -360,7 +360,7 @@ public ByteBuffer keyMetadata() {\n     if (list != null) {\n       List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n       copy.addAll(list);\n-      return Collections.unmodifiableList(copy);", "originalCommit": "4d277a0115f9521c82806fc35d71c1640bad20a4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcwOTc3MA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r448709770", "bodyText": "Because flink is depending on the com.esotericsoftware.kryo.serializers to serialize & deserialize the class/object, it will fill few null value into the fields which is a collection data type. the code is here: https://github.com/EsotericSoftware/kryo/blob/46ef9788fa1d3fb020ce6e8f33f431c9fb54cb35/src/com/esotericsoftware/kryo/serializers/CollectionSerializer.java#L102.\nAnd if we don't make it modifiable, then it will throw the stacktrace :\njava.lang.UnsupportedOperationException\nSerialization trace:\nsplitOffsets (org.apache.iceberg.GenericDataFile)\ncom.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException\nSerialization trace:\nsplitOffsets (org.apache.iceberg.GenericDataFile)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125)\n\tat com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:528)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:657)\n\tat org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.copy(KryoSerializer.java:262)\n\tat org.apache.flink.api.java.typeutils.runtime.PojoSerializer.copy(PojoSerializer.java:239)\n\tat org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness$MockOutput.collect(AbstractStreamOperatorTestHarness.java:693)\n\tat org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness$MockOutput.collect(AbstractStreamOperatorTestHarness.java:661)\n\tat org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:730)\n\tat org.apache.flink.streaming.api.operators.AbstractStreamOperator$CountingOutput.collect(AbstractStreamOperator.java:708)\n\tat org.apache.iceberg.flink.IcebergStreamWriter.emit(IcebergStreamWriter.java:149)\n\tat org.apache.iceberg.flink.IcebergStreamWriter.prepareSnapshotPreBarrier(IcebergStreamWriter.java:119)\n\tat org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.prepareSnapshotPreBarrier(AbstractStreamOperatorTestHarness.java:565)\n\tat org.apache.iceberg.flink.TestIcebergStreamWriter.testWritingTable(TestIcebergStreamWriter.java:101)\nCaused by: java.lang.UnsupportedOperationException\n\tat java.util.Collections$UnmodifiableCollection.add(Collections.java:1057)\n\tat com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)\n\tat com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:22)\n\tat com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:679)\n\tat com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106)\n\t... 70 more", "author": "openinx", "createdAt": "2020-07-02T02:13:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxMTcxMw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r448711713", "bodyText": "OK,  the serializers is not filling the unexpected null value into the collection.  It's just initialize a collection instance firstly, then fill the element one by one.", "author": "openinx", "createdAt": "2020-07-02T02:21:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxNTI3NQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r448715275", "bodyText": "When serializing the DataFile, the kyro will reconstruct all the fields inside the object so that every field could be serializable in kyro way.   The problem is: it will  create the BaseFile instance firstly then fill its element into collection one by one, finally this problem happen.", "author": "openinx", "createdAt": "2020-07-02T02:35:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM0NzAxMQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449347011", "bodyText": "Why must use com.esotericsoftware.kryo.serializers? You can specify a serializer for BaseFile, for example, use AvroSerializer.", "author": "JingsongLi", "createdAt": "2020-07-03T02:33:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk2NTQ5Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449965497", "bodyText": "OK,  I did not know theres' a way to specify the customized serializer.  Let me check how to handle this.", "author": "openinx", "createdAt": "2020-07-06T03:23:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDU1NTA1OQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r464555059", "bodyText": "I think we might want to convert the field to an array instead of a List. Lists are causing serialization problems, but arrays are fine. This would be similar to how we handle keyMetadata, which uses byte[] for the field, but returns ByteBuffer through the API.", "author": "rdblue", "createdAt": "2020-08-03T17:27:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDc4MjU0Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r464782546", "bodyText": "Sounds like a great suggestion.  I tried this way and it works, we don't need the customized flink serializer any more.", "author": "openinx", "createdAt": "2020-08-04T03:50:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODUzNzc2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTk1Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449359952", "bodyText": "Operator in Flink will be serialized into byte[] whatever. So I think it is true about set all the non-serializable members to be null. You don't need add this comment.", "author": "JingsongLi", "createdAt": "2020-07-03T03:37:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwNjQxMg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449706412", "bodyText": "This is also more of a code comment than user-facing documentation, right?", "author": "rdblue", "createdAt": "2020-07-03T21:24:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk2MDUyOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449960529", "bodyText": "Yes, IIUC, and I think it is the behavior of java Serializable.", "author": "JingsongLi", "createdAt": "2020-07-06T02:55:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTk1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk2NjQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449966475", "bodyText": "Yes,  It's a code comment and  we expect that the IcebergStreamWriter  won't be exposed to the upper layer  iceberg users,  because we will have a wrapper to hidden the implementation details of IcebergStreamWriter and IcebergFilesCommitter and only expose the DataStream<Row> to the end user.  I can remove this comment if you guys think it's OK.  NOTICE:  there will be a next patch to implement the IcebergFilesCommitter to collect all the data files emitted by IcebergStreamWriter and commit the iceberg transaction in one parallelism.", "author": "openinx", "createdAt": "2020-07-06T03:28:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM1OTk1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MTI0OQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449361249", "bodyText": "createFileFormat ?", "author": "JingsongLi", "createdAt": "2020-07-03T03:45:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(new SerializableDataFile(dataFile)));\n+  }\n+\n+  private FileFormat getFileFormat() {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3OTc1Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449979753", "bodyText": "Here we don't create a file format, just parse the  FileFormat  from properties. the get sounds good to me.", "author": "openinx", "createdAt": "2020-07-06T04:38:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MTI0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MzY3Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449363672", "bodyText": "A reset in TaskWriter is just clear complete files?", "author": "JingsongLi", "createdAt": "2020-07-03T03:57:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/TaskWriter.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+\n+/**\n+ * The writer interface which could accept records and provide the generated data files.\n+ *\n+ * @param <T> to indicate the record data type.\n+ */\n+public interface TaskWriter<T> {\n+  int ROW_DIVISOR = 1000;\n+\n+  /**\n+   * Append the row into the data files.\n+   */\n+  void append(T record) throws IOException;\n+\n+  /**\n+   * Close the writer.\n+   */\n+  void close() throws IOException;\n+\n+  /**\n+   * To get the full list of complete files, we should call this method after {@link TaskWriter#close()} because the\n+   * close method will close all the opening data files and build {@link DataFile} to the return array list.\n+   *\n+   * @return the cached completed data files of this task writer.\n+   */\n+  List<DataFile> getCompleteFiles();\n+\n+  /**\n+   * Reset to clear all the cached complete files.\n+   */\n+  void reset();", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDAzNzI1Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450037252", "bodyText": "Yes, as we discussed above,  unifying the getCompleteFiles and reset into a pollCompleteFiles   sounds good to me.", "author": "openinx", "createdAt": "2020-07-06T07:37:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2MzY3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2ODY5MQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449368691", "bodyText": "I can see always a reset after getCompleteFiles. Can just provides a pollCompleteFiles?", "author": "JingsongLi", "createdAt": "2020-07-03T04:24:18Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3MjExMg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449972112", "bodyText": "Well, sounds good to me.", "author": "openinx", "createdAt": "2020-07-06T03:59:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM2ODY5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MTQyOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449371428", "bodyText": "computeIfAbsent?", "author": "JingsongLi", "createdAt": "2020-07-03T04:37:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MTYxNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449371617", "bodyText": "We don't need store partitionKey, it is already in map?", "author": "JingsongLi", "createdAt": "2020-07-03T04:38:46Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {\n+    private final PartitionKey partitionKey;", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA2MjE1Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450062157", "bodyText": "It's will be simpler to write the following logic with attached the partitionKey in WrappedFileAppender.", "author": "openinx", "createdAt": "2020-07-06T08:25:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MTYxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MjE1Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372156", "bodyText": "spec.fields().size() == 0 ? null : partitionKey just be partitionKey?", "author": "JingsongLi", "createdAt": "2020-07-03T04:41:17Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)\n+        .withEncryptedOutputFile(currentFile)\n+        .withPath(currentFile.encryptingOutputFile().location())\n+        .withFileSizeInBytes(fileSizeInBytes)\n+        .withPartition(spec.fields().size() == 0 ? null : partitionKey)", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4NTk5Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449985992", "bodyText": "It is not necessary to keep the partitionKey reference when building DataFiles.", "author": "openinx", "createdAt": "2020-07-06T05:08:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MjE1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MjY5MA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372690", "bodyText": "add a method in this class: add(...), you can increment currentRows here.", "author": "JingsongLi", "createdAt": "2020-07-03T04:43:47Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {\n+    private final PartitionKey partitionKey;\n+    private final EncryptedOutputFile encryptedOutputFile;\n+    private final FileAppender<T> fileAppender;\n+\n+    private long currentRows = 0;\n+\n+    WrappedFileAppender(PartitionKey partitionKey,\n+                        EncryptedOutputFile encryptedOutputFile,\n+                        FileAppender<T> fileAppender) {\n+      this.partitionKey = partitionKey;\n+      this.encryptedOutputFile = encryptedOutputFile;\n+      this.fileAppender = fileAppender;\n+    }", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3Mjg2Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449372863", "bodyText": "Can you add comments to explain why need align to ROW_DIVISOR?", "author": "JingsongLi", "createdAt": "2020-07-03T04:44:41Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/UnpartitionedWriter.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+class UnpartitionedWriter<T> extends BaseTaskWriter<T> {\n+\n+  private final FileAppenderFactory<T> factory;\n+  private final Supplier<EncryptedOutputFile> outputFileSupplier;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  private long currentRows = 0;\n+  private EncryptedOutputFile currentOutputFile;\n+  private FileAppender<T> currentAppender = null;\n+\n+  UnpartitionedWriter(FileAppenderFactory<T> factory,\n+                      Supplier<EncryptedOutputFile> outputFileSupplier,\n+                      long targetFileSize,\n+                      FileFormat fileFormat) {\n+    this.factory = factory;\n+    this.outputFileSupplier = outputFileSupplier;\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = new ArrayList<>();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    if (currentAppender == null) {\n+      currentOutputFile = outputFileSupplier.get();\n+      currentAppender = factory.newAppender(currentOutputFile.encryptingOutputFile(), fileFormat);\n+    }\n+    currentAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    currentRows++;\n+    if (currentRows % ROW_DIVISOR == 0 && currentAppender.length() >= targetFileSize) {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM3MzAzNg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449373036", "bodyText": "Extract this class for NonPartitionWriter too?", "author": "JingsongLi", "createdAt": "2020-07-03T04:45:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();\n+  }\n+\n+  private WrappedFileAppender<T> createWrappedFileAppender(PartitionKey partitionKey) {\n+    EncryptedOutputFile outputFile = outputFileGetter.apply(partitionKey);\n+    FileAppender<T> appender = factory.newAppender(outputFile.encryptingOutputFile(), fileFormat);\n+    return new WrappedFileAppender<>(partitionKey, outputFile, appender);\n+  }\n+\n+  private void closeCurrentWriter(WrappedFileAppender<T> wrap) throws IOException {\n+    DataFile dataFile = closeFileAppender(wrap.fileAppender, wrap.encryptedOutputFile, spec, wrap.partitionKey);\n+    completeDataFiles.add(dataFile);\n+  }\n+\n+  private static class WrappedFileAppender<T> {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM4NzQzMA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449387430", "bodyText": "You may need to check the metrics.recordCount(), we don't need construct the DataFile without records.", "author": "JingsongLi", "createdAt": "2020-07-03T05:48:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4NDMyOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449984329", "bodyText": "We actually have skipped the empty DataFile construction in the upper layer,   you can see the PartitionWriter and UnpartitionedWriter (we also have an unit test to address it). So no need to check it here anymore.", "author": "openinx", "createdAt": "2020-07-06T05:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM4NzQzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM4ODQ4Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449388486", "bodyText": "NIT: add (name = \"format = {0}, partitioned= {1}\")", "author": "JingsongLi", "createdAt": "2020-07-03T05:52:34Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestIcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergStreamWriter {\n+  private static final Configuration CONF = new Configuration();\n+\n+  @Rule\n+  public TemporaryFolder tempFolder = new TemporaryFolder();\n+\n+  private String tablePath;\n+  private Table table;\n+\n+  private final FileFormat format;\n+  private final boolean partitioned;\n+\n+  // TODO add AVRO, ORC unit test once the readers and writers are ready.\n+  @Parameterized.Parameters", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTM5MTQwOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449391409", "bodyText": "Maybe you can not just apply from objects in row.\nIIUC, In Flink Row, the structure of bytes is byte[], and in Iceberg, the structure of bytes is ByteBuffer.", "author": "JingsongLi", "createdAt": "2020-07-03T06:03:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {\n+      Object[] partitionTuple = new Object[size];\n+      for (int i = 0; i < partitionTuple.length; i += 1) {\n+        partitionTuple[i] = transforms[i].apply(row.getField(pos[i]));", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwNzE1Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449707152", "bodyText": "Is there a way to tell which methods are intended to be called before serialization and which ones will be called after serialization? This one is clearly before serialization because it uses table, which is set in the constructor and is transient. I think it would help readability if we had an annotation or some way to highlight the methods that will be used after serialization, so we can check that they don't use transient objects without initializing them.", "author": "rdblue", "createdAt": "2020-07-03T21:30:28Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(new SerializableDataFile(dataFile)));\n+  }\n+\n+  private FileFormat getFileFormat() {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3OTk4MA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449979980", "bodyText": "Well, let me consider about this.", "author": "openinx", "createdAt": "2020-07-06T04:39:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwNzE1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708127", "bodyText": "I don't think it is a good idea to mimic the behavior of Spark 2 here. That's really limited. What about using job configuration to instantiate a catalog and load a table by name, like the newer Spark 3 integration does? Here's the javadoc: https://github.com/apache/iceberg/blob/master/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L64-L77\nThat uses this logic to determine the catalog and load a table:\n  /**\n   * Build an Iceberg {@link Catalog} to be used by this Spark catalog adapter.\n   *\n   * @param name Spark's catalog name\n   * @param options Spark's catalog options\n   * @return an Iceberg catalog\n   */\n  protected Catalog buildIcebergCatalog(String name, CaseInsensitiveStringMap options) {\n    Configuration conf = SparkSession.active().sparkContext().hadoopConfiguration();\n    String catalogType = options.getOrDefault(\"type\", \"hive\");\n    switch (catalogType) {\n      case \"hive\":\n        int clientPoolSize = options.getInt(\"clients\", 2);\n        String uri = options.get(\"uri\");\n        return new HiveCatalog(name, uri, clientPoolSize, conf);\n\n      case \"hadoop\":\n        String warehouseLocation = options.get(\"warehouse\");\n        return new HadoopCatalog(name, conf, warehouseLocation);\n\n      default:\n        throw new UnsupportedOperationException(\"Unknown catalog type: \" + catalogType);\n    }\n  }", "author": "rdblue", "createdAt": "2020-07-03T21:38:33Z", "path": "flink/src/main/java/org/apache/iceberg/flink/TableUtil.java", "diffHunk": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+\n+class TableUtil {\n+\n+  private TableUtil() {\n+  }\n+\n+  static Table findTable(String path, Configuration conf) {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk2ODI0Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449968242", "bodyText": "This is a very good suggestion. And the extension is, in Flink, also has Catalog interface, it is better to integrate iceberg catalog to Flink catalog, just like what Spark 3 do.", "author": "JingsongLi", "createdAt": "2020-07-06T03:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3NDYyOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449974629", "bodyText": "Can/Should I have a try to contribute integrating iceberg catalog to Flink catalog?", "author": "JingsongLi", "createdAt": "2020-07-06T04:12:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4MzYyOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449983628", "bodyText": "@JingsongLi seems we could work those things together.  I will focus on the streaming writer and keep the simple findTable here. and you could provide a pull request in buildIcebergCatalog way to integrate iceberg catalog to flink catalog (if you want).", "author": "openinx", "createdAt": "2020-07-06T04:57:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA0NDcxMA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450044710", "bodyText": "I created #1170 for further discussion.", "author": "JingsongLi", "createdAt": "2020-07-06T07:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODEyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODI1OA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708258", "bodyText": "Minor: in other modules, we don't use this. when reading a field, only when assigning to it so it is clear whether the assignment is to a local variable or a field.", "author": "rdblue", "createdAt": "2020-07-03T21:39:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk2NzIxOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449967218", "bodyText": "Fine, let's keep the rule.", "author": "openinx", "createdAt": "2020-07-06T03:33:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODI1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODMyOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449708328", "bodyText": "Why is this reassigning the read schema's IDs?", "author": "rdblue", "createdAt": "2020-07-03T21:40:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3MTExNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449971117", "bodyText": "Because the readSchema was converted  from user-provided flink table schema,  its field id(s) is not matching the written iceberg table's. Then the following validateWriteSchema will regard those two tables as incompatible because of the diff id. for example:\nCannot write incompatible dataset to table with schema:\ntable {\n  0: id: optional int\n  1: data: optional string\n}\nwrite schema:table {\n  1: id: optional int\n  2: data: optional string\n}\nProblems:\n* data: int cannot be promoted to string\njava.lang.IllegalArgumentException: Cannot write incompatible dataset to table with schema:\ntable {\n  0: id: optional int\n  1: data: optional string\n}\nwrite schema:table {\n  1: id: optional int\n  2: data: optional string\n}\nProblems:\n* data: int cannot be promoted to string\n\tat org.apache.iceberg.types.TypeUtil.validateWriteSchema(TypeUtil.java:216)\n\tat org.apache.iceberg.flink.IcebergStreamWriter.open(IcebergStreamWriter.java:97)\n\tat org.apache.flink.streaming.util.AbstractStreamOperatorTestHarness.open(AbstractStreamOperatorTestHarness.java:558)\n\tat org.apache.iceberg.flink.TestIcebergStreamWriter.testWritingTable(TestIcebergStreamWriter.java:94)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:55)\n\tat java.lang.Thread.run(Thread.java:748)", "author": "openinx", "createdAt": "2020-07-06T03:54:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODMyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM5NjU5MA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450396590", "bodyText": "Okay, that sounds correct to me.\nOne thing that doesn't make sense is why the read schema is coming from the user with IDs. We would normally want the projection to be provided as names to project from the table schema, then use table.schema().select(\"col1\", \"col2\", ...) to get the read schema.\nThe way we do this for Spark SQL is we convert the Iceberg schema to the SQL representation, then are passed back a subset of that schema. Next, we convert the SQL schema back to Iceberg using a method like the reassignIds you use here. I think what I didn't expect was that you'd get an Iceberg schema passed in as the user's schema. Does Flink have a schema representation we could use instead?", "author": "rdblue", "createdAt": "2020-07-06T18:11:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODMyOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg5MDY2Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450890667", "bodyText": "Thanks for the question,  Flink provide both DataStream API and Table API.  For DataStream API, actually we don't have to provide the table schema so it could be null. While for Table API, it's required to provide a table schema to specify where the data will be written into, so we need to do the validation for the user-provided schema. I written a flink connector before in here  ( unit test is here ) , you may want to take a look.", "author": "openinx", "createdAt": "2020-07-07T14:05:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwODMyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTEwNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709107", "bodyText": "This won't handle nested data. I think it would be better to use a wrapper for Row and then use the standard accessors that are provided by schema.accessorForField(fieldId). That Row wrapper would implement StructLike, which is what the accessors use. It would also be responsible for converting to the internal representation of data values, like the wrapper for Iceberg generic rows.", "author": "rdblue", "createdAt": "2020-07-03T21:46:28Z", "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4MjIxNA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449982214", "bodyText": "In my thought, the  PartitionSpec  will only use the root-level fields so I simplified the accessor to buildFieldId2PosMap... I'm not quite sure whether we need the complex tree-traverse,  let me take a deeper look..", "author": "openinx", "createdAt": "2020-07-06T04:50:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTEwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM5NDMyNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450394327", "bodyText": "Iceberg allows partitioning by fields that are nested in structs. For simple example of when you'd want to do this, think about an HTTP log stream, with request and response structs. You might want to partition by truncate(response.status, 100) to get partitions of 200, 300, 400, and 500 response codes. Since a struct is just a logical grouping, we want to be able to partition by its fields.", "author": "rdblue", "createdAt": "2020-07-06T18:06:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTEwNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDY4MDA4Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450680087", "bodyText": "@rdblue I created a pull request to address the thing about \"generalize the PartitionKey from Spark and use it in flink here\",  please take a look. #1175", "author": "openinx", "createdAt": "2020-07-07T08:00:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTEwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTQ4Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709483", "bodyText": "In other cases, we avoid allocating a new key for the partition tuple every row, and we defensively copy partition keys. It might be a good idea to generalize the PartitionKey from Spark and use it here.", "author": "rdblue", "createdAt": "2020-07-03T21:49:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/PartitionKey.java", "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.types.Types;\n+\n+public class PartitionKey implements StructLike {\n+\n+  private final Object[] partitionTuple;\n+\n+  private PartitionKey(Object[] partitionTuple) {\n+    this.partitionTuple = partitionTuple;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (!(o instanceof PartitionKey)) {\n+      return false;\n+    }\n+\n+    PartitionKey that = (PartitionKey) o;\n+    return Arrays.equals(partitionTuple, that.partitionTuple);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Arrays.hashCode(partitionTuple);\n+  }\n+\n+  @Override\n+  public int size() {\n+    return partitionTuple.length;\n+  }\n+\n+  @Override\n+  public <T> T get(int pos, Class<T> javaClass) {\n+    return javaClass.cast(partitionTuple[pos]);\n+  }\n+\n+  public Object[] getPartitionTuple() {\n+    return partitionTuple;\n+  }\n+\n+  @Override\n+  public <T> void set(int pos, T value) {\n+    partitionTuple[pos] = value;\n+  }\n+\n+  private static Map<Integer, Integer> buildFieldId2PosMap(Schema schema) {\n+    Map<Integer, Integer> fieldId2Position = Maps.newHashMap();\n+    List<Types.NestedField> nestedFields = schema.asStruct().fields();\n+    for (int i = 0; i < nestedFields.size(); i++) {\n+      fieldId2Position.put(nestedFields.get(i).fieldId(), i);\n+    }\n+    return fieldId2Position;\n+  }\n+\n+  public static Builder builder(PartitionSpec spec) {\n+    return new Builder(spec);\n+  }\n+\n+  public static class Builder {\n+    private final int size;\n+\n+    private final int[] pos;\n+    private final Transform[] transforms;\n+\n+    private Builder(PartitionSpec spec) {\n+      List<PartitionField> fields = spec.fields();\n+      this.size = fields.size();\n+      this.pos = new int[size];\n+      this.transforms = new Transform[size];\n+\n+      Map<Integer, Integer> fieldId2Pos = buildFieldId2PosMap(spec.schema());\n+\n+      for (int i = 0; i < size; i += 1) {\n+        PartitionField field = fields.get(i);\n+        Integer position = fieldId2Pos.get(field.sourceId());\n+        Preconditions.checkArgument(position != null,\n+            \"Field source id from PartitionSpec MUST exist in the original schema\");\n+        this.pos[i] = position;\n+        this.transforms[i] = field.transform();\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public PartitionKey build(Row row) {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcwOTY4Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449709682", "bodyText": "Why is this needed? The DataFile implementations used internally are Serializable. Although we don't implement Serializable in the DataFile API, as long as you're using the internal implementation of DataFile, you should be able to serialize it.", "author": "rdblue", "createdAt": "2020-07-03T21:51:32Z", "path": "flink/src/main/java/org/apache/iceberg/flink/SerializableDataFile.java", "diffHunk": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.DataFile;\n+\n+public class SerializableDataFile implements Serializable {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449710143", "bodyText": "Table is not serializable. That's why the Spark output file factory has fields for LocationProvider, FileIO, EncryptionManager, and PartitionSpec that are serializable. Those are held by the task and this is created on each task after serialization to workers.", "author": "rdblue", "createdAt": "2020-07-03T21:55:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/OutputFileFactory.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.util.UUID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public class OutputFileFactory {\n+  private final String uuid = UUID.randomUUID().toString();\n+\n+  private final Table table;\n+  private final FileFormat format;\n+  private final long taskId;\n+  private int fileCount;\n+\n+  public OutputFileFactory(Table table, FileFormat format, long taskId) {", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA2MTI2Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450061263", "bodyText": "OK, I think I get the difference you mentioned between spark writer implementation and flink writer implementation:\nFor spark,   it will load the iceberg table firstly at driver side, then create the DataWriterFactory  and serialize and dispatch it to each executor, then the executor will create its DataWriter, so here each executor won't need to load the iceberg table.\nFor flink , I currently implemented the IcebergStreamWriter by loading the iceberg table for each sub task, then each task get the table path and open the iceberg table, so the Table instance won't need to be serializable. Seems the iceberg table will be loaded 100 times if we have 100 parallerism for IcebergStreamWriter.  @JingsongLi Any thought about this issue ?  I mean: do we have some similar ways as spark did to optimize the iceberg table loading ?", "author": "openinx", "createdAt": "2020-07-06T08:23:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA4NDcyNQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450084725", "bodyText": "I don't quite understand what you mean.\nFirst, the StreamOperator.open is invoked in tasks instead of client.\nMaybe we should avoid loading table in tasks, image the catalog is hive catalog, too many tasks visit the catalog will lead to collapse of hive metastore.\nSo in this direction, we should store serializable LocationProvider, FileIO, EncryptionManager, and PartitionSpec in StreamOperator instead of loading table at runtime.", "author": "JingsongLi", "createdAt": "2020-07-06T09:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM5MjcxNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450392717", "bodyText": "I'd prefer to avoid loading the table in each task. There is no need for the table reference on workers, and not having it will prevent patterns that are bad, like attempting to commit to the table from every task.", "author": "rdblue", "createdAt": "2020-07-06T18:04:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg2ODkwMw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450868903", "bodyText": "Agree.   I changed the design in commit 0026d16, say it will load the table firstly, then initialize the IcebergStreamWriter  instances and then deploy the flink stream job to flink jobManager, and finally the flink jobManager will allocate slot and dispatch the task to each slot. In general,  we currently load the table once.. Thanks for the discussion.", "author": "openinx", "createdAt": "2020-07-07T13:35:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDE0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDU5NA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449710594", "bodyText": "If you're following the Spark file naming convention, then the the first number is equivalent to getRuntimeContext().getIndexOfThisSubtask(). The second number is the unique ID for that task in Spark, so that task reattempts don't collide.\nIs there an attempt ID or something in Flink? I don't see much value in using a hash code, unless it is System.identityHashCode and can take on more values than this.", "author": "rdblue", "createdAt": "2020-07-03T21:59:31Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/OutputFileFactory.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.util.UUID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.OutputFile;\n+\n+public class OutputFileFactory {\n+  private final String uuid = UUID.randomUUID().toString();\n+\n+  private final Table table;\n+  private final FileFormat format;\n+  private final long taskId;\n+  private int fileCount;\n+\n+  public OutputFileFactory(Table table, FileFormat format, long taskId) {\n+    this.table = table;\n+    this.format = format;\n+    this.taskId = taskId;\n+    this.fileCount = 0;\n+  }\n+\n+  /**\n+   * All the data files inside the same task will share the same uuid identifier but could be distinguished by the\n+   * increasing file count.\n+   *\n+   * @return the data file name to be written.\n+   */\n+  private String generateFilename() {\n+    int hashCode = Math.abs(this.hashCode() % 10 ^ 5);\n+    return format.addExtension(String.format(\"%05d-%d-%s-%05d\", hashCode, taskId, uuid, fileCount++));", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA0NzA2Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450047062", "bodyText": "Fine, I guess I misunderstood the meanings of spark file naming. Yeah, the flink task has the an attempt ID as you said. Let me address it in the next pach.", "author": "openinx", "createdAt": "2020-07-06T07:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMDU5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTUzOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711539", "bodyText": "withEncryptedOutputFile sets all of the information from the file, so you don't need to set the path separately.", "author": "rdblue", "createdAt": "2020-07-03T22:08:14Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.\n+    return DataFiles.builder(spec)\n+        .withEncryptedOutputFile(currentFile)\n+        .withPath(currentFile.encryptingOutputFile().location())", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4NDU2Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449984562", "bodyText": "OK, thanks for the reminding.", "author": "openinx", "createdAt": "2020-07-06T05:02:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTUzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTU3NQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711575", "bodyText": "Looks like this comment is no longer correct because completeDataFiles is private in the subclasses.", "author": "rdblue", "createdAt": "2020-07-03T22:08:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/BaseTaskWriter.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.FileAppender;\n+\n+abstract class BaseTaskWriter<T> implements TaskWriter<T> {\n+\n+  protected DataFile closeFileAppender(FileAppender<T> fileAppender, EncryptedOutputFile currentFile,\n+                                       PartitionSpec spec, StructLike partitionKey) throws IOException {\n+    // Close the file appender firstly.\n+    fileAppender.close();\n+\n+    // metrics are only valid after the appender is closed.\n+    Metrics metrics = fileAppender.metrics();\n+    long fileSizeInBytes = fileAppender.length();\n+    List<Long> splitOffsets = fileAppender.splitOffsets();\n+\n+    // Construct the DataFile and add it into the completeDataFiles.", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk4Mzc5MQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449983791", "bodyText": "It's true,  let remove it.", "author": "openinx", "createdAt": "2020-07-06T04:58:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTg4OQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449711889", "bodyText": "Shouldn't this also ensure that all of the appenders are closed?", "author": "rdblue", "createdAt": "2020-07-03T22:11:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/writer/PartitionWriter.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.writer;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.flink.util.Preconditions;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.flink.PartitionKey;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The PartitionFanoutWriter will open a writing data file for each partition and route the given record to the\n+ * corresponding data file in the correct partition.\n+ *\n+ * @param <T> defines the data type of record to write.\n+ */\n+class PartitionWriter<T> extends BaseTaskWriter<T> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(PartitionWriter.class);\n+\n+  private final PartitionSpec spec;\n+  private final FileAppenderFactory<T> factory;\n+  private final Function<PartitionKey, EncryptedOutputFile> outputFileGetter;\n+  private final Function<T, PartitionKey> keyGetter;\n+  private final Map<PartitionKey, WrappedFileAppender<T>> writers;\n+  private final long targetFileSize;\n+  private final FileFormat fileFormat;\n+  private final List<DataFile> completeDataFiles;\n+\n+  PartitionWriter(PartitionSpec spec,\n+                  FileAppenderFactory<T> factory,\n+                  Function<PartitionKey, EncryptedOutputFile> outputFileGetter,\n+                  Function<T, PartitionKey> keyGetter,\n+                  long targetFileSize,\n+                  FileFormat fileFormat) {\n+    this.spec = spec;\n+    this.factory = factory;\n+    this.outputFileGetter = outputFileGetter;\n+    this.keyGetter = keyGetter;\n+    this.writers = Maps.newHashMap();\n+    this.targetFileSize = targetFileSize;\n+    this.fileFormat = fileFormat;\n+    this.completeDataFiles = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public void append(T record) throws IOException {\n+    PartitionKey partitionKey = keyGetter.apply(record);\n+    Preconditions.checkArgument(partitionKey != null, \"Partition key shouldn't be null\");\n+\n+    WrappedFileAppender<T> writer = writers.get(partitionKey);\n+    if (writer == null) {\n+      writer = createWrappedFileAppender(partitionKey);\n+      writers.put(partitionKey, writer);\n+    }\n+    writer.fileAppender.add(record);\n+\n+    // Roll the writer if reach the target file size.\n+    writer.currentRows++;\n+    if (writer.currentRows % ROW_DIVISOR == 0 && writer.fileAppender.length() >= targetFileSize) {\n+      closeCurrentWriter(writer);\n+      writers.remove(partitionKey);\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    for (WrappedFileAppender<T> wrap : writers.values()) {\n+      closeCurrentWriter(wrap);\n+      LOG.debug(\"Close file appender: {}, completeDataFiles: {}\",\n+          wrap.encryptedOutputFile.encryptingOutputFile().location(),\n+          completeDataFiles.size());\n+    }\n+    this.writers.clear();\n+  }\n+\n+  @Override\n+  public List<DataFile> getCompleteFiles() {\n+    if (completeDataFiles.size() > 0) {\n+      return ImmutableList.copyOf(this.completeDataFiles);\n+    } else {\n+      return Collections.emptyList();\n+    }\n+  }\n+\n+  @Override\n+  public void reset() {\n+    this.completeDataFiles.clear();", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA0NTM5NA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450045394", "bodyText": "The data file will be put into completeDataFiles only after we have closed the appenders.  So we can guarantee that all the appenders are closed.", "author": "openinx", "createdAt": "2020-07-06T07:53:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMTg4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjU5Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449712596", "bodyText": "I would normally expect reset to restore the writer to a new state for reuse. Looks like in this case it is clearing the files that have already been consumed. I think that this could have a better name for the operation.\nAlso, I would rather improve this API. It seems brittle to get the list and then reset it through the writer. I think it would be cleaner to have a poll call that retrieves one data file and removes it from the internal tracking, if one is ready. Then this could call poll until there are no more files.", "author": "rdblue", "createdAt": "2020-07-03T22:18:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449712639", "bodyText": "When this close happens, what emits the data files?", "author": "rdblue", "createdAt": "2020-07-03T22:19:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,225 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.java.ClosureCleaner;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.functions.sink.SinkFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.OutputFileFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<SerializableDataFile>\n+    implements OneInputStreamOperator<Row, SerializableDataFile> {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final SerializableConfiguration conf;\n+  private Schema readSchema;\n+\n+  private transient Table table;\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+\n+  /**\n+   * Be careful to do the initialization in this constructor, because in {@link DataStream#addSink(SinkFunction)}\n+   * it will call {@link ClosureCleaner#clean(Object, ExecutionConfig.ClosureCleanerLevel, boolean)} to set all the\n+   * non-serializable members to be null.\n+   *\n+   * @param tablePath  The base path of the iceberg table.\n+   * @param readSchema The schema of source data.\n+   * @param conf       The hadoop's configuration.\n+   */\n+  private IcebergStreamWriter(String tablePath, Schema readSchema, Configuration conf) {\n+    this.tablePath = tablePath;\n+    this.conf = new SerializableConfiguration(conf);\n+    this.readSchema = readSchema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.table = TableUtil.findTable(tablePath, conf.get());\n+    if (this.readSchema != null) {\n+      // reassign ids to match the existing table schema\n+      readSchema = TypeUtil.reassignIds(readSchema, table.schema());\n+      TypeUtil.validateWriteSchema(readSchema, table.schema(), true, true);\n+    }\n+\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(table);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(table, fileFormat, subTaskId);\n+    this.writer = TaskWriterFactory.createTaskWriter(table.spec(),\n+        appenderFactory,\n+        outputFileFactory,\n+        getTargetFileSizeBytes(),\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer {} subtask {} begin preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.getCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    // Remember to clear the writer's cached complete files.\n+    writer.reset();\n+    LOG.info(\"Iceberg writer {} subtask {} completed preparing for checkpoint {}\", tablePath, subTaskId, checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.getCompleteFiles();\n+    if (!completeFiles.isEmpty()) {\n+      completeFiles.forEach(this::emit);\n+      // Remember to clear the writer's cached complete files.\n+      writer.reset();\n+    }\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    if (writer != null) {\n+      writer.close();", "originalCommit": "60b566fbe23e118823020827ba79cea178f2089e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTgyMjQwNA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449822404", "bodyText": "Nobody now...\nFlink does not allow data to be sent in close. And there are two cases to call close: exception and normal termination (Source is bounded).\nYou can let operator implement BoundedOneInput, emits the data files in endInput.", "author": "JingsongLi", "createdAt": "2020-07-05T02:12:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTg3NzI2Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449877267", "bodyText": "Hi Jingsong, if a collector.collect happens in close, what the next behavior will be ? Will data be processed by processElement in next operator and then called close in next operator ?", "author": "simonsssu", "createdAt": "2020-07-05T13:23:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk1OTMxNA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449959314", "bodyText": "Hi Simon0806, yes, it works, but it is not be recommended. If there is a BoundedOneInput operator in downstream, this bounded operator may misunderstand the real \"end input\".", "author": "JingsongLi", "createdAt": "2020-07-06T02:49:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTk3OTQ3Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r449979473", "bodyText": "@rdblue ,  I guess the real question you are curious is:   here we close the writer without emitting the data files to downstream, then will it break the exactly-once semantic ?  Let's take an example, we have the record stream:\n1 2 3 4  5 6 7 8 9 10 11 12  ...\n\nand the emitted data file list would be:\ndatafile0:   1,2,3\ndatafile1:   4,5,6\ndatafile2:   7,8,9\ndatafile3:   10,11,12\n\nAssuming that the checkpoint happen between  6 and 7, then we will emit the datafile0 and datafile1 to downstream operator.   If we close the writer (exceptionally or intentionally) after record 11, then we will emit the datafile2  to downstream operator, but will ignore the opening datafile3.  It actually don't break the exactly-once semantics  because the snapshot think that the record [1,6] has been processed by IcebergStreamWriter and snapshot state of the downstream operator contains the data file datafile0, datafile1.  Once recovered,  we will replay the records 7, 8, 9, 10, 11, 12..., the datafile-2 and datafile-3 will be re-generated and re-emitted to downstream operator, so here we don't need to emit the data file to downstream operator when closing. Besides, even if we emit the datafile-3 to downstream operator in the close method, it will be discarded when recovering.\n(I provided a unit test to address  this case).", "author": "openinx", "createdAt": "2020-07-06T04:36:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDA4MDIyNQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450080225", "bodyText": "@openinx It seems you can implement \"release all resources\" in StreamOperator.dispose(be invoked always) and emit data to downstream in BoundedOneInput.endInput(only be invoked in case of bounded source).", "author": "JingsongLi", "createdAt": "2020-07-06T08:57:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDg2NTk3Nw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r450865977", "bodyText": "@JingsongLi  I've addressed those things you mentioned in commit 99df013, Thanks for the suggestions.", "author": "openinx", "createdAt": "2020-07-07T13:31:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTcxMjYzOQ=="}], "type": "inlineReview"}, {"oid": "3e297442b9d4c98f5fdc20ee318856b43d575b0a", "url": "https://github.com/apache/iceberg/commit/3e297442b9d4c98f5fdc20ee318856b43d575b0a", "message": "Rebase the origin/master and add Avro writers and unit tests.", "committedDate": "2020-07-06T10:10:35Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM0MzUyNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451343527", "bodyText": "isEmpty redundant?", "author": "JingsongLi", "createdAt": "2020-07-08T07:38:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(spec,\n+        appenderFactory,\n+        outputFileFactory,\n+        targetFileSize,\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    List<DataFile> completeFiles = writer.pollCompleteFiles();\n+    if (!completeFiles.isEmpty()) {", "originalCommit": "c91d89a0ab414e3cb021de5164d5dbe40030ec87", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTM0Mzg0Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451343843", "bodyText": "NIT: writer.pollCompleteFiles().forEach(this::emit);", "author": "JingsongLi", "createdAt": "2020-07-08T07:39:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(spec,\n+        appenderFactory,\n+        outputFileFactory,\n+        targetFileSize,\n+        fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {", "originalCommit": "c91d89a0ab414e3cb021de5164d5dbe40030ec87", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r451934464", "bodyText": "Revert this?", "author": "JingsongLi", "createdAt": "2020-07-09T02:52:23Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -360,7 +360,7 @@ public ByteBuffer keyMetadata() {\n     if (list != null) {\n       List<E> copy = Lists.newArrayListWithExpectedSize(list.size());\n       copy.addAll(list);\n-      return Collections.unmodifiableList(copy);", "originalCommit": "c91d89a0ab414e3cb021de5164d5dbe40030ec87", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjAyMTg5MQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r452021891", "bodyText": "The AvroSerializer don't work here because we need the schema to serialize and deserialize the DataFile,  while in our current design we're hiding the schema details inside  GenericDataFile and BaseDataFile.  The current kyro serializer don't have problem actually, so I'd prefer to keep the current version.", "author": "openinx", "createdAt": "2020-07-09T07:35:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxODI1Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r457718256", "bodyText": "Can we register something with Kryo to work with the unmodifiable list?", "author": "rdblue", "createdAt": "2020-07-20T22:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxMzgxNA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r457813814", "bodyText": "+1 We can register a custom serializer to Flink.", "author": "JingsongLi", "createdAt": "2020-07-21T03:25:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODA0NTM2OA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r458045368", "bodyText": "@rdblue @JingsongLi   I followed the suggestion from flink community and prepared a patch for the  serializer issue here,  How do you think about this patch ?", "author": "openinx", "createdAt": "2020-07-21T12:06:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODA1OTg3NQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r458059875", "bodyText": "For me, it sounds not a good solution for our iceberg here because:\n\nit introduced a java dependency from an individual developer (https://github.com/magro/kryo-serializers).\nIt use a hack way (loading an invisible class) to register the UnmodifiableCollectionsSerializer.", "author": "openinx", "createdAt": "2020-07-21T12:32:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgxNzQ2Ng==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r459817466", "bodyText": "Sorry if it's inappropriate for me to but in here, as I'm not a committer or part of the project.\nBut I maintain and help developers at my company with dozens of flink programs. I can attest that we have had issues with using de.javakaffee:kryo-serializers in our flink programs, specifically with UnmodifiableListSerializer. As one example, flink allows users to override the class loading to either be parent first or child first. This, combined with the loading of an invisible class via Class.forName to register the UnmodifiableCollectionsSerializer has lead to developers encountering class not found issues depending on how they compile their programs.\nAdditionally, in the newer versions of flink (I belive >1.9, which current is 1.11 recently released), certain internal classes use their own class loading order, ignoring the configured class loading order (parent vs child) which the user configured. This has lead to some unexpected issues at run time, namely class not found errors as the class is not on the correct class path for flink's kryo dependency to find it.\nIn general, kryo is typically attempted to be avoided in flink-landia in favor of custom serializers which use the Flink serializer / deserializer interface.\nSo I'd +1 with @JingsongLi to registering a custom serializer a custom flink serializer to be used in the flink-iceberg subproject.", "author": "kbendick", "createdAt": "2020-07-24T01:52:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTgxODMzOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r459818338", "bodyText": "Additionally, I've not contributed meaningfully to the code base (I fixed some white space issues), but I have been following along for quite some time and I'm very interested in attempting to using Flink to write Iceberg files at my work. If there's any way I could be of assistance with this, please let me know. Possibly I could help fill in the gaps of Flink knowledge.\nPlease let me know where a better place to discuss new contributor opportunities would be! But I thought it would be prudent to share my experience with users using de.javakaffee:kryo-serializers in their flink programs.\nI'd also be open to testing out flink + iceberg integration if there's a need for that.", "author": "kbendick", "createdAt": "2020-07-24T01:56:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTg4MzYxNw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r459883617", "bodyText": "Thanks for your  sharing, @kbendick.  So let's remove the kyro serializer way, and turn to register a custom serializer.", "author": "openinx", "createdAt": "2020-07-24T07:02:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM1NDkyOA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r460354928", "bodyText": "No problem @openinx. There's essentially an issue with kryo's JaveSerializer when used with flink. See here: https://issues.apache.org/jira/browse/FLINK-6025 and here for more info: https://ci.apache.org/projects/flink/flink-docs-stable/dev/custom_serializers.html#issue-with-using-kryos-javaserializer\nIf there's anyway I could be of assistance with this issue or any other work for someone hopeful to start contributing to the project in a meaningful way, I'd be very happy to. I'm personally evaluating iceberg for usage between flink and spark as there are many flink users that I help oversee and we also have need to integrate that work with spark.\nI'd be happy to attempt a PR for a custom serializer, or would be happy to work on anything that anybody feels would be good for a hopeful new committer / contributor. Including tests, automation set up, flink support, etc.\nSorry if there's a better place to find this information; I am just getting my hands dirty in contributing to an apache OSS project (though I have forked some and then made changes and used those forks in production). I've also browsed the issues etc, but I'm not quite sure how to go about being allowed to get assigned one. My apologies if this is covered elsewhere; I'm used to being able to browse the JIRA for apache projects (though I much prefer github).\ncc @JingsongLi for potential guidance if you have a minute :)", "author": "kbendick", "createdAt": "2020-07-25T02:48:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkzNDQ2NA=="}], "type": "inlineReview"}, {"oid": "2355608252b282dc00357654d9dcd53bd3e46e30", "url": "https://github.com/apache/iceberg/commit/2355608252b282dc00357654d9dcd53bd3e46e30", "message": "Address serveral minor issues", "committedDate": "2020-07-09T04:27:16Z", "type": "forcePushed"}, {"oid": "c62f38af5db893d15e6221baa32ae6adce8ef024", "url": "https://github.com/apache/iceberg/commit/c62f38af5db893d15e6221baa32ae6adce8ef024", "message": "Rebase and use the abstracted PartitionKey in iceberg-api", "committedDate": "2020-07-16T03:57:44Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzgxOTM0Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r457819343", "bodyText": "Can we just pass a Iceberg Table instead of path here? We can get ride of TableUtil.findTable (Or just move it to test).", "author": "JingsongLi", "createdAt": "2020-07-21T03:49:24Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(schema, spec, appenderFactory, outputFileFactory,\n+        targetFileSize, fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+\n+    writer.pollCompleteFiles().forEach(this::emit);\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    writer.pollCompleteFiles().forEach(this::emit);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  @Override\n+  public void endInput() throws IOException {\n+    // For bounded stream, it may don't enable the checkpoint mechanism so we'd better to emit the remaining\n+    // data files to downstream before closing the writer so that we won't miss any of them.\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return MoreObjects.toStringHelper(this)\n+        .add(\"table_path\", tablePath)\n+        .add(\"subtask_id\", subTaskId)\n+        .add(\"attempt_id\", attemptId)\n+        .toString();\n+  }\n+\n+  private void emit(DataFile dataFile) {\n+    output.collect(new StreamRecord<>(dataFile));\n+  }\n+\n+  private FileFormat getFileFormat() {\n+    String formatString = properties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT);\n+    return FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  private long getTargetFileSizeBytes() {\n+    return PropertyUtil.propertyAsLong(properties,\n+        WRITE_TARGET_FILE_SIZE_BYTES,\n+        WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+  }\n+\n+  private static class FlinkFileAppenderFactory implements FileAppenderFactory<Row> {\n+    private final Schema schema;\n+    private final Map<String, String> props;\n+\n+    private FlinkFileAppenderFactory(Schema schema, Map<String, String> props) {\n+      this.schema = schema;\n+      this.props = props;\n+    }\n+\n+    @Override\n+    public FileAppender<Row> newAppender(OutputFile outputFile, FileFormat format) {\n+      MetricsConfig metricsConfig = MetricsConfig.fromProperties(props);\n+      try {\n+        switch (format) {\n+          case PARQUET:\n+            return Parquet.write(outputFile)\n+                .createWriterFunc(FlinkParquetWriters::buildWriter)\n+                .setAll(props)\n+                .metricsConfig(metricsConfig)\n+                .schema(schema)\n+                .overwrite()\n+                .build();\n+\n+          case AVRO:\n+            return Avro.write(outputFile)\n+                .createWriterFunc(FlinkAvroWriter::new)\n+                .setAll(props)\n+                .schema(schema)\n+                .overwrite()\n+                .build();\n+\n+          case ORC:\n+          default:\n+            throw new UnsupportedOperationException(\"Cannot write unknown format: \" + format);\n+        }\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    }\n+  }\n+\n+  static IcebergStreamWriter createStreamWriter(String path, TableSchema tableSchema, Configuration conf) {", "originalCommit": "95d5affa6a772198c9bd162636b87d336f463f05", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA5ODc0Mg==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r461098742", "bodyText": "Is it allowed to call pollCompleteFiles() after close(). IIUC, the writer is still needed when emitting the remaining datafiles as well as when calling pollCompleteFiles(). But possibly the contract around close is different than I expect it to be.\nAdditionally, the comment here is somewhat confusing. Based on the discussion above, and based on the task lifecycle of Flink operators outlined here, it seems like it might be more appropriate to say something along the lines of \"Once the close method of this IcebergStreamWriter is invoked, we'll no longer be able to emit any remaining data files downstream. To get around this, we implement the BoundedOneInput interface in order to finish processing and emit any remaining data before graceful shutdown\".\nMy third and final concern is that if we're closing the TaskWriter here during a graceful shutdown, if we only poll for complete files, what will happen to any remaining data that's being buffered in files that are not marked as complete? IIUC, the TaskWriter does not keep its opened / in-process files in flink's state such that it can be recovered after restore or replayed from the last checkpoint after a fatal. If I have that correct, it seems to me that we would want to poll for simply any remaining open files and emit them, otherwise we risk data loss. Can somebody help me better understand how we avoid losing data from in-process but incomplete files during a graceful shutdown (say a user takes a savepoint in order to restart their application and deploy a new version of their code)?", "author": "kbendick", "createdAt": "2020-07-27T18:51:25Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,226 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.OutputFileFactory;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroWriter;\n+import org.apache.iceberg.flink.data.FlinkParquetWriters;\n+import org.apache.iceberg.flink.writer.FileAppenderFactory;\n+import org.apache.iceberg.flink.writer.TaskWriter;\n+import org.apache.iceberg.flink.writer.TaskWriterFactory;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n+import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT_DEFAULT;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES;\n+import static org.apache.iceberg.TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT;\n+\n+class IcebergStreamWriter extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<Row, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergStreamWriter.class);\n+\n+  private final String tablePath;\n+  private final PartitionSpec spec;\n+  private final LocationProvider locations;\n+  private final Map<String, String> properties;\n+  private final FileIO io;\n+  private final EncryptionManager encryptionManager;\n+  private final Schema schema;\n+\n+  private transient TaskWriter<Row> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  private IcebergStreamWriter(String tablePath, PartitionSpec spec, LocationProvider locations,\n+                              Map<String, String> properties, FileIO io, EncryptionManager encryptionManager,\n+                              Schema schema) {\n+    this.tablePath = tablePath;\n+    this.spec = spec;\n+    this.locations = locations;\n+    this.properties = properties;\n+    this.io = io;\n+    this.encryptionManager = encryptionManager;\n+    this.schema = schema;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer.\n+    FileFormat fileFormat = getFileFormat();\n+    long targetFileSize = getTargetFileSizeBytes();\n+    FileAppenderFactory<Row> appenderFactory = new FlinkFileAppenderFactory(schema, properties);\n+    OutputFileFactory outputFileFactory = new OutputFileFactory(spec, fileFormat, locations, io,\n+        encryptionManager, subTaskId, attemptId);\n+    this.writer = TaskWriterFactory.createTaskWriter(schema, spec, appenderFactory, outputFileFactory,\n+        targetFileSize, fileFormat);\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {\n+    LOG.info(\"Iceberg writer({}) begin preparing for checkpoint {}\", toString(), checkpointId);\n+    // close all open files and emit files to downstream committer operator\n+    writer.close();\n+\n+    writer.pollCompleteFiles().forEach(this::emit);\n+    LOG.info(\"Iceberg writer({}) completed preparing for checkpoint {}\", toString(), checkpointId);\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<Row> element) throws Exception {\n+    Row value = element.getValue();\n+    writer.append(value);\n+\n+    // Emit the data file entries to downstream committer operator if there exist any complete files.\n+    writer.pollCompleteFiles().forEach(this::emit);\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+    if (writer != null) {\n+      writer.close();\n+      writer = null;\n+    }\n+  }\n+\n+  @Override\n+  public void endInput() throws IOException {\n+    // For bounded stream, it may don't enable the checkpoint mechanism so we'd better to emit the remaining\n+    // data files to downstream before closing the writer so that we won't miss any of them.\n+    writer.close();\n+    for (DataFile dataFile : writer.pollCompleteFiles()) {\n+      emit(dataFile);\n+    }", "originalCommit": "95d5affa6a772198c9bd162636b87d336f463f05", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTI3ODM0Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r461278343", "bodyText": "But possibly the contract around close is different than I expect it to be.\n\n@kbendick  It's better to rename the close to flush here, because the current writer.close is just closing all the opening data files and the writer could still accept record and write to the new opened files.  In the task writer abstraction pull request,  we've discussed this and changed to the way similar with spark.  Say  once we need to get the whole completed data file list, we will close the writer. If there's new record to write, then  we will initialize a new TaskWriter to write it.\n\nwhat will happen to any remaining data that's being buffered in files that are not marked as complete?\n\nOnce the writer#close is invoked, then all the opening file handlers will flush their data to file system and close them,  finally the completed DataFile (s) will be emitted to downstream operator IcebergFileCommitter. When a checkpoint come,  then the committer will flush its DataFile states to flink StateBackend.  So actually, we could recover the completed files from it.", "author": "openinx", "createdAt": "2020-07-28T02:23:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTA5ODc0Mg=="}], "type": "inlineReview"}, {"oid": "25d8f8404500db432c7015c3a4e83f9ffa9d9af3", "url": "https://github.com/apache/iceberg/commit/25d8f8404500db432c7015c3a4e83f9ffa9d9af3", "message": "Rebase to use the abstracted task writers.", "committedDate": "2020-08-03T07:55:06Z", "type": "forcePushed"}, {"oid": "b54d02430bf9813849c79ff93e7a5360b9dd5ff6", "url": "https://github.com/apache/iceberg/commit/b54d02430bf9813849c79ff93e7a5360b9dd5ff6", "message": "Add customized data file serializer.", "committedDate": "2020-08-03T12:55:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMTExOQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465211119", "bodyText": "Could you revert this change? We try to avoid changes that are non-functional and can cause commit conflicts between pull requests and while cherry-picking commits. In addition, keeping these aligned just leads to more changes. I think it's fine to format these aligned in new code, but I don't think it is a good idea to go back and make extra changes just to align.", "author": "rdblue", "createdAt": "2020-08-04T17:25:33Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -134,14 +134,14 @@ public PartitionData copy() {\n     this.nullValueCounts = nullValueCounts;\n     this.lowerBounds = SerializableByteBufferMap.wrap(lowerBounds);\n     this.upperBounds = SerializableByteBufferMap.wrap(upperBounds);\n-    this.splitOffsets = copy(splitOffsets);\n+    this.splitOffsets = splitOffsets == null ? null : splitOffsets.toArray(new Long[0]);\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n   }\n \n   /**\n    * Copy constructor.\n    *\n-   * @param toCopy a generic data file to copy.\n+   * @param toCopy   a generic data file to copy.", "originalCommit": "7989956bff8f3770afe9d0a853bde181fb2918c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1NjE2Mw==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465456163", "bodyText": "OK", "author": "openinx", "createdAt": "2020-08-05T03:52:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMTExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTIxMjMxMA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465212310", "bodyText": "The corresponding get implementation also needs to be updated. That's what is causing test failures.", "author": "rdblue", "createdAt": "2020-08-04T17:27:34Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -234,7 +235,7 @@ public void put(int i, Object value) {\n         this.keyMetadata = ByteBuffers.toByteArray((ByteBuffer) value);\n         return;\n       case 12:\n-        this.splitOffsets = (List<Long>) value;\n+        this.splitOffsets = value != null ? ((List<Long>) value).toArray(new Long[0]) : null;", "originalCommit": "7989956bff8f3770afe9d0a853bde181fb2918c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA2MA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465410060", "bodyText": "Minor: I recommend using table.name() instead of table.location(). The name is set by the catalog, so it should be how the table was identified. For Hive, it is catalog.db.table and for Hadoop it is the location. So it is a more natural identifier to use.", "author": "rdblue", "createdAt": "2020-08-05T00:55:20Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+class IcebergStreamWriter<T> extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<T, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final String tablePath;\n+\n+  private transient TaskWriterFactory<T> taskWriterFactory;\n+  private transient TaskWriter<T> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  IcebergStreamWriter(String tablePath, TaskWriterFactory<T> taskWriterFactory) {", "originalCommit": "7989956bff8f3770afe9d0a853bde181fb2918c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2MTY0NA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465461644", "bodyText": "You mean Table.toString()?", "author": "JingsongLi", "createdAt": "2020-08-05T04:15:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2NzIxMQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465467211", "bodyText": "the name is hidden inside BaseTable, yeah, we could use Table.toString()  to get that name.", "author": "openinx", "createdAt": "2020-08-05T04:37:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA2MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAwNDI4OA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r466004288", "bodyText": "Yeah. We can also add name to the table interface. I didn't realize we were still using toString.", "author": "rdblue", "createdAt": "2020-08-05T21:07:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDA2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDU2NA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465410564", "bodyText": "Is this guaranteed to be called from the same thread as processElement?", "author": "rdblue", "createdAt": "2020-08-05T00:57:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/IcebergStreamWriter.java", "diffHunk": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.BoundedOneInput;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.io.TaskWriter;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+class IcebergStreamWriter<T> extends AbstractStreamOperator<DataFile>\n+    implements OneInputStreamOperator<T, DataFile>, BoundedOneInput {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final String tablePath;\n+\n+  private transient TaskWriterFactory<T> taskWriterFactory;\n+  private transient TaskWriter<T> writer;\n+  private transient int subTaskId;\n+  private transient int attemptId;\n+\n+  IcebergStreamWriter(String tablePath, TaskWriterFactory<T> taskWriterFactory) {\n+    this.tablePath = tablePath;\n+    this.taskWriterFactory = taskWriterFactory;\n+  }\n+\n+  @Override\n+  public void open() {\n+    this.subTaskId = getRuntimeContext().getIndexOfThisSubtask();\n+    this.attemptId = getRuntimeContext().getAttemptNumber();\n+\n+    // Initialize the task writer factory.\n+    this.taskWriterFactory.initialize(subTaskId, attemptId);\n+\n+    // Initialize the task writer.\n+    this.writer = taskWriterFactory.create();\n+  }\n+\n+  @Override\n+  public void prepareSnapshotPreBarrier(long checkpointId) throws Exception {", "originalCommit": "7989956bff8f3770afe9d0a853bde181fb2918c7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ1ODgzMQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465458831", "bodyText": "Not the same thread, but:\nBefore invoking them, the caller will take a same lock.", "author": "JingsongLi", "createdAt": "2020-08-05T04:03:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDU2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQ2MjgzMQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r465462831", "bodyText": "In recent versions, Flink has refactored the lock model, using queue and single thread.\nAny way, Flink has ensured the thread safety of each method for a StreamOperator.", "author": "JingsongLi", "createdAt": "2020-08-05T04:20:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTQxMDU2NA=="}], "type": "inlineReview"}, {"oid": "d271f6ffde40f912d276c91083ed119b05d3b2db", "url": "https://github.com/apache/iceberg/commit/d271f6ffde40f912d276c91083ed119b05d3b2db", "message": "Rebase to use the abstracted task writers.", "committedDate": "2020-08-06T02:01:28Z", "type": "commit"}, {"oid": "550345ba07b1428ff52687fecb97fb3e4a604071", "url": "https://github.com/apache/iceberg/commit/550345ba07b1428ff52687fecb97fb3e4a604071", "message": "Add customized data file serializer.", "committedDate": "2020-08-06T02:01:28Z", "type": "commit"}, {"oid": "9e5aa9dcbc9222ba177eeaec28443cbfc818d829", "url": "https://github.com/apache/iceberg/commit/9e5aa9dcbc9222ba177eeaec28443cbfc818d829", "message": "Address the kyro serialize issues", "committedDate": "2020-08-06T02:01:28Z", "type": "commit"}, {"oid": "09f5f4edc37a685674daf70a14711685bae05b55", "url": "https://github.com/apache/iceberg/commit/09f5f4edc37a685674daf70a14711685bae05b55", "message": "Address comments", "committedDate": "2020-08-06T02:06:58Z", "type": "commit"}, {"oid": "09f5f4edc37a685674daf70a14711685bae05b55", "url": "https://github.com/apache/iceberg/commit/09f5f4edc37a685674daf70a14711685bae05b55", "message": "Address comments", "committedDate": "2020-08-06T02:06:58Z", "type": "forcePushed"}, {"oid": "b5790f37a3cf369af2e942b4a5a2e3615caa7e38", "url": "https://github.com/apache/iceberg/commit/b5790f37a3cf369af2e942b4a5a2e3615caa7e38", "message": "Remove the avro building in FlinkFileAppenderFactory.", "committedDate": "2020-08-06T02:15:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1MDYzNQ==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r467150635", "bodyText": "Should this return an ImmutableList so that it cannot be modified? That's what was happening before, although I think it matter less if this is creating a new list each time it is returned.", "author": "rdblue", "createdAt": "2020-08-07T16:41:10Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -357,7 +358,7 @@ public ByteBuffer keyMetadata() {\n \n   @Override\n   public List<Long> splitOffsets() {\n-    return splitOffsets;\n+    return splitOffsets != null ? Lists.newArrayList(splitOffsets) : null;", "originalCommit": "b5790f37a3cf369af2e942b4a5a2e3615caa7e38", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMwNjk3OA==", "url": "https://github.com/apache/iceberg/pull/1145#discussion_r467306978", "bodyText": "I thought that we use unmodified collection before because we don't wanna to change the contents inside splitOffsets.  Now we've accomplished the same purpose.  Returning it as modifiable or unmodifiable collection, both of them sounds good to me.", "author": "openinx", "createdAt": "2020-08-07T22:07:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE1MDYzNQ=="}], "type": "inlineReview"}]}