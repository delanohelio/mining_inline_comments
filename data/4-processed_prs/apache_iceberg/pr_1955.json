{"pr_number": 1955, "pr_title": "Spark: Sort retained rows in DELETE FROM by file and position", "pr_createdAt": "2020-12-18T01:16:52Z", "pr_url": "https://github.com/apache/iceberg/pull/1955", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDIzNA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510234", "bodyText": "Needed to allow projecting _file even though it isn't in the data file.", "author": "rdblue", "createdAt": "2020-12-18T01:17:35Z", "path": "core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java", "diffHunk": "@@ -96,7 +96,7 @@ public Schema record(Schema record, List<String> names, Iterable<Schema.Field> s\n \n       } else {\n         Preconditions.checkArgument(\n-            field.isOptional() || field.fieldId() == MetadataColumns.ROW_POSITION.fieldId(),\n+            field.isOptional() || MetadataColumns.metadataFieldIds().contains(field.fieldId()),", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjAzMzI4NA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546033284", "bodyText": "This reminds me we need to fix that projection bug / selection bug", "author": "RussellSpitzer", "createdAt": "2020-12-18T19:05:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDIzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMDM3Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545510376", "bodyText": "This adds _file to the constants map so it is set in records like a partition value.", "author": "rdblue", "createdAt": "2020-12-18T01:18:02Z", "path": "core/src/main/java/org/apache/iceberg/util/PartitionUtil.java", "diffHunk": "@@ -39,13 +40,17 @@ private PartitionUtil() {\n   }\n \n   public static Map<Integer, ?> constantsMap(FileScanTask task, BiFunction<Type, Object, Object> convertConstant) {\n-    return constantsMap(task.spec(), task.file().partition(), convertConstant);\n-  }\n+    PartitionSpec spec = task.spec();\n+    StructLike partitionData = task.file().partition();\n \n-  private static Map<Integer, ?> constantsMap(PartitionSpec spec, StructLike partitionData,\n-                                              BiFunction<Type, Object, Object> convertConstant) {\n     // use java.util.HashMap because partition data may contain null values\n     Map<Integer, Object> idToConstant = new HashMap<>();\n+\n+    // add _file\n+    idToConstant.put(\n+        MetadataColumns.FILE_PATH.fieldId(),\n+        convertConstant.apply(Types.StringType.get(), task.file().path()));", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTUxMTA2MA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545511060", "bodyText": "Spark's contract is that the scan's schema is the one that should be used, not the original table schema. This allows the merge scan to return the extra _file and _pos columns and matches the behavior of normal scans that are configured with PushDownUtils.pruneColumns.", "author": "rdblue", "createdAt": "2020-12-18T01:20:08Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545694599", "bodyText": "nit: should it be isNonMetadataColumn to indicate it is a boolean flag?", "author": "aokolnychyi", "createdAt": "2020-12-18T09:15:57Z", "path": "core/src/main/java/org/apache/iceberg/MetadataColumns.java", "diffHunk": "@@ -55,4 +55,16 @@ private MetadataColumns() {\n   public static Set<Integer> metadataFieldIds() {\n     return META_IDS;\n   }\n+\n+  public static NestedField get(String name) {\n+    return META_COLUMNS.get(name);\n+  }\n+\n+  public static boolean isMetadataColumn(String name) {\n+    return META_COLUMNS.containsKey(name);\n+  }\n+\n+  public static boolean nonMetadataColumn(String name) {", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NTQ4Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545985486", "bodyText": "Yeah, we could do that. I was following the slightly shorter Scala convention, where there are methods like nonEmpty. I prefer this way, but if others agree I'm happy to change it.", "author": "rdblue", "createdAt": "2020-12-18T17:38:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYxMTY0OA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546611648", "bodyText": "Let's leave it, I don't mind as long as it wasn't an oversight and there is an idea/convention being followed.", "author": "aokolnychyi", "createdAt": "2020-12-21T09:56:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTY5NDU5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwMjM4Mg==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545702382", "bodyText": "+1", "author": "aokolnychyi", "createdAt": "2020-12-18T09:24:14Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n \n     val scanPlan = scan match {\n-      case _: SupportsFileFilter =>\n+      case filterable: SupportsFileFilter =>\n         val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n-        val dynamicFileFilter = DynamicFileFilter(scanRelation, matchingFilePlan)\n+        val dynamicFileFilter = DynamicFileFilter(ExtendedScanRelation(scanRelation), matchingFilePlan, filterable)\n         dynamicFileFilter\n       case _ =>\n         scanRelation\n     }\n \n-    // include file name so that we can group data back\n-    val fileNameExpr = Alias(InputFileName(), FILE_NAME_COL)()\n-    Project(scanPlan.output :+ fileNameExpr, scanPlan)\n+    scanPlan\n   }\n \n   private def buildWritePlan(\n       remainingRowsPlan: LogicalPlan,\n       output: Seq[AttributeReference]): LogicalPlan = {\n \n-    // TODO: sort by _pos to keep the original ordering of rows\n-    // TODO: consider setting a file size limit\n-\n     val fileNameCol = findOutputAttr(remainingRowsPlan, FILE_NAME_COL)\n+    val rowPosCol = findOutputAttr(remainingRowsPlan, ROW_POS_COL)\n+    val order = Seq(SortOrder(fileNameCol, Ascending), SortOrder(rowPosCol, Ascending))", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTI3OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545709279", "bodyText": "Are there any cases when pruneColumns is going to be called multiple times? Should we worry about it at all?", "author": "aokolnychyi", "createdAt": "2020-12-18T09:31:06Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java", "diffHunk": "@@ -131,28 +138,52 @@ public SparkScanBuilder caseSensitive(boolean isCaseSensitive) {\n \n   @Override\n   public void pruneColumns(StructType requestedSchema) {\n-    this.requestedProjection = requestedSchema;\n+    this.requestedProjection = new StructType(Stream.of(requestedSchema.fields())\n+        .filter(field -> MetadataColumns.nonMetadataColumn(field.name()))\n+        .toArray(StructField[]::new));\n+\n+    Stream.of(requestedSchema.fields())", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTY5NQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545709695", "bodyText": "I see we call distinct in schemaWithMetadataColumns, never mind.", "author": "aokolnychyi", "createdAt": "2020-12-18T09:31:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwOTI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545810195", "bodyText": "I am a bit worried about adding this class as I am not sure we want to maintain it in Spark later. There is another idea how to solve the rewrite rule: we can simply disable column pruning for DynamicFileFilter nodes. I think it should be sufficient to extend the node references to also cover all output attributes of the scan.\nAttributeSet(scanRelation.output ++ fileFilterPlan.output)\n\nI've submitted a PR with this idea to your branch, @rdblue. Feel free to discard/modify as needed.", "author": "aokolnychyi", "createdAt": "2020-12-18T12:49:04Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/ExtendedScanRelation.scala", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanRelation\n+\n+case class ExtendedScanRelation(relation: DataSourceV2ScanRelation) extends LogicalPlan {", "originalCommit": "934a375adbf9efca155936d63f4003f94adb070b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk4NjgxNQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r545986815", "bodyText": "I think the more general problem is that Spark couldn't rewrite DynamicFileFilter node at all, which led to some situations where the plan worked fine and some where it would fail. I think we should generally fit the pattern of being able to rewrite nodes.", "author": "rdblue", "createdAt": "2020-12-18T17:40:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzMzE5NA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546633194", "bodyText": "I get the intention here now. I did a quick look and see that Spark injects the extended planner strategies before the data source V2 strategy. That got me thinking that we can get rid of this extra logical plan and just use the scan node directly and apply the new logic only if we have a scan that supports file filtering.\ncase PhysicalOperation(project, filters, DataSourceV2ScanRelation(_, scan: SupportsFileFilter, output)) =>\n  // projection and filters were already pushed down in the optimizer.\n  // this uses PhysicalOperation to get the projection and ensure that if the batch scan does\n  // not support columnar, a projection is added to convert the rows to UnsafeRow.\n  val batchExec = ExtendedBatchScanExec(output, scan)\n  withProjectAndFilter(project, filters, batchExec, !batchExec.supportsColumnar) :: Nil\n\n  ....\n\nprivate def withProjectAndFilter(\n    project: Seq[NamedExpression],\n    filters: Seq[Expression],\n    scan: LeafExecNode,\n    needsUnsafeConversion: Boolean): SparkPlan = {\n  val filterCondition = filters.reduceLeftOption(And)\n  val withFilter = filterCondition.map(FilterExec(_, scan)).getOrElse(scan)\n\n  if (withFilter.output != project || needsUnsafeConversion) {\n    ProjectExec(project, withFilter)\n  } else {\n    withFilter\n  }\n}\n\nThis also takes care of adding a projection if needed.", "author": "aokolnychyi", "createdAt": "2020-12-21T10:40:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNDI2Mg==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546634262", "bodyText": "I think that's close how a native Spark solution would work.", "author": "aokolnychyi", "createdAt": "2020-12-21T10:43:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNDgzMQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546634831", "bodyText": "There are no issues we use the same scan node in multiple places given the new logic and allowing rewrites, right?", "author": "aokolnychyi", "createdAt": "2020-12-21T10:44:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg3MDE3NQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546870175", "bodyText": "Scan nodes shouldn't be reused in general, but it is usually okay if there is not a join. The problem is reused attribute IDs.", "author": "rdblue", "createdAt": "2020-12-21T18:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg5MTcwNA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546891704", "bodyText": "+1 to using this rule that gets rid of the extra node.", "author": "rdblue", "createdAt": "2020-12-21T19:35:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTgxMDE5NQ=="}], "type": "inlineReview"}, {"oid": "7a87a438309f57eb83ee21178c01f8bae336b89f", "url": "https://github.com/apache/iceberg/commit/7a87a438309f57eb83ee21178c01f8bae336b89f", "message": "Fix vectorized Parquet _file and _pos.", "committedDate": "2020-12-19T00:13:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzU5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157599", "bodyText": "This is needed for cases where Arrow checks the validity buffer.", "author": "rdblue", "createdAt": "2020-12-19T00:14:55Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzY2NA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546157664", "bodyText": "Looks like this was an oversight in the original PR. FYI @chenjunjiedada.", "author": "rdblue", "createdAt": "2020-12-19T00:15:14Z", "path": "arrow/src/main/java/org/apache/iceberg/arrow/vectorized/VectorizedArrowReader.java", "diffHunk": "@@ -381,9 +382,13 @@ public VectorHolder read(VectorHolder reuse, int numValsToRead) {\n         for (int i = 0; i < numValsToRead; i += 1) {\n           vec.getDataBuffer().setLong(i * Long.BYTES, rowStart + i);\n         }\n+        for (int i = 0; i < numValsToRead; i += 1) {\n+          BitVectorHelper.setValidityBitToOne(vec.getValidityBuffer(), i);\n+        }\n         nulls = new NullabilityHolder(numValsToRead);\n       }\n \n+      rowStart += numValsToRead;", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE4NDk5OQ==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546184999", "bodyText": "Great catch! Let me update the unit test as well.", "author": "chenjunjiedada", "createdAt": "2020-12-19T03:39:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1NzY2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjE1ODAyMw==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546158023", "bodyText": "It isn't necessary to check whether there are projected ID columns. The code is shorter if the values are available by default, even if they aren't used. This fixes the problem where there are constants to add (like _file) but no identity partition values are projected.", "author": "rdblue", "createdAt": "2020-12-19T00:16:50Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/BatchDataReader.java", "diffHunk": "@@ -68,18 +68,7 @@\n     // update the current file for Spark's filename() function\n     InputFileBlockHolder.set(file.path().toString(), task.start(), task.length());\n \n-    // schema or rows returned by readers\n-    PartitionSpec spec = task.spec();\n-    Set<Integer> idColumns = spec.identitySourceIds();\n-    Schema partitionSchema = TypeUtil.select(expectedSchema, idColumns);\n-    boolean projectsIdentityPartitionColumns = !partitionSchema.columns().isEmpty();\n-\n-    Map<Integer, ?> idToConstant;\n-    if (projectsIdentityPartitionColumns) {\n-      idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);\n-    } else {\n-      idToConstant = ImmutableMap.of();\n-    }\n+    Map<Integer, ?> idToConstant = PartitionUtil.constantsMap(task, BatchDataReader::convertConstant);", "originalCommit": "7a87a438309f57eb83ee21178c01f8bae336b89f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzMzkxNw==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546633917", "bodyText": "I think this block can be simplified a bit.\n    scan match {\n      case filterable: SupportsFileFilter =>\n        val matchingFilePlan = buildFileFilterPlan(cond, scanRelation)\n        DynamicFileFilter(scanRelation, matchingFilePlan, filterable)\n      case _ =>\n        scanRelation\n    }", "author": "aokolnychyi", "createdAt": "2020-12-21T10:42:10Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDelete.scala", "diffHunk": "@@ -77,33 +101,30 @@ object RewriteDelete extends Rule[LogicalPlan] with PredicateHelper with Logging\n     PushDownUtils.pushFilters(scanBuilder, normalizedPredicates)\n \n     val scan = scanBuilder.build()\n-    val scanRelation = DataSourceV2ScanRelation(table, scan, output)\n+    val scanRelation = DataSourceV2ScanRelation(table, scan, toOutputAttrs(scan.readSchema(), output))\n \n     val scanPlan = scan match {", "originalCommit": "d9213621701f7061dde785e821d588f0ab9020c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0Njg1MzM3MA==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546853370", "bodyText": "Good catch. Updated.", "author": "rdblue", "createdAt": "2020-12-21T18:09:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzMzkxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNTQ0Ng==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546635446", "bodyText": "Used for local testing?", "author": "aokolnychyi", "createdAt": "2020-12-21T10:45:18Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java", "diffHunk": "@@ -48,32 +48,32 @@ public SparkRowLevelOperationsTestBase(String catalogName, String implementation\n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, vectorized = {4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-        { \"testhive\", SparkCatalog.class.getName(),\n-            ImmutableMap.of(\n-                \"type\", \"hive\",\n-                \"default-namespace\", \"default\"\n-            ),\n-            \"orc\",\n-            true\n-        },\n+//        { \"testhive\", SparkCatalog.class.getName(),\n+//            ImmutableMap.of(\n+//                \"type\", \"hive\",\n+//                \"default-namespace\", \"default\"", "originalCommit": "d9213621701f7061dde785e821d588f0ab9020c9", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjgzODgyMw==", "url": "https://github.com/apache/iceberg/pull/1955#discussion_r546838823", "bodyText": "Yes, will remove. Sorry about that, I usually look through the PR to catch these before review!", "author": "rdblue", "createdAt": "2020-12-21T17:37:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NjYzNTQ0Ng=="}], "type": "inlineReview"}, {"oid": "6ae2a917c7cd905420db715be5fc43e4eb1f057e", "url": "https://github.com/apache/iceberg/commit/6ae2a917c7cd905420db715be5fc43e4eb1f057e", "message": "Spark: Sort retained rows in DELETE FROM by file and position.", "committedDate": "2020-12-21T18:10:26Z", "type": "commit"}, {"oid": "2bb4199772e19967f58c7b86b2d95f34c67c05c1", "url": "https://github.com/apache/iceberg/commit/2bb4199772e19967f58c7b86b2d95f34c67c05c1", "message": "Fix vectorized Parquet _file and _pos.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "url": "https://github.com/apache/iceberg/commit/545f5db1d3c7d4a0fbcb58ec5ba5f804a79dbab1", "message": "Fix checkstyle.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "eb90c18be19583c5fbfb219c9b54cc28a7074a17", "url": "https://github.com/apache/iceberg/commit/eb90c18be19583c5fbfb219c9b54cc28a7074a17", "message": "Fix test parameters from debugging.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "message": "Simplify statement in buildScanPlan.", "committedDate": "2020-12-21T18:10:27Z", "type": "commit"}, {"oid": "5b3b8d83141bd637a48b2499519a2db1ad819ef0", "url": "https://github.com/apache/iceberg/commit/5b3b8d83141bd637a48b2499519a2db1ad819ef0", "message": "Simplify statement in buildScanPlan.", "committedDate": "2020-12-21T18:10:27Z", "type": "forcePushed"}, {"oid": "ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "url": "https://github.com/apache/iceberg/commit/ab4505c3aee81d005d2f48e5ef7b689ba80a834a", "message": "Remove ExtendedScanRelation node.", "committedDate": "2020-12-21T19:28:50Z", "type": "commit"}]}