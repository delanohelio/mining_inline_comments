{"pr_number": 1939, "pr_title": "Flink: Commit both data files and delete files to iceberg transaction.", "pr_createdAt": "2020-12-16T09:44:50Z", "pr_url": "https://github.com/apache/iceberg/pull/1939", "timeline": [{"oid": "14c028ba1cbe1ea7e3c4b7935fa00566a8e13d5d", "url": "https://github.com/apache/iceberg/commit/14c028ba1cbe1ea7e3c4b7935fa00566a8e13d5d", "message": "Flink: Commit both data files and delete files to iceberg transaction.", "committedDate": "2020-12-16T09:43:30Z", "type": "commit"}, {"oid": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "url": "https://github.com/apache/iceberg/commit/b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "message": "Minor changes.", "committedDate": "2020-12-16T14:28:23Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDM0MzQ3Mg==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544343472", "bodyText": "I will provide an unit test to address it.", "author": "openinx", "createdAt": "2020-12-16T14:30:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -229,33 +232,71 @@ private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n     }\n   }\n \n-  private void replacePartitions(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n+  private void replacePartitions(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId,\n+                                 long checkpointId) {\n+    // Merge all the pending results into a single write result.\n+    WriteResult result = WriteResult.builder().add(pendingResults.values()).build();\n+\n+    // Partition overwrite does not support delete files.\n+    Preconditions.checkArgument(result.deleteFiles().length == 0,\n+        \"Cannot overwrite partitions with delete files.\");\n     ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n \n+    // Commit the overwrite transaction.\n     int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n+    for (DataFile file : result.dataFiles()) {\n       numFiles += 1;\n       dynamicOverwrite.addFile(file);\n     }\n \n-    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n+    commitOperation(dynamicOverwrite, numFiles, 0, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n   }\n \n-  private void append(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n-    AppendFiles appendFiles = table.newAppend();\n+  private void commitDeltaTxn(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId, long checkpointId) {\n+    // Merge all pending results into a single write result.\n+    WriteResult mergedResult = WriteResult.builder().add(pendingResults.values()).build();\n \n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      appendFiles.appendFile(file);\n-    }\n+    if (mergedResult.deleteFiles().length < 1) {\n+      // To be compatible with iceberg format V1.\n+      AppendFiles appendFiles = table.newAppend();\n+\n+      int numFiles = 0;\n+      for (DataFile file : mergedResult.dataFiles()) {\n+        numFiles += 1;\n+        appendFiles.appendFile(file);\n+      }\n+\n+      commitOperation(appendFiles, numFiles, 0, \"append\", newFlinkJobId, checkpointId);\n+    } else {\n+      // To be compatible with iceberg format V2.\n+      for (Map.Entry<Long, WriteResult> e : pendingResults.entrySet()) {\n+        // We don't commit the merged result into a single transaction because for the sequential transaction txn1 and", "originalCommit": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwMzQzOQ==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545703439", "bodyText": "I've addressed this case in this unit test here.", "author": "openinx", "createdAt": "2020-12-18T09:25:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDM0MzQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTA0Mg==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544749042", "bodyText": "Is this used?", "author": "rdblue", "createdAt": "2020-12-17T01:44:11Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/DeltaManifests.java", "diffHunk": "@@ -19,30 +19,42 @@\n \n package org.apache.iceberg.flink.sink;\n \n-import java.io.IOException;\n-import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import java.util.Iterator;\n+import java.util.List;\n import org.apache.iceberg.ManifestFile;\n-import org.apache.iceberg.ManifestFiles;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.jetbrains.annotations.NotNull;", "originalCommit": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2MzcwNw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544863707", "bodyText": "Em,  this could be removed now.", "author": "openinx", "createdAt": "2020-12-17T07:21:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTA0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTYzNg==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544749636", "bodyText": "Typically, we would follow the Java collection convention and use addAll.", "author": "rdblue", "createdAt": "2020-12-17T01:45:52Z", "path": "core/src/main/java/org/apache/iceberg/io/WriteResult.java", "diffHunk": "@@ -76,6 +76,11 @@ public Builder add(WriteResult result) {\n       return this;\n     }\n \n+    public Builder add(Iterable<WriteResult> results) {", "originalCommit": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg1OTczNw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544859737", "bodyText": "OK,  rename it to addAll sound great to me.", "author": "openinx", "createdAt": "2020-12-17T07:11:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc0OTYzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc1MTgxMw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544751813", "bodyText": "Is it correct for this to be a list of write results if a write result keeps track of a list of data files and a list of delete files?", "author": "rdblue", "createdAt": "2020-12-17T01:52:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -85,9 +88,9 @@\n   // iceberg table when the next checkpoint happen.\n   private final NavigableMap<Long, byte[]> dataFilesPerCheckpoint = Maps.newTreeMap();\n \n-  // The data files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n+  // The completed files cache for current checkpoint. Once the snapshot barrier received, it will be flushed to the\n   // 'dataFilesPerCheckpoint'.\n-  private final List<DataFile> dataFilesOfCurrentCheckpoint = Lists.newArrayList();\n+  private final List<WriteResult> writeResultsOfCurrentCkpt = Lists.newArrayList();", "originalCommit": "b5d2b09be04560167ccf2c5dd9cc10f3ba0d05a2", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg2Mjk5Ng==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544862996", "bodyText": "Yes, it's correct here. Because if there're 5 IcebergStreamWriter, then each writer will emit a WriteResult.  For the one parallelism IcebergFilesCommitter,  it will collect all the WriteResult(s) in this writeResultsOfCurrentCkpt cache,  and then merge them into a single WriteResult.  Finally,  write those files into delete + data manifests and update the flink statebackend.", "author": "openinx", "createdAt": "2020-12-17T07:19:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDc1MTgxMw=="}], "type": "inlineReview"}, {"oid": "c00716509e57423ba3e7cf21212a0f674004a2c7", "url": "https://github.com/apache/iceberg/commit/c00716509e57423ba3e7cf21212a0f674004a2c7", "message": "Addressing the comments.", "committedDate": "2020-12-17T07:13:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDg4NTY1OA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r544885658", "bodyText": "We will need to maintain the flink state's compatibility.  If the encoding version is 1, then we should use the FlinkManifestSerializer way to read the byte[].", "author": "openinx", "createdAt": "2020-12-17T08:04:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -184,36 +187,36 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> deltaManifestsMap,\n                                     String newFlinkJobId,\n                                     long checkpointId) throws IOException {\n-    NavigableMap<Long, byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+    NavigableMap<Long, byte[]> pendingMap = deltaManifestsMap.headMap(checkpointId, true);\n \n-    List<ManifestFile> manifestFiles = Lists.newArrayList();\n-    List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (byte[] manifestData : pendingManifestMap.values()) {\n-      if (Arrays.equals(EMPTY_MANIFEST_DATA, manifestData)) {\n+    List<DeltaManifests> deltaManifestsList = Lists.newArrayList();\n+    NavigableMap<Long, WriteResult> pendingResults = Maps.newTreeMap();\n+    for (Map.Entry<Long, byte[]> e : pendingMap.entrySet()) {\n+      if (Arrays.equals(EMPTY_MANIFEST_DATA, e.getValue())) {\n         // Skip the empty flink manifest.\n         continue;\n       }\n \n-      ManifestFile manifestFile =\n-          SimpleVersionedSerialization.readVersionAndDeSerialize(FlinkManifestSerializer.INSTANCE, manifestData);\n+      DeltaManifests deltaManifests =\n+          SimpleVersionedSerialization.readVersionAndDeSerialize(DeltaManifestsSerializer.INSTANCE, e.getValue());", "originalCommit": "c00716509e57423ba3e7cf21212a0f674004a2c7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1eae61a63938e85eb40eee9c27f39889b9bf2e07", "url": "https://github.com/apache/iceberg/commit/1eae61a63938e85eb40eee9c27f39889b9bf2e07", "message": "Add unit tests to address the state compatibility issues.", "committedDate": "2020-12-17T09:15:18Z", "type": "commit"}, {"oid": "0fdec6764e2b8d4962bc027512895581ba31b89f", "url": "https://github.com/apache/iceberg/commit/0fdec6764e2b8d4962bc027512895581ba31b89f", "message": "Minor changes.", "committedDate": "2020-12-17T09:43:33Z", "type": "commit"}, {"oid": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "url": "https://github.com/apache/iceberg/commit/4e769b2c522c8ab7df7bf10eb267c758361e0a01", "message": "Add unit tests: addressing the case that commit two failure checkpoints in the lastest sucessful checkpoint.", "committedDate": "2020-12-17T11:45:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2NTAxNg==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545265016", "bodyText": "does this need to extend from Iterable? It seems only needed for using Iterables.addAll(manifests, deltaManifests);. is it simpler to directly to cal the two getters?", "author": "stevenzwu", "createdAt": "2020-12-17T17:23:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/DeltaManifests.java", "diffHunk": "@@ -19,30 +19,40 @@\n \n package org.apache.iceberg.flink.sink;\n \n-import java.io.IOException;\n-import org.apache.flink.core.io.SimpleVersionedSerializer;\n+import java.util.Iterator;\n+import java.util.List;\n import org.apache.iceberg.ManifestFile;\n-import org.apache.iceberg.ManifestFiles;\n-import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n-class FlinkManifestSerializer implements SimpleVersionedSerializer<ManifestFile> {\n-  private static final int VERSION_NUM = 1;\n-  static final FlinkManifestSerializer INSTANCE = new FlinkManifestSerializer();\n+class DeltaManifests implements Iterable<ManifestFile> {\n \n-  @Override\n-  public int getVersion() {\n-    return VERSION_NUM;\n+  private final ManifestFile dataManifest;\n+  private final ManifestFile deleteManifest;\n+\n+  DeltaManifests(ManifestFile dataManifest, ManifestFile deleteManifest) {\n+    this.dataManifest = dataManifest;\n+    this.deleteManifest = deleteManifest;\n   }\n \n-  @Override\n-  public byte[] serialize(ManifestFile manifestFile) throws IOException {\n-    Preconditions.checkNotNull(manifestFile, \"ManifestFile to be serialized should not be null\");\n+  ManifestFile dataManifest() {\n+    return dataManifest;\n+  }\n \n-    return ManifestFiles.encode(manifestFile);\n+  ManifestFile deleteManifest() {\n+    return deleteManifest;\n   }\n \n   @Override\n-  public ManifestFile deserialize(int version, byte[] serialized) throws IOException {\n-    return ManifestFiles.decode(serialized);\n+  public Iterator<ManifestFile> iterator() {", "originalCommit": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NjEzMw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545556133", "bodyText": "OK,  Agreed we don't have to introduce the complex Iterable.", "author": "openinx", "createdAt": "2020-12-18T03:50:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2NTAxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2OTQ0OA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545269448", "bodyText": "do we need to check if WriteResult is empty (no data and delete files)?", "author": "stevenzwu", "createdAt": "2020-12-17T17:29:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java", "diffHunk": "@@ -63,4 +66,53 @@ static ManifestOutputFileFactory createOutputFileFactory(Table table, String fli\n     TableOperations ops = ((HasTableOperations) table).operations();\n     return new ManifestOutputFileFactory(ops, table.io(), table.properties(), flinkJobId, subTaskId, attemptNumber);\n   }\n+\n+  static DeltaManifests writeCompletedFiles(WriteResult result,\n+                                            Supplier<OutputFile> outputFileSupplier,\n+                                            PartitionSpec spec) throws IOException {\n+\n+    ManifestFile dataManifest = null;\n+    ManifestFile deleteManifest = null;\n+\n+    // Write the completed data files into a newly created data manifest file.\n+    if (result.dataFiles() != null && result.dataFiles().length > 0) {\n+      dataManifest = writeDataFiles(outputFileSupplier.get(), spec, Lists.newArrayList(result.dataFiles()));\n+    }\n+\n+    // Write the completed delete files into a newly created delete manifest file.\n+    if (result.deleteFiles() != null && result.deleteFiles().length > 0) {\n+      OutputFile deleteManifestFile = outputFileSupplier.get();\n+\n+      ManifestWriter<DeleteFile> deleteManifestWriter = ManifestFiles.writeDeleteManifest(FORMAT_V2, spec,\n+          deleteManifestFile, DUMMY_SNAPSHOT_ID);\n+      try (ManifestWriter<DeleteFile> writer = deleteManifestWriter) {\n+        for (DeleteFile deleteFile : result.deleteFiles()) {\n+          writer.add(deleteFile);\n+        }\n+      }\n+\n+      deleteManifest = deleteManifestWriter.toManifestFile();\n+    }\n+\n+    return new DeltaManifests(dataManifest, deleteManifest);", "originalCommit": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwMDE1Mw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545700153", "bodyText": "We have a similar discussion here.  Even if the WriteResult is empty ( NOT null, null means there's nobody emitted a result to the IcebergFilesCommitter, while empty WriteResult means the IcebergStreamWriter did not write any new data but still emit a WriterResult with zero data files and zero delete files to downstream IcebergFilesCommitter),  we'd better to commit to iceberg txn so that the flink streaming job won't be failure easily when expiring a old snapshot (since that time we did not even write any new records).", "author": "openinx", "createdAt": "2020-12-18T09:21:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2OTQ0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcwMTM3MA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545701370", "bodyText": "About this question,  I think we'd better to keep the dummy DeltaManifests in state , although it has no delete files and data files.", "author": "openinx", "createdAt": "2020-12-18T09:23:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI2OTQ0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTgxMg==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545271812", "bodyText": "just for my own education, referencedDataFiles from WriteResult doesn't seem to be used (except for unit test). What is it for? do we need to serialize it too?", "author": "stevenzwu", "createdAt": "2020-12-17T17:33:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/FlinkManifestUtil.java", "diffHunk": "@@ -63,4 +66,53 @@ static ManifestOutputFileFactory createOutputFileFactory(Table table, String fli\n     TableOperations ops = ((HasTableOperations) table).operations();\n     return new ManifestOutputFileFactory(ops, table.io(), table.properties(), flinkJobId, subTaskId, attemptNumber);\n   }\n+\n+  static DeltaManifests writeCompletedFiles(WriteResult result,", "originalCommit": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI5MTM2MQ==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545291361", "bodyText": "We should serialize it and add it to the commit. This is the set of files that is referenced by any positional delete, which identifies deleted rows by file and row position. The commit will validate that all of the files still exist in the table.\nThis isn't strictly needed for this use case because we know that the position deletes only refer to files that are created in this commit. Since the files are being added in the commit, it isn't possible for some other process to delete some of them from metadata. But it is still good to configure the commit properly in case this gets reused later.", "author": "rdblue", "createdAt": "2020-12-17T18:00:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTU1NzgwOA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545557808", "bodyText": "Thanks for the explanation, @rdblue .   I think it's correct to validate the data files in RowDelta#commit.  Will provide an extra unit test to address it.", "author": "openinx", "createdAt": "2020-12-18T03:57:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTgxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcyMTU0OA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545721548", "bodyText": "This unit test addressed the data files validation issue", "author": "openinx", "createdAt": "2020-12-18T09:52:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTI3MTgxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3MTM1OQ==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545471359", "bodyText": "We are using this API from AppendFiles interface. When we had an extended outage and accumulated a few hundreds of transactions/manifests in Flink checkpoint, this help avoiding rewrite of those manifest files. Otherwise, commit can take very long. @rdblue can probably explain it better than I do.\n  AppendFiles appendManifest(ManifestFile file);\n\nhere we are merging data files potentially from multiple checkpoint cycles/manifests into a single manifest file. Maybe we can add a similar API in DeleteFiles interface?\n  DeleteFiles deleteManifest(ManifestFile file);", "author": "stevenzwu", "createdAt": "2020-12-17T23:25:54Z", "path": "flink/src/main/java/org/apache/iceberg/flink/sink/IcebergFilesCommitter.java", "diffHunk": "@@ -184,78 +185,106 @@ public void notifyCheckpointComplete(long checkpointId) throws Exception {\n     }\n   }\n \n-  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> manifestsMap,\n+  private void commitUpToCheckpoint(NavigableMap<Long, byte[]> deltaManifestsMap,\n                                     String newFlinkJobId,\n                                     long checkpointId) throws IOException {\n-    NavigableMap<Long, byte[]> pendingManifestMap = manifestsMap.headMap(checkpointId, true);\n+    NavigableMap<Long, byte[]> pendingMap = deltaManifestsMap.headMap(checkpointId, true);\n \n-    List<ManifestFile> manifestFiles = Lists.newArrayList();\n-    List<DataFile> pendingDataFiles = Lists.newArrayList();\n-    for (byte[] manifestData : pendingManifestMap.values()) {\n-      if (Arrays.equals(EMPTY_MANIFEST_DATA, manifestData)) {\n+    List<ManifestFile> manifests = Lists.newArrayList();\n+    NavigableMap<Long, WriteResult> pendingResults = Maps.newTreeMap();\n+    for (Map.Entry<Long, byte[]> e : pendingMap.entrySet()) {\n+      if (Arrays.equals(EMPTY_MANIFEST_DATA, e.getValue())) {\n         // Skip the empty flink manifest.\n         continue;\n       }\n \n-      ManifestFile manifestFile =\n-          SimpleVersionedSerialization.readVersionAndDeSerialize(FlinkManifestSerializer.INSTANCE, manifestData);\n-\n-      manifestFiles.add(manifestFile);\n-      pendingDataFiles.addAll(FlinkManifestUtil.readDataFiles(manifestFile, table.io()));\n+      DeltaManifests deltaManifests = SimpleVersionedSerialization\n+          .readVersionAndDeSerialize(DeltaManifestsSerializer.INSTANCE, e.getValue());\n+      pendingResults.put(e.getKey(), FlinkManifestUtil.readCompletedFiles(deltaManifests, table.io()));\n+      Iterables.addAll(manifests, deltaManifests);\n     }\n \n     if (replacePartitions) {\n-      replacePartitions(pendingDataFiles, newFlinkJobId, checkpointId);\n+      replacePartitions(pendingResults, newFlinkJobId, checkpointId);\n     } else {\n-      append(pendingDataFiles, newFlinkJobId, checkpointId);\n+      commitDeltaTxn(pendingResults, newFlinkJobId, checkpointId);\n     }\n \n-    pendingManifestMap.clear();\n+    pendingMap.clear();\n \n-    // Delete the committed manifests and clear the committed data files from dataFilesPerCheckpoint.\n-    for (ManifestFile manifestFile : manifestFiles) {\n+    // Delete the committed manifests.\n+    for (ManifestFile manifest : manifests) {\n       try {\n-        table.io().deleteFile(manifestFile.path());\n+        table.io().deleteFile(manifest.path());\n       } catch (Exception e) {\n         // The flink manifests cleaning failure shouldn't abort the completed checkpoint.\n         String details = MoreObjects.toStringHelper(this)\n             .add(\"flinkJobId\", newFlinkJobId)\n             .add(\"checkpointId\", checkpointId)\n-            .add(\"manifestPath\", manifestFile.path())\n+            .add(\"manifestPath\", manifest.path())\n             .toString();\n         LOG.warn(\"The iceberg transaction has been committed, but we failed to clean the temporary flink manifests: {}\",\n             details, e);\n       }\n     }\n   }\n \n-  private void replacePartitions(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n+  private void replacePartitions(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId,\n+                                 long checkpointId) {\n+    // Partition overwrite does not support delete files.\n+    int deleteFilesNum = pendingResults.values().stream().mapToInt(r -> r.deleteFiles().length).sum();\n+    Preconditions.checkState(deleteFilesNum == 0, \"Cannot overwrite partitions with delete files.\");\n+\n+    // Commit the overwrite transaction.\n     ReplacePartitions dynamicOverwrite = table.newReplacePartitions();\n \n     int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      dynamicOverwrite.addFile(file);\n+    for (WriteResult result : pendingResults.values()) {\n+      numFiles += result.dataFiles().length;\n+      Arrays.stream(result.dataFiles()).forEach(dynamicOverwrite::addFile);\n     }\n \n-    commitOperation(dynamicOverwrite, numFiles, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n+    commitOperation(dynamicOverwrite, numFiles, 0, \"dynamic partition overwrite\", newFlinkJobId, checkpointId);\n   }\n \n-  private void append(List<DataFile> dataFiles, String newFlinkJobId, long checkpointId) {\n-    AppendFiles appendFiles = table.newAppend();\n+  private void commitDeltaTxn(NavigableMap<Long, WriteResult> pendingResults, String newFlinkJobId, long checkpointId) {\n+    int deleteFilesNum = pendingResults.values().stream().mapToInt(r -> r.deleteFiles().length).sum();\n \n-    int numFiles = 0;\n-    for (DataFile file : dataFiles) {\n-      numFiles += 1;\n-      appendFiles.appendFile(file);\n-    }\n+    if (deleteFilesNum == 0) {\n+      // To be compatible with iceberg format V1.\n+      AppendFiles appendFiles = table.newAppend();\n \n-    commitOperation(appendFiles, numFiles, \"append\", newFlinkJobId, checkpointId);\n+      int numFiles = 0;\n+      for (WriteResult result : pendingResults.values()) {", "originalCommit": "4e769b2c522c8ab7df7bf10eb267c758361e0a01", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTcxNTM3MA==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545715370", "bodyText": "It sounds like a separate improvement , so I created an issue for this , let's discuss there, #1959.", "author": "openinx", "createdAt": "2020-12-18T09:41:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3MTM1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTk5NDgxMw==", "url": "https://github.com/apache/iceberg/pull/1939#discussion_r545994813", "bodyText": "Maybe we can add a similar API in DeleteFiles interface?\n\nWe don't currently do this because we need delete entries to exist when we delete files. That way we can track when something was deleted and clean it up incrementally in ExpireSnapshots. If we did have a method like this, it would always rewrite the manifest with deletes, or would need to ensure that the manifest that is added contains only deletes, and these requirements are not very obvious. I think it is better to pass the deleted files through the existing methods.", "author": "rdblue", "createdAt": "2020-12-18T17:55:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTQ3MTM1OQ=="}], "type": "inlineReview"}, {"oid": "0c6d008515af75c9f2cc13864c6fed4ead6b4915", "url": "https://github.com/apache/iceberg/commit/0c6d008515af75c9f2cc13864c6fed4ead6b4915", "message": "Address the comments and add more unit tests.", "committedDate": "2020-12-18T09:11:59Z", "type": "commit"}]}