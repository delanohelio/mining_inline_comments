{"pr_number": 843, "pr_title": "InputFormat support for Iceberg", "pr_createdAt": "2020-03-16T00:51:25Z", "pr_url": "https://github.com/apache/iceberg/pull/843", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r392831257", "bodyText": "Why do we need to depend on iceberg-hive module? Not sure if this will potentially introduce circular dependency.", "author": "jerryshao", "createdAt": "2020-03-16T07:39:15Z", "path": "build.gradle", "diffHunk": "@@ -193,6 +193,24 @@ project(':iceberg-hive') {\n   }\n }\n \n+project(':iceberg-mr') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-core')\n+    compile project(':iceberg-hive')\n+    compile project(':iceberg-orc')\n+    compile project(':iceberg-parquet')\n+\n+    compileOnly(\"org.apache.hadoop:hadoop-client\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+    }\n+\n+    testCompile project(path: ':iceberg-hive', configuration: 'testArtifacts')", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAwMDc4NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393000784", "bodyText": "This is so that I can rely on HiveCatalog. Yes you are right though, we'd have to figure out how to arrange out modules when we have a Hive storage handler", "author": "rdsr", "createdAt": "2020-03-16T12:51:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIwODcxOQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393208719", "bodyText": "Maybe this could just rely on a catalog from the API, and include iceberg-hive for tests. That wouldn't introduce a dependency cycle.", "author": "rdblue", "createdAt": "2020-03-16T17:54:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDMzOTI1NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394339254", "bodyText": "We'd need the org.apache.iceberg.hive.HiveCatalogs#loadCatalog which is available in iceberg-hive", "author": "rdsr", "createdAt": "2020-03-18T13:19:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2NzI1Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394467253", "bodyText": "I think we have some options. We could add a config option to the builder to set the catalog, or we could use reflection to call HiveCatalogs.loadCatalog if the class is present.", "author": "rdblue", "createdAt": "2020-03-18T16:10:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MDU3Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395780577", "bodyText": "I can see a future where we have Hive SerDes and StorageHandlers in the hive subproject which in turn will need to depend on the input format(s) here so we need to avoid a circular dependency right? In our InputFormat up to now we've just use HadoopTables and the JobConf to get everything we need.", "author": "massdosage", "createdAt": "2020-03-20T17:16:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxMjA1OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396012059", "bodyText": "I fixed this. For now, I've adding iceberg-hive as a test dep", "author": "rdsr", "createdAt": "2020-03-21T17:35:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxNjU5Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396016593", "bodyText": "Maybe iceberg-hive as a test dep can also cause recursive deps. If that's the case we can simply test with HadoopCatalog instead and remove iceberg-hive altogether", "author": "rdsr", "createdAt": "2020-03-21T18:28:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQ0MTg2Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402441867", "bodyText": "I've removed iceberg-hive altogether. Relying on HadoopCatalog to test the Catalog plugin", "author": "rdsr", "createdAt": "2020-04-02T16:21:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjgzMTI1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg2MTc2NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r392861765", "bodyText": "I read the pig & spark source reader, seems we could abstract the FileScanTask open(..) method with readSchema,  so that all the compute engine can share that part of code,  maybe we can defined it as:\nIterator<T> open(FileScanTask currentTask, Schema readSchema, ReadSupport<T> readSupport).\n\nI think it could be a file-level abstraction ..", "author": "openinx", "createdAt": "2020-03-16T08:49:20Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {\n+      return conf;\n+    }\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private ReadSupport<T> readSupport;\n+    private Closeable currentCloseable;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext context) {\n+      Configuration conf = context.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = context;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.readSupport = readSupport(conf);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private ReadSupport<T> readSupport(Configuration conf) {\n+      String readSupportClassName = conf.get(READ_SUPPORT);\n+      try {\n+        return DynClasses\n+            .builder()\n+            .impl(readSupportClassName)\n+            .<ReadSupport<T>>buildChecked()\n+            .newInstance();\n+      } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {\n+        throw new RuntimeException(String.format(\"Unable to instantiate read support %s\", readSupportClassName), e);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema partitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> readSupport.withPartitionColumns(row, partitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjkzNTMwNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r392935307", "bodyText": "I also have the same feelings, with more framework supported, there could be a bunch of duplicated codes. It would be better to have an abstract layer which could possibly be adapted to all the frameworks.", "author": "jerryshao", "createdAt": "2020-03-16T10:55:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg2MTc2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzAwMzYzOQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393003639", "bodyText": "Pig will eventually rely on this InputFormat, but I agree generally, if we can abstract away common code, it would be great.", "author": "rdsr", "createdAt": "2020-03-16T12:56:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg2MTc2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTEzNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393221137", "bodyText": "Yeah, there may be a good way to extract common code. For now, let's do that in a separate PR though to keep this focused on the MR support.", "author": "rdblue", "createdAt": "2020-03-16T18:11:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mjg2MTc2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxMDk0NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393210945", "bodyText": "Looks like this serialization (java serializable/gzip/base64) is only used for the filter expression. We should consider (in a separate commit) using a parser for Iceberg expressions so that we can convert them to infix strings instead and parse them when needed. That makes the job configuration readable, which is always a good thing.", "author": "rdblue", "createdAt": "2020-03-16T17:57:55Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxMTQ3Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393211472", "bodyText": "We could also serialize to/from JSON if we wanted something easier than including or building a parser.", "author": "rdblue", "createdAt": "2020-03-16T17:58:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxMDk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxOTc3Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393219773", "bodyText": "Rather than allowing a user to pass in ReadSupport, I think it makes more sense to configure at a higher level. There are good built-in options that don't require exposing classes from Parquet, Avro, and ORC here.\nI think the config builder could expose methods to control the in-memory format. By default, it would use Iceberg generic records. Optionally, we could expose Pig's in-memory types and Hive's in-memory types. Maybe Avro as well. Those seem like reasonable options for an InputFormat given that users will probably not be customizing their in-memory classes if they are still using the MR read interface. At this point, I would expect the input format to be primarily used for integration with Pig and Hive.", "author": "rdblue", "createdAt": "2020-03-16T18:09:40Z", "path": "mr/src/main/java/org/apache/iceberg/mr/ReadSupport.java", "diffHunk": "@@ -0,0 +1,50 @@\n+package org.apache.iceberg.mr;\n+\n+import java.util.function.BiFunction;\n+import org.apache.avro.io.DatumReader;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.orc.OrcValueReader;\n+import org.apache.iceberg.parquet.ParquetValueReader;\n+import org.apache.iceberg.parquet.VectorizedReader;\n+import org.apache.orc.TypeDescription;\n+import org.apache.parquet.schema.MessageType;\n+\n+import java.util.function.Function;\n+\n+\n+/**\n+ * ReadSupport for MR InputFormat, providing value readers\n+ * for different data formats and appending identity partition columns\n+ * to the input row\n+ * @param <T>\n+ */\n+public interface ReadSupport<T> {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMjg3Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393222877", "bodyText": "I think that makes sense. We either allow Pig's Tuples, Hive's ObjectInspectors or default Iceberg GenericRecords", "author": "rdsr", "createdAt": "2020-03-16T18:14:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIxOTc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTM3MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393221370", "bodyText": "No need for \"Expression\" in this API. How about just filter?", "author": "rdblue", "createdAt": "2020-03-16T18:12:07Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTc2OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393221769", "bodyText": "I'd rather not throw IOException here. Can we update serializeToBase64 to not throw it?", "author": "rdblue", "createdAt": "2020-03-16T18:12:52Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMzIyMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393223221", "bodyText": "+1 .  This was mainly the result of copy pasting error.", "author": "rdsr", "createdAt": "2020-03-16T18:15:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMTc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMjkyNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393222924", "bodyText": "If we are going with the builder pattern, then let's set the table/path and in-memory data model in builder methods instead of here. I prefer not to minimize constructor args with builders by removing anything that might not need to be set (e.g. use Iceberg generics by default).", "author": "rdblue", "createdAt": "2020-03-16T18:14:48Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNDc2Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393224767", "bodyText": "in-memory data model +1.\nFor table path, which is mandatory, does it make more sense in the constructor? Then users cannot omit it.  Granted that if they omit it, the code will just fail, but I general add required args as const. params and options and methods to the builder.", "author": "rdsr", "createdAt": "2020-03-16T18:18:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMjkyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Mzk5MzkyMg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393993922", "bodyText": "Decided to set everything as builder methods except configuration, similar to Kite", "author": "rdsr", "createdAt": "2020-03-17T21:56:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMjkyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMzU4Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393223586", "bodyText": "Can we add documentation?", "author": "rdblue", "createdAt": "2020-03-16T18:15:49Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxMjEyNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396012124", "bodyText": "Added.", "author": "rdsr", "createdAt": "2020-03-21T17:36:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyMzU4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNDg1MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393224851", "bodyText": "I'm not sure that this is needed. Unless this builder makes a copy of the Configuration, it is configuring the one passed in as methods are called. I think that's probably fine.", "author": "rdblue", "createdAt": "2020-03-16T18:18:14Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNzEyMA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393227120", "bodyText": "I like the builder pattern here. I used something similar for Kite's InputFormat.\nYou might consider how that builder worked. The static methods there would accept a Job when creating the builder. That way, there was no need for the user to also set the input format class. It was also not necessary to build the resulting Configuration and do something with it:\n  /**\n   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n   * returns a helper to add further configuration.\n   *\n   * @param job the {@code Job} to configure\n   */\n  public static ConfigBuilder configure(Job job) {\n    job.setInputFormatClass(IcebergInputFormat.class);\n    return new ConfigBuilder(job.getConfiguration());\n  }", "author": "rdblue", "createdAt": "2020-03-16T18:22:16Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyODM0Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393228347", "bodyText": "This looks great. Will use", "author": "rdsr", "createdAt": "2020-03-16T18:24:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyNzEyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIyODg3MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393228871", "bodyText": "The base scan will combine the requested columns with the ones used by row filters to produce the required schema. If you don't intend to use that, then it isn't needed. But you could add the schema to the splits to know what to project.\nSee https://github.com/apache/incubator-iceberg/blob/master/core/src/main/java/org/apache/iceberg/BaseTableScan.java#L298", "author": "rdblue", "createdAt": "2020-03-16T18:25:28Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MzIzMjg2OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r393232869", "bodyText": "You might want to use DynConstructors and build instead of buildChecked. That will get you better error messages and some of this error handling for free.", "author": "rdblue", "createdAt": "2020-03-16T18:32:42Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,345 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynClasses;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String READ_SUPPORT = \"iceberg.mr.read.support\";\n+\n+  private transient Table table;\n+  private transient List<InputSplit> splits;\n+\n+  public IcebergInputFormat() {\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    table = getTable(conf);\n+    TableScan scan = table.newScan();\n+    //TODO add caseSensitive, snapshot id etc..\n+\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    final String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      // Not sure if this is having any effect?\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader();\n+  }\n+\n+  public static ConfBuilder updateConf(\n+      Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+    return new ConfBuilder(conf, path, readSupportClass);\n+  }\n+\n+  public static class ConfBuilder {\n+    private final Configuration conf;\n+\n+    public ConfBuilder(Configuration conf, String path, Class<? extends ReadSupport<?>> readSupportClass) {\n+      this.conf = conf;\n+      conf.set(TABLE_PATH, path);\n+      conf.set(READ_SUPPORT, readSupportClass.getName());\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    }\n+\n+    public ConfBuilder filterExpression(Expression expression) throws IOException {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    //TODO: other options split-size, snapshotid etc..\n+    public Configuration updatedConf() {\n+      return conf;\n+    }\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private ReadSupport<T> readSupport;\n+    private Closeable currentCloseable;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext context) {\n+      Configuration conf = context.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = context;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.readSupport = readSupport(conf);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private ReadSupport<T> readSupport(Configuration conf) {\n+      String readSupportClassName = conf.get(READ_SUPPORT);\n+      try {\n+        return DynClasses\n+            .builder()\n+            .impl(readSupportClassName)\n+            .<ReadSupport<T>>buildChecked()\n+            .newInstance();\n+      } catch (InstantiationException | IllegalAccessException | ClassNotFoundException e) {", "originalCommit": "f8dc2b09feac246310e6c8139bfa734330cf49a0", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "url": "https://github.com/apache/iceberg/commit/5f5ffbd30f0ae7881225d886a27bdcb87c69078c", "message": "Address review comments", "committedDate": "2020-03-18T06:51:19Z", "type": "forcePushed"}, {"oid": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "url": "https://github.com/apache/iceberg/commit/1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "message": "Address review comments", "committedDate": "2020-03-18T13:17:58Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MDYwMA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394470600", "bodyText": "We should note in documentation that this InputFormat must perform its own split planning and doesn't accept FileSplit instances. I think it's fairly common to create and pass file splits, so we may just want to accept them and treat them as single-file tasks.", "author": "rdblue", "createdAt": "2020-03-18T16:15:21Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4Mzk2Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394483967", "bodyText": "If the splits are only created using Iceberg apis, I'd imagine we will only be getting IcebergSpits , no?", "author": "rdsr", "createdAt": "2020-03-18T16:34:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MDYwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4ODQzNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394488434", "bodyText": "Yes, if splits are produced using getSplits then we'd be fine. I seem to remember some problem doing that with Hive for Parquet, though. We ended up using regular FileSplit there.", "author": "rdblue", "createdAt": "2020-03-18T16:41:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MDYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394472584", "bodyText": "I just realized that we also need to assert that the residual for each file in the task is either null or Expressions.alwaysTrue. That will guarantee that the filter expression is completely satisfied and no additional rows will be returned.\nAlternatively, we could run the residual to filter every row in the reader, but it's probably simpler for now to open that as a follow-up issue.", "author": "rdblue", "createdAt": "2020-03-18T16:18:12Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxMjQ3OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396012478", "bodyText": "I think Pig and Hive could apply the residuals on their own if we return the pushed down filters as is, like we do in Spark.  For MR standalone, we can run the residual like we do for IcebergGenerics for now I do as you mentioned but only for MR standalone.", "author": "rdsr", "createdAt": "2020-03-21T17:40:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1ODgwMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396158801", "bodyText": "I like that this applies the residuals for generics, but I think we need to be a bit safer for the others. I think we should make a config option to enable the current behavior with Pig and Hive object models. If it is set, then Pig and Hive are expected to apply the final filter on their own. If not, then we assert that the residual is null or alwaysTrue. I think it's much safer to make skipping residuals opt-in like that.", "author": "rdblue", "createdAt": "2020-03-22T23:24:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQxNjcyNQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397416725", "bodyText": "Fixed.", "author": "rdsr", "createdAt": "2020-03-24T19:45:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQyNTM0NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397425345", "bodyText": "Added follow up for residual evaluation for Iceberg generics - #866", "author": "rdsr", "createdAt": "2020-03-24T20:00:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NDU1Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394474553", "bodyText": "I think we need to update how we handle partition columns in general. #585 points out that the current row-join approach doesn't work for nested columns. We'll need to update row materialization to fix it. I can work on that sometime soon because we also need it for metadata columns in Spark (in particular, row ordinal in a file).\nFYI @aokolnychyi.", "author": "rdblue", "createdAt": "2020-03-18T16:20:57Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NjI5MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394476290", "bodyText": "What about changing this to usePigTuples() and useHiveRows()? That eliminates the need for a public enum and I think is easy to read.", "author": "rdblue", "createdAt": "2020-03-18T16:23:28Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NzAwNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394477007", "bodyText": "+1. I like yours better!\nI'll keep the enum but make it private and expose methods as u described above", "author": "rdsr", "createdAt": "2020-03-18T16:24:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3NjI5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394480678", "bodyText": "In Spark, this is an instance method so that user can override it and use their own catalog. I think that would be a good idea here as well:\npublic class CustomCatalogIcebergInputFormat extends IcebergInputFormat {\n  @Override\n  public Table getTable(String tablePath) {\n    CustomCatalog.get().loadTable(conf.get(tablePath));\n  }\n}\nIn that example, I'm assuming that we'd have a final getTable(Configuration) method that pulls TABLE_PATH out of the config and passes it to getTable.", "author": "rdblue", "createdAt": "2020-03-18T16:29:37Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM3NDc0MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395374741", "bodyText": "I think this is slightly related to the HiveCatalog discussion above where we want to load HiveCatalog through reflection if HiveCatalog is on the classpath [since we do not want to add the iceberg-hive dependency]\nThe problem here is that this getTable method needs to be static since it is being called from the configBuilder.\nTo solve both these issues - not relying on hive-deps at compile time and ability to plug in a custom catalog, can we define a CatalogFactory which allows us to load our own custom catalogs. The CatalogFactory can be passed as a config property. Since this factory is an interface, it should be easier to instantiate [through reflection] and then we can load our custom catalog.\nWhat do u think?", "author": "rdsr", "createdAt": "2020-03-19T23:28:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4MTQ3OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395381478", "bodyText": "Yeah, I guess that's not a good way to inject a catalog. In addition, the configuration methods sets up the IcebergInputFormat and not a user's subclass.\nI think the factory idea would work, but it's more of a Supplier<Catalog> since there is no configuration. Like this?\n  ConfigBuilder catalogSupplier(Class<? extends Supplier<Catalog>> supplierClass)) {\n    this.supplierClass = supplierClass;\n    return this;\n  }", "author": "rdblue", "createdAt": "2020-03-19T23:52:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM4OTg1OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395389858", "bodyText": "Hmm, don't all our Catalogs rely on Configuration during instantiation?", "author": "rdsr", "createdAt": "2020-03-20T00:25:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTM5MDM4NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395390384", "bodyText": "You're right, we probably want to use Function instead.", "author": "rdblue", "createdAt": "2020-03-20T00:28:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MDY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MTE0MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394481140", "bodyText": "Is this duplicated? Maybe we should have a locality util class.", "author": "rdblue", "createdAt": "2020-03-18T16:30:21Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      Catalog catalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    }\n+  }\n+\n+  private static class IcebergSplit extends InputSplit implements Writable {\n+    private static final String[] ANYWHERE = new String[]{\"*\"};\n+    private CombinedScanTask task;\n+    private transient String[] locations;\n+    private transient Configuration conf;\n+\n+    IcebergSplit(Configuration conf, CombinedScanTask task) {\n+      this.task = task;\n+      this.conf = conf;\n+    }\n+\n+    @Override\n+    public long getLength() {\n+      return task.files().stream().mapToLong(FileScanTask::length).sum();\n+    }\n+\n+    @Override\n+    public String[] getLocations() {\n+      boolean localityPreferred = conf.getBoolean(LOCALITY, false);\n+      if (!localityPreferred) {\n+        return ANYWHERE;\n+      }\n+      if (locations != null) {\n+        return locations;\n+      }\n+\n+      Set<String> locationSets = Sets.newHashSet();", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MjYwMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394482601", "bodyText": "We should make this an assertion before committing this PR.", "author": "rdblue", "createdAt": "2020-03-18T16:32:34Z", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.avro.RandomAvroData;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+public class TestIcebergInputFormat {\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  private File tableLocation;\n+\n+  private static final Schema SCHEMA = new Schema(\n+      required(1, \"id\", Types.LongType.get()),\n+      optional(2, \"data\", Types.StringType.get()),\n+      required(3, \"date\", Types.StringType.get()));\n+\n+  private static final PartitionSpec PARTITION_BY_DATE = PartitionSpec\n+      .builderFor(SCHEMA)\n+      .identity(\"date\")\n+      .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  IcebergInputFormat<GenericData.Record> icebergInputFormat;\n+\n+  @Test\n+  public void test() throws IOException, InterruptedException {\n+    tableLocation = new File(temp.newFolder(), \"table\");\n+    Table table = TABLES.create(SCHEMA, PARTITION_BY_DATE, tableLocation.toString());\n+    List<GenericData.Record> records = RandomAvroData.generate(SCHEMA, 5, 0L);\n+    File file = temp.newFile();\n+    Assert.assertTrue(file.delete());\n+    try (FileAppender<GenericData.Record> appender = Avro.write(Files.localOutput(file))\n+                                                         .schema(SCHEMA)\n+                                                         .named(\"avro\")\n+                                                         .build()) {\n+      appender.addAll(records);\n+    }\n+\n+    DataFile dataFile = DataFiles.builder(PARTITION_BY_DATE)\n+                                 .withPartition(partitionData(\"2020-03-15\"))\n+                                 .withRecordCount(records.size())\n+                                 .withFileSizeInBytes(file.length())\n+                                 .withPath(file.toString())\n+                                 .withFormat(\"avro\")\n+                                 .build();\n+\n+    table.newAppend().appendFile(dataFile).commit();\n+\n+    Job job = Job.getInstance(new Configuration());\n+    IcebergInputFormat\n+        .configure(job)\n+        .readFrom(tableLocation.getAbsolutePath());\n+\n+    TaskAttemptContext context = new TaskAttemptContextImpl(new JobConf(job.getConfiguration()), new TaskAttemptID());\n+    icebergInputFormat = new IcebergInputFormat<>();\n+    List<InputSplit> splits = icebergInputFormat.getSplits(context);\n+    final RecordReader<Void, GenericData.Record> recordReader =\n+        icebergInputFormat.createRecordReader(splits.get(0), context);\n+    recordReader.initialize(splits.get(0), context);\n+    while (recordReader.nextKeyValue()) {\n+      System.out.println(recordReader.getCurrentValue());", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4NDk0NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394484945", "bodyText": "Tests are not done well. I plan to write better tests before publishing this PR", "author": "rdsr", "createdAt": "2020-03-18T16:35:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MjYwMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxMjUxNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396012516", "bodyText": "Fixed.", "author": "rdsr", "createdAt": "2020-03-21T17:41:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ4MjYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc2OTcyNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394769727", "bodyText": "Maybe we'd also consider adding incremental support here, like what current Spark Reader did.", "author": "jerryshao", "createdAt": "2020-03-19T03:23:47Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE1ODI0MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395158240", "bodyText": "Let's do that in a follow-up.", "author": "rdblue", "createdAt": "2020-03-19T16:29:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc2OTcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc3Mzk3NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r394773974", "bodyText": "For a simple MR application, do we need to involve HiveCatalog?\nIt seems strange to me that a simple MR application requires to communicate with Hive. I was thinking that if we could have an abstract catalog layer here, or only support Hadoop tables in MR as default implementation. Besides let Hive, Pig and others to choose a specific implementation, because each engine may have their own preference, and we don't have to bind to Hive or others here.", "author": "jerryshao", "createdAt": "2020-03-19T03:39:52Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  private static final String LOCALITY = \"iceberg.mr.locality\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  public enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = getTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder inMemoryDataModel(InMemoryDataModel inMemoryDataModel) {\n+      conf.set(IN_MEMORY_DATA_MODEL, inMemoryDataModel.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = getTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      return iterable.iterator();\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) icebergRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record icebergRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table getTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      Catalog catalog = HiveCatalogs.loadCatalog(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTE3MzIzNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395173236", "bodyText": "I like this idea, but I don't think it works. The problem is that getSplits isn't called in the client/driver process. It's called in the setup task that needs to instantiate the InputFormat with the no-arg constructor. So everything needs to be passed through the Configuration.\nTables themselves aren't serializable, so we can't pass a table or a scan that way. We need to be able to load the table in the setup task, configure the scan, and get splits there.", "author": "rdblue", "createdAt": "2020-03-19T16:51:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDc3Mzk3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r395781356", "bodyText": "Can I suggest putting this and the related classes into a org.apache.iceberg.mr.mapreduce package? So we can then add the org.apache.iceberg.mr.mapred implementation alongside it and the package names match the Hadoop API package names?", "author": "massdosage", "createdAt": "2020-03-20T17:18:01Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,494 @@\n+/*", "originalCommit": "1d07d8c8a88ef9c9ee56c750c519fc13e455d697", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxMTgyMg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396011822", "bodyText": "@massdosage . I'll make this change once all other comments are addressed. I think making this change before can trip up github and it may consider mr.InputFormat and mr.mapreduce.InputFormat as two different files.", "author": "rdsr", "createdAt": "2020-03-21T17:32:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MjM3Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397342373", "bodyText": "OK, sure.", "author": "massdosage", "createdAt": "2020-03-24T17:40:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3NjQ5OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402076499", "bodyText": "Fixed.", "author": "rdsr", "createdAt": "2020-04-02T06:22:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NTc4MTM1Ng=="}], "type": "inlineReview"}, {"oid": "8d0c77791a77118abdeb1f170efbcea71993a9ea", "url": "https://github.com/apache/iceberg/commit/8d0c77791a77118abdeb1f170efbcea71993a9ea", "message": "Address review comments [Take 2]", "committedDate": "2020-03-21T17:38:34Z", "type": "forcePushed"}, {"oid": "0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "url": "https://github.com/apache/iceberg/commit/0bf3319633f76d8c5cea5b6462a7c19b6928c03a", "message": "Address review comments [Take 2]", "committedDate": "2020-03-21T18:20:43Z", "type": "forcePushed"}, {"oid": "88a9c1d9e8db1413549afb254788d6b59db0fb42", "url": "https://github.com/apache/iceberg/commit/88a9c1d9e8db1413549afb254788d6b59db0fb42", "message": "Address review comments [Take 2]", "committedDate": "2020-03-21T18:28:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjAxNjc5OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396016799", "bodyText": "Will add orc once ORC patches are committed which read/writer Iceberg generics", "author": "rdsr", "createdAt": "2020-03-21T18:30:35Z", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.Database;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.HiveCatalogs;\n+import org.apache.iceberg.hive.HiveClientPool;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {\n+  private static final Schema SCHEMA = new Schema(\n+      required(1, \"data\", Types.StringType.get()),\n+      required(3, \"id\", Types.LongType.get()),\n+      required(2, \"date\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+                                                         .identity(\"date\")\n+                                                         .bucket(\"id\", 1)\n+                                                         .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private HadoopTables tables;\n+  private Configuration conf;\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][]{\n+        new Object[]{\"parquet\"},\n+        new Object[]{\"avro\"}", "originalCommit": "88a9c1d9e8db1413549afb254788d6b59db0fb42", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "url": "https://github.com/apache/iceberg/commit/58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "message": "Address review comments [Take 2]", "committedDate": "2020-03-21T19:32:56Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTAzNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159037", "bodyText": "Yes. FileIO is Serializable and should be used to create input files. We should also use the encryption manager to be correct (could open a follow-up).", "author": "rdblue", "createdAt": "2020-03-22T23:26:30Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?", "originalCommit": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE3NDg1Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396174856", "bodyText": "I'll create a follow up for this if that's OK. Should this be serialized as base64 and added to job conf? I think job configuration generally has a default size limit of 5MB", "author": "rdsr", "createdAt": "2020-03-23T01:23:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTAzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc4NzM4Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396787383", "bodyText": "Yeah, sounds good.", "author": "rdblue", "createdAt": "2020-03-23T22:11:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTAzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQyNjA1OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397426058", "bodyText": "Created a follow up #865", "author": "rdsr", "createdAt": "2020-03-24T20:02:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTAzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTEyNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159126", "bodyText": "Is it possible to throw this in getSplits so it isn't an error that may happen 95% of the way through a job?", "author": "rdblue", "createdAt": "2020-03-22T23:27:27Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filterExpression = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filterExpression != null) {\n+      scan = scan.filter(filterExpression);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> splits.add(new IcebergSplit(conf, task)));\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(", "originalCommit": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQxOTA0Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397419046", "bodyText": "Fixed", "author": "rdsr", "createdAt": "2020-03-24T19:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTEyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTM2MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159361", "bodyText": "Minor: could we use > 0 instead of != -1?", "author": "rdblue", "createdAt": "2020-03-22T23:30:17Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,525 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    public ConfigBuilder locality(boolean localityPreferred) {\n+      conf.setBoolean(LOCALITY, localityPreferred);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, -1);\n+    if (splitSize != -1) {", "originalCommit": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396159757", "bodyText": "Are Hive dependencies still needed for testCompile?\nTo test this with HMS, we could make it a test dependency of iceberg-hive and test out the catalog function.", "author": "rdblue", "createdAt": "2020-03-22T23:33:52Z", "path": "build.gradle", "diffHunk": "@@ -219,6 +219,58 @@ project(':iceberg-hive') {\n   }\n }\n \n+project(':iceberg-mr') {\n+  dependencies {\n+    compile project(':iceberg-api')\n+    compile project(':iceberg-core')\n+    compile project(':iceberg-orc')\n+    compile project(':iceberg-parquet')\n+    compile project(':iceberg-data')\n+\n+    compileOnly(\"org.apache.hadoop:hadoop-client\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+    }\n+\n+    testCompile project(':iceberg-hive')\n+    testCompile project(path: ':iceberg-data', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-hive', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-api', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-core', configuration: 'testArtifacts')\n+\n+    // By default, hive-exec is a fat/uber jar and it exports a guava library\n+    // that's really old. We use the core classifier to be able to override our guava\n+    // version. Luckily, hive-exec seems to work okay so far with this version of guava\n+    // See: https://github.com/apache/hive/blob/master/ql/pom.xml#L911 for more context.\n+    testCompile(\"org.apache.hive:hive-exec::core\") {", "originalCommit": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE3MjgzMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396172831", "bodyText": "I'l also worried that iceberg-hive in test scope can create a circular dep. Maybe we can just test with HadoopCatalog ?", "author": "rdsr", "createdAt": "2020-03-23T01:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc4Nzc1Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r396787753", "bodyText": "Why would it create a circular dependency? This doesn't need to depend on iceberg-hive anymore, right?", "author": "rdblue", "createdAt": "2020-03-23T22:12:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM4NjM5Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397386397", "bodyText": "We'd need to depend on iceberg-hive in test scope so that we can use HiveCatalog", "author": "rdsr", "createdAt": "2020-03-24T18:51:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM5NDQ4Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397394482", "bodyText": "What about putting the test case for HiveCatalog in iceberg-hive using the catalog function? Then we get a test that uses the catalog plugin system and we don't have to include Hive here. Instead of having this rely on iceberg-hive, have iceberg-hive tests depend on iceberg-mr.", "author": "rdblue", "createdAt": "2020-03-24T19:04:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQxOTYzNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397419634", "bodyText": "I've removed the iceberg-hive and related deps from iceberg-mr module and the plugin system is being tested using HadoopCatalog.  I can still add the test for HiveCatalog in the iceberg-hive module. What do you think?", "author": "rdsr", "createdAt": "2020-03-24T19:50:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE1OTc1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397341626", "bodyText": "Is it OK if this happens? We don't want to rethrow this?", "author": "massdosage", "createdAt": "2020-03-24T17:39:22Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/Util.java", "diffHunk": "@@ -36,4 +47,21 @@ public static FileSystem getFs(Path path, Configuration conf) {\n       throw new RuntimeIOException(e, \"Failed to get file system for path: %s\", path);\n     }\n   }\n+\n+  public static String[] blockLocations(CombinedScanTask task, Configuration conf) {\n+    Set<String> locationSets = Sets.newHashSet();\n+    for (FileScanTask f : task.files()) {\n+      Path path = new Path(f.file().path().toString());\n+      try {\n+        FileSystem fs = path.getFileSystem(conf);\n+        for (BlockLocation b : fs.getFileBlockLocations(path, f.start(), f.length())) {\n+          locationSets.addAll(Arrays.asList(b.getHosts()));\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to get block locations for path {}\", path, ioe);", "originalCommit": "58bf83c3b5c0f2dc14f8cf4f6addc8ad641e5bd3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzUwOTgwNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397509806", "bodyText": "For now, I've kept it as it was for Spark. Seem like locality is not really needed for cloud stores and it is an optimization for HDFS. I can throw an exception and then handle it across Spark and MR if you guys think this is necessary..", "author": "rdsr", "createdAt": "2020-03-24T22:53:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYxOTE5OQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397619199", "bodyText": "Locality preference is just a nice-to-have thing, to throw an exception for this seems not so necessary.", "author": "jerryshao", "createdAt": "2020-03-25T05:38:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzY4OTAzMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397689031", "bodyText": "OK, makes sense.", "author": "massdosage", "createdAt": "2020-03-25T08:48:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA0OTMwMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r398049301", "bodyText": "Yes, locality is optional and not being able to get it should not cause a job to fail.", "author": "rdblue", "createdAt": "2020-03-25T17:44:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0MTYyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyMjUwMA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397622500", "bodyText": "Does MR treat \"*\" as ANYWHERE?", "author": "jerryshao", "createdAt": "2020-03-25T05:52:26Z", "path": "mr/src/main/java/org/apache/iceberg/mr/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,567 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.case.sensitive\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note. This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied . Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Set<Integer> idColumns = spec.identitySourceIds();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+      if (hasJoinedPartitionColumns) {\n+        readSchema = TypeUtil.selectNot(tableSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(tableSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .schema(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)\n+          DynConstructors.builder(Function.class)\n+                         .impl(catalogFuncClass)\n+                         .build()\n+                         .newInstance();\n+      Catalog catalog = catalogFunc.apply(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    } else if (path.contains(\"/\")) {\n+      HadoopTables tables = new HadoopTables(conf);\n+      return tables.load(path);\n+    } else {\n+      throw new IllegalArgumentException(\"No custom catalog specified to load table \" + path);\n+    }\n+  }\n+\n+  private static class IcebergSplit extends InputSplit implements Writable {\n+    private static final String[] ANYWHERE = new String[]{\"*\"};\n+    private CombinedScanTask task;\n+    private transient String[] locations;\n+    private transient Configuration conf;\n+\n+    IcebergSplit(Configuration conf, CombinedScanTask task) {\n+      this.task = task;\n+      this.conf = conf;\n+    }\n+\n+    @Override\n+    public long getLength() {\n+      return task.files().stream().mapToLong(FileScanTask::length).sum();\n+    }\n+\n+    @Override\n+    public String[] getLocations() {\n+      boolean localityPreferred = conf.getBoolean(LOCALITY, false);\n+      if (!localityPreferred) {\n+        return ANYWHERE;", "originalCommit": "046ee4ad19c2fbd42596013d2eba5c8b7c760820", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA5OTI2NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402099264", "bodyText": "I copied this from IcebergPigInputFormat. I'll double check", "author": "rdsr", "createdAt": "2020-04-02T07:16:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyMjUwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397626247", "bodyText": "Maybe we should add more tests to cover some of the configurations defined above.", "author": "jerryshao", "createdAt": "2020-03-25T06:06:57Z", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {", "originalCommit": "046ee4ad19c2fbd42596013d2eba5c8b7c760820", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA1NDY0Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r398054646", "bodyText": "We also need tests that use the MR framework, not just tests that independently use the input format API.\nYou can use this s3committer test as an example. That test uses a mock to test s3 that isn't needed, but it shows how to bring up a test DFS and MR cluster.", "author": "rdblue", "createdAt": "2020-03-25T17:52:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjA3NTkxMg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402075912", "bodyText": "I tried writing a test with DFS and Mini MR cluster. I'm facing class-path issues particularly with Guava. Any suggestions on how to go about it? I was thinking maybe we can have an integration testing module per  framework. For example for MR we'd have mr-runtime module which would be shaded. Now to test MR we could have a mr-runtime-test module which can setup DFS and MR clusters. Since mr runtime would be shaded completely there wouldn't be any classpath issues\nI've added more unit tests though.", "author": "rdsr", "createdAt": "2020-04-02T06:21:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQzMTc4NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402431784", "bodyText": "This is the actual error\njava.lang.NoSuchMethodError: com.google.common.base.Objects.toStringHelper(Ljava/lang/Object;)Lcom/google/common/base/Objects$ToStringHelper;\n\n\tat org.apache.hadoop.metrics2.lib.MetricsRegistry.toString(MetricsRegistry.java:406)\n\tat java.lang.String.valueOf(String.java:2994)\n\tat java.lang.StringBuilder.append(StringBuilder.java:131)\n\tat org.apache.hadoop.ipc.metrics.RpcMetrics.<init>(RpcMetrics.java:74)\n\tat org.apache.hadoop.ipc.metrics.RpcMetrics.create(RpcMetrics.java:80)\n\tat org.apache.hadoop.ipc.Server.<init>(Server.java:2218)\n\tat org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:951)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server.<init>(ProtobufRpcEngine.java:534)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine.getServer(ProtobufRpcEngine.java:509)\n\tat org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:796)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:351)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:674)\n\nI have the MR framework added in my own repo https://github.com/rdsr/incubator-iceberg/blob/mr_generic_job/mr/src/test/java/org/apache/iceberg/mr/TestMRJob.java", "author": "rdsr", "createdAt": "2020-04-02T16:06:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjY5OTIyOA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402699228", "bodyText": "Yes, this could be a problem. I agree that we should have a mr-runtime module to shade some conflict dependencies.", "author": "jerryshao", "createdAt": "2020-04-03T02:17:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE3NzYxMw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403177613", "bodyText": "Do you know what version of Guava is getting pulled in? The Iceberg version?", "author": "rdblue", "createdAt": "2020-04-03T17:34:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE5MzQ3MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403193470", "bodyText": "My guess would be yes as toStringHelper is not defined in our version of guava 28.0-jre", "author": "rdsr", "createdAt": "2020-04-03T17:52:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzk2NzQwMQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403967401", "bodyText": "We've had similar issues in our branch where we are trying to get the Hive InputFormat to work. Hive 2.3.6 requires Guava 11.0.2, if a newer Guava version is on the classpath Hive is unable to use the InputFormat due to exceptions similar to the one above.  So we have to remove Guava as an exposed dependency from all Iceberg artifacts which appear on the Hive classpath. The only way we've managed to get it to work is by doing the following:\n\nAlter every Iceberg module that uses Guava to shade and relocate it (which IMHO is a good thing to do anyway so external users of Iceberg can use their own versions of Guava).\nDepend on the shaded version of these modules from iceberg-mr.\nRemove Guava from versions.props so that different subprojects can depend on different versions of it.\nThe Guava version that then gets used in iceberg-mr is the transitive one from Hive 2.3.6 (in this case) which is Guava 11.0.2.\n\nYou can see these changes here: https://github.com/ExpediaGroup/incubator-iceberg/blob/078a06ddd78d08648127d8b2e8dc41e0febf7f49/build.gradle We're not Gradle experts so hopefully there is an easier way to do all of this but I think the general steps outlined above will still be required.\nUltimately I think this issue is going to have to be solved for both InputFormats so any changes that would allow different versions of Guava to be used would be great.", "author": "massdosage", "createdAt": "2020-04-06T09:54:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQzNjc1Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r404436756", "bodyText": "I think it's reasonable to build a mr-runtime library. It's really too bad that this requires one, since it is difficult to get the licensing right for a binary Jar like that and it's more maintenance to have 2 more modules. Hopefully we can reuse some of the work from Spark's runtime Jar.", "author": "rdblue", "createdAt": "2020-04-06T22:55:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNjI0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzYyNzIyOQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r397627229", "bodyText": "one more empty line.", "author": "jerryshao", "createdAt": "2020-03-25T06:10:23Z", "path": "core/src/main/java/org/apache/iceberg/hadoop/Util.java", "diffHunk": "@@ -19,13 +19,24 @@\n \n package org.apache.iceberg.hadoop;\n \n+import com.google.common.collect.Sets;\n import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Set;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.BlockLocation;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+", "originalCommit": "046ee4ad19c2fbd42596013d2eba5c8b7c760820", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA1MjU2MA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r398052560", "bodyText": "Nit: extra newline.", "author": "rdblue", "createdAt": "2020-03-25T17:49:40Z", "path": "mr/src/test/java/org/apache/iceberg/mr/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+", "originalCommit": "046ee4ad19c2fbd42596013d2eba5c8b7c760820", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "637ed5ce6958b884e3437ec1a1d2bc394632e820", "url": "https://github.com/apache/iceberg/commit/637ed5ce6958b884e3437ec1a1d2bc394632e820", "message": "Added more test cases", "committedDate": "2020-04-02T06:12:47Z", "type": "forcePushed"}, {"oid": "da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "url": "https://github.com/apache/iceberg/commit/da8f03e0d78d2057e21bc0daa84c1baac9ba2d68", "message": "Added more test cases", "committedDate": "2020-04-02T06:16:53Z", "type": "forcePushed"}, {"oid": "0e250f954dc6388aea3bf3bce695ba9b593b5463", "url": "https://github.com/apache/iceberg/commit/0e250f954dc6388aea3bf3bce695ba9b593b5463", "message": "Added more test cases", "committedDate": "2020-04-02T07:09:02Z", "type": "forcePushed"}, {"oid": "306f3f845fcb896b753364a24a352aeddc618483", "url": "https://github.com/apache/iceberg/commit/306f3f845fcb896b753364a24a352aeddc618483", "message": "Added more test cases", "committedDate": "2020-04-02T07:14:42Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjQxODc1NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402418755", "bodyText": "Had to change this logic slightly so that whatever schema the user projected, that is what is returned [we drop identity partition columns not projected by the user]", "author": "rdsr", "createdAt": "2020-04-02T15:48:13Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -346,14 +348,15 @@ public void close() throws IOException {\n       DataFile file = currentTask.file();\n       // schema of rows returned by readers\n       PartitionSpec spec = currentTask.spec();\n-      Set<Integer> idColumns = spec.identitySourceIds();\n       Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n       boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n       if (hasJoinedPartitionColumns) {\n-        readSchema = TypeUtil.selectNot(tableSchema, idColumns);", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc0ODMxNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402748314", "bodyText": "additional line.", "author": "jerryshao", "createdAt": "2020-04-03T05:44:18Z", "path": "mr/src/main/java/org/apache/iceberg/mr/SerializationUtil.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.nio.charset.StandardCharsets;\n+import java.util.Base64;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+\n+", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjc1MDA0Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r402750042", "bodyText": "Also here.", "author": "jerryshao", "createdAt": "2020-04-03T05:50:39Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzNTQzOA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403135438", "bodyText": "Why is this necessary? Can't the work be done in either method when there is enough information to load the table?", "author": "rdblue", "createdAt": "2020-04-03T16:40:13Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc0NzU5NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403747595", "bodyText": "You are right. We don't need this.  If readFrom is called before calling catalogFunc we through a well defined error saying no custom catalog provided.", "author": "rdsr", "createdAt": "2020-04-05T19:41:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzNTQzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzODk5MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403138991", "bodyText": "I think that if the job configured plaformAppliesFilter then it is okay to return records even if the model is generics.", "author": "rdblue", "createdAt": "2020-04-03T16:46:29Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDEwNg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754106", "bodyText": "fixed.", "author": "rdsr", "createdAt": "2020-04-05T20:39:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzODk5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzOTU0NQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403139545", "bodyText": "I think this should be GENERIC instead of DEFAULT. Defaults depend on context, but this enum doesn't need to encode what some other part of the code will do by default. It just needs to encode the data model.", "author": "rdblue", "createdAt": "2020-04-03T16:47:32Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDE2Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754166", "bodyText": "fixed", "author": "rdsr", "createdAt": "2020-04-05T20:39:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzOTU0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjEzOQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142139", "bodyText": "I find this a little hard to reason about and I think a different name could make it more straight-forward. Rather than configuring \"someone will do task\" it is easier for the caller and this code to phrase it as \"do task\" or \"skip task\". That also avoids the need to refer to the \"platform\" because users won't know what that is and we have to explain it.\nWhat about changing this to skipResidualFiltering()? That is clear that the InputFormat is not responsible for filtering, but doesn't define who or what is responsible. It is also clearly dangerous, even if you don't know what a residual is.", "author": "rdblue", "createdAt": "2020-04-03T16:52:24Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDE4Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754183", "bodyText": "fixed", "author": "rdsr", "createdAt": "2020-04-05T20:40:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjEzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjM5MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142391", "bodyText": "Nit: the line wrapping here is strange.", "author": "rdblue", "createdAt": "2020-04-03T16:52:48Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDE5NA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754194", "bodyText": "fixed", "author": "rdsr", "createdAt": "2020-04-05T20:40:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MjM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0Mjc5MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403142791", "bodyText": "Minor: this calls getBoolean and getEnum for every split. It would be nice to get those outside the loop and pass them in.", "author": "rdblue", "createdAt": "2020-04-03T16:53:32Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403143224", "bodyText": "How about UnsupportedOperationException since this will eventually be implemented?", "author": "rdblue", "createdAt": "2020-04-03T16:54:10Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzQxNjU2Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403416566", "bodyText": "Well, this would only be implemented for GENERIC data model, not for Pig and Hive. Thoughts on keeping the same exception or do u think we should implement residual eval for Pig and Hive data models?", "author": "rdsr", "createdAt": "2020-04-04T03:19:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzUxNDg2Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403514867", "bodyText": "My rationale was that we would accept a patch to add residual filtering for Hive and Pig, so why not consider it a future feature? Should someone choose to contribute it I think it would be valuable.", "author": "rdblue", "createdAt": "2020-04-04T20:44:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzUxNzk2Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403517962", "bodyText": "OK that make sense. Thanks!", "author": "rdsr", "createdAt": "2020-04-04T21:16:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDIxOA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754218", "bodyText": "Kept as UnsupportedException", "author": "rdsr", "createdAt": "2020-04-05T20:40:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0MzIyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDE0Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403144142", "bodyText": "Nit: I prefer separating state that won't change (caseSensitive, reuseContainers) from state that does change (currentRow, currentCloseable), but those are mixed together here.", "author": "rdblue", "createdAt": "2020-04-03T16:55:40Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDk0Ng==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403144946", "bodyText": "Maybe leave a TODO item to improve this.", "author": "rdblue", "createdAt": "2020-04-03T16:56:36Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1NzM5Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403157397", "bodyText": "@rdblue , any thoughts on what we need to improve here?", "author": "rdsr", "createdAt": "2020-04-03T17:11:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE3NDU3MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403174571", "bodyText": "Just that we could give a more accurate progress based on records read from the file. I don't think that context.getProgress has enough information to give an accurate progress value.\nThis isn't that easy, since we don't know how much of the input split has been processed and we are pushing filters into Parquet and ORC. But we do know when a file is opened and could count the number of rows returned, so we can estimate. And we could also add a row count to the readers so that we can get an accurate count of rows that have been either returned or filtered out.", "author": "rdblue", "createdAt": "2020-04-03T17:30:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDk0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDI0Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754247", "bodyText": "Added", "author": "rdsr", "createdAt": "2020-04-05T20:40:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0NDk0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE0ODYyNQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403148625", "bodyText": "In the future, we should update HadoopFileIO to use a transient Configuration and to keep a set of properties that it extracts from its original configuration. Then we can serialize HadoopFileIO more easily, and rebuild a working IO config on tasks.", "author": "rdblue", "createdAt": "2020-04-03T17:00:39Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1NzE3Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403157172", "bodyText": "Nit: = should be on the previous line.", "author": "rdblue", "createdAt": "2020-04-03T17:10:44Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .project(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO: implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException(\"In memory representation not yet supported for Pig and Hive\");\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    Preconditions.checkArgument(path != null, \"Table path should not be null\");\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1Nzg1Mw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403157853", "bodyText": "I think this should be the first branch of the if because it is the easiest and is less likely to fail.", "author": "rdblue", "createdAt": "2020-04-03T17:11:35Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());\n+\n+      if (reuseContainers) {\n+        avroReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          avroReadBuilder.createReaderFunc(DataReader::create);\n+      }\n+      return avroReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newParquetIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Parquet.ReadBuilder parquetReadBuilder = Parquet.read(inputFile)\n+                                                      .project(readSchema)\n+                                                      .filter(task.residual())\n+                                                      .caseSensitive(caseSensitive)\n+                                                      .split(task.start(), task.length());\n+      if (reuseContainers) {\n+        parquetReadBuilder.reuseContainers();\n+      }\n+\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          parquetReadBuilder.createReaderFunc(\n+              fileSchema -> GenericParquetReaders.buildReader(readSchema, fileSchema));\n+      }\n+      return parquetReadBuilder.build();\n+    }\n+\n+    private CloseableIterable<T> newOrcIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      ORC.ReadBuilder orcReadBuilder = ORC.read(inputFile)\n+                                          .project(readSchema)\n+                                          .caseSensitive(caseSensitive)\n+                                          .split(task.start(), task.length());\n+      // ORC does not support reuse containers yet\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          //TODO: implement value readers for Pig and Hive\n+          throw new UnsupportedOperationException(\"In memory representation not yet supported for Pig and Hive\");\n+        case DEFAULT:\n+          //TODO: We do not have support for Iceberg generics for ORC\n+          throw new UnsupportedOperationException();\n+      }\n+\n+      return orcReadBuilder.build();\n+    }\n+  }\n+\n+  private static Table findTable(Configuration conf) {\n+    String path = conf.get(TABLE_PATH);\n+    Preconditions.checkArgument(path != null, \"Table path should not be null\");\n+    String catalogFuncClass = conf.get(CATALOG);\n+    if (catalogFuncClass != null) {\n+      Function<Configuration, Catalog> catalogFunc\n+          = (Function<Configuration, Catalog>)\n+          DynConstructors.builder(Function.class)\n+                         .impl(catalogFuncClass)\n+                         .build()\n+                         .newInstance();\n+      Catalog catalog = catalogFunc.apply(conf);\n+      TableIdentifier tableIdentifier = TableIdentifier.parse(path);\n+      return catalog.loadTable(tableIdentifier);\n+    } else if (path.contains(\"/\")) {", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1ODc4MQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403158781", "bodyText": "Nit: typical style is to use 2 indents / 4 spaces from the next line, not to indent to the same level as the last function call.", "author": "rdblue", "createdAt": "2020-04-03T17:12:39Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      List<Types.NestedField> fields = Lists.newArrayList(record.struct().fields());\n+      fields.addAll(identityPartitionSchema.asStruct().fields());\n+      GenericRecord row = GenericRecord.create(Types.StructType.of(fields));\n+      int size = record.struct().fields().size();\n+      for (int i = 0; i < size; i++) {\n+        row.set(i, record.get(i));\n+      }\n+      List<PartitionField> partitionFields = spec.fields();\n+      List<Types.NestedField> identityColumns = identityPartitionSchema.columns();\n+      for (int i = 0; i < identityColumns.size(); i++) {\n+        Types.NestedField identityColumn = identityColumns.get(i);\n+\n+        for (int j = 0; j < partitionFields.size(); j++) {\n+          PartitionField partitionField = partitionFields.get(j);\n+          if (identityColumn.fieldId() == partitionField.sourceId() &&\n+              \"identity\".equals(partitionField.transform().toString())) {\n+            //TODO: identity partitions are being added to the end, this changes\n+            // the position of the column. This seems like a potential bug\n+            row.set(size + i, partition.get(j, spec.javaClasses()[i]));\n+          }\n+        }\n+      }\n+      return row;\n+    }\n+\n+    private CloseableIterable<T> newAvroIterable(InputFile inputFile, FileScanTask task, Schema readSchema) {\n+      Avro.ReadBuilder avroReadBuilder = Avro.read(inputFile)\n+                                             .project(readSchema)\n+                                             .split(task.start(), task.length());", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1OTgwOQ==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403159809", "bodyText": "Why not ORC?", "author": "rdblue", "createdAt": "2020-04-03T17:13:49Z", "path": "mr/src/test/java/org/apache/iceberg/mr/mapreduce/TestIcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,382 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.collect.FluentIterable;\n+import com.google.common.collect.ImmutableMap;\n+import com.google.common.collect.ImmutableSet;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.hadoop.mapreduce.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;\n+import org.apache.iceberg.AssertHelpers;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataWriter;\n+import org.apache.iceberg.data.parquet.GenericParquetWriter;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+\n+@RunWith(Parameterized.class)\n+public class TestIcebergInputFormat {\n+  static final Schema SCHEMA = new Schema(\n+      required(1, \"data\", Types.StringType.get()),\n+      required(2, \"id\", Types.LongType.get()),\n+      required(3, \"date\", Types.StringType.get()));\n+\n+  static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+                                                 .identity(\"date\")\n+                                                 .bucket(\"id\", 1)\n+                                                 .build();\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+  private HadoopTables tables;\n+  private Configuration conf;\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][]{\n+        new Object[]{\"parquet\"},\n+        new Object[]{\"avro\"}", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE3NjE0Mg==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403176142", "bodyText": "(Now that generics for ORC have been added)", "author": "rdblue", "createdAt": "2020-04-03T17:32:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1OTgwOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDI1Nw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754257", "bodyText": "Added!", "author": "rdsr", "createdAt": "2020-04-05T20:40:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE1OTgwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE2MTk5OA==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403161998", "bodyText": "There are tests for this in Spark here: 6cafdab\nYou might consider adding some similar tests.", "author": "rdblue", "createdAt": "2020-04-03T17:16:16Z", "path": "mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java", "diffHunk": "@@ -0,0 +1,571 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.mapreduce;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Iterators;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Sets;\n+import java.io.Closeable;\n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.function.Function;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapreduce.InputFormat;\n+import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobContext;\n+import org.apache.hadoop.mapreduce.RecordReader;\n+import org.apache.hadoop.mapreduce.TaskAttemptContext;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionField;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.common.DynConstructors;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.data.avro.DataReader;\n+import org.apache.iceberg.data.parquet.GenericParquetReaders;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopInputFile;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mr.SerializationUtil;\n+import org.apache.iceberg.orc.ORC;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * Generic Mrv2 InputFormat API for Iceberg.\n+ * @param <T> T is the in memory data model which can either be Pig tuples, Hive rows. Default is Iceberg records\n+ */\n+public class IcebergInputFormat<T> extends InputFormat<Void, T> {\n+  private static final Logger LOG = LoggerFactory.getLogger(IcebergInputFormat.class);\n+\n+  static final String AS_OF_TIMESTAMP = \"iceberg.mr.as.of.time\";\n+  static final String CASE_SENSITIVE = \"iceberg.mr.case.sensitive\";\n+  static final String FILTER_EXPRESSION = \"iceberg.mr.filter.expression\";\n+  static final String IN_MEMORY_DATA_MODEL = \"iceberg.mr.in.memory.data.model\";\n+  static final String READ_SCHEMA = \"iceberg.mr.read.schema\";\n+  static final String REUSE_CONTAINERS = \"iceberg.mr.reuse.containers\";\n+  static final String SNAPSHOT_ID = \"iceberg.mr.snapshot.id\";\n+  static final String SPLIT_SIZE = \"iceberg.mr.split.size\";\n+  static final String TABLE_PATH = \"iceberg.mr.table.path\";\n+  static final String TABLE_SCHEMA = \"iceberg.mr.table.schema\";\n+  static final String LOCALITY = \"iceberg.mr.locality\";\n+  static final String CATALOG = \"iceberg.mr.catalog\";\n+  static final String PLATFORM_APPLIES_FILTER_RESIDUALS = \"iceberg.mr.platform.applies.filter.residuals\";\n+\n+  private transient List<InputSplit> splits;\n+\n+  private enum InMemoryDataModel {\n+    PIG,\n+    HIVE,\n+    DEFAULT // Default data model is of Iceberg Generics\n+  }\n+\n+  /**\n+   * Configures the {@code Job} to use the {@code IcebergInputFormat} and\n+   * returns a helper to add further configuration.\n+   *\n+   * @param job the {@code Job} to configure\n+   */\n+  public static ConfigBuilder configure(Job job) {\n+    job.setInputFormatClass(IcebergInputFormat.class);\n+    return new ConfigBuilder(job.getConfiguration());\n+  }\n+\n+  public static class ConfigBuilder {\n+    private final Configuration conf;\n+\n+    public ConfigBuilder(Configuration conf) {\n+      this.conf = conf;\n+    }\n+\n+    public ConfigBuilder readFrom(String path) {\n+      conf.set(TABLE_PATH, path);\n+      Table table = findTable(conf);\n+      conf.set(TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+      return this;\n+    }\n+\n+    public ConfigBuilder filter(Expression expression) {\n+      conf.set(FILTER_EXPRESSION, SerializationUtil.serializeToBase64(expression));\n+      return this;\n+    }\n+\n+    public ConfigBuilder project(Schema schema) {\n+      conf.set(READ_SCHEMA, SchemaParser.toJson(schema));\n+      return this;\n+    }\n+\n+    public ConfigBuilder reuseContainers(boolean reuse) {\n+      conf.setBoolean(REUSE_CONTAINERS, reuse);\n+      return this;\n+    }\n+\n+    public ConfigBuilder caseSensitive(boolean caseSensitive) {\n+      conf.setBoolean(CASE_SENSITIVE, caseSensitive);\n+      return this;\n+    }\n+\n+    public ConfigBuilder snapshotId(long snapshotId) {\n+      conf.setLong(SNAPSHOT_ID, snapshotId);\n+      return this;\n+    }\n+\n+    public ConfigBuilder asOfTime(long asOfTime) {\n+      conf.setLong(AS_OF_TIMESTAMP, asOfTime);\n+      return this;\n+    }\n+\n+    public ConfigBuilder splitSize(long splitSize) {\n+      conf.setLong(SPLIT_SIZE, splitSize);\n+      return this;\n+    }\n+\n+    /**\n+     * If this API is called. The input splits\n+     * constructed will have host location information\n+     */\n+    public ConfigBuilder preferLocality() {\n+      conf.setBoolean(LOCALITY, true);\n+      return this;\n+    }\n+\n+    public ConfigBuilder catalogFunc(Class<? extends Function<Configuration, Catalog>> catalogFuncClass) {\n+      Preconditions.checkState(\n+          conf.get(TABLE_PATH) == null,\n+          \"Please provide custom catalog before specifying the table to read from\");\n+      conf.setClass(CATALOG, catalogFuncClass, Function.class);\n+      return this;\n+    }\n+\n+    public ConfigBuilder useHiveRows() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.HIVE.name());\n+      return this;\n+    }\n+\n+    public ConfigBuilder usePigTuples() {\n+      conf.set(IN_MEMORY_DATA_MODEL, InMemoryDataModel.PIG.name());\n+      return this;\n+    }\n+\n+    /**\n+     * Compute platforms pass down filters to data sources.\n+     * If the data source cannot apply some filters, or only\n+     * partially applies the filter, it will return the\n+     * residual filter back. If the platform\n+     * can correctly apply the residual filters, then it\n+     * should call this api. Otherwise the current\n+     * api will throw an exception if the passed in\n+     * filter is not completely satisfied. Note: This\n+     * does not apply to standalone MR application\n+     */\n+    public ConfigBuilder platformAppliesFilterResiduals() {\n+      conf.setBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, true);\n+      return this;\n+    }\n+  }\n+\n+  @Override\n+  public List<InputSplit> getSplits(JobContext context) {\n+    if (splits != null) {\n+      LOG.info(\"Returning cached splits: {}\", splits.size());\n+      return splits;\n+    }\n+\n+    Configuration conf = context.getConfiguration();\n+    Table table = findTable(conf);\n+    TableScan scan = table.newScan()\n+                          .caseSensitive(conf.getBoolean(CASE_SENSITIVE, true));\n+    long snapshotId = conf.getLong(SNAPSHOT_ID, -1);\n+    if (snapshotId != -1) {\n+      scan = scan.useSnapshot(snapshotId);\n+    }\n+    long asOfTime = conf.getLong(AS_OF_TIMESTAMP, -1);\n+    if (asOfTime != -1) {\n+      scan = scan.asOfTime(asOfTime);\n+    }\n+    long splitSize = conf.getLong(SPLIT_SIZE, 0);\n+    if (splitSize > 0) {\n+      scan = scan.option(TableProperties.SPLIT_SIZE, String.valueOf(splitSize));\n+    }\n+    String schemaStr = conf.get(READ_SCHEMA);\n+    if (schemaStr != null) {\n+      scan.project(SchemaParser.fromJson(schemaStr));\n+    }\n+\n+    // TODO add a filter parser to get rid of Serialization\n+    Expression filter = SerializationUtil.deserializeFromBase64(conf.get(FILTER_EXPRESSION));\n+    if (filter != null) {\n+      scan = scan.filter(filter);\n+    }\n+\n+    splits = Lists.newArrayList();\n+    try (CloseableIterable<CombinedScanTask> tasksIterable = scan.planTasks()) {\n+      tasksIterable.forEach(task -> {\n+        checkResiduals(conf, task);\n+        splits.add(new IcebergSplit(conf, task));\n+      });\n+    } catch (IOException e) {\n+      throw new RuntimeIOException(e, \"Failed to close table scan: %s\", scan);\n+    }\n+\n+    return splits;\n+  }\n+\n+  private static void checkResiduals(Configuration conf, CombinedScanTask task) {\n+    boolean platformAppliesFilter = conf.getBoolean(PLATFORM_APPLIES_FILTER_RESIDUALS, false);\n+    //TODO remove the check on dataModel once we start supporting\n+    // residual evaluation for Iceberg Generics in InputFormat\n+    InMemoryDataModel dataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+    if (dataModel == InMemoryDataModel.DEFAULT || !platformAppliesFilter) {\n+      task.files().forEach(fileScanTask -> {\n+        Expression residual = fileScanTask.residual();\n+        if (residual != null && !residual.equals(Expressions.alwaysTrue())) {\n+          throw new RuntimeException(\n+              String.format(\n+                  \"Filter expression %s is not completely satisfied. Additional rows \" +\n+                      \"can be returned not satisfied by the filter expression\", residual));\n+        }\n+      });\n+    }\n+  }\n+\n+  @Override\n+  public RecordReader<Void, T> createRecordReader(InputSplit split, TaskAttemptContext context) {\n+    return new IcebergRecordReader<>();\n+  }\n+\n+  private static final class IcebergRecordReader<T> extends RecordReader<Void, T> {\n+    private TaskAttemptContext context;\n+    private Iterator<FileScanTask> tasks;\n+    private Iterator<T> currentIterator;\n+    private T currentRow;\n+    private Schema expectedSchema;\n+    private Schema tableSchema;\n+    private InMemoryDataModel inMemoryDataModel;\n+    private Closeable currentCloseable;\n+    private boolean reuseContainers;\n+    private boolean caseSensitive;\n+\n+    @Override\n+    public void initialize(InputSplit split, TaskAttemptContext newContext) {\n+      Configuration conf = newContext.getConfiguration();\n+      // For now IcebergInputFormat does its own split planning and does not\n+      // accept FileSplit instances\n+      CombinedScanTask task = ((IcebergSplit) split).task;\n+      this.context = newContext;\n+      this.tasks = task.files().iterator();\n+      this.tableSchema = SchemaParser.fromJson(conf.get(TABLE_SCHEMA));\n+      String readSchemaStr = conf.get(READ_SCHEMA);\n+      if (readSchemaStr != null) {\n+        this.expectedSchema = SchemaParser.fromJson(readSchemaStr);\n+      }\n+      this.reuseContainers = conf.getBoolean(REUSE_CONTAINERS, false);\n+      this.caseSensitive = conf.getBoolean(CASE_SENSITIVE, true);\n+      this.inMemoryDataModel = conf.getEnum(IN_MEMORY_DATA_MODEL, InMemoryDataModel.DEFAULT);\n+      this.currentIterator = open(tasks.next());\n+    }\n+\n+    @Override\n+    public boolean nextKeyValue() throws IOException {\n+      while (true) {\n+        if (currentIterator.hasNext()) {\n+          currentRow = currentIterator.next();\n+          return true;\n+        } else if (tasks.hasNext()) {\n+          currentCloseable.close();\n+          currentIterator = open(tasks.next());\n+        } else {\n+          return false;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public Void getCurrentKey() {\n+      return null;\n+    }\n+\n+    @Override\n+    public T getCurrentValue() {\n+      return currentRow;\n+    }\n+\n+    @Override\n+    public float getProgress() {\n+      return context.getProgress();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+      currentCloseable.close();\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask) {\n+      DataFile file = currentTask.file();\n+      // schema of rows returned by readers\n+      PartitionSpec spec = currentTask.spec();\n+      Schema readSchema = expectedSchema != null ? expectedSchema : tableSchema;\n+      Set<Integer> idColumns =  Sets.intersection(spec.identitySourceIds(), TypeUtil.getProjectedIds(readSchema));\n+      boolean hasJoinedPartitionColumns = !idColumns.isEmpty();\n+\n+      if (hasJoinedPartitionColumns) {\n+        Schema readDataSchema = TypeUtil.selectNot(readSchema, idColumns);\n+        Schema identityPartitionSchema = TypeUtil.select(readSchema, idColumns);\n+        return Iterators.transform(\n+            open(currentTask, readDataSchema),\n+            row -> withPartitionColumns(row, identityPartitionSchema, spec, file.partition()));\n+      } else {\n+        return open(currentTask, readSchema);\n+      }\n+    }\n+\n+    private Iterator<T> open(FileScanTask currentTask, Schema readSchema) {\n+      DataFile file = currentTask.file();\n+      // TODO should we somehow make use of FileIO to create inputFile?\n+      InputFile inputFile = HadoopInputFile.fromLocation(file.path(), context.getConfiguration());\n+      CloseableIterable<T> iterable;\n+      switch (file.format()) {\n+        case AVRO:\n+          iterable = newAvroIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case ORC:\n+          iterable = newOrcIterable(inputFile, currentTask, readSchema);\n+          break;\n+        case PARQUET:\n+          iterable = newParquetIterable(inputFile, currentTask, readSchema);\n+          break;\n+        default:\n+          throw new UnsupportedOperationException(\n+              String.format(\"Cannot read %s file: %s\", file.format().name(), file.path()));\n+      }\n+      currentCloseable = iterable;\n+      if (inMemoryDataModel == InMemoryDataModel.DEFAULT) {\n+        Expression filter = currentTask.residual();\n+        return applyResidualsOnGenericRecords(iterable, filter).iterator();\n+      } else {\n+        return iterable.iterator();\n+      }\n+    }\n+\n+    private Iterable<T> applyResidualsOnGenericRecords(CloseableIterable<T> iterable, Expression filter) {\n+      if (filter == null || filter.equals(Expressions.alwaysTrue())) {\n+        return iterable;\n+      } else {\n+        throw new UnsupportedOperationException(\n+            String.format(\"Filter expression %s is not completely satisfied. Additional rows can be returned\" +\n+                              \" not satisfied by the filter expression\", filter));\n+      }\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    private T withPartitionColumns(T row, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {\n+      switch (inMemoryDataModel) {\n+        case PIG:\n+        case HIVE:\n+          // TODO implement adding partition columns to records for Pig and Hive\n+          throw new UnsupportedOperationException();\n+        case DEFAULT:\n+          return (T) genericRecordWithPartitionsColumns((Record) row, identityPartitionSchema, spec, partition);\n+      }\n+      return row;\n+    }\n+\n+    private static Record genericRecordWithPartitionsColumns(\n+        Record record, Schema identityPartitionSchema, PartitionSpec spec, StructLike partition) {", "originalCommit": "306f3f845fcb896b753364a24a352aeddc618483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzc1NDMzNw==", "url": "https://github.com/apache/iceberg/pull/843#discussion_r403754337", "bodyText": "Added similar tests. Had to change the logic of handling identity columns.", "author": "rdsr", "createdAt": "2020-04-05T20:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzE2MTk5OA=="}], "type": "inlineReview"}, {"oid": "15de263bee96fd49c4390f7bdec5f450d052a6dd", "url": "https://github.com/apache/iceberg/commit/15de263bee96fd49c4390f7bdec5f450d052a6dd", "message": "InputFormat support for Iceberg", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "7a287b1e1c629d3d851372d69aebc3bcade3c546", "url": "https://github.com/apache/iceberg/commit/7a287b1e1c629d3d851372d69aebc3bcade3c546", "message": "Address review comments", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "a38b8e326fab5946b0039013d7bd2ed4a3cc8cd4", "url": "https://github.com/apache/iceberg/commit/a38b8e326fab5946b0039013d7bd2ed4a3cc8cd4", "message": "Address review comments [Take 2]", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "3567b02d47ccf9b102b78c273fd4ceac8fce469f", "url": "https://github.com/apache/iceberg/commit/3567b02d47ccf9b102b78c273fd4ceac8fce469f", "message": "c", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "25bb2cd77283fedec001a2fcfd32212c3f2e7083", "url": "https://github.com/apache/iceberg/commit/25bb2cd77283fedec001a2fcfd32212c3f2e7083", "message": "[Address comments - take 3] Using Hadoop catalog to test custom catalog", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "406802109c232d2aeb5912f1e25ef6141d26390c", "url": "https://github.com/apache/iceberg/commit/406802109c232d2aeb5912f1e25ef6141d26390c", "message": "Better name for filter residuals option", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "a1c8e6eff60e2bc00c2211b6eb98fecdd8ef021c", "url": "https://github.com/apache/iceberg/commit/a1c8e6eff60e2bc00c2211b6eb98fecdd8ef021c", "message": "Added more test cases", "committedDate": "2020-04-05T20:55:22Z", "type": "commit"}, {"oid": "54b6bc56606c41b477b298eb1a065025d7a25e68", "url": "https://github.com/apache/iceberg/commit/54b6bc56606c41b477b298eb1a065025d7a25e68", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-05T20:55:22Z", "type": "forcePushed"}, {"oid": "d08465340556d3bc0d3abfba08517b9e28ebdaa0", "url": "https://github.com/apache/iceberg/commit/d08465340556d3bc0d3abfba08517b9e28ebdaa0", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-05T23:41:39Z", "type": "forcePushed"}, {"oid": "1aef1bd601d76742b6c60b4b075943466b57bf86", "url": "https://github.com/apache/iceberg/commit/1aef1bd601d76742b6c60b4b075943466b57bf86", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-05T23:55:56Z", "type": "forcePushed"}, {"oid": "7cb3dd0f9ef257f9928a283393fae0c733817411", "url": "https://github.com/apache/iceberg/commit/7cb3dd0f9ef257f9928a283393fae0c733817411", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-06T00:14:49Z", "type": "forcePushed"}, {"oid": "e327bd7e6798a00b4860659708da031f60c96058", "url": "https://github.com/apache/iceberg/commit/e327bd7e6798a00b4860659708da031f60c96058", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-06T01:12:01Z", "type": "commit"}, {"oid": "e327bd7e6798a00b4860659708da031f60c96058", "url": "https://github.com/apache/iceberg/commit/e327bd7e6798a00b4860659708da031f60c96058", "message": "Address review comments. Added more test cases", "committedDate": "2020-04-06T01:12:01Z", "type": "forcePushed"}]}