{"pr_number": 1523, "pr_title": "ISSUE-1520 Document writing against partitioned table in Spark", "pr_createdAt": "2020-09-28T07:49:32Z", "pr_url": "https://github.com/apache/iceberg/pull/1523", "timeline": [{"oid": "230650d36f81d39d55ea46a85c81c25c4a32e483", "url": "https://github.com/apache/iceberg/commit/230650d36f81d39d55ea46a85c81c25c4a32e483", "message": "ISSUE-1520 Document writing against partitioned table in Spark", "committedDate": "2020-09-28T07:45:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NTQxNg==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496255416", "bodyText": "Could you add a SQL example as well?", "author": "rdblue", "createdAt": "2020-09-28T21:49:42Z", "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.\n+This applies both Writing with SQL and Writing with DataFrames.\n+\n+Assuming we would like to write the data against below sample table:\n+\n+```sql\n+CREATE TABLE prod.db.sample (\n+    id bigint,\n+    data string,\n+    category string,\n+    ts timestamp)\n+USING iceberg\n+PARTITIONED BY (bucket(16, id), days(ts), category)\n+```\n+\n+then your data needs to be sorted by `bucket(16, id), days(ts), category` before writing to the table, like below:\n+\n+```scala\n+val sorted = data.sortWithinPartitions(expr(\"iceberg_bucket16(id)\"), expr(\"iceberg_days(ts)\"), col(\"category\"))\n+```\n+\n+You can create a temporary view from the resulting DataFrame to write with SQL, or write with Dataframe directly.\n+\n+If the partition spec of the table consists of `transformation` (non-identity), you need to register the Iceberg\n+transform function in Spark to specify it during sort. For example, to register `iceberg_bucket16` function in above query:\n+\n+```scala\n+import org.apache.iceberg.transforms.Transforms\n+import org.apache.iceberg.types.Types\n+import org.apache.spark.sql.types.IntegerType\n+import org.apache.spark.sql.types.StringType\n+\n+// Load the bucket transform from Iceberg to use as a UDF.\n+// Here the source type is `Types.LongType`, and matching Java type is `java.lang.Long`.\n+val bucketTransform = Transforms.bucket[java.lang.Long](Types.LongType.get(), 16)\n+\n+// Needed because Scala has trouble with the Java transform type.\n+// The return type of `bucket` transform is int, so the method's return type is.\n+def bucketFunc(id: Long): Int = bucketTransform.apply(id)\n+\n+// create and register a UDF\n+spark.udf.register(\"iceberg_bucket16\", bucketFunc _)", "originalCommit": "230650d36f81d39d55ea46a85c81c25c4a32e483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMzNTE0NA==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496335144", "bodyText": "OK. Will add.", "author": "HeartSaVioR", "createdAt": "2020-09-29T02:02:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NTQxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NjgzOQ==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496256839", "bodyText": "This is a little to specific because Iceberg actually just requires that your data is clustered by partition in each task. Maybe this is the easiest way to get the idea across initially, but it would be nice to have a note box that explains that both global and local sorts will work.", "author": "rdblue", "createdAt": "2020-09-28T21:51:21Z", "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.", "originalCommit": "230650d36f81d39d55ea46a85c81c25c4a32e483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMzOTczNQ==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496339735", "bodyText": "Please correct me if I'm missing something. (Sorry for being too correct technically - I'm also a learner of Iceberg so just to understand correctly.)\nIf I understand correctly, at least \"Iceberg Spark writer\" requires the data to be sorted according to the partition spec in task (Spark partition), not just the data to be clustered by partition.\nBelow query fails:\nspark.sql(\"\"\"\nCREATE TABLE iceberg_catalog.default.sample1 (\n    id bigint,\n    data string,\n    category string)\nUSING iceberg\nPARTITIONED BY (category)\n\"\"\")\n\nval data = (0 to 100000).map { id =>\n  (id, s\"hello$id\", s\"category-${id % 100}\")\n}\n\ndata.toDF(\"id\", \"data\", \"category\").repartition(100, col(\"category\")).sortWithinPartitions(\"id\").writeTo(\"iceberg_catalog.default.sample1\").append()\n\nMentioning the global and local sorts would be nice to have. Thanks! Will add.", "author": "HeartSaVioR", "createdAt": "2020-09-29T02:21:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NjgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjkwMzU1Nw==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496903557", "bodyText": "You're right that the query fails. Although the data is partitioned to group by category, Spark will use hash partitioning so there is no guarantee that each partition contains only one category. If it there were, it would work. Some partitions are going to contain more than one category, and a partition is a task. Sorting by id mixes the categories together, which causes the failure. I think it would also fail without the sort because the data coming from the map side of the shuffle would also be mixed together.\nFor repartitioning, it appears like Spark will cluster the data, but you actually need a sort to do it. However, there are other cases where you don't need to add an explicit sort, like when you can guarantee that only one partition is written by a job (writing an agg table for one day) or when the input data already aligns with the partitioning of the output.", "author": "rdblue", "createdAt": "2020-09-29T17:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NjgzOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzEzODkyNg==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r497138926", "bodyText": "Yeah you're right that repartition wouldn't make each partition to have only one category, and I think that's normal as we probably don't want to have large number of partitions (tasks) due to the unpredictable cardinality of category.\nI understand there're cases where we don't need to add an explicit sort. Probably they would become the cases no implicit sort is performed as well if Spark knows enough information on data alignment & required alignment - it'd be ideal SPARK-23889 would also cover the case.", "author": "HeartSaVioR", "createdAt": "2020-09-29T23:30:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1NjgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1Nzg2NA==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496257864", "bodyText": "Should this example be a little simpler?\nI think it makes sense to have an example for bucketing, but the other two don't require creating a UDF with the Iceberg transform. Splitting this into two examples might make sense: one with days(ts) and category to show how to add the sort with a SQL ORDER BY and using sortWithinPartitions, and then a more complicated one for bucketing.", "author": "rdblue", "createdAt": "2020-09-28T21:53:36Z", "path": "site/docs/spark.md", "diffHunk": "@@ -519,6 +519,59 @@ data.writeTo(\"prod.db.table\")\n     .createOrReplace()\n ```\n \n+## Writing against partitioned table\n+\n+Iceberg requires the data to be sorted according to the partition spec in prior to write against partitioned table.\n+This applies both Writing with SQL and Writing with DataFrames.\n+\n+Assuming we would like to write the data against below sample table:\n+\n+```sql\n+CREATE TABLE prod.db.sample (\n+    id bigint,\n+    data string,\n+    category string,\n+    ts timestamp)\n+USING iceberg\n+PARTITIONED BY (bucket(16, id), days(ts), category)", "originalCommit": "230650d36f81d39d55ea46a85c81c25c4a32e483", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjMzMTEyMQ==", "url": "https://github.com/apache/iceberg/pull/1523#discussion_r496331121", "bodyText": "Good suggestion. Simple example and the complicated one would be good. Will do.", "author": "HeartSaVioR", "createdAt": "2020-09-29T01:55:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI1Nzg2NA=="}], "type": "inlineReview"}, {"oid": "f4d4bcb56b466e50ddb7d2888f7c092c4689c19a", "url": "https://github.com/apache/iceberg/commit/f4d4bcb56b466e50ddb7d2888f7c092c4689c19a", "message": "Reflect review comment, before having utility method for transform.", "committedDate": "2020-09-29T03:23:19Z", "type": "commit"}, {"oid": "2b2e1772ae6d6783229daaed750c3bfb7ea01f74", "url": "https://github.com/apache/iceberg/commit/2b2e1772ae6d6783229daaed750c3bfb7ea01f74", "message": "Add notes", "committedDate": "2020-09-29T03:48:08Z", "type": "commit"}, {"oid": "82915b6b50015520516b7875f5eba3250d25dfd0", "url": "https://github.com/apache/iceberg/commit/82915b6b50015520516b7875f5eba3250d25dfd0", "message": "Introduce util class to ease use of registering bucket function to UDF", "committedDate": "2020-09-29T04:37:04Z", "type": "commit"}, {"oid": "f118cbcbd2c944974537fb5183f14d074dccd20d", "url": "https://github.com/apache/iceberg/commit/f118cbcbd2c944974537fb5183f14d074dccd20d", "message": "Add test for IcebergSpark", "committedDate": "2020-09-29T04:50:38Z", "type": "commit"}, {"oid": "e6ab83d3a5caf3dc9d95deb8300e601a52e81402", "url": "https://github.com/apache/iceberg/commit/e6ab83d3a5caf3dc9d95deb8300e601a52e81402", "message": "Fix test issue", "committedDate": "2020-09-29T05:51:15Z", "type": "commit"}]}