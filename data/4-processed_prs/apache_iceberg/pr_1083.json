{"pr_number": 1083, "pr_title": "Add DataFile rewrite Action support for Iceberg", "pr_createdAt": "2020-06-01T06:36:28Z", "pr_url": "https://github.com/apache/iceberg/pull/1083", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQ0NzY5NA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433447694", "bodyText": "Nit: should be deletedDataFiles.", "author": "rdblue", "createdAt": "2020-06-01T19:39:38Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesActionResult.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+\n+public class RewriteDataFilesActionResult {\n+\n+  private static final RewriteDataFilesActionResult EMPTY =\n+      new RewriteDataFilesActionResult(ImmutableList.of(), ImmutableList.of());\n+\n+  private List<DataFile> deletedDataFiles;\n+  private List<DataFile> addedDataFiles;\n+\n+  public RewriteDataFilesActionResult(List<DataFile> deletedDataFiles, List<DataFile> addedDataFiles) {\n+    this.deletedDataFiles = deletedDataFiles;\n+    this.addedDataFiles = addedDataFiles;\n+  }\n+\n+  static RewriteDataFilesActionResult empty() {\n+    return EMPTY;\n+  }\n+\n+  public List<DataFile> deleteDataFiles() {", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODY1Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538657", "bodyText": "It looks like if the aborts succeed, then the task returns instead of throwing t. Is that correct behavior? I see that this is calling context.markTaskFailed(originalThrowable). Is that done instead of throwing the exception?", "author": "rdblue", "createdAt": "2020-06-01T23:19:39Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc0NTMwOQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433745309", "bodyText": "This is an issue, I will fix it.", "author": "jerryshao", "createdAt": "2020-06-02T09:31:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODY1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODczMw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538733", "bodyText": "Why not throw the original throwable?", "author": "rdblue", "createdAt": "2020-06-01T23:19:54Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        dataReader.close();\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n+\n+      } catch (Throwable inner) {\n+        if (originalThrowable != inner) {\n+          originalThrowable.addSuppressed(inner);\n+          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n+        }\n+\n+        // Wrap throwable in an Exception\n+        throw new Exception(originalThrowable);", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc1Mzc3Mg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433753772", "bodyText": "This is mainly because the map function in RDD only supports Exception rather Throwable, so here I wrapped again to throw an Exception instead.", "author": "jerryshao", "createdAt": "2020-06-02T09:46:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODczMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODk2Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433538967", "bodyText": "Close should be called within a finally block, I think.", "author": "rdblue", "createdAt": "2020-06-01T23:20:49Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzc0MDgyNw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433740827", "bodyText": "This part of code is mostly referred from Spark DS related ones.\nI think we invoked dataReader.close in catch Throwable to close reader in any case, so there should be no necessary to use finally block.", "author": "jerryshao", "createdAt": "2020-06-02T09:24:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzUzODk2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MDc2Mg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433540762", "bodyText": "Why does this set the epoch ID? I think this is unnecessary and could just default to 0.", "author": "rdblue", "createdAt": "2020-06-01T23:27:15Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzczNjU1OQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433736559", "bodyText": "Sure, will change to 0.", "author": "jerryshao", "createdAt": "2020-06-02T09:17:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MDc2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0MTY2Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433541667", "bodyText": "This sets the partition spec that will be used when writing new data files, but that isn't obvious from the configuration method name. How about outputSpecId or something more descriptive? I think this should also be documented for callers.", "author": "rdblue", "createdAt": "2020-06-01T23:30:18Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0NjkyNg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433546926", "bodyText": "This method iterates through tasks and builds the grouped result in memory. The result doesn't need to be a CloseableIterable because it is in memory and there shouldn't be any resources to close.\nThis also doesn't close tasks or the iterator used (which is what closing tasks would do). Instead, it combines each value list into a CloseableIterable with the original tasks. I think the intent was to close tasks when the result of this method is consumed. But tasks is never really closed because the collect doesn't handle a CloseableIterable any differently than an Iterable. In addition, by combining each partition list with tasks, closing each one would close tasks again, which isn't a good idea.\nLet's refactor this by passing in tasks.iterator(). That will be a CloseableIterator that should be closed after it is consumed. Like this:\ntry {\n  tasksIter.forEachRemaining(task -> {\n    ...\n  });\n} finally {\n  tasksIter.close()\n}", "author": "rdblue", "createdAt": "2020-06-01T23:49:40Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU0ODI1OQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433548259", "bodyText": "While I agree that using the table values as defaults is a good idea, I think this should also expose configuration methods for combined size and lookback.\nThis is important because lookback(1) is used to prevent files from being reordered. Because only one bin is open at a time, either a file fits or it doesn't so the order of files is not changed. That's important when compacting a partition written with a global ORDER BY, which will distribute the sort column data across files. Iceberg uses that distribution to prune files when users filter by those fields.", "author": "rdblue", "createdAt": "2020-06-01T23:54:19Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MTE0NQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433551145", "bodyText": "We typically use the boolean threw pattern instead of using a broad catch like this. The purpose is to still try to clean up even if the problem wasn't an Exception (like AssertionError):\nboolean threw = true;\ntry {\n  RewriteFiles = table.newRewrite().rewriteFiles(...).commit()\n  threw = false;\n} finally {\n  if (threw) {\n    Tasks.foreach(...)\n        .run(fileIO::deleteFile);\n  }\n}", "author": "rdblue", "createdAt": "2020-06-02T00:04:48Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {\n+      List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(task.file().partition(), Lists.newArrayList());\n+      taskList.add(task);\n+      tasksGroupedByPartition.put(task.file().partition(), taskList);\n+    });\n+\n+    return tasksGroupedByPartition.values().stream()\n+        .map(list -> CloseableIterable.combine(list, tasks))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+    } catch (Exception e) {", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MjM5OA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433552398", "bodyText": "The purpose of this class doesn't look well-defined. The Spark context is passed in, but so are already broadcasted variables. The main function here looks like it handles calling the DSv2 components and is serializable. If that's the case, then I think it would be cleaner to refactor the Spark context out and just pass in an RDD of CombinedScanTask.", "author": "rdblue", "createdAt": "2020-06-02T00:09:08Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1MjYxOQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433552619", "bodyText": "You can omit empty constructors.", "author": "rdblue", "createdAt": "2020-06-02T00:09:58Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,300 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  public TestRewriteDataFilesAction() {\n+  }", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1NTI4OQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433555289", "bodyText": "This is going to iterate through fileScanTasks before groupTasksByPartition also iterates through fileScanTasks, so it will cause job planning to happen twice. Instead, I think that this check should be moved after groupTasksByPartition, which would return a Map. If the map has no entries, then there is no work to do.\nThis should also be more aggressive. If a partition has just one file to compact, then that partition should be removed because there is no work to do.", "author": "rdblue", "createdAt": "2020-06-02T00:19:46Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzY1NTI4Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433655283", "bodyText": "I see, will change to that way.", "author": "jerryshao", "createdAt": "2020-06-02T06:48:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1NTI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzU1NTYxNg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433555616", "bodyText": "This should use StructLikeWrapper for the key instead of StructLike to handle CharSequence values correctly.", "author": "rdblue", "createdAt": "2020-06-02T00:21:09Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = groupedTasks.stream()\n+        .map(tasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(tasks, splitSize);\n+          return TableScanUtil.planTasks(splitTasks, splitSize, lookbak, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    CloseableIterable<CombinedScanTask> combinedScanTasks = CloseableIterable.concat(groupedCombinedTasks);\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, sparkContext, spec,\n+        caseSensitive, io, encryption, targetDataFileSizeBytes);\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(combinedScanTasks);\n+\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private List<CloseableIterable<FileScanTask>> groupTasksByPartition(CloseableIterable<FileScanTask> tasks) {\n+    Map<StructLike, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    tasks.forEach(task -> {\n+      List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(task.file().partition(), Lists.newArrayList());\n+      taskList.add(task);\n+      tasksGroupedByPartition.put(task.file().partition(), taskList);", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMDc2Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433820767", "bodyText": "typo", "author": "Jiayi-Liao", "createdAt": "2020-06-02T11:58:09Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyMTE1NA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433821154", "bodyText": "shoudn't be SPLIT_OPEN_FILE_COST_DEFAULT?", "author": "Jiayi-Liao", "createdAt": "2020-06-02T11:58:58Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,186 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final long targetDataFileSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.targetDataFileSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteDataFilesAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+    List<DataFile> currentDataFiles =\n+        Lists.newArrayList(CloseableIterable.transform(fileScanTasks, FileScanTask::file));\n+    if (currentDataFiles.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    List<CloseableIterable<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks);\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    int lookbak = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMzQ1Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r433833453", "bodyText": "Same as Line 118?", "author": "Jiayi-Liao", "createdAt": "2020-06-02T12:22:22Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.execution.streaming.MicroBatchExecution;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final transient JavaSparkContext sparkContext;\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, JavaSparkContext sparkContext, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.sparkContext = sparkContext;\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(CloseableIterable<CombinedScanTask> tasks) {\n+    List<CombinedScanTask> taskList = Lists.newArrayList(tasks);\n+    int parallelism = taskList.size();\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(taskList, parallelism);\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(task -> rewriteDataForTask(task));\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) throws Exception {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    long epochId = Long.parseLong(\n+        Optional.ofNullable(context.getLocalProperty(MicroBatchExecution.BATCH_ID_KEY())).orElse(\"0\"));\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, epochId);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        dataReader.close();\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",", "originalCommit": "2dc587785ba1d1a2dd1b72aa85aea3bd30966f68", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI3NDg0NQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434274845", "bodyText": "No, it is not the same. \"Aborting\" -> \"Aborted\".", "author": "jerryshao", "createdAt": "2020-06-03T02:32:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMzQ1Mw=="}], "type": "inlineReview"}, {"oid": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "url": "https://github.com/apache/iceberg/commit/dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "message": "Address the comments", "committedDate": "2020-06-02T13:21:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2MjE3MA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434062170", "bodyText": "I think that rewriteDataFile is redundant because this is a rewrite data files action. How about targetSizeInBytes instead?", "author": "rdblue", "createdAt": "2020-06-02T17:50:15Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2NTAwNQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434065005", "bodyText": "I think it would help to give a bit more context here. This is the number of \"bins\" considered when trying to pack the next file split into a task. Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single task, but with extra planning cost.\nThis should also mention that the bin packing can reorder the incoming file regions, so to preserve order for lower/upper bounds in file metadata, the caller can use a lookback of 1.", "author": "rdblue", "createdAt": "2020-06-02T17:55:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDA2NzIyOA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434067228", "bodyText": "I think it is important to mention that all files that may contain data matching the filter may be rewritten.\nWhen planning, the filter is used to match files inclusively: if a file may contain a matching row, the file matches. When matching file stats, the same inclusive approach is used. So if a user has a bucked table, bucket(id, 16) and asks to rewrite where id = X, then this will match any data file in id_bucket = bucket(X, 16).", "author": "rdblue", "createdAt": "2020-06-02T17:58:42Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMjQ4Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434132483", "bodyText": "This needs to re-throw the exception, not just swallow it.\nAnd since this should throw the exception, I don't think that it is necessary to log it. Whatever catches it above should log it. So maybe just remove this catch.", "author": "rdblue", "createdAt": "2020-06-02T19:41:01Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), rewriteDataFileSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, rewriteDataFileSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = groupedCombinedTasks.stream()\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter = new RowDataRewriter(table, spec, caseSensitive, io, encryption,\n+        rewriteDataFileSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());\n+        taskList.add(task);\n+        tasksGroupedByPartition.put(structLike, taskList);\n+      });\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Faile to close task iterator\", ioe);\n+      }\n+    }\n+\n+    return tasksGroupedByPartition;\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    boolean threw = true;\n+\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+      threw = false;\n+\n+    } catch (Throwable t) {\n+      LOG.error(\"Failed to rewrite DataFiles\", t);", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMzgwNQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434133805", "bodyText": "Could this use ManifestGroup instead of a scan? The scan will emit a scan event, but this isn't really a table scan.\nAlso, this should be wrapped in a try-with-resources block so that the CloseableIterable is closed.", "author": "rdblue", "createdAt": "2020-06-02T19:43:45Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4MjM5NQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434282395", "bodyText": "AFAIK, we already called close in groupTasksByPartition, do we need to call wrap the code here in try-with-resources again?", "author": "jerryshao", "createdAt": "2020-06-03T03:04:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMzgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4NDMxOA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434284318", "bodyText": "Besides, ManifestGroup is package private, so it cannot be used here.", "author": "jerryshao", "createdAt": "2020-06-03T03:13:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMzgwNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUyMQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434738521", "bodyText": "Yes, we need to close the CloseableIterable. That will close all iterators that are still open. The close in groupTasksByPartition closes the CloseableIterator that was passed in, not the group of iterators created from the iterable. Ideally, there shouldn't be other open iterators, but closing the iterable helps ensure that we catch any that are accidentally missed.", "author": "rdblue", "createdAt": "2020-06-03T17:33:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzMzgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzNDYxNQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434134615", "bodyText": "Since this is checked below, it's not necessary to have 2. Not a big deal to keep it though.", "author": "rdblue", "createdAt": "2020-06-02T19:45:20Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDEzNTk5Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434135993", "bodyText": "Why not combine this with the previous statement?", "author": "rdblue", "createdAt": "2020-06-02T19:47:54Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long rewriteDataFileSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.rewriteDataFileSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.rewriteDataFileSizeInBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the rewrite data file size in bytes\n+   *\n+   * @param rewriteDataFileSize size of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction rewriteDataFileSizeInBytes(long rewriteDataFileSize) {\n+    Preconditions.checkArgument(rewriteDataFileSize > 0L, \"Invalid rewrite data file size in bytes %d\",\n+        rewriteDataFileSize);\n+    this.rewriteDataFileSizeInBytes = rewriteDataFileSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the split lookback\n+   *\n+   * @param lookback lookback number to split\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. This is typically used when only rewriting DatFiles\n+   * under some partitions.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    // Nothing to rewrite if the table is empty.\n+    if (groupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CloseableIterable<CombinedScanTask>> groupedCombinedTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), rewriteDataFileSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, rewriteDataFileSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .collect(Collectors.toList());\n+    List<CombinedScanTask> combinedScanTasks = groupedCombinedTasks.stream()\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NTQ0OA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434175448", "bodyText": "Why not catch (Throwable originalThrowable) instead? I don't see the value of having this outside of the catch block.", "author": "rdblue", "createdAt": "2020-06-02T21:06:16Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(this::rewriteDataForTask);\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, 0);\n+\n+    Throwable originalThrowable = null;", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDI4ODI3Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434288277", "bodyText": "I see, will change it.", "author": "jerryshao", "createdAt": "2020-06-03T03:30:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NTQ0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDE3NzUxMg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434177512", "bodyText": "I think this should check whether originalThrowable is an Exception and throw it if so. That way we don't needlessly wrap it. Only throwables that aren't exceptions should be wrapped.", "author": "rdblue", "createdAt": "2020-06-02T21:10:47Z", "path": "spark/src/main/java/org/apache/iceberg/spark/source/RowDataRewriter.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.Serializable;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.SchemaParser;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.spark.TaskContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.sources.v2.writer.DataWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RowDataRewriter implements Serializable {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RowDataRewriter.class);\n+\n+  private final Broadcast<FileIO> fileIO;\n+  private final Broadcast<EncryptionManager> encryptionManager;\n+  private final String tableSchema;\n+  private final Writer.WriterFactory writerFactory;\n+  private final boolean caseSensitive;\n+\n+  public RowDataRewriter(Table table, PartitionSpec spec, boolean caseSensitive,\n+                         Broadcast<FileIO> fileIO, Broadcast<EncryptionManager> encryptionManager,\n+                         long targetDataFileSizeInBytes) {\n+    this.fileIO = fileIO;\n+    this.encryptionManager = encryptionManager;\n+\n+    this.caseSensitive = caseSensitive;\n+    this.tableSchema = SchemaParser.toJson(table.schema());\n+\n+    String formatString = table.properties().getOrDefault(\n+        TableProperties.DEFAULT_FILE_FORMAT, TableProperties.DEFAULT_FILE_FORMAT_DEFAULT);\n+    FileFormat fileFormat = FileFormat.valueOf(formatString.toUpperCase(Locale.ENGLISH));\n+    this.writerFactory = new Writer.WriterFactory(spec, fileFormat, table.locationProvider(), table.properties(),\n+        fileIO, encryptionManager, targetDataFileSizeInBytes, table.schema(), SparkSchemaUtil.convert(table.schema()));\n+  }\n+\n+  public List<DataFile> rewriteDataForTasks(JavaRDD<CombinedScanTask> taskRDD) {\n+    JavaRDD<Writer.TaskCommit> taskCommitRDD = taskRDD.map(this::rewriteDataForTask);\n+\n+    return taskCommitRDD.collect().stream()\n+        .flatMap(taskCommit -> Arrays.stream(taskCommit.files()))\n+        .collect(Collectors.toList());\n+  }\n+\n+  private Writer.TaskCommit rewriteDataForTask(CombinedScanTask task) {\n+    TaskContext context = TaskContext.get();\n+\n+    RowDataReader dataReader = new RowDataReader(task, SchemaParser.fromJson(tableSchema),\n+        SchemaParser.fromJson(tableSchema), fileIO.value(), encryptionManager.value(), caseSensitive);\n+\n+    int partitionId = context.partitionId();\n+    long taskId = context.taskAttemptId();\n+    DataWriter<InternalRow> dataWriter = writerFactory.createDataWriter(partitionId, taskId, 0);\n+\n+    Throwable originalThrowable = null;\n+    try {\n+      while (dataReader.next()) {\n+        InternalRow row = dataReader.get();\n+        dataWriter.write(row);\n+      }\n+\n+      dataReader.close();\n+      dataReader = null;\n+      return (Writer.TaskCommit) dataWriter.commit();\n+\n+    } catch (Throwable t) {\n+      originalThrowable = t;\n+      try {\n+        LOG.error(\"Aborting task\", originalThrowable);\n+        context.markTaskFailed(originalThrowable);\n+\n+        LOG.error(\"Aborting commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.attemptNumber(), context.stageId(), context.stageAttemptNumber());\n+        if (dataReader != null) {\n+          dataReader.close();\n+        }\n+        dataWriter.abort();\n+        LOG.error(\"Aborted commit for partition {} (task {}, attempt {}, stage {}.{})\",\n+            partitionId, taskId, context.taskAttemptId(), context.stageId(), context.stageAttemptNumber());\n+\n+      } catch (Throwable inner) {\n+        if (originalThrowable != inner) {\n+          originalThrowable.addSuppressed(inner);\n+          LOG.warn(\"Suppressing exception in catch: {}\", inner.getMessage(), inner);\n+        }\n+      }\n+\n+      throw new RuntimeException(originalThrowable);", "originalCommit": "dca22e0fae297feb6bddcf4d36ec76f2efdaeccc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MDgxMw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434740813", "bodyText": "Nit: typo Faile -> Failed.", "author": "rdblue", "createdAt": "2020-06-03T17:37:04Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());\n+        taskList.add(task);\n+        tasksGroupedByPartition.put(structLike, taskList);\n+      });\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Faile to close task iterator\", ioe);", "originalCommit": "1008765e306a97813637bb8f603d4ebe117b6e4c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjUyMg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434742522", "bodyText": "Javadoc requires <p> on empty lines if you want a paragraph break.", "author": "rdblue", "createdAt": "2020-06-03T17:40:05Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *", "originalCommit": "1008765e306a97813637bb8f603d4ebe117b6e4c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NjA5Ng==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r434746096", "bodyText": "This creates a new ArrayList for every element in the iterator to pass in as a default value. It would be better to use a ListMultimap instead so that you pass a supplier to create the lists:\n  ListMultimap<...> groups = Multimaps.newListMultimap(Maps.newHashMap(), Lists::newArrayList);\n  ...\n  groups.put(wrapper, task);", "author": "rdblue", "createdAt": "2020-06-03T17:46:28Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   *\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = table.newScan()\n+        .filter(filter)\n+        .planFiles();\n+\n+    Map<StructLikeWrapper, List<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, List<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+    boolean caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, List<FileScanTask>> groupTasksByPartition(CloseableIterator<FileScanTask> tasksIter) {\n+    Map<StructLikeWrapper, List<FileScanTask>> tasksGroupedByPartition = Maps.newHashMap();\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        List<FileScanTask> taskList = tasksGroupedByPartition.getOrDefault(structLike, Lists.newArrayList());", "originalCommit": "1008765e306a97813637bb8f603d4ebe117b6e4c", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxMDc2OA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436010768", "bodyText": "Will it make sense to move this logic into BaseAction as it is used also used while rewriting manifests?", "author": "aokolnychyi", "createdAt": "2020-06-05T15:50:46Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxMjU3Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436012573", "bodyText": "nit: typo in PartitionSepc", "author": "aokolnychyi", "createdAt": "2020-06-05T15:53:53Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436014391", "bodyText": "Shouldn't we use openFileCost as zero while bin-packing?", "author": "aokolnychyi", "createdAt": "2020-06-05T15:57:00Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU2NzA0Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436567043", "bodyText": "Yes, it seems meaningful to set openFileCost to zero, but will it increase the overhead to pack many small files into one task?", "author": "jerryshao", "createdAt": "2020-06-08T09:27:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MDkzNg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r437150936", "bodyText": "@rdblue what's your opinion on this?", "author": "jerryshao", "createdAt": "2020-06-09T05:45:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU2NzU1Ng==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r437567556", "bodyText": "I think there is still value for the open file cost. If there are a lot of small files, we don't want to pack them all into a single task for this action, just like we don't want to scan them all in a single task when reading. I think the only thing we would need to do is to allow the user to override this setting. We could do that in this one or a follow-up.", "author": "rdblue", "createdAt": "2020-06-09T16:35:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzgyOTAyOA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r437829028", "bodyText": "I will add a configuration method for user to override.", "author": "jerryshao", "createdAt": "2020-06-10T02:44:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxNDM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxODE5MA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436018190", "bodyText": "Can we simplify this block by using catch (Exception e) and rethrowing the original exception? Otherwise, we won't know what happened.", "author": "aokolnychyi", "createdAt": "2020-06-05T16:03:44Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)\n+          .filter(filter)\n+          .planFiles();\n+    } finally {\n+      try {\n+        if (fileScanTasks != null) {\n+          fileScanTasks.close();\n+        }\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterable\", ioe);\n+      }\n+    }\n+\n+    Map<StructLikeWrapper, Collection<FileScanTask>> groupedTasks = groupTasksByPartition(fileScanTasks.iterator());\n+    Map<StructLikeWrapper, Collection<FileScanTask>> filteredGroupedTasks = groupedTasks.entrySet().stream()\n+        .filter(kv -> kv.getValue().size() > 1)\n+        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n+\n+    // Nothing to rewrite if there's only one DataFile in each partition.\n+    if (filteredGroupedTasks.isEmpty()) {\n+      return RewriteDataFilesActionResult.empty();\n+    }\n+\n+    long openFileCost = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_OPEN_FILE_COST,\n+        TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n+\n+    // Split and combine tasks under each partition\n+    List<CombinedScanTask> combinedScanTasks = filteredGroupedTasks.values().stream()\n+        .map(scanTasks -> {\n+          CloseableIterable<FileScanTask> splitTasks = TableScanUtil.splitFiles(\n+              CloseableIterable.withNoopClose(scanTasks), targetSizeInBytes);\n+          return TableScanUtil.planTasks(splitTasks, targetSizeInBytes, splitLookback, openFileCost);\n+        })\n+        .flatMap(Streams::stream)\n+        .collect(Collectors.toList());\n+\n+    JavaRDD<CombinedScanTask> taskRDD = sparkContext.parallelize(combinedScanTasks, combinedScanTasks.size());\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    Broadcast<EncryptionManager> encryption = sparkContext.broadcast(encryptionManager);\n+\n+    RowDataRewriter rowDataRewriter =\n+        new RowDataRewriter(table, spec, caseSensitive, io, encryption, targetSizeInBytes);\n+\n+    List<DataFile> addedDataFiles = rowDataRewriter.rewriteDataForTasks(taskRDD);\n+    List<DataFile> currentDataFiles = filteredGroupedTasks.values().stream()\n+        .flatMap(tasks -> tasks.stream().map(FileScanTask::file))\n+        .collect(Collectors.toList());\n+    replaceDataFiles(currentDataFiles, addedDataFiles);\n+\n+    return new RewriteDataFilesActionResult(currentDataFiles, addedDataFiles);\n+  }\n+\n+  private Map<StructLikeWrapper, Collection<FileScanTask>> groupTasksByPartition(\n+      CloseableIterator<FileScanTask> tasksIter) {\n+    ListMultimap<StructLikeWrapper, FileScanTask> tasksGroupedByPartition = Multimaps.newListMultimap(\n+        Maps.newHashMap(), Lists::newArrayList);\n+\n+    try {\n+      tasksIter.forEachRemaining(task -> {\n+        StructLikeWrapper structLike = StructLikeWrapper.wrap(task.file().partition());\n+        tasksGroupedByPartition.put(structLike, task);\n+      });\n+\n+    } finally {\n+      try {\n+        tasksIter.close();\n+      } catch (IOException ioe) {\n+        LOG.warn(\"Failed to close task iterator\", ioe);\n+      }\n+    }\n+\n+    return tasksGroupedByPartition.asMap();\n+  }\n+\n+  private void replaceDataFiles(Iterable<DataFile> deletedDataFiles, Iterable<DataFile> addedDataFiles) {\n+    boolean threw = true;\n+\n+    try {\n+      RewriteFiles rewriteFiles = table.newRewrite();\n+      rewriteFiles.rewriteFiles(Sets.newHashSet(deletedDataFiles), Sets.newHashSet(addedDataFiles));\n+      commit(rewriteFiles);\n+      threw = false;\n+\n+    } finally {\n+      if (threw) {", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg1MDA5Mg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436850092", "bodyText": "We typically use this pattern with a threw boolean to always clean up even if the throwable isn't an Exception. It's more important to do this when the finally block needs to execute for all cases, though. I think it would be probably be fine to catch Exception here like it was originally -- sorry for the bad review comment.", "author": "rdblue", "createdAt": "2020-06-08T16:50:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxODE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAxOTY3Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436019677", "bodyText": "This is the place where we need to call ignoreResiduals() to make sure we read complete files.", "author": "aokolnychyi", "createdAt": "2020-06-05T16:06:20Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,275 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);\n+\n+    this.splitLookback = PropertyUtil.propertyAsInt(\n+        table.properties(),\n+        TableProperties.SPLIT_LOOKBACK,\n+        TableProperties.SPLIT_LOOKBACK_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+    this.encryptionManager = table.encryption();\n+  }\n+\n+  @Override\n+  protected RewriteDataFilesAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  /**\n+   * Pass a PartitionSepc id to specify which PartitionSpec should be used in DataFile rewrite\n+   *\n+   * @param specId PartitionSpec id to rewrite\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction outputSpecId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the target rewrite data file size in bytes\n+   *\n+   * @param targetSize size in bytes of rewrite data file\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction targetSizeInBytes(long targetSize) {\n+    Preconditions.checkArgument(targetSize > 0L, \"Invalid target rewrite data file size in bytes %d\",\n+        targetSize);\n+    this.targetSizeInBytes = targetSize;\n+    return this;\n+  }\n+\n+  /**\n+   * Specify the number of \"bins\" considered when trying to pack the next file split into a task.\n+   * Increasing this usually makes tasks a bit more even by considering more ways to pack file regions into a single\n+   * task with extra planning cost.\n+   * <p>\n+   * This configuration can reorder the incoming file regions, to preserve order for lower/upper bounds in file\n+   * metadata, user can use a lookback of 1.\n+   *\n+   * @param lookback number of \"bins\" considered when trying to pack the next file split into a task.\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction splitLookback(int lookback) {\n+    Preconditions.checkArgument(lookback > 0L, \"Invalid split lookback %d\", lookback);\n+    this.splitLookback = lookback;\n+    return this;\n+  }\n+\n+  /**\n+   * Pass a row Expression to filter DataFiles to be rewritten. Note that all files that may contain data matching the\n+   * filter may be rewritten.\n+   *\n+   * @param expr Expression to filter out DataFiles\n+   * @return this for method chaining\n+   */\n+  public RewriteDataFilesAction filter(Expression expr) {\n+    this.filter = Expressions.and(filter, expr);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteDataFilesActionResult execute() {\n+    CloseableIterable<FileScanTask> fileScanTasks = null;\n+    try {\n+      fileScanTasks = table.newScan()\n+          .caseSensitive(caseSensitive)", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjAyNTA3Mg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r436025072", "bodyText": "I would advise adding a test where a file we rewrite has multiple row groups and our filter expression matches only one of them. It is not an easy test but should be worth it.\nThis can be done by setting TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES to a small value like 100, inserting a relatively large number of rows sorted by a column, and using a filter on that column that matches only last 1-2 records.", "author": "aokolnychyi", "createdAt": "2020-06-05T16:16:20Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,297 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {", "originalCommit": "c70da1b6275f5ab8c0c260405eda8b038aa37673", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "5d00e101627b83af956e45adb05667835d6fb0fe", "url": "https://github.com/apache/iceberg/commit/5d00e101627b83af956e45adb05667835d6fb0fe", "message": "Add rewrite DataFile support for Iceberg Action", "committedDate": "2020-06-08T09:31:48Z", "type": "commit"}, {"oid": "4a7b98b5a1b1eae0855c4489c9aa0f71a9bd09dd", "url": "https://github.com/apache/iceberg/commit/4a7b98b5a1b1eae0855c4489c9aa0f71a9bd09dd", "message": "Fix guava relocate issue", "committedDate": "2020-06-08T09:31:48Z", "type": "commit"}, {"oid": "1c26f2415a1fdfd6d5479454ca879b541f94fb25", "url": "https://github.com/apache/iceberg/commit/1c26f2415a1fdfd6d5479454ca879b541f94fb25", "message": "Address the comments", "committedDate": "2020-06-08T09:31:48Z", "type": "commit"}, {"oid": "daa1097af2802b21c87339f348218e48af7a808c", "url": "https://github.com/apache/iceberg/commit/daa1097af2802b21c87339f348218e48af7a808c", "message": "Address the comments", "committedDate": "2020-06-08T09:31:48Z", "type": "commit"}, {"oid": "4b64ba99ec6ee6110a6886f6583a5168de5bb194", "url": "https://github.com/apache/iceberg/commit/4b64ba99ec6ee6110a6886f6583a5168de5bb194", "message": "Address the comments", "committedDate": "2020-06-08T09:31:48Z", "type": "commit"}, {"oid": "986518ac2c6bc8147356691d55df00ac350e5a37", "url": "https://github.com/apache/iceberg/commit/986518ac2c6bc8147356691d55df00ac350e5a37", "message": "Address the comments", "committedDate": "2020-06-09T03:31:40Z", "type": "commit"}, {"oid": "986518ac2c6bc8147356691d55df00ac350e5a37", "url": "https://github.com/apache/iceberg/commit/986518ac2c6bc8147356691d55df00ac350e5a37", "message": "Address the comments", "committedDate": "2020-06-09T03:31:40Z", "type": "forcePushed"}, {"oid": "2bd9f4aee4714ec519370efe4929965b75b6e5c0", "url": "https://github.com/apache/iceberg/commit/2bd9f4aee4714ec519370efe4929965b75b6e5c0", "message": "Address the comments", "committedDate": "2020-06-10T02:58:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MjE0Nw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438482147", "bodyText": "How do we know there will be 2 files for c3=0?\nLooks like the table is unpartitioned, so each task will create a single output file. Are there only 2 tasks?", "author": "rdblue", "createdAt": "2020-06-11T00:44:09Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    actions.rewriteDataFiles().execute();\n+\n+    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, null, \"AAAA\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"CCCCCCCCCC\", \"CCCC\"),\n+        new ThreeColumnRecord(2, \"DDDDDDDDDD\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files before rewrite\", 1, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .filter(Expressions.equal(\"c1\", 1))\n+        .filter(Expressions.startsWith(\"c2\", \"AA\"))\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 7 data files before rewrite\", 7, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteLargeTableHasResiduals() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, \"100\");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    // all records belong to the same partition\n+    List<ThreeColumnRecord> records = Lists.newArrayList();\n+    for (int i = 0; i < 100; i++) {\n+      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i % 4)));\n+    }\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n+    writeDF(df);", "originalCommit": "2bd9f4aee4714ec519370efe4929965b75b6e5c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ5Nzk5NA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438497994", "bodyText": "Yes, because the Spark master we set is local[2], so we will only have 2 task slots, which will only have 2 files.", "author": "jerryshao", "createdAt": "2020-06-11T01:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4MjE0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4Mjc5Ng==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438482796", "bodyText": "I don't think it is necessary to have this assertion or to run the scan planning above. This tests that ignoreResiduals works properly, but we can assume that for this test.", "author": "rdblue", "createdAt": "2020-06-11T00:46:25Z", "path": "spark/src/test/java/org/apache/iceberg/actions/TestRewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.File;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.source.ThreeColumnRecord;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestRewriteDataFilesAction {\n+\n+  private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n+  private static final Schema SCHEMA = new Schema(\n+      optional(1, \"c1\", Types.IntegerType.get()),\n+      optional(2, \"c2\", Types.StringType.get()),\n+      optional(3, \"c3\", Types.StringType.get())\n+  );\n+\n+  private static SparkSession spark;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestRewriteDataFilesAction.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .getOrCreate();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestRewriteDataFilesAction.spark;\n+    TestRewriteDataFilesAction.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private String tableLocation = null;\n+\n+  @Before\n+  public void setupTableLocation() throws Exception {\n+    File tableDir = temp.newFolder();\n+    this.tableLocation = tableDir.toURI().toString();\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesEmptyTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    actions.rewriteDataFiles().execute();\n+\n+    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesUnpartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.unpartitioned();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, null, \"AAAA\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"CCCCCCCCCC\", \"CCCC\"),\n+        new ThreeColumnRecord(2, \"DDDDDDDDDD\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 1 data files before rewrite\", 1, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesPartitionedTable() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions.rewriteDataFiles().execute();\n+    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 4 data files before rewrite\", 4, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteDataFilesWithFilter() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA)\n+        .identity(\"c1\")\n+        .truncate(\"c2\", 2)\n+        .build();\n+    Map<String, String> options = Maps.newHashMap();\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    List<ThreeColumnRecord> records1 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"),\n+        new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"CCCC\")\n+    );\n+    writeRecords(records1);\n+\n+    List<ThreeColumnRecord> records2 = Lists.newArrayList(\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"),\n+        new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"DDDD\")\n+    );\n+    writeRecords(records2);\n+\n+    List<ThreeColumnRecord> records3 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"EEEE\"),\n+        new ThreeColumnRecord(2, \"AAAAAAAAAA\", \"GGGG\")\n+    );\n+    writeRecords(records3);\n+\n+    List<ThreeColumnRecord> records4 = Lists.newArrayList(\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"FFFF\"),\n+        new ThreeColumnRecord(2, \"BBBBBBBBBB\", \"HHHH\")\n+    );\n+    writeRecords(records4);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan().planFiles();\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 8 data files before rewrite\", 8, dataFiles.size());\n+\n+    Actions actions = Actions.forTable(table);\n+\n+    RewriteDataFilesActionResult result = actions\n+        .rewriteDataFiles()\n+        .filter(Expressions.equal(\"c1\", 1))\n+        .filter(Expressions.startsWith(\"c2\", \"AA\"))\n+        .execute();\n+    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.deletedDataFiles().size());\n+    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFiles().size());\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks1 = table.newScan().planFiles();\n+    List<DataFile> dataFiles1 = Lists.newArrayList(CloseableIterable.transform(tasks1, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 7 data files before rewrite\", 7, dataFiles1.size());\n+\n+    List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n+    expectedRecords.addAll(records1);\n+    expectedRecords.addAll(records2);\n+    expectedRecords.addAll(records3);\n+    expectedRecords.addAll(records4);\n+\n+    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+    List<ThreeColumnRecord> actualRecords = resultDF.sort(\"c1\", \"c2\", \"c3\")\n+        .as(Encoders.bean(ThreeColumnRecord.class))\n+        .collectAsList();\n+\n+    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+  }\n+\n+  @Test\n+  public void testRewriteLargeTableHasResiduals() {\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, \"100\");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    // all records belong to the same partition\n+    List<ThreeColumnRecord> records = Lists.newArrayList();\n+    for (int i = 0; i < 100; i++) {\n+      records.add(new ThreeColumnRecord(i, String.valueOf(i), String.valueOf(i % 4)));\n+    }\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class);\n+    writeDF(df);\n+\n+    table.refresh();\n+\n+    CloseableIterable<FileScanTask> tasks = table.newScan()\n+        .ignoreResiduals()\n+        .filter(Expressions.equal(\"c3\", \"0\"))\n+        .planFiles();\n+    for (FileScanTask task : tasks) {\n+      Assert.assertEquals(\"Residuals must be ignored\", Expressions.alwaysTrue(), task.residual());\n+    }\n+    List<DataFile> dataFiles = Lists.newArrayList(CloseableIterable.transform(tasks, FileScanTask::file));\n+    Assert.assertEquals(\"Should have 2 data files before rewrite\", 2, dataFiles.size());", "originalCommit": "2bd9f4aee4714ec519370efe4929965b75b6e5c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5NDAyOQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438894029", "bodyText": "I think the best way to verify is by checking that the test fails if we remove ignoreResiduals() from the action.", "author": "aokolnychyi", "createdAt": "2020-06-11T15:53:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODQ4Mjc5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438890925", "bodyText": "Hm, if my split size is 128 MB and target file size is 512 MB, what is the size of files that we will create?", "author": "aokolnychyi", "createdAt": "2020-06-11T15:50:21Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteDataFilesAction.java", "diffHunk": "@@ -0,0 +1,281 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import java.io.IOException;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ListMultimap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Multimaps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n+import org.apache.iceberg.spark.source.RowDataRewriter;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.StructLikeWrapper;\n+import org.apache.iceberg.util.TableScanUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.SparkSession;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteDataFilesAction\n+    extends BaseSnapshotUpdateAction<RewriteDataFilesAction, RewriteDataFilesActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteDataFilesAction.class);\n+\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final EncryptionManager encryptionManager;\n+  private final boolean caseSensitive;\n+  private long targetSizeInBytes;\n+  private int splitLookback;\n+  private long splitOpenFileCost;\n+\n+  private PartitionSpec spec = null;\n+  private Expression filter;\n+\n+  RewriteDataFilesAction(SparkSession spark, Table table) {\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.filter = Expressions.alwaysTrue();\n+    this.caseSensitive = Boolean.parseBoolean(spark.conf().get(\"spark.sql.caseSensitive\", \"false\"));\n+\n+    long splitSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.SPLIT_SIZE,\n+        TableProperties.SPLIT_SIZE_DEFAULT);\n+    long targetFileSize = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES,\n+        TableProperties.WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT);\n+    this.targetSizeInBytes = Math.min(splitSize, targetFileSize);", "originalCommit": "2bd9f4aee4714ec519370efe4929965b75b6e5c0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5ODM2Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438898363", "bodyText": "Hm. Split size is how much data per task while reading, and write target size is how much data to put in a file when writing. The behavior of the write target size is more of an upper bound -- files could be smaller and Iceberg has no ability to coalesce to larger sizes. I think this should ignore the target file size because that's enforced in the writer regardless. Only the split size should matter.", "author": "rdblue", "createdAt": "2020-06-11T15:59:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkxNDMwMA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438914300", "bodyText": "I'd consider packing files into bins that are around targetFileSize as defined by the table property and ignoring the threshold during writes by setting target-file-size-bytes to the max long value so that we avoid closing files near the end of bins. That way, we will get files that are close to the target file size.", "author": "aokolnychyi", "createdAt": "2020-06-11T16:24:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMzU4Mg==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438923582", "bodyText": "After thinking about this a bit more, I think I wasn't correct that target write size is a maximum. It's currently used that way in Spark, but the property is \"target\" and that implies that it is the size files should be, ideally. So Anton is right and that's the size of files we should attempt to create. This also makes sense because it is a write property, not a read property.\nI think we should remove the min with split size and just use the table's target size as the default.", "author": "rdblue", "createdAt": "2020-06-11T16:40:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk1MDA3OQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r438950079", "bodyText": "Let's also set target-file-size-bytes to the max long value while writing so that we avoid closing files near the end of bins. For example, if we ended up with 520 MB of data in a task, it is better to write it as one file instead of 512MB and 8 MB.", "author": "aokolnychyi", "createdAt": "2020-06-11T17:27:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTAzNzEwOA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r439037108", "bodyText": "I am also OK addressing this in a follow-up as the PR itself is large.", "author": "aokolnychyi", "createdAt": "2020-06-11T20:04:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE3OTU2OA==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r439179568", "bodyText": "@aokolnychyi @rdblue , let me rephrase the purpose you mentioned above:\n\nUsing the targetFileSize to do the split, and also the written file size.\nSetting write.target-file-size-bytes to Long.MAX_VALUE to make sure there's no separated files for one written task.\n\nAm I understanding right?\nBesides, what is the default value of targetFileSize, should it be explicitly defined by user, or use SPLIT_SIZE as default?", "author": "jerryshao", "createdAt": "2020-06-12T02:53:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE4MDk2MQ==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r439180961", "bodyText": "I think the conclusion was that the planning in this feature should use write.target-file-size-bytes for the default targetFileSize when planning the splits because we should use the target write size (instead of the target read/split size for most reads) for our task size.\nNext, we also need to override write.target-file-size-bytes for the write itself because we expect the write to already produce files close to that size, and leaving it at the normal target is likely to produce tiny files when a task's output is just over the limit.", "author": "rdblue", "createdAt": "2020-06-12T02:59:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTE4MjQ5Mw==", "url": "https://github.com/apache/iceberg/pull/1083#discussion_r439182493", "bodyText": "Thanks @rdblue ,\nThe default value of write.target-file-size-bytes is Long.MAX_VALUE. If users don't override this configuration explicitly, and use this to do planning ( bin-packing), will this lead to the problem where all the FileScanTasks combined into one?", "author": "jerryshao", "createdAt": "2020-06-12T03:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODg5MDkyNQ=="}], "type": "inlineReview"}]}