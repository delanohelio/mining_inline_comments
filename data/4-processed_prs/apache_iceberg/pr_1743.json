{"pr_number": 1743, "pr_title": "Spark: Add stored procedure API", "pr_createdAt": "2020-11-09T17:33:12Z", "pr_url": "https://github.com/apache/iceberg/pull/1743", "timeline": [{"oid": "5d23e76c259f8d500347863e1b72938c794e03b4", "url": "https://github.com/apache/iceberg/commit/5d23e76c259f8d500347863e1b72938c794e03b4", "message": "Spark: Add stored procedure API", "committedDate": "2020-11-09T17:32:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519993784", "bodyText": "This is ugly. I'd appreciate any ideas.", "author": "aokolnychyi", "createdAt": "2020-11-09T17:34:12Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5NDYyMw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519994623", "bodyText": "In the current approach, methodHandle is expected to be either void or return java.lang.Iterable of Spark Rows.", "author": "aokolnychyi", "createdAt": "2020-11-09T17:35:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1Mzg2Mg==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953862", "bodyText": "Got rid of this.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:08:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5Mzc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTk5ODM3OA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r519998378", "bodyText": "Presto uses method handles for stored procedures too.", "author": "aokolnychyi", "createdAt": "2020-11-09T17:41:27Z", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();\n+  MethodHandle methodHandle();", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTM3Mg==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065372", "bodyText": "The ignored argument is a SparkSession, which I think should be passed into ResolveProcedures so that it doesn't need to reference SparkSession.active.", "author": "rdblue", "createdAt": "2020-11-09T19:26:53Z", "path": "spark3-extensions/src/main/scala/org/apache/iceberg/spark/extensions/IcebergSparkSessionExtensions.scala", "diffHunk": "@@ -20,13 +20,15 @@\n package org.apache.iceberg.spark.extensions\n \n import org.apache.spark.sql.SparkSessionExtensions\n+import org.apache.spark.sql.catalyst.analysis.ResolveProcedures\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergSparkSqlExtensionsParser\n import org.apache.spark.sql.execution.datasources.v2.ExtendedDataSourceV2Strategy\n \n class IcebergSparkSessionExtensions extends (SparkSessionExtensions => Unit) {\n \n   override def apply(extensions: SparkSessionExtensions): Unit = {\n     extensions.injectParser { case (_, parser) => new IcebergSparkSqlExtensionsParser(parser) }\n+    extensions.injectResolutionRule { _ => ResolveProcedures }", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1Mzc3NA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953774", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:08:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTM3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTU3MQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520065571", "bodyText": "The session should be passed in from rule injection.", "author": "rdblue", "createdAt": "2020-11-09T19:27:13Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzkwNg==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953906", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:09:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NTU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NjQyMg==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520066422", "bodyText": "Should this be in ExtendedDataSourceV2Strategy? It doesn't seem related to DSv2.", "author": "rdblue", "createdAt": "2020-11-09T19:28:45Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/ExtendedDataSourceV2Strategy.scala", "diffHunk": "@@ -20,14 +20,21 @@\n package org.apache.spark.sql.execution.datasources.v2\n \n import org.apache.spark.sql.{AnalysisException, Strategy}\n-import org.apache.spark.sql.catalyst.plans.logical.{CallStatement, LogicalPlan}\n+import org.apache.spark.sql.catalyst.CatalystTypeConverters\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, LogicalPlan}\n import org.apache.spark.sql.execution.SparkPlan\n \n object ExtendedDataSourceV2Strategy extends Strategy {\n \n   override def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {\n-    case _: CallStatement =>\n-      throw new AnalysisException(\"CALL statements are not currently supported\")\n+    case c @ Call(procedure, args) =>\n+      CallExec(c.output, procedure.methodHandle, args.map(toScalaValue)) :: Nil", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1NDE1NQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520954155", "bodyText": "@rdblue, I think this is debatable. It does depend on the catalog API part of DS V2 so I've put it here. Any suggestions?", "author": "aokolnychyi", "createdAt": "2020-11-11T00:09:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDA2NjQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMTg2OA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520131868", "bodyText": "Does this need to be a struct?", "author": "rdblue", "createdAt": "2020-11-09T21:29:11Z", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import java.lang.invoke.MethodHandle;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {\n+  ProcedureParameter[] parameters();\n+  StructType outputType();", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzU5NQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953595", "bodyText": "Procedures are supposed to return Spark rows that will be reported as output. So it is the type of those rows.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:08:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDEzMTg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163076", "bodyText": "This is going to force people to use value literals, like 12L. Can we insert a cast and execute it if the type is compatible and the cast is safe? I'm thinking it would be fine for int -> long, but not for string -> int.", "author": "rdblue", "createdAt": "2020-11-09T22:31:48Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkxODM0Mw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520918343", "bodyText": "I think we can try to use Cast$canUpCast for this.", "author": "aokolnychyi", "createdAt": "2020-11-10T22:36:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjI5OA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952298", "bodyText": "I added a cast if it is safe.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:04:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjU4MQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952581", "bodyText": "All expressions will undergo optimizations so we will get rid of casts before creating an exec node.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:04:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzgxOQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520163819", "bodyText": "This is Presto's current behavior?", "author": "rdblue", "createdAt": "2020-11-09T22:33:33Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+object ResolveProcedures extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = SparkSession.active.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+\n+      validateParams(procedure)\n+      validateMethodHandle(procedure)\n+\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def validateMethodHandle(procedure: Procedure): Unit = {\n+    val params = procedure.parameters\n+    val outputType = procedure.outputType\n+\n+    val methodHandle = procedure.methodHandle\n+    val methodType = methodHandle.`type`\n+    val methodReturnType = methodType.returnType\n+\n+    // method cannot accept var ags\n+    if (methodHandle.isVarargsCollector) {\n+      throw new AnalysisException(\"Method must have fixed arity\")\n+    }\n+\n+    // verify the number of params in the procedure match the number of params in the method\n+    if (params.length != methodType.parameterCount) {\n+      throw new AnalysisException(\"Method parameter count must match the number of procedure parameters\")\n+    }\n+\n+    // the MethodHandle API does not allow us to check the generic type\n+    // so we only verify the return type is either void or iterable\n+\n+    if (outputType.nonEmpty && methodReturnType != classOf[java.lang.Iterable[_]]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines $outputType \" +\n+        \"as its output so must return java.lang.Iterable of Spark Rows\")\n+    }\n+\n+    if (outputType.isEmpty && methodReturnType != classOf[Void]) {\n+      throw new AnalysisException(\n+        s\"Wrong method return type: $methodReturnType; the procedure defines no output columns \" +\n+        \"so must be void\")\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+      if (paramType != argType) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+      argExprs(position) = arg.expr\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>\n+        val position = nameToPositionMap(p.name)\n+        argExprs(position) = Literal.create(null, p.dataType)\n+      case _ =>\n+    }\n+\n+    argExprs\n+  }\n+\n+  private def buildNameToArgMap(\n+      params: Seq[ProcedureParameter],\n+      args: Seq[CallArgument],\n+      nameToPositionMap: Map[String, Int]): Map[String, CallArgument] = {\n+\n+    val containsNamedArg = args.exists(_.isInstanceOf[NamedArgument])\n+    val containsPositionalArg = args.exists(_.isInstanceOf[PositionalArgument])\n+\n+    if (containsNamedArg && containsPositionalArg) {\n+      throw new AnalysisException(\"Named and positional arguments cannot be mixed\")", "originalCommit": "5d23e76c259f8d500347863e1b72938c794e03b4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTE5MA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520921190", "bodyText": "Yes, Presto does not allow mixing named and positional args.", "author": "aokolnychyi", "createdAt": "2020-11-10T22:42:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzgxOQ=="}], "type": "inlineReview"}, {"oid": "588a116401751f3d59ed46aae74a38aa54947367", "url": "https://github.com/apache/iceberg/commit/588a116401751f3d59ed46aae74a38aa54947367", "message": "Get rid of MethodHandle & minor fixes", "committedDate": "2020-11-11T00:02:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjExNg==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520952116", "bodyText": "We've discussed how handy this rule is multiple times. I'd remove it if there are no objections.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:03:30Z", "path": "project/scalastyle_config.xml", "diffHunk": "@@ -85,11 +85,6 @@\n             <parameter name=\"maxTypes\"><![CDATA[30]]></parameter>\n         </parameters>\n     </check>\n-    <check level=\"error\" class=\"org.scalastyle.scalariform.CyclomaticComplexityChecker\" enabled=\"true\">", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1NTAyMQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520955021", "bodyText": "I can move this into a separate PR and disable in Java too.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:12:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MjExNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk1MzI4Mw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520953283", "bodyText": "@rdblue, I do not pass a type alongside the row since the rules rely on parameters reported by the procedure to align input expressions.", "author": "aokolnychyi", "createdAt": "2020-11-11T00:07:03Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/CallExec.scala", "diffHunk": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.execution.datasources.v2\n+\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.Attribute\n+import org.apache.spark.sql.connector.catalog.Procedure\n+\n+case class CallExec(\n+    output: Seq[Attribute],\n+    procedure: Procedure,\n+    input: InternalRow) extends V2CommandExec {\n+\n+  override protected def run(): Seq[InternalRow] = {\n+    procedure.call(input)", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NjE4OQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520966189", "bodyText": "We know from validation above that if the map doesn't contain an argument, that it must be required. Not a big deal to have the additional check, though.", "author": "rdblue", "createdAt": "2020-11-11T00:46:06Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/analysis/ResolveProcedures.scala", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.analysis\n+\n+import org.apache.spark.sql.{AnalysisException, SparkSession}\n+import org.apache.spark.sql.catalyst.expressions.{Cast, Expression, Literal}\n+import org.apache.spark.sql.catalyst.plans.logical.{Call, CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.rules.Rule\n+import org.apache.spark.sql.connector.catalog.{CatalogManager, CatalogPlugin, LookupCatalog, Procedure, ProcedureCatalog, ProcedureParameter}\n+import scala.collection.Seq\n+\n+case class ResolveProcedures(spark: SparkSession) extends Rule[LogicalPlan] with LookupCatalog {\n+\n+  protected lazy val catalogManager: CatalogManager = spark.sessionState.catalogManager\n+\n+  override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators {\n+    case CallStatement(CatalogAndIdentifier(catalog, ident), args) =>\n+      val procedure = catalog.asProcedureCatalog.loadProcedure(ident)\n+      validateParams(procedure)\n+      Call(procedure, args = buildArgExprs(procedure, args))\n+  }\n+\n+  private def validateParams(procedure: Procedure): Unit = {\n+    // should not be any duplicate param names\n+    val duplicateParamNames = procedure.parameters.groupBy(_.name).collect {\n+      case (name, matchingParams) if matchingParams.length > 1 => name\n+    }\n+\n+    if (duplicateParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Duplicate parameter names: ${duplicateParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    // optional params should be at the end\n+    procedure.parameters.sliding(2).foreach {\n+      case Array(previousParam, currentParam) if previousParam.required && !currentParam.required =>\n+        throw new AnalysisException(\"Optional parameters must be after required ones\")\n+      case _ =>\n+    }\n+  }\n+\n+  private def buildArgExprs(procedure: Procedure, args: Seq[CallArgument]): Seq[Expression] = {\n+    val params = procedure.parameters\n+\n+    // build a map of declared parameter names to their positions\n+    val nameToPositionMap = params.map(_.name).zipWithIndex.toMap\n+\n+    // build a map of parameter names to args\n+    val nameToArgMap = buildNameToArgMap(params, args, nameToPositionMap)\n+\n+    // verify all required parameters are provided\n+    val missingParamNames = params.filter(_.required).collect {\n+      case param if !nameToArgMap.contains(param.name) => param.name\n+    }\n+\n+    if (missingParamNames.nonEmpty) {\n+      throw new AnalysisException(s\"Missing required parameters: ${missingParamNames.mkString(\"[\", \",\", \"]\")}\")\n+    }\n+\n+    val argExprs = new Array[Expression](params.size)\n+\n+    nameToArgMap.foreach { case (name, arg) =>\n+      val position = nameToPositionMap(name)\n+      val param = params(position)\n+      val paramType = param.dataType\n+      val argType = arg.expr.dataType\n+\n+      if (paramType != argType && !Cast.canUpCast(argType, paramType)) {\n+        throw new AnalysisException(s\"Wrong arg type for ${param.name}: expected $paramType but got $argType\")\n+      }\n+\n+      if (paramType != argType) {\n+        argExprs(position) = Cast(arg.expr, paramType)\n+      } else {\n+        argExprs(position) = arg.expr\n+      }\n+    }\n+\n+    // assign nulls to optional params that were not set\n+    params.foreach {\n+      case p if !p.required && !nameToArgMap.contains(p.name) =>", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzE5Nw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520967197", "bodyText": "I think this needs to be a lazy val, or else the attributes will have different IDs every time it is called.", "author": "rdblue", "createdAt": "2020-11-11T00:47:50Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override def output: Seq[Attribute] = procedure.outputType.toAttributes", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ4MTY5MA==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521481690", "bodyText": "Fixed.", "author": "aokolnychyi", "createdAt": "2020-11-11T16:31:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzE5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2ODI1Nw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520968257", "bodyText": "I think for all of the connector interfaces, we should add clear Javadoc.", "author": "rdblue", "createdAt": "2020-11-11T00:49:19Z", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/ProcedureParameter.java", "diffHunk": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.types.DataType;\n+\n+public interface ProcedureParameter {", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ4MTE1Ng==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521481156", "bodyText": "I'll add Javadoc in a follow-up PR. I need to document other places like IcebergSparkSessionExtensions. I want to have at least one procedure to be merged using these APIs.", "author": "aokolnychyi", "createdAt": "2020-11-11T16:30:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2ODI1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2OTg3NQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r520969875", "bodyText": "One more thing: we should also add a describe method so that we can show these correctly in plans.", "author": "rdblue", "createdAt": "2020-11-11T00:53:11Z", "path": "spark3/src/main/java/org/apache/spark/sql/connector/catalog/Procedure.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.connector.catalog;\n+\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.types.StructType;\n+\n+public interface Procedure {", "originalCommit": "588a116401751f3d59ed46aae74a38aa54947367", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ3OTc5Mw==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521479793", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-11T16:29:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2OTg3NQ=="}], "type": "inlineReview"}, {"oid": "b6ade274936b6fb810d3999a8c95efb83cba6520", "url": "https://github.com/apache/iceberg/commit/b6ade274936b6fb810d3999a8c95efb83cba6520", "message": "Lazy val & description", "committedDate": "2020-11-11T16:25:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTQ3OTQxNQ==", "url": "https://github.com/apache/iceberg/pull/1743#discussion_r521479415", "bodyText": "@rdblue, I implemented simpleString in Call and CallExec that use description in Procedure.", "author": "aokolnychyi", "createdAt": "2020-11-11T16:28:31Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/Call.scala", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.{Attribute, Expression}\n+import org.apache.spark.sql.catalyst.util.truncatedString\n+import org.apache.spark.sql.connector.catalog.Procedure\n+import scala.collection.Seq\n+\n+case class Call(procedure: Procedure, args: Seq[Expression]) extends Command {\n+  override lazy val output: Seq[Attribute] = procedure.outputType.toAttributes\n+\n+  override def simpleString(maxFields: Int): String = {", "originalCommit": "b6ade274936b6fb810d3999a8c95efb83cba6520", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}