{"pr_number": 1517, "pr_title": "Flink: apply row-level delete when reading", "pr_createdAt": "2020-09-27T13:39:09Z", "pr_url": "https://github.com/apache/iceberg/pull/1517", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495577348", "bodyText": "@JingsongLi , This is used to fix the UT.  How can we copy the RowData?", "author": "chenjunjiedada", "createdAt": "2020-09-27T14:12:46Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -92,12 +102,12 @@\n     return iter;\n   }\n \n-  private CloseableIterable<RowData> newAvroIterable(FileScanTask task, Map<Integer, ?> idToConstant) {\n+  private CloseableIterable<RowData> newAvroIterable(FileScanTask task, Schema schema, Map<Integer, ?> idToConstant) {\n     Avro.ReadBuilder builder = Avro.read(getInputFile(task))\n-        .reuseContainers()\n-        .project(projectedSchema)\n+        .reuseContainers(false)", "originalCommit": "bb46c7f7bd8ba27af2094635a6ca4714df8d7418", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTYyMDg1MA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495620850", "bodyText": "@chenjunjiedada can you please clarify for me what the UT stands for?", "author": "kbendick", "createdAt": "2020-09-27T21:55:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY3NTcyNQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495675725", "bodyText": "I mean the unit tests in TestFlinkScan.  The getRows puts each row from inputformat.nexRecord(null) into List while the row is reused when the file format is Avro, so the result in List is wrong.\nI didn't find a simple way to copy the GenericRowData, so I set reuseContainer to false to align with Parquet and ORC cases.  I changed it back in 8526b6d since I found the converter could be used to copy the row. But there comes new concern about the double copies for Parquet and ORC. @openinx @JingsongLi @rdblue ,  Should we reuse the container for Flink read?", "author": "chenjunjiedada", "createdAt": "2020-09-28T03:50:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTY5ODkyNQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495698925", "bodyText": "The inputformat.nexRecord returns reused record, it is OK in Flink.", "author": "JingsongLi", "createdAt": "2020-09-28T05:39:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcwMTQ5NQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495701495", "bodyText": "I think we can create a PR to reuse Parquet container for Flink and Spark.", "author": "JingsongLi", "createdAt": "2020-09-28T05:48:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcxODY3MQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495718671", "bodyText": "@JingsongLi , Thanks for your comments. I think it would be better to use an option to set reuse, let me create one.", "author": "chenjunjiedada", "createdAt": "2020-09-28T06:41:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcxOTgzNA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495719834", "bodyText": "I am slight -1 for option, If there are no side effects, why do we need to provide this option? (testing is not a good example, we should consider user-face interface)", "author": "JingsongLi", "createdAt": "2020-09-28T06:44:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTcyNDY0Mg==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495724642", "bodyText": "And if this reuse flap is false, I think there may also be some risks.\nNote in Flink and Spark reader, we are reusing binary for StringReader.\nMaybe these binaries are chunk buffers that have been reused by parquet reader (CC: @rdblue), so even if reuse flag is false, users cannot assume returning row's security.", "author": "JingsongLi", "createdAt": "2020-09-28T06:55:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTczODI2MQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r495738261", "bodyText": "@JingsongLi, I created #1522. We could also discuss there.", "author": "chenjunjiedada", "createdAt": "2020-09-28T07:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTU3NzM0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEzMzU3Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r496133573", "bodyText": "Why rename this variable?", "author": "rdblue", "createdAt": "2020-09-28T17:56:58Z", "path": "data/src/test/java/org/apache/iceberg/data/DeletesReadTest.java", "diffHunk": "@@ -240,17 +252,11 @@ public void testEqualityDeleteByNull() throws IOException {\n     Assert.assertEquals(\"Table should contain expected rows\", expected, actual);\n   }\n \n-  private static StructLikeSet rowSet(Table table) throws IOException {\n-    return rowSet(table, \"*\");\n+  private StructLikeSet rowSet(Table tbl) throws IOException {\n+    return rowSet(tbl, \"*\");", "originalCommit": "8526b6deedb747200fb21507f5716cf5e8396470", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjQ3NzUwMw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r496477503", "bodyText": "The checkstyle reports hidden variable issue because it has the same name between the parameter and class member. I updated the member table to testTable in the PR.  will rebase this accordingly.", "author": "chenjunjiedada", "createdAt": "2020-09-29T07:30:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEzMzU3Mw=="}], "type": "inlineReview"}, {"oid": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "url": "https://github.com/apache/iceberg/commit/dc2b9dbb61bad6eb4bb366817108ea63375d4803", "message": "Flink: apply row-level delete when reading\n\nConflicts:\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\tflink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "committedDate": "2020-10-10T08:00:20Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMzc5MQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502833791", "bodyText": "distinct compares files using equals, which is not overridden for data or delete files. This should instead use the approach that Spark uses:\n    Map<String, ByteBuffer> keyMetadata = Maps.newHashMap();\n    task.files().stream()\n        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n        .forEach(file -> keyMetadata.put(file.path().toString(), file.keyMetadata()));\n    Stream<EncryptedInputFile> encrypted = keyMetadata.entrySet().stream()\n        .map(entry -> EncryptedFiles.encryptedInput(io.newInputFile(entry.getKey()), entry.getValue()));\n\n    // decrypt with the batch call to avoid multiple RPCs to a key server, if possible\n    Iterable<InputFile> decryptedFiles = encryptionManager.decrypt(encrypted::iterator);", "author": "rdblue", "createdAt": "2020-10-10T21:24:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -50,23 +54,36 @@\n abstract class DataIterator<T> implements CloseableIterator<T> {\n \n   private Iterator<FileScanTask> tasks;\n-  private final FileIO io;\n-  private final EncryptionManager encryption;\n+  private final Map<String, InputFile> inputFiles;\n \n   private CloseableIterator<T> currentIterator;\n \n   DataIterator(CombinedScanTask task, FileIO io, EncryptionManager encryption) {\n     this.tasks = task.files().iterator();\n-    this.io = io;\n-    this.encryption = encryption;\n+\n+    Stream<EncryptedInputFile> encrypted = task.files().stream()\n+        .flatMap(fileScanTask -> Stream.concat(Stream.of(fileScanTask.file()), fileScanTask.deletes().stream()))\n+        .distinct()", "originalCommit": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI5MDU3OA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503290578", "bodyText": "I see, updated.", "author": "chenjunjiedada", "createdAt": "2020-10-12T13:19:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzMzc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNDM1Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502834353", "bodyText": "If this used a Hive table instead, then it wouldn't be necessary to keep state that isn't passed as method arguments. I think that would be less brittle.", "author": "rdblue", "createdAt": "2020-10-10T21:30:51Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private final Configuration conf = new Configuration();\n+  private final HadoopTables tables = new HadoopTables(conf);\n+  private final FileFormat format;\n+\n+  private String tableLocation;", "originalCommit": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI5MDc3Nw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503290777", "bodyText": "Make sense to me, updated.", "author": "chenjunjiedada", "createdAt": "2020-10-12T13:19:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNDM1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTQwOA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502835408", "bodyText": "Why half-configure the builder above and then finish it here? I think it would be simpler to use this:\nSchema projected = testTable.schema().select(columns);\nRowType rowType = FlinkSchemaUtil.convert(projected);\nFlinkInputFormat inputFormat = FlinkSource.forRowData()\n    .tableLoader(TableLoader.fromHadoopTable(tableLocation))\n    .project(FlinkSchemaUtil.toSchema(rowType))\n    .buildFormat();", "author": "rdblue", "createdAt": "2020-10-10T21:43:01Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.Assert;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private final Configuration conf = new Configuration();\n+  private final HadoopTables tables = new HadoopTables(conf);\n+  private final FileFormat format;\n+\n+  private String tableLocation;\n+\n+  @Parameterized.Parameters(name = \"fileFormat={0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { FileFormat.PARQUET },\n+        new Object[] { FileFormat.AVRO },\n+        new Object[] { FileFormat.ORC }\n+    };\n+  }\n+\n+  public TestFlinkInputFormatReaderDeletes(FileFormat inputFormat) {\n+    this.format = inputFormat;\n+  }\n+\n+  @Override\n+  protected Table createTable(String name, Schema schema, PartitionSpec spec) throws IOException {\n+    File location = temp.newFolder(format.name(), name);\n+    Assert.assertTrue(location.delete());\n+    this.tableLocation = location.toURI().toString();\n+\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+\n+    Table table = tables.create(schema, spec, props, tableLocation);\n+    TableOperations ops = ((BaseTable) table).operations();\n+    TableMetadata meta = ops.current();\n+    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+\n+    return table;\n+  }\n+\n+  @Override\n+  protected void dropTable(String name) {\n+    tables.dropTable(tableLocation, true);\n+  }\n+\n+  @Override\n+  protected StructLikeSet rowSet(String name, Table testTable, String... columns) throws IOException {\n+    FlinkSource.Builder builder = FlinkSource.forRowData().tableLoader(TableLoader.fromHadoopTable(tableLocation));\n+    Schema projected = testTable.schema().select(columns);\n+    RowType rowType = FlinkSchemaUtil.convert(projected);\n+    FlinkInputFormat inputFormat = builder.project(FlinkSchemaUtil.toSchema(rowType)).buildFormat();", "originalCommit": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI5MDkzMg==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503290932", "bodyText": "Updated.", "author": "chenjunjiedada", "createdAt": "2020-10-12T13:19:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTQwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTY4Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r502835683", "bodyText": "I think it is a bad practice to make helper methods in one test suite public and use them in another suite. Instead, helper methods should be moved to an appropriate test utility class. That way, we don't have test utility code that is hard to find because it lives in whatever test was written first.", "author": "rdblue", "createdAt": "2020-10-10T21:46:03Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -104,6 +105,22 @@ public void testNestedProjection() throws Exception {\n   }\n \n   private List<Row> runFormat(FlinkInputFormat inputFormat) throws IOException {\n+    return getRows(inputFormat);\n+  }\n+\n+  public static List<RowData> getRowData(FlinkInputFormat inputFormat) throws IOException {\n+    RowType rowType = FlinkSchemaUtil.convert(inputFormat.projectedSchema());\n+\n+    DataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+        TypeConversions.fromLogicalToDataType(rowType));\n+\n+    return getRows(inputFormat).stream()\n+        .map(converter::toInternal)\n+        .map(RowData.class::cast)\n+        .collect(Collectors.toList());\n+  }\n+\n+  public static List<Row> getRows(FlinkInputFormat inputFormat) throws IOException {", "originalCommit": "dc2b9dbb61bad6eb4bb366817108ea63375d4803", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzI5MTM2Nw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503291367", "bodyText": "OK, I added TestHelpers to contain getRows and getRowData.", "author": "chenjunjiedada", "createdAt": "2020-10-12T13:20:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjgzNTY4Mw=="}], "type": "inlineReview"}, {"oid": "5656be83274c4e64b754e27cddac2957380e8039", "url": "https://github.com/apache/iceberg/commit/5656be83274c4e64b754e27cddac2957380e8039", "message": "use hive catalog to create table", "committedDate": "2020-10-11T13:47:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MzQ3Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503063473", "bodyText": "@JingsongLi , is this the right way to create the CatalogLoader?", "author": "chenjunjiedada", "createdAt": "2020-10-12T06:26:48Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormatReaderDeletes.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.DeleteReadTests;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataWrapper;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.flink.TestHelpers;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public class TestFlinkInputFormatReaderDeletes extends DeleteReadTests {\n+  private static HiveConf hiveConf = null;\n+  private static HiveCatalog catalog = null;\n+  private static TestHiveMetastore metastore = null;\n+\n+  private final FileFormat format;\n+\n+  @Parameterized.Parameters(name = \"fileFormat={0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { FileFormat.PARQUET },\n+        new Object[] { FileFormat.AVRO },\n+        new Object[] { FileFormat.ORC }\n+    };\n+  }\n+\n+  public TestFlinkInputFormatReaderDeletes(FileFormat inputFormat) {\n+    this.format = inputFormat;\n+  }\n+\n+  @BeforeClass\n+  public static void startMetastore() {\n+    TestFlinkInputFormatReaderDeletes.metastore = new TestHiveMetastore();\n+    metastore.start();\n+    TestFlinkInputFormatReaderDeletes.hiveConf = metastore.hiveConf();\n+    TestFlinkInputFormatReaderDeletes.catalog = new HiveCatalog(hiveConf);\n+  }\n+\n+  @AfterClass\n+  public static void stopMetastore() {\n+    metastore.stop();\n+    catalog.close();\n+    TestFlinkInputFormatReaderDeletes.catalog = null;\n+  }\n+\n+  @Override\n+  protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n+    Map<String, String> props = Maps.newHashMap();\n+    props.put(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", name), schema, spec, props);\n+    TableOperations ops = ((BaseTable) table).operations();\n+    TableMetadata meta = ops.current();\n+    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+\n+    return table;\n+  }\n+\n+  @Override\n+  protected void dropTable(String name) {\n+    catalog.dropTable(TableIdentifier.of(\"default\", name));\n+  }\n+\n+  @Override\n+  protected StructLikeSet rowSet(String name, Table testTable, String... columns) throws IOException {\n+    Schema projected = testTable.schema().select(columns);\n+    RowType rowType = FlinkSchemaUtil.convert(projected);\n+    CatalogLoader hiveCatalogLoader = CatalogLoader.hive(catalog.name(),", "originalCommit": "5656be83274c4e64b754e27cddac2957380e8039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA3MjU3MQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503072571", "bodyText": "Yes, but it is under refactor. #1565", "author": "JingsongLi", "createdAt": "2020-10-12T06:50:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MzQ3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzE4MDUyMw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503180523", "bodyText": "Thanks for the information!", "author": "chenjunjiedada", "createdAt": "2020-10-12T09:57:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzA2MzQ3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0NjUxNQ==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503446515", "bodyText": "This seems strange to me. Why convert rows to external and convert them back to internal in the getRowData method? Why not move this implementation into getRowData and convert to external in this one?\nAlso, is there a better name for these methods? What about readRows or scan? Those would be a bit more clear about what is going on in these. The original was called runFormat, which is also a good name.", "author": "rdblue", "createdAt": "2020-10-12T17:50:24Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java", "diffHunk": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.flink.source.FlinkInputFormat;\n+import org.apache.iceberg.flink.source.FlinkInputSplit;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class TestHelpers {\n+\n+  private TestHelpers() {\n+  }\n+\n+  public static List<Row> getRows(FlinkInputFormat inputFormat, RowType rowType) throws IOException {\n+    FlinkInputSplit[] splits = inputFormat.createInputSplits(0);\n+    List<Row> results = Lists.newArrayList();\n+\n+    DataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(\n+        TypeConversions.fromLogicalToDataType(rowType));\n+\n+    for (FlinkInputSplit s : splits) {\n+      inputFormat.open(s);\n+      while (!inputFormat.reachedEnd()) {\n+        RowData row = inputFormat.nextRecord(null);\n+        results.add((Row) converter.toExternal(row));", "originalCommit": "5656be83274c4e64b754e27cddac2957380e8039", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcwMDU3Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503700573", "bodyText": "inputFormat.nextRecord() returns the record which will be reused, so it needs to copy the returned RowData otherwise the element in the result list are same. Currently, there's no explicitly API in Flink to copy RowData. Only the serializer RowDataSerializer is a class in Flink for copying RowData while it is an internal class. DataStructureConverter.toExternal and toInteranl can construct the record according to RowData and Row.  @JingsongLi @openinx Do you have any suggestion on this?", "author": "chenjunjiedada", "createdAt": "2020-10-13T06:40:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0NjUxNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcxMjk3OA==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503712978", "bodyText": "I think you can get serializer from FlinkSource.Builder.build().getType().createSerializer()", "author": "JingsongLi", "createdAt": "2020-10-13T07:05:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ0NjUxNQ=="}], "type": "inlineReview"}, {"oid": "ab3cad9d5496cbe21d3843aa19ba32a5e5a20527", "url": "https://github.com/apache/iceberg/commit/ab3cad9d5496cbe21d3843aa19ba32a5e5a20527", "message": "Flink: apply row-level delete when reading\n\nConflicts:\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java\n\tflink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java\n\tflink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "committedDate": "2020-10-13T06:53:55Z", "type": "commit"}, {"oid": "3115544e11e808756a06b39b2ba1479d00143698", "url": "https://github.com/apache/iceberg/commit/3115544e11e808756a06b39b2ba1479d00143698", "message": "use hive catalog to create table", "committedDate": "2020-10-13T06:53:55Z", "type": "commit"}, {"oid": "75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "url": "https://github.com/apache/iceberg/commit/75dce7f98c0dc26fd6a27cf931130e7b69c8dceb", "message": "rename the helper function and merge test helpers", "committedDate": "2020-10-13T06:53:56Z", "type": "forcePushed"}, {"oid": "265524fb0c1eba181a2c3376a94600358b5e0bba", "url": "https://github.com/apache/iceberg/commit/265524fb0c1eba181a2c3376a94600358b5e0bba", "message": "rename the helper function and merge test helpers", "committedDate": "2020-10-13T07:07:28Z", "type": "commit"}, {"oid": "265524fb0c1eba181a2c3376a94600358b5e0bba", "url": "https://github.com/apache/iceberg/commit/265524fb0c1eba181a2c3376a94600358b5e0bba", "message": "rename the helper function and merge test helpers", "committedDate": "2020-10-13T07:07:28Z", "type": "forcePushed"}, {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4", "url": "https://github.com/apache/iceberg/commit/3a61448aa453806183a7c9aed5d906974bdc0cf4", "message": "copy RowData instead of converting back and forth", "committedDate": "2020-10-13T09:36:19Z", "type": "commit"}, {"oid": "3a61448aa453806183a7c9aed5d906974bdc0cf4", "url": "https://github.com/apache/iceberg/commit/3a61448aa453806183a7c9aed5d906974bdc0cf4", "message": "copy RowData instead of converting back and forth", "committedDate": "2020-10-13T09:36:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzgxMDY5Mw==", "url": "https://github.com/apache/iceberg/pull/1517#discussion_r503810693", "bodyText": "@JingsongLi , I can not use RowDataSerializer directly since the returned RowData may contain metadata column after merging with position deletes. So I created this function to do the copy job.", "author": "chenjunjiedada", "createdAt": "2020-10-13T09:38:15Z", "path": "flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java", "diffHunk": "@@ -49,6 +61,49 @@\n   private TestHelpers() {\n   }\n \n+  public static RowData copyRowData(RowData from, RowType rowType) {", "originalCommit": "3a61448aa453806183a7c9aed5d906974bdc0cf4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "9fb7c941951c06c498e3585b7784c81447240ea7", "url": "https://github.com/apache/iceberg/commit/9fb7c941951c06c498e3585b7784c81447240ea7", "message": "fix failed unit tests", "committedDate": "2020-10-14T02:32:15Z", "type": "commit"}]}