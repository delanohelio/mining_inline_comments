{"pr_number": 937, "pr_title": "Spark: Implement an adapter to wrap Row into DataFile", "pr_createdAt": "2020-04-18T02:14:49Z", "pr_url": "https://github.com/apache/iceberg/pull/937", "timeline": [{"oid": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "url": "https://github.com/apache/iceberg/commit/31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "message": "Spark: Implement an adapter to wrap Row into DataFile", "committedDate": "2020-04-18T02:11:02Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r410559603", "bodyText": "As much as I don't like to use the index, referring by column name seems to incur an additional penalty as we will resolve the column name every time (based on what I see in Spark Row).", "author": "aokolnychyi", "createdAt": "2020-04-18T02:18:01Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final Type keyMetadataType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.keyMetadataType = type.fieldType(\"key_metadata\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;", "originalCommit": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NjU2Mg==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411596562", "bodyText": "If we assume that the type passed in here matches the Row, then we should be able to derive the ordinals from that type instead of hard-coding them. I think this would produce an int[] of expected offset to actual offset, like we use in the Avro generics. What do you think?", "author": "rdblue", "createdAt": "2020-04-20T18:26:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NjkyNQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411596925", "bodyText": "Otherwise, I think it would probably be worth the performance penalty to ensure correctness.", "author": "rdblue", "createdAt": "2020-04-20T18:27:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NDA1NQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411644055", "bodyText": "@rdblue, do you mean passing the Spark type alongside the Iceberg type and building a mapping in the constructor?", "author": "aokolnychyi", "createdAt": "2020-04-20T19:45:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NzEzOQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411647139", "bodyText": "Ideally, yes. But if that's not possible we know that the DataFile that will be passed in comes from a metadata table (because we control the implementations that use this) so it would be okay to assume that we know what the Spark schema is.", "author": "rdblue", "createdAt": "2020-04-20T19:50:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3Mzk4MA==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411773980", "bodyText": "Okay, I've updated this to handle arbitrary projections.", "author": "aokolnychyi", "createdAt": "2020-04-21T00:04:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU1OTYwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU2MDU5NQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r410560595", "bodyText": "I am using getAs instead of getLong as getLong and other methods specific to primitives perform a check if the value is null. The downside might be boxing/unboxing.", "author": "aokolnychyi", "createdAt": "2020-04-18T02:20:13Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,134 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final Type keyMetadataType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.keyMetadataType = type.fieldType(\"key_metadata\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;\n+  }\n+\n+  public SparkDataFile wrap(Row row) {\n+    this.wrapped = row;\n+    if (wrappedPartition.size() > 0) {\n+      this.wrappedPartition.wrap(row.getAs(2));\n+    }\n+    return this;\n+  }\n+\n+  @Override\n+  public CharSequence path() {\n+    return wrapped.getAs(0);\n+  }\n+\n+  @Override\n+  public FileFormat format() {\n+    String formatAsString = wrapped.getString(1).toUpperCase(Locale.ROOT);\n+    return FileFormat.valueOf(formatAsString);\n+  }\n+\n+  @Override\n+  public StructLike partition() {\n+    return wrappedPartition;\n+  }\n+\n+  @Override\n+  public long recordCount() {\n+    return wrapped.getAs(fieldShift + 2);\n+  }\n+\n+  @Override\n+  public long fileSizeInBytes() {\n+    return wrapped.getAs(fieldShift + 3);", "originalCommit": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTA1MzQ5NQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411053495", "bodyText": "Actually, Spark row uses Array[Any] which I assume allocates an array of Objects. So, Spark stores boxed versions and we have to unbox it anyway. If we used getLong, we would also perform a null check.", "author": "aokolnychyi", "createdAt": "2020-04-20T02:27:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMDU2MDU5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NDI5Ng==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411594296", "bodyText": "Why is this necessary? It looks like all of the code here is added to Spark.", "author": "rdblue", "createdAt": "2020-04-20T18:23:22Z", "path": "build.gradle", "diffHunk": "@@ -307,6 +307,7 @@ project(':iceberg-spark') {\n     compile project(':iceberg-api')\n     compile project(':iceberg-common')\n     compile project(':iceberg-core')\n+    compile project(':iceberg-data')", "originalCommit": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTYzOTgzOQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411639839", "bodyText": "SparkValueConverter uses the following classes:\nimport org.apache.iceberg.data.GenericRecord;\nimport org.apache.iceberg.data.Record;", "author": "aokolnychyi", "createdAt": "2020-04-20T19:38:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NDI5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NzU0Ng==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411597546", "bodyText": "It would be nice to have some Javadoc about this, like that values are converted to Iceberg's internal representation.", "author": "rdblue", "createdAt": "2020-04-20T18:28:30Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkValueConverter.java", "diffHunk": "@@ -0,0 +1,116 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.nio.ByteBuffer;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils;\n+\n+public class SparkValueConverter {", "originalCommit": "31e7d9a98f9c32ecbdc827040767a4a9c4722cb0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTY0NDIxOQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411644219", "bodyText": "Yep, will add.", "author": "aokolnychyi", "createdAt": "2020-04-20T19:45:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NzU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTc3MzQwNQ==", "url": "https://github.com/apache/iceberg/pull/937#discussion_r411773405", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-04-21T00:02:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMTU5NzU0Ng=="}], "type": "inlineReview"}, {"oid": "2a1c840e546d41613d18b8fe07dd5adae78b001c", "url": "https://github.com/apache/iceberg/commit/2a1c840e546d41613d18b8fe07dd5adae78b001c", "message": "Switch to position array", "committedDate": "2020-04-20T23:56:41Z", "type": "commit"}, {"oid": "668c83fc9a1fd7d40c799f5381a3c7ac46c14dc5", "url": "https://github.com/apache/iceberg/commit/668c83fc9a1fd7d40c799f5381a3c7ac46c14dc5", "message": "Fix javadoc", "committedDate": "2020-04-21T00:02:20Z", "type": "commit"}]}