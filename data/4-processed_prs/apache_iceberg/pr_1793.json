{"pr_number": 1793, "pr_title": "Flink: Support streaming reader.", "pr_createdAt": "2020-11-20T13:34:25Z", "pr_url": "https://github.com/apache/iceberg/pull/1793", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgxNzE2Ng==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529817166", "bodyText": "I think returning a different builder breaks the builder pattern. Why not call these methods on this instead?", "author": "rdblue", "createdAt": "2020-11-24T19:13:07Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -98,115 +112,203 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n     this.projectedSchema = projectedSchema;\n     this.filterExpressions = filterExpressions;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions);\n+  boolean isStreaming() {\n+    return isStreaming;\n   }\n \n-  String nameMapping() {\n-    return nameMapping;\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions);\n+  String nameMapping() {\n+    return nameMapping;\n   }\n \n   Schema projectedSchema() {\n     return projectedSchema;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions);\n-  }\n-\n   List<Expression> filterExpressions() {\n     return filterExpressions;\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters);\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  static class Builder {\n+    private boolean caseSensitive;\n+    private Long snapshotId;\n+    private Long startSnapshotId;\n+    private Long endSnapshotId;\n+    private Long asOfTimestamp;\n+    private Long splitSize;\n+    private Integer splitLookback;\n+    private Long splitOpenFileCost;\n+    private boolean isStreaming;\n+    private Duration monitorInterval;\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filterExpressions;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndsnapshotId) {\n+      this.endSnapshotId = newEndsnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    Builder streaming(boolean streaming) {\n+      this.isStreaming = streaming;\n+      return this;\n+    }\n+\n+    Builder monitorInterval(Duration newMonitorInterval) {\n+      this.monitorInterval = newMonitorInterval;\n+      return this;\n+    }\n+\n+    Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    Builder projectedSchema(Schema newProjectedSchema) {\n+      this.projectedSchema = newProjectedSchema;\n+      return this;\n+    }\n+\n+    Builder filterExpression(List<Expression> newFilterExpressions) {\n+      this.filterExpressions = newFilterExpressions;\n+      return this;\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return new Builder()", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg4NjcyNQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r535886725", "bodyText": "Yeah, you are right !  If use a new builder, then it should be copying the builder , maybe named it as mergeProperties, here we should use this .", "author": "openinx", "createdAt": "2020-12-04T07:21:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgxNzE2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzMjQyNA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529932424", "bodyText": "Shouldn't a builder supply the defaults for these? Otherwise, all of the values need to be supplied. Why not use CASE_SENSITIVE.defaultValue() and similar here?", "author": "rdblue", "createdAt": "2020-11-24T22:19:26Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -98,115 +112,203 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n     this.projectedSchema = projectedSchema;\n     this.filterExpressions = filterExpressions;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions);\n+  boolean isStreaming() {\n+    return isStreaming;\n   }\n \n-  String nameMapping() {\n-    return nameMapping;\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions);\n+  String nameMapping() {\n+    return nameMapping;\n   }\n \n   Schema projectedSchema() {\n     return projectedSchema;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions);\n-  }\n-\n   List<Expression> filterExpressions() {\n     return filterExpressions;\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters);\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .nameMapping(nameMapping)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .build();\n+  }\n+\n+  static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  static class Builder {\n+    private boolean caseSensitive;\n+    private Long snapshotId;\n+    private Long startSnapshotId;\n+    private Long endSnapshotId;\n+    private Long asOfTimestamp;\n+    private Long splitSize;\n+    private Integer splitLookback;\n+    private Long splitOpenFileCost;\n+    private boolean isStreaming;\n+    private Duration monitorInterval;\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filterExpressions;", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzNzEwNA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529937104", "bodyText": "It would be good to include what _should _ be used to configure the stream. Looks like it should be startSnapshotId with no endSnapshotId.", "author": "rdblue", "createdAt": "2020-11-24T22:23:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTg5ODkxNg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r535898916", "bodyText": "If user did not provide a start-snapshot-id, then this function will use the snapshot id (it's a DUMMY_START_SNAPSHOT_ID in code) before current snapshot as the start-snapshot-id.   So we are allowed to pass null start-snapshot-id  but are not allowed to pass non-null end-snapshot-id.", "author": "openinx", "createdAt": "2020-12-04T07:47:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzNzEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxNjQ3Mg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552916472", "bodyText": "we should probably also check this\n    Preconditions.checkArgument(scanContext.asOfTimestamp() == null,\n        \"Can't set asOfTimestamp in ScanContext for continuous enumerator\");", "author": "stevenzwu", "createdAt": "2021-01-06T19:23:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzNzEwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzIyNTI1NA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553225254", "bodyText": "The current condition ctxt.snapshotId() == null && ctxt.asOfTimestamp() == null will require both the snapshotId and asOfTimestamp  to be null.  So we don't have to do the extra scanContext.asOfTimestamp() == null check now.  But I prefer to use the separate Preconditions.checkArgument for snapshotId and asOfTimestamp, so that people could distingush the cause quickly.", "author": "openinx", "createdAt": "2021-01-07T09:58:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTkzNzEwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0MDIzMw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529940233", "bodyText": "Should this use newOptions?", "author": "rdblue", "createdAt": "2020-11-24T22:26:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);\n+      }\n+      Thread.sleep(options.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newOptions = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          options.copyWithSnapshotId(snapshotId) :\n+          options.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, options);", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0NDU1Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529944557", "bodyText": "Okay, so it looks like state is always tracked at the snapshot granularity, and the checkpointLock is used to guarantee that a snapshot's data is fully emitted or not emitted at all. Is that right?", "author": "rdblue", "createdAt": "2020-11-24T22:30:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);\n+      }\n+      Thread.sleep(options.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newOptions = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          options.copyWithSnapshotId(snapshotId) :\n+          options.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, options);\n+      for (FlinkInputSplit split : splits) {\n+        ctx.collect(split);\n+      }\n+      startSnapshotId = snapshotId;\n+    }\n+  }\n+\n+  @Override\n+  public void cancel() {\n+    // this is to cover the case where cancel() is called before the run()\n+    if (checkpointLock != null) {\n+      synchronized (checkpointLock) {\n+        isRunning = false;\n+      }\n+    } else {\n+      isRunning = false;\n+    }\n+\n+    if (tableLoader != null) {\n+      try {\n+        tableLoader.close();\n+      } catch (IOException e) {\n+        throw new UncheckedIOException(e);\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void close() {\n+    cancel();\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    snapshotIdState.clear();\n+    snapshotIdState.add(startSnapshotId);", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0NjY1NA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529946654", "bodyText": "I think this also need to check isRunning. Otherwise, this thread could check isRunning and then stop before getting the lock. Then cancel could run in another thread. Once cancel succeeds, I'm assuming that it is okay for Flink to call snapshotState to produce state with the current snapshot. But then this thread might wake up and start producing splits and update startSnapshotId.", "author": "rdblue", "createdAt": "2020-11-24T22:33:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,172 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final List<Expression> filterExpressions;\n+  private final ScanContext options;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, Schema projectedSchema, List<Expression> filterExpressions,\n+                                  ScanContext options) {\n+    Preconditions.checkArgument(\n+        options.snapshotId() == null && options.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(\n+        options.endSnapshotId() == null, \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.filterExpressions = filterExpressions;\n+    this.options = options;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = options.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        monitorAndForwardSplits(ctx);", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk3MDE0OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r535970149", "bodyText": "Think about this again,  it's true that we need to have a isRunning checking inside the synchronized block.", "author": "openinx", "createdAt": "2020-12-04T09:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk0NjY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk1MTY5OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529951699", "bodyText": "Hm, didn't we decide not to set these because we don't actually want to guarantee Java serialization across Iceberg versions? I'm okay with this, but I think it makes it appear that we support Java serialization across versions when we probably will not.", "author": "rdblue", "createdAt": "2020-11-24T22:37:35Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2MzUyNQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529963525", "bodyText": "Just to be sure, collect is the right method to call to emit a record? That seems oddly named to me.", "author": "rdblue", "createdAt": "2020-11-24T22:48:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(\n+      FlinkInputFormat format, ProcessingTimeService timeService, MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(\n+        mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzI4NTY4OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553285689", "bodyText": "Yes, it's the right method to emit record.  Pls see the javadoc here: https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/functions/source/SourceFunction.java#L200", "author": "openinx", "createdAt": "2021-01-07T12:01:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2MzUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r529964197", "bodyText": "Why does this submit itself again?", "author": "rdblue", "createdAt": "2020-11-24T22:49:24Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,220 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have DOP > 1.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(\n+      FlinkInputFormat format, ProcessingTimeService timeService, MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(\n+        mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();\n+    } while (executor.isIdle());\n+    enqueueProcessSplits();", "originalCommit": "329c60af56936dc99c784c60a1b4f3511e06abd1", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzMwNDAwMA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553304000", "bodyText": "That's a good question.   This method processSplits will emit all records from one CombinedScanTask to downstream and then yield the current thread (run out of the while loop ) &  re-enqueue the processSplits in Mailbox (Which is similar to the producer-consumer model, the caller do the enqueue then the single thread executor to consume from it).\nWe will always yield from the executor thread after it has processed a split and then re-enqueue because we don't want to hold the checkpointLock too long  (step.1) so that flink runtime won't wait long-time to trigger a new checkpoint (step.2) .   Otherwise if we hold the checkpointLock  and then process the splits one by one in the same processSplits,  finally the checkpoint will timeout so many times.  In an extreme case,  the flink streaming job will never checkpoint successfully if the splits data is huge.\n(Notice, for step.1,  when processing splits in executor thread we will be granted the checkpointLock,  that means nobody can start a new checkpoint unless the executor finished the current split processing;  for step.2     anybody who want to trigger a new checkpoint will need to grab the checkpointLock firstly )", "author": "openinx", "createdAt": "2021-01-07T12:41:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ2MTQ4MQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555461481", "bodyText": "I agree that the checkpoint lock should not be held for more than one input split.\nThis still doesn't make much sense to me. When a new split is added, enqueueProcessSplits runs, so the number of times processSplits is called should match the number of input splits. If there is a concern about executors and splits not aligning, then the mailbox executor should receive a task like split -> processSplit(split) so that we can ensure the mailbox has exactly one task per split.\nEither way, the logic here doesn't seem to do what you're describing. Calling enqueueProcessSplits again probably submits a task that won't actually process any data because tasks are already created for each incoming split.\nI also don't see where the checkpoint lock is passed to the executor, so how is the executor holding the checkpoint lock?\nLast, why does this loop until executor.isIdle? Isn't this what is running in the mailbox loop? I interpret isIdle to be true when there is not another task for the executor to process. So if there isn't another call to processSplits, this one will keep running? But that contradicts the idea of one split processed at a time -- assuming I'm missing something about the lock not being passed to the mailbox executor. Alternatively, if there is another task for the executor, the loop will exit so it can run, but then enqueue another run. The end result seems like adding increasingly more tasks to the mailbox executor's queue because each task replaces itself and processElement keeps adding more.\nI think that the problem is that the mailbox executor holds its own state queue, but we want to keep unprocessed splits in the splits queue so they can be included in checkpoint state. This can't add a task for each split to the mailbox because the mailbox task queue isn't part of state. I think the solution is to have a single thread that runs a loop that polls the queue and handles the checkpoint lock. That should be simpler than the mailbox executor architecture, while still handling one split at a time in a single thread and holding the checkpoint lock.", "author": "rdblue", "createdAt": "2021-01-12T01:57:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc5NTc0OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555795749", "bodyText": "Thanks for the careful reviewing.  After checked the code carefully again,  I think your concern is reasonable because the current implementation did not handle the issue correctly.\nLet me re-describe the problem to be solved so far.  I think everyone could design the following code in straightforward when there's a new coming split:\n  @Override\n  public void processElement(StreamRecord<FlinkInputSplit> element) {\n    FlinkInputSplit split = element.getValue();\n\n    LOG.debug(\"Start to process the split: {}\", split);\n    format.open(split);\n\n    try {\n      RowData nextElement = null;\n      while (!format.reachedEnd()) {\n        nextElement = format.nextRecord(nextElement);\n        sourceContext.collect(nextElement);\n      }\n    } finally {\n      format.close();\n    }\n  }\nIt's quite simple and straightforward, but we can not do that.  Because when reading the current split1 in processElement,  there will be more splits coming from upstream operator,  such as split2, split3, split4.    Those pending splits will buffered in the flink's network queue,  it won't process the next split unless the previous one has been finished.  At this time,  if there's a checkpoint barrier coming ( Message requesting to create a new checkpoint snapshot),  then the network queue will be : split2, split3, split4, checkpoint-barrier.  The checkpoint snapshot could not start unless all the split2, split3, split4 has been processed in this processElement.   Finally,  the checkpoint snapshot will be blocked for long-time, in an extreme case, the flink streaming job will never checkpoint successfully if those splits data are huge.\nOK, above is the problem that we want to solve.\nThe idea way is:   the mail box executor will only enqueue one split and checkpoint barrier could be inserted at any time.  For example,  we are reading the records from split1. At the same time, there're more splits coming from upstream operator :  split2, split3, split4,  then the three splits will be buffered in the in-memory queue ( the variable Queue<FlinkInputSplit> splits in PR).   The current flink network queue is empty because we've just buffered those split2, split3, split4 in splits queue when processElement.    Now the checkpoint barrier comes,   the operator read it from network queue and put it in mailbox,  then finally we could trigger the checkpoint snapshot once finished reading  records from splits,  don't have to wait all split2, split3, split4 's records reading.", "author": "openinx", "createdAt": "2021-01-12T14:09:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTc5NzI2OA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555797268", "bodyText": "After reconsider the code design,   I think the current version is the correct one: https://github.com/apache/iceberg/pull/1793/files", "author": "openinx", "createdAt": "2021-01-12T14:12:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTgwNzYwNw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555807607", "bodyText": "I also don't see where the checkpoint lock is passed to the executor, so how is the executor holding the checkpoint lock?\n\nAbout the checkpoint lock question,  Flink 1.10.0 has changed the checkpoint-lock model to a mailbox-based model (https://issues.apache.org/jira/browse/FLINK-12477), means we don't have to grab the checkpoint lock when coming a checkpoint barrier because we only have one thread to consume from mailbox so we could just enqueue the checkpoint action in the mailbox and wait for being invoked.  (this #1793 (comment) is actually stale now).    The SourceStreamTask still retain the checkpoint lock because there're lots of legacy flink connector which are using this checkpoint lock way to synchronized between normal records and checkpoint barrier.", "author": "openinx", "createdAt": "2021-01-12T14:23:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2OTY1Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555969657", "bodyText": "I thought that MailboxExecutor ran a separate thread. If so, how is it automatically synchronized with the thread running processElement?", "author": "rdblue", "createdAt": "2021-01-12T18:01:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjI1MjgzMA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556252830", "bodyText": "Let's discuss in this comment .https://github.com/apache/iceberg/pull/1793/files#r556249432", "author": "openinx", "createdAt": "2021-01-13T04:18:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTk2NDE5Nw=="}], "type": "inlineReview"}, {"oid": "a63e4c6d0658c8cb02e13426375ba84cfe38eda2", "url": "https://github.com/apache/iceberg/commit/a63e4c6d0658c8cb02e13426375ba84cfe38eda2", "message": "Flink: Support streaming reader.", "committedDate": "2021-01-04T13:12:37Z", "type": "commit"}, {"oid": "3250db974998ec3b6411fb44dcfa99e7979bff5a", "url": "https://github.com/apache/iceberg/commit/3250db974998ec3b6411fb44dcfa99e7979bff5a", "message": "Minor fixes", "committedDate": "2021-01-04T13:13:32Z", "type": "commit"}, {"oid": "abf9e2b529781f2981973b3161d4ce939ce98665", "url": "https://github.com/apache/iceberg/commit/abf9e2b529781f2981973b3161d4ce939ce98665", "message": "Addressing comments.", "committedDate": "2021-01-04T13:15:48Z", "type": "commit"}, {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "url": "https://github.com/apache/iceberg/commit/5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "message": "Minor changes.", "committedDate": "2021-01-04T13:15:48Z", "type": "commit"}, {"oid": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "url": "https://github.com/apache/iceberg/commit/5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "message": "Minor changes.", "committedDate": "2021-01-04T13:15:48Z", "type": "forcePushed"}, {"oid": "506a35f3012c8f278988795b6685474c818532e2", "url": "https://github.com/apache/iceberg/commit/506a35f3012c8f278988795b6685474c818532e2", "message": "Address minor issues", "committedDate": "2021-01-04T13:35:42Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MTMyMjIxMg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r551322212", "bodyText": "I think it's good to make this unit tests to extend FlinkTestBase class.", "author": "openinx", "createdAt": "2021-01-04T13:40:17Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestStreamScanSql.java", "diffHunk": "@@ -0,0 +1,190 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.Iterator;\n+import java.util.List;\n+import org.apache.flink.streaming.api.environment.ExecutionCheckpointingOptions;\n+import org.apache.flink.table.api.EnvironmentSettings;\n+import org.apache.flink.table.api.TableEnvironment;\n+import org.apache.flink.table.api.TableResult;\n+import org.apache.flink.table.api.config.TableConfigOptions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.flink.util.CloseableIterator;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+public class TestStreamScanSql extends AbstractTestBase {", "originalCommit": "5c6a0b7ad3a8d4e88f049c7e51510edcba258eb7", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "243e3ba6c1c2829bfedf8e36b6744650d4c4d9f7", "url": "https://github.com/apache/iceberg/commit/243e3ba6c1c2829bfedf8e36b6744650d4c4d9f7", "message": "Fix the broken unit tests.", "committedDate": "2021-01-05T10:05:11Z", "type": "commit"}, {"oid": "1b757fd8fdd6997a4681652608bf14e1c949a3bb", "url": "https://github.com/apache/iceberg/commit/1b757fd8fdd6997a4681652608bf14e1c949a3bb", "message": "Fix broken unit tests.", "committedDate": "2021-01-05T10:54:45Z", "type": "commit"}, {"oid": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "url": "https://github.com/apache/iceberg/commit/6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "message": "Fix javadoc", "committedDate": "2021-01-05T11:44:41Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxMDg0NQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552910845", "bodyText": "I have been wondering if really need to duplicate every ScanContext/Builder API in the FlinkSource. Should we just let FlinkSource builder take the ScanContext, which is constructed separately before the FlinkSource?", "author": "stevenzwu", "createdAt": "2021-01-06T19:11:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -91,7 +89,7 @@ public Builder env(StreamExecutionEnvironment newEnv) {\n     }\n \n     public Builder filters(List<Expression> filters) {", "originalCommit": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzE3Njg1MQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553176851", "bodyText": "We've discussed this before in. #1346 (comment) and #1509 (comment).   The conclusion is :  exposing only one simple FlinkSource  API to end users , rather than expose two builders.     That make sense to me,  flink data stream developers will find it's easier to construct the streaming source, though we iceberg developers need to write those duplicated setters .", "author": "openinx", "createdAt": "2021-01-07T08:30:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxMDg0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxOTU3NA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552919574", "bodyText": "Should we introduce a start strategy config?\nhttps://github.com/stevenzwu/iceberg/blob/flip27IcebergSource/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousEnumConfig.java#L35\nI am also wondering if we should move the continuous monitoring config outside ScanContext?", "author": "stevenzwu", "createdAt": "2021-01-06T19:29:41Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,163 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext ctxt) {\n+    Preconditions.checkArgument(ctxt.snapshotId() == null && ctxt.asOfTimestamp() == null,\n+        \"The streaming reader does not support using snapshot-id or as-of-timestamp to select the table snapshot.\");\n+    Preconditions.checkArgument(ctxt.endSnapshotId() == null,\n+        \"The streaming reader does not support using end snapshot id.\");\n+    this.tableLoader = tableLoader;\n+    this.ctxt = ctxt;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = ctxt.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;", "originalCommit": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzIzMjY5MQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553232691", "bodyText": "The current read strategy support LATEST_SNAPSHOT and SPECIFIC_START_SNAPSHOT_ID.  I think it's great to introduce an extra read strategy config outside  ScanContext when we integrate flink source/sink with flip-27, for now we can let it go.", "author": "openinx", "createdAt": "2021-01-07T10:12:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkxOTU3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkyNDE5MA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552924190", "bodyText": "I opened issue-1698 a while back regarding a more stable serializer (than Java serializable) for Flink checkpointing. While Java serialization works well for batch jobs, we need a more stable serialization to support schema evolution for long-running stream jobs. We need sth like DeltaManifestsSerializer and ManifestFiles.encode for CombinedScanTask.\nIt doesn't have to be done in the initial version. So I don't think it is a blocker for this PR.", "author": "stevenzwu", "createdAt": "2021-01-06T19:39:26Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(", "originalCommit": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzIzMzQwMA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553233400", "bodyText": "Sorry , I missed this issue before.  Let me take a look .", "author": "openinx", "createdAt": "2021-01-07T10:13:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjkyNDE5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mjk3MDA3Ng==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r552970076", "bodyText": "should we use addAll instead, which translates to one merge call in rocksdb?", "author": "stevenzwu", "createdAt": "2021-01-06T21:29:27Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,216 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();\n+    } while (executor.isIdle());\n+    enqueueProcessSplits();\n+  }\n+\n+  @Override\n+  public void processWatermark(Watermark mark) {\n+    // we do nothing because we emit our own watermarks if needed.\n+  }\n+\n+  @Override\n+  public void dispose() throws Exception {\n+    super.dispose();\n+\n+    if (format != null) {\n+      format.close();\n+      format.closeInputFormat();\n+      format = null;\n+    }\n+\n+    readerContext = null;\n+  }\n+\n+  @Override\n+  public void close() throws Exception {\n+    super.close();\n+    output.close();\n+    if (readerContext != null) {\n+      readerContext.emitWatermark(Watermark.MAX_WATERMARK);\n+      readerContext.close();\n+      readerContext = null;\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+\n+    checkpointState.clear();\n+    for (FlinkInputSplit split : splits) {", "originalCommit": "6ea85e37fded0c67538f3c7e1f8c33a47e033eb5", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0afaf8be1878e6daebee4779392fab4ebfc5a379", "url": "https://github.com/apache/iceberg/commit/0afaf8be1878e6daebee4779392fab4ebfc5a379", "message": "Adress comments from stevenwu", "committedDate": "2021-01-07T10:22:38Z", "type": "commit"}, {"oid": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "url": "https://github.com/apache/iceberg/commit/ac6febbe89682726970075a25ccc4e5f1e6f3d16", "message": "Minor changes.", "committedDate": "2021-01-07T13:26:50Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTI1Ng==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699256", "bodyText": "Minor: I strongly prefer full words to removing vowels from them, so contextBuilder instead of ctxtBuilder.\nWhile the other is shorter, we aren't trying to save space in code. I also find it no easier to type ctxt than context and find it is distracting when reading the code because it isn't a word so I have to think harder about saying it.", "author": "rdblue", "createdAt": "2021-01-08T02:05:19Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -101,57 +99,62 @@ public Builder project(TableSchema schema) {\n     }\n \n     public Builder limit(long newLimit) {\n-      this.limit = newLimit;\n+      ctxtBuilder.limit(newLimit);\n       return this;\n     }\n \n     public Builder properties(Map<String, String> properties) {\n-      this.context = context.fromProperties(properties);\n+      ctxtBuilder.fromProperties(properties);\n       return this;\n     }\n \n     public Builder caseSensitive(boolean caseSensitive) {\n-      this.context = context.setCaseSensitive(caseSensitive);\n+      ctxtBuilder.caseSensitive(caseSensitive);\n       return this;\n     }\n \n     public Builder snapshotId(Long snapshotId) {\n-      this.context = context.useSnapshotId(snapshotId);\n+      ctxtBuilder.useSnapshotId(snapshotId);\n       return this;\n     }\n \n     public Builder startSnapshotId(Long startSnapshotId) {\n-      this.context = context.startSnapshotId(startSnapshotId);\n+      ctxtBuilder.startSnapshotId(startSnapshotId);\n       return this;\n     }\n \n     public Builder endSnapshotId(Long endSnapshotId) {\n-      this.context = context.endSnapshotId(endSnapshotId);\n+      ctxtBuilder.endSnapshotId(endSnapshotId);\n       return this;\n     }\n \n     public Builder asOfTimestamp(Long asOfTimestamp) {\n-      this.context = context.asOfTimestamp(asOfTimestamp);\n+      ctxtBuilder.asOfTimestamp(asOfTimestamp);\n       return this;\n     }\n \n     public Builder splitSize(Long splitSize) {\n-      this.context = context.splitSize(splitSize);\n+      ctxtBuilder.splitSize(splitSize);\n       return this;\n     }\n \n     public Builder splitLookback(Integer splitLookback) {\n-      this.context = context.splitLookback(splitLookback);\n+      ctxtBuilder.splitLookback(splitLookback);\n       return this;\n     }\n \n     public Builder splitOpenFileCost(Long splitOpenFileCost) {\n-      this.context = context.splitOpenFileCost(splitOpenFileCost);\n+      ctxtBuilder.splitOpenFileCost(splitOpenFileCost);\n+      return this;\n+    }\n+\n+    public Builder streaming(boolean streaming) {\n+      ctxtBuilder.streaming(streaming);\n       return this;\n     }\n \n     public Builder nameMapping(String nameMapping) {\n-      this.context = context.nameMapping(nameMapping);", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzczMzk3OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553733979", "bodyText": "Thanks for sharing the feeling from an English native speaker,  I didn't notice the slight difference in it before but I think it's worth to respect this feedback.", "author": "openinx", "createdAt": "2021-01-08T04:24:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTI1Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQzNTcxMg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555435712", "bodyText": "Thank you! It helps make everything more readable.", "author": "rdblue", "createdAt": "2021-01-12T00:49:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTI1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTg0Mw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699843", "bodyText": "If you're using -1 then why is this Long and not a primitive?", "author": "rdblue", "createdAt": "2021-01-08T02:07:35Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzc2Mjg1OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553762859", "bodyText": "OK, here can use long .", "author": "openinx", "createdAt": "2021-01-08T06:20:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTg0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTkyOA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553699928", "bodyText": "Is there a case where it is acceptable to pass null here?", "author": "rdblue", "createdAt": "2021-01-08T02:07:50Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzc2NDgwNQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553764805", "bodyText": "When we do the incremental read, it will pass a start-snapshot-id & end-snapshot-id with null snapshot-id.  So the answer is yes.   ( see here.)", "author": "openinx", "createdAt": "2021-01-08T06:27:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzY5OTkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDA0Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700047", "bodyText": "Same with these other methods. Should these be primitives?", "author": "rdblue", "createdAt": "2021-01-08T02:08:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzc2NTU0Mg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553765542", "bodyText": "Its default value is null.  So it should not use primitive here.   The limit is a special case because its default value is -1.", "author": "openinx", "createdAt": "2021-01-08T06:29:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDA0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDU1Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700557", "bodyText": "Can get accept a default value so we don't need to use boxed objects?", "author": "rdblue", "createdAt": "2021-01-08T02:10:08Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanContext.java", "diffHunk": "@@ -100,126 +96,224 @@ private ScanContext(boolean caseSensitive, Long snapshotId, Long startSnapshotId\n     this.splitSize = splitSize;\n     this.splitLookback = splitLookback;\n     this.splitOpenFileCost = splitOpenFileCost;\n+    this.isStreaming = isStreaming;\n+    this.monitorInterval = monitorInterval;\n+\n     this.nameMapping = nameMapping;\n-    this.projectedSchema = projectedSchema;\n-    this.filterExpressions = filterExpressions;\n+    this.schema = schema;\n+    this.filters = filters;\n     this.limit = limit;\n   }\n \n-  ScanContext fromProperties(Map<String, String> properties) {\n-    Configuration config = new Configuration();\n-    properties.forEach(config::setString);\n-    return new ScanContext(config.get(CASE_SENSITIVE), config.get(SNAPSHOT_ID), config.get(START_SNAPSHOT_ID),\n-        config.get(END_SNAPSHOT_ID), config.get(AS_OF_TIMESTAMP), config.get(SPLIT_SIZE), config.get(SPLIT_LOOKBACK),\n-        config.get(SPLIT_FILE_OPEN_COST), properties.get(DEFAULT_NAME_MAPPING), projectedSchema, filterExpressions,\n-        limit);\n-  }\n-\n   boolean caseSensitive() {\n     return caseSensitive;\n   }\n \n-  ScanContext setCaseSensitive(boolean isCaseSensitive) {\n-    return new ScanContext(isCaseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long snapshotId() {\n     return snapshotId;\n   }\n \n-  ScanContext useSnapshotId(Long scanSnapshotId) {\n-    return new ScanContext(caseSensitive, scanSnapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long startSnapshotId() {\n     return startSnapshotId;\n   }\n \n-  ScanContext startSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, id, endSnapshotId, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long endSnapshotId() {\n     return endSnapshotId;\n   }\n \n-  ScanContext endSnapshotId(Long id) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, id, asOfTimestamp, splitSize, splitLookback,\n-        splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long asOfTimestamp() {\n     return asOfTimestamp;\n   }\n \n-  ScanContext asOfTimestamp(Long timestamp) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, timestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitSize() {\n     return splitSize;\n   }\n \n-  ScanContext splitSize(Long size) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, size,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Integer splitLookback() {\n     return splitLookback;\n   }\n \n-  ScanContext splitLookback(Integer lookback) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        lookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, limit);\n-  }\n-\n   Long splitOpenFileCost() {\n     return splitOpenFileCost;\n   }\n \n-  ScanContext splitOpenFileCost(Long fileCost) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, fileCost, nameMapping, projectedSchema, filterExpressions, limit);\n+  boolean isStreaming() {\n+    return isStreaming;\n+  }\n+\n+  Duration monitorInterval() {\n+    return monitorInterval;\n   }\n \n   String nameMapping() {\n     return nameMapping;\n   }\n \n-  ScanContext nameMapping(String mapping) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, mapping, projectedSchema, filterExpressions, limit);\n+  Schema project() {\n+    return schema;\n   }\n \n-  Schema projectedSchema() {\n-    return projectedSchema;\n+  List<Expression> filters() {\n+    return filters;\n   }\n \n-  ScanContext project(Schema schema) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, schema, filterExpressions, limit);\n+  long limit() {\n+    return limit;\n   }\n \n-  List<Expression> filterExpressions() {\n-    return filterExpressions;\n+  ScanContext copyWithAppendsBetween(long newStartSnapshotId, long newEndSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(null)\n+        .startSnapshotId(newStartSnapshotId)\n+        .endSnapshotId(newEndSnapshotId)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  ScanContext filterRows(List<Expression> filters) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filters, limit);\n+  ScanContext copyWithSnapshotId(long newSnapshotId) {\n+    return ScanContext.builder()\n+        .caseSensitive(caseSensitive)\n+        .useSnapshotId(newSnapshotId)\n+        .startSnapshotId(null)\n+        .endSnapshotId(null)\n+        .asOfTimestamp(null)\n+        .splitSize(splitSize)\n+        .splitLookback(splitLookback)\n+        .splitOpenFileCost(splitOpenFileCost)\n+        .streaming(isStreaming)\n+        .monitorInterval(monitorInterval)\n+        .nameMapping(nameMapping)\n+        .project(schema)\n+        .filters(filters)\n+        .limit(limit)\n+        .build();\n   }\n \n-  long limit() {\n-    return limit;\n+  static Builder builder() {\n+    return new Builder();\n   }\n \n-  ScanContext limit(Long newLimit) {\n-    return new ScanContext(caseSensitive, snapshotId, startSnapshotId, endSnapshotId, asOfTimestamp, splitSize,\n-        splitLookback, splitOpenFileCost, nameMapping, projectedSchema, filterExpressions, newLimit);\n+  static class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private boolean isStreaming = STREAMING.defaultValue();\n+    private Duration monitorInterval = MONITOR_INTERVAL.defaultValue();\n+    private String nameMapping;\n+    private Schema projectedSchema;\n+    private List<Expression> filters;\n+    private Long limit = -1L;\n+\n+    private Builder() {\n+    }\n+\n+    Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    Builder useSnapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    Builder streaming(boolean streaming) {\n+      this.isStreaming = streaming;\n+      return this;\n+    }\n+\n+    Builder monitorInterval(Duration newMonitorInterval) {\n+      this.monitorInterval = newMonitorInterval;\n+      return this;\n+    }\n+\n+    Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    Builder project(Schema newProjectedSchema) {\n+      this.projectedSchema = newProjectedSchema;\n+      return this;\n+    }\n+\n+    Builder filters(List<Expression> newFilters) {\n+      this.filters = newFilters;\n+      return this;\n+    }\n+\n+    Builder limit(long newLimit) {\n+      this.limit = newLimit;\n+      return this;\n+    }\n+\n+    Builder fromProperties(Map<String, String> properties) {\n+      Configuration config = new Configuration();\n+      properties.forEach(config::setString);\n+\n+      return this.useSnapshotId(config.get(SNAPSHOT_ID))\n+          .caseSensitive(config.get(CASE_SENSITIVE))\n+          .asOfTimestamp(config.get(AS_OF_TIMESTAMP))", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzc2ODY4MA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553768680", "bodyText": "The default value of a given configure key is provided  in the AS_OF_TIMESTAMP  ConfigOption.    For those keys:\n\nsnapshot-id\nas-of-timestamp\nstart-snapshot-id\nend-snapshot-id\nsplit-size\nsplit-lookback\nsplit-file-open-cost\n\nWe provides null  as their default values,  so it's better to use boxed objects here.", "author": "openinx", "createdAt": "2021-01-08T06:41:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDU1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMDg5Mg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553700892", "bodyText": "Here as well, I think the code is more readable using context instead of ctxt.", "author": "rdblue", "createdAt": "2021-01-08T02:11:33Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMTIxNw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553701217", "bodyText": "@openinx, @stevenzwu, if you have any pointers to docs that would help me understand the Flink API here, please let me know. I'm still learning the Flink side.", "author": "rdblue", "createdAt": "2021-01-08T02:12:57Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,162 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext ctxt;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext ctxt) {\n+    Preconditions.checkArgument(ctxt.snapshotId() == null, \"Cannot set snapshot-id option for streaming reader\");\n+    Preconditions.checkArgument(ctxt.asOfTimestamp() == null, \"Cannot set as-of-timestamp option for streaming reader\");\n+    Preconditions.checkArgument(ctxt.endSnapshotId() == null, \"Cannot set end-snapshot-id option for streaming reader\");\n+    this.tableLoader = tableLoader;\n+    this.ctxt = ctxt;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {", "originalCommit": "ac6febbe89682726970075a25ccc4e5f1e6f3d16", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Mzc3MTQ5Ng==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r553771496", "bodyText": "@rdblue  I think this is the post that you may want to read.", "author": "openinx", "createdAt": "2021-01-08T06:51:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MzcwMTIxNw=="}], "type": "inlineReview"}, {"oid": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "url": "https://github.com/apache/iceberg/commit/cd46095d4890809b31d7d5b8c4b3cde75522b135", "message": "Addressing comments from ryan", "committedDate": "2021-01-08T06:46:55Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0MjE2Mw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555442163", "bodyText": "Should this be called in dispose instead? The doc you pointed to says that dispose is when resources should be released, and that close may not be called:\n\nIn the case of a termination due to a failure or due to manual cancellation, the execution jumps directly to the dispose() and skips any intermediate phases between the phase the operator was in when the failure happened and the dispose().", "author": "rdblue", "createdAt": "2021-01-12T01:10:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext scanContext;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;\n+\n+  public StreamingMonitorFunction(TableLoader tableLoader, ScanContext scanContext) {\n+    Preconditions.checkArgument(scanContext.snapshotId() == null,\n+        \"Cannot set snapshot-id option for streaming reader\");\n+    Preconditions.checkArgument(scanContext.asOfTimestamp() == null,\n+        \"Cannot set as-of-timestamp option for streaming reader\");\n+    Preconditions.checkArgument(scanContext.endSnapshotId() == null,\n+        \"Cannot set end-snapshot-id option for streaming reader\");\n+    this.tableLoader = tableLoader;\n+    this.scanContext = scanContext;\n+  }\n+\n+  @Override\n+  public void initializeState(FunctionInitializationContext context) throws Exception {\n+    snapshotIdState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\n+            \"snapshot-id-state\",\n+            LongSerializer.INSTANCE));\n+    tableLoader.open();\n+    table = tableLoader.loadTable();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {}.\", getClass().getSimpleName());\n+      startSnapshotId = snapshotIdState.get().iterator().next();\n+    } else {\n+      Long optionStartSnapshot = scanContext.startSnapshotId();\n+      if (optionStartSnapshot != null) {\n+        if (!SnapshotUtil.ancestorOf(table, table.currentSnapshot().snapshotId(), optionStartSnapshot)) {\n+          throw new IllegalStateException(\"The option start-snapshot-id \" + optionStartSnapshot +\n+              \" is not an ancestor of the current snapshot\");\n+        }\n+\n+        startSnapshotId = optionStartSnapshot;\n+      } else {\n+        startSnapshotId = DUMMY_START_SNAPSHOT_ID;\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(FunctionSnapshotContext context) throws Exception {\n+    snapshotIdState.clear();\n+    snapshotIdState.add(startSnapshotId);\n+  }\n+\n+  @Override\n+  public void run(SourceContext<FlinkInputSplit> ctx) throws Exception {\n+    checkpointLock = ctx.getCheckpointLock();\n+    while (isRunning) {\n+      synchronized (checkpointLock) {\n+        if (isRunning) {\n+          monitorAndForwardSplits(ctx);\n+        }\n+      }\n+      Thread.sleep(scanContext.monitorInterval().toMillis());\n+    }\n+  }\n+\n+  private void monitorAndForwardSplits(SourceContext<FlinkInputSplit> ctx) {\n+    table.refresh();\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot != null && snapshot.snapshotId() != startSnapshotId) {\n+      long snapshotId = snapshot.snapshotId();\n+      // Read current static table if startSnapshotId not set.\n+      ScanContext newScanContext = startSnapshotId == DUMMY_START_SNAPSHOT_ID ?\n+          scanContext.copyWithSnapshotId(snapshotId) :\n+          scanContext.copyWithAppendsBetween(startSnapshotId, snapshotId);\n+\n+      FlinkInputSplit[] splits = FlinkSplitGenerator.createInputSplits(table, newScanContext);\n+      for (FlinkInputSplit split : splits) {\n+        ctx.collect(split);\n+      }\n+      startSnapshotId = snapshotId;\n+    }\n+  }\n+\n+  @Override\n+  public void cancel() {\n+    // this is to cover the case where cancel() is called before the run()\n+    if (checkpointLock != null) {\n+      synchronized (checkpointLock) {\n+        isRunning = false;\n+      }\n+    } else {\n+      isRunning = false;\n+    }\n+\n+    if (tableLoader != null) {\n+      try {\n+        tableLoader.close();", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTU5MDQzNw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555590437", "bodyText": "Yes, you're right.  It's recommended to release all resources in an AbstractStreamOperator's dispose method.  Here the StreamingMonitorFuntion is a  RichSourceFunction which does not have a dispose method.  But I've checked the  AbstractUdfStreamOperator (which is inherited by the StreamSource operator) ,  it's dispose method will call the function's close method directly.  So I think it's OK to release those resources here.", "author": "openinx", "createdAt": "2021-01-12T08:28:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0MjE2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0MjcxOQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555442719", "bodyText": "This appears to be used as the last processed snapshot ID. I think it would make more sense to name it lastSnapshotId so it is clear that this snapshot has already been processed.", "author": "rdblue", "createdAt": "2021-01-12T01:12:14Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingMonitorFunction.java", "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.api.common.typeutils.base.LongSerializer;\n+import org.apache.flink.runtime.state.FunctionInitializationContext;\n+import org.apache.flink.runtime.state.FunctionSnapshotContext;\n+import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;\n+import org.apache.flink.streaming.api.functions.source.RichSourceFunction;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.SnapshotUtil;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * This is the single (non-parallel) monitoring task which takes a {@link FlinkInputFormat},\n+ * it is responsible for:\n+ *\n+ * <ol>\n+ *     <li>Monitoring snapshots of the Iceberg table.</li>\n+ *     <li>Creating the {@link FlinkInputSplit splits} corresponding to the incremental files</li>\n+ *     <li>Assigning them to downstream tasks for further processing.</li>\n+ * </ol>\n+ *\n+ * <p>The splits to be read are forwarded to the downstream {@link StreamingReaderOperator}\n+ * which can have parallelism greater than one.\n+ */\n+public class StreamingMonitorFunction extends RichSourceFunction<FlinkInputSplit> implements CheckpointedFunction {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingMonitorFunction.class);\n+\n+  private static final long DUMMY_START_SNAPSHOT_ID = -1;\n+\n+  private final TableLoader tableLoader;\n+  private final ScanContext scanContext;\n+\n+  private volatile boolean isRunning = true;\n+  private transient Object checkpointLock;\n+  private transient Table table;\n+  private transient ListState<Long> snapshotIdState;\n+  private transient long startSnapshotId;", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0NTk3Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555445977", "bodyText": "Adding an element with offer will return false if the element cannot be added. If that happens, then the split will be silently dropped because the return value is ignored. I think this should use add to throw an exception, or handle the boolean return value.", "author": "rdblue", "createdAt": "2021-01-12T01:22:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0ODg4NQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555448885", "bodyText": "In what case would the element be null, but format.reachedEnd() will not be true? That's the only case where break is needed, right? I would expect this to emit records until reachedEnd is true, regardless of whether any of them are null. This may choose to suppress null records, but I don't understand why that would change the loop.", "author": "rdblue", "createdAt": "2021-01-12T01:30:27Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTYxNzA0Ng==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555617046", "bodyText": "I checked the parquet/orc/avro read iterator,  there should be no one that will return null elements when reachedEnd.   So the else-break block could be removed now. Thanks for the remanding.", "author": "openinx", "createdAt": "2021-01-12T09:13:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ0ODg4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ1NDQyMg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555454422", "bodyText": "Why use the MailboxExecutorImpl class rather than the MailboxExecutor interface?", "author": "rdblue", "createdAt": "2021-01-12T01:36:21Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTU5MTcxOA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555591718", "bodyText": "Because we're using the MailboxExecutorImpl's isIdle  here.", "author": "openinx", "createdAt": "2021-01-12T08:31:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ1NDQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTQ2MTU5Mg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555461592", "bodyText": "Shouldn't this be called in a finally block to ensure that close is called?", "author": "rdblue", "createdAt": "2021-01-12T01:58:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,214 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.streaming.runtime.tasks.mailbox.MailboxExecutorImpl;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+  private final MailboxExecutorImpl executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> readerContext;\n+\n+  private transient ListState<FlinkInputSplit> checkpointState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = (MailboxExecutorImpl) Preconditions.checkNotNull(mailboxExecutor,\n+        \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    checkpointState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      LOG.info(\"Restoring state for the {} (taskIdx={}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : checkpointState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.readerContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.offer(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    executor.execute(this::processSplits, this.getClass().getSimpleName());\n+  }\n+\n+  private void processSplits() throws IOException {\n+    do {\n+      FlinkInputSplit split = splits.poll();\n+      if (split == null) {\n+        return;\n+      }\n+\n+      LOG.debug(\"load split: {}\", split);\n+      format.open(split);\n+\n+      RowData nextElement = null;\n+      while (!format.reachedEnd()) {\n+        nextElement = format.nextRecord(nextElement);\n+        if (nextElement != null) {\n+          readerContext.collect(nextElement);\n+        } else {\n+          break;\n+        }\n+      }\n+\n+      format.close();", "originalCommit": "cd46095d4890809b31d7d5b8c4b3cde75522b135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ea3b7e118b7503a3b2d2e7208805ab997840f866", "url": "https://github.com/apache/iceberg/commit/ea3b7e118b7503a3b2d2e7208805ab997840f866", "message": "Minor changes & address comments from Ryan.", "committedDate": "2021-01-12T09:05:46Z", "type": "commit"}, {"oid": "33554971c192cd6b94f5d7ae7766219cd7e7b6d9", "url": "https://github.com/apache/iceberg/commit/33554971c192cd6b94f5d7ae7766219cd7e7b6d9", "message": "Execute the processSplits once per split", "committedDate": "2021-01-12T13:40:05Z", "type": "commit"}, {"oid": "77d9aa45fa63f66f93d514501abbb79bd988bf4b", "url": "https://github.com/apache/iceberg/commit/77d9aa45fa63f66f93d514501abbb79bd988bf4b", "message": "Fix the snapshot ancestor issue", "committedDate": "2021-01-12T14:49:33Z", "type": "commit"}, {"oid": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "url": "https://github.com/apache/iceberg/commit/0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "message": "Minor changes.", "committedDate": "2021-01-12T15:08:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2ODU3OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555968579", "bodyText": "Split state should be set before starting a concurrent operation right?", "author": "rdblue", "createdAt": "2021-01-12T17:59:52Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = Preconditions.checkNotNull(mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    // TODO Replace Java serialization with Avro approach to keep state compatibility.\n+    // See issue: https://github.com/apache/iceberg/issues/1698\n+    inputSplitsState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    // Initialize the current split state to IDLE.\n+    currentSplitState = SplitState.IDLE;\n+\n+    // Recover splits state from flink state backend if possible.\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+      LOG.info(\"Restoring state for the {} (taskIdx: {}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : inputSplitsState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.sourceContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.add(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    if (currentSplitState == SplitState.IDLE) {\n+      executor.execute(this::processSplits, this.getClass().getSimpleName());\n+      currentSplitState = SplitState.RUNNING;", "originalCommit": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjI1MjY4MA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556252680", "bodyText": "Moving this line before line-130 is OK for me.", "author": "openinx", "createdAt": "2021-01-13T04:18:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2ODU3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3NDY3MA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556974670", "bodyText": "If there aren't multiple threads, then I agree it doesn't matter much.", "author": "rdblue", "createdAt": "2021-01-14T00:55:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2ODU3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2OTg2Mg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555969862", "bodyText": "This should be volatile if it is shared across threads.", "author": "rdblue", "createdAt": "2021-01-12T18:02:05Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;", "originalCommit": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjI1MjMwNQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556252305", "bodyText": "There's only one thread (the thread that running StreamTask ) to access this currentSplitState,  so we don't have to make it to be volatile.   Hope I explained the mechanism clearly in above comment,  if it's still no clear, Pls bring it up ,  I'd like to provide more background :-)", "author": "openinx", "createdAt": "2021-01-13T04:16:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk2OTg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r555971461", "bodyText": "I don't think that using MailboxExecutor is a good idea. If I understand correctly, the mailbox queue for the executor cannot be used to hold state because that state would not be checkpointed (if, for example, it held the splits waiting to be processed). The result is that this operator has an elaborate way to keep state in a queue and continuously submit stateless mailbox tasks to keep running. But a simpler option is to create a thread directly that polls the splits queue and keeps running endlessly.", "author": "rdblue", "createdAt": "2021-01-12T18:04:54Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,233 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;", "originalCommit": "0ce3ce6e1b4a52f10313b3bf2c24762042741ef0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NjI0OTQzMg==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556249432", "bodyText": "We cannot use a newly created thread to process the split asynchronously because it will break the checkpoint mechanism which depends on mail-box model in flink runtime,  Assume the asynchronously thread keep processing the records of the newly split (which is polled from splits queue),  now the flink checkpoint barrier come,  How should we coordinate the checkpoint barrier and the processing split so that the barrier could effect ( trigger to persist all states of this operator ) once the current split is finished ?  Will we go back to use the checkpoint lock to synchronize between checkpoint barrier event and the newly introduced async thread ?\nIn the current mail-box model,  both flink's internal controlling action and user-provided events will be processed in the same thread ( Which is StreamTask in flink runtime).   The StreamTask will run in a endless loop to process the event from mail box queue ( see the code here).  For each loop,  it will:\nStep.1.    Try to take mails which has been enqueued in mail-box queue  ( code), those mails are flink's controlling actions, such as action to trigger flink's checkpoint , action to notify checkpoint complete etc.   If there's no mail enqueued in mail-box queue,  then the processMail will do nothing.\nStep.2  Then read one completed record from the flink's network queue and invoke the processElement(record) ,  that's the process about incremental compute.  Take the sum as an example,   once a record come, the processElement will increment its counter.\nSo all the events (Regardless of flink's control events or user's events )  are being processed in the same thread StreamTask.   In our flink streaming reader,  we only need to control that there's only one split that is being processing,  then the newly triggered checkpoint action could just enqueue the mail-box queue.  Once the processing splits is finished,  the StreamTask will execute the checkpoint action in step.1 .    We don't need any extra checkpoint lock or extra synchronization.\nThat's why we use Mailbox to enqueue the action  ( processSplits )to process whole elements from a given split.", "author": "openinx", "createdAt": "2021-01-13T04:05:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk2NjQ2OA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556966468", "bodyText": "Okay, I think I understand now. The missing piece of information was that the MailboxExecutor where these tasks are added is the same mailbox executor that is running the operator. Is that correct?\nI thought that it was a separate mailbox executor and thread, which wouldn't make much sense. I also noted the problem with checkpoints that you pointed out, but thought that it must be handled within the mailbox.\nIf it is the same executor, then it makes sense because the same thread processing incoming checkpoint events will also run processSplits. I think I see what you're doing now.", "author": "rdblue", "createdAt": "2021-01-14T00:30:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk2ODE2OA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556968168", "bodyText": "If this is the same mailbox executor that is running the operator, then it would be helpful to add a comment here that says it for future readers.", "author": "rdblue", "createdAt": "2021-01-14T00:36:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzAwMTM5OQ==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r557001399", "bodyText": "The missing piece of information was that the MailboxExecutor where these tasks are added is the same mailbox executor that is running the operator. Is that correct?\n\nYes, you're right.  Let me add few comment to explain this more cleaner in code.", "author": "openinx", "createdAt": "2021-01-14T02:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NTk3MTQ2MQ=="}], "type": "inlineReview"}, {"oid": "5899a9f21329a782c54e99b56abe157128f2cf35", "url": "https://github.com/apache/iceberg/commit/5899a9f21329a782c54e99b56abe157128f2cf35", "message": "More unit tests for StreamingReaderOperator.", "committedDate": "2021-01-13T10:08:17Z", "type": "commit"}, {"oid": "b8e91f189c640ad90a840970555735267e06cd9e", "url": "https://github.com/apache/iceberg/commit/b8e91f189c640ad90a840970555735267e06cd9e", "message": "More UT for streaming-monitor-funciton.", "committedDate": "2021-01-13T13:10:24Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MTE4Nw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556971187", "bodyText": "Minor: Should this only queue a task if splits is non-empty?\nIf splits is currently empty and this is called from processSplits, then a new task will be queued. That task will process a split if one is waiting in the mailbox queue to be processed, but often it will do nothing and set the split state back to IDLE.\nI'd probably only add a new task if splits is non-empty, or update processSplits to always submit a new task and not set IDLE in the finally block.", "author": "rdblue", "createdAt": "2021-01-14T00:44:24Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;\n+\n+  private StreamingReaderOperator(FlinkInputFormat format, ProcessingTimeService timeService,\n+                                  MailboxExecutor mailboxExecutor) {\n+    this.format = Preconditions.checkNotNull(format, \"The InputFormat should not be null.\");\n+    this.processingTimeService = timeService;\n+    this.executor = Preconditions.checkNotNull(mailboxExecutor, \"The mailboxExecutor should not be null.\");\n+  }\n+\n+  @Override\n+  public void initializeState(StateInitializationContext context) throws Exception {\n+    super.initializeState(context);\n+\n+    // TODO Replace Java serialization with Avro approach to keep state compatibility.\n+    // See issue: https://github.com/apache/iceberg/issues/1698\n+    inputSplitsState = context.getOperatorStateStore().getListState(\n+        new ListStateDescriptor<>(\"splits\", new JavaSerializer<>()));\n+\n+    // Initialize the current split state to IDLE.\n+    currentSplitState = SplitState.IDLE;\n+\n+    // Recover splits state from flink state backend if possible.\n+    splits = Lists.newLinkedList();\n+    if (context.isRestored()) {\n+      int subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n+      LOG.info(\"Restoring state for the {} (taskIdx: {}).\", getClass().getSimpleName(), subtaskIdx);\n+\n+      for (FlinkInputSplit split : inputSplitsState.get()) {\n+        splits.add(split);\n+      }\n+    }\n+\n+    this.sourceContext = StreamSourceContexts.getSourceContext(\n+        getOperatorConfig().getTimeCharacteristic(),\n+        getProcessingTimeService(),\n+        new Object(), // no actual locking needed\n+        getContainingTask().getStreamStatusMaintainer(),\n+        output,\n+        getRuntimeContext().getExecutionConfig().getAutoWatermarkInterval(),\n+        -1);\n+\n+    if (!splits.isEmpty()) {\n+      enqueueProcessSplits();\n+    }\n+  }\n+\n+  @Override\n+  public void snapshotState(StateSnapshotContext context) throws Exception {\n+    super.snapshotState(context);\n+\n+    inputSplitsState.clear();\n+    inputSplitsState.addAll(Lists.newArrayList(splits));\n+  }\n+\n+  @Override\n+  public void processElement(StreamRecord<FlinkInputSplit> element) {\n+    splits.add(element.getValue());\n+    enqueueProcessSplits();\n+  }\n+\n+  private void enqueueProcessSplits() {\n+    if (currentSplitState == SplitState.IDLE) {", "originalCommit": "b8e91f189c640ad90a840970555735267e06cd9e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1NzAwNTg3Mw==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r557005873", "bodyText": "Yeah, I like this improvement. Will update it in the next patch.", "author": "openinx", "createdAt": "2021-01-14T02:34:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MTE4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1Njk3MzQwOA==", "url": "https://github.com/apache/iceberg/pull/1793#discussion_r556973408", "bodyText": "I think I would rephrase the comment here. I didn't really understand it when I read it the first time. Here's what I would suggest, assuming that I understand what's happening here:\n\nSplits are read by the same thread that calls processElement. Each read task is submitted to that thread by adding them to the executor. This state is used to ensure that only one read task is in that queue at a time, so that read tasks do not accumulate ahead of checkpoint tasks. When there is a read task in the queue, this is set to RUNNING. When there are no more files to read, this will be set to IDLE.", "author": "rdblue", "createdAt": "2021-01-14T00:51:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/StreamingReaderOperator.java", "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.Queue;\n+import org.apache.flink.api.common.state.ListState;\n+import org.apache.flink.api.common.state.ListStateDescriptor;\n+import org.apache.flink.runtime.state.JavaSerializer;\n+import org.apache.flink.runtime.state.StateInitializationContext;\n+import org.apache.flink.runtime.state.StateSnapshotContext;\n+import org.apache.flink.streaming.api.functions.source.SourceFunction;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperator;\n+import org.apache.flink.streaming.api.operators.AbstractStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.MailboxExecutor;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperator;\n+import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.streaming.api.operators.StreamOperator;\n+import org.apache.flink.streaming.api.operators.StreamOperatorParameters;\n+import org.apache.flink.streaming.api.operators.StreamSourceContexts;\n+import org.apache.flink.streaming.api.operators.YieldingOperatorFactory;\n+import org.apache.flink.streaming.api.watermark.Watermark;\n+import org.apache.flink.streaming.runtime.streamrecord.StreamRecord;\n+import org.apache.flink.streaming.runtime.tasks.ProcessingTimeService;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The operator that reads the {@link FlinkInputSplit splits} received from the preceding {@link\n+ * StreamingMonitorFunction}. Contrary to the {@link StreamingMonitorFunction} which has a parallelism of 1,\n+ * this operator can have multiple parallelism.\n+ *\n+ * <p>As soon as a split descriptor is received, it is put in a queue, and use {@link MailboxExecutor}\n+ * read the actual data of the split. This architecture allows the separation of the reading thread from the one split\n+ * processing the checkpoint barriers, thus removing any potential back-pressure.\n+ */\n+public class StreamingReaderOperator extends AbstractStreamOperator<RowData>\n+    implements OneInputStreamOperator<FlinkInputSplit, RowData> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingReaderOperator.class);\n+\n+  private final MailboxExecutor executor;\n+  private FlinkInputFormat format;\n+\n+  private transient SourceFunction.SourceContext<RowData> sourceContext;\n+\n+  private transient ListState<FlinkInputSplit> inputSplitsState;\n+  private transient Queue<FlinkInputSplit> splits;\n+\n+  // This state is used to control that only one split is occupying the executor for record reading. If there're more\n+  // splits coming, we will buffer them in flink's state. Once the executor is idle (the currentSplitState will be\n+  // marked as IDLE), it will schedule one new split from buffered splits in flink's state to executor (the\n+  // currentSplitState will be marked as RUNNING). After finished all records processing, the currentSplitState will\n+  // be marked as IDLE again.\n+  // NOTICE: all the reader and writer of this variable are the same thread, so we don't need extra synchronization.\n+  private transient SplitState currentSplitState;", "originalCommit": "b8e91f189c640ad90a840970555735267e06cd9e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ea532f4ba26f19612eacb7f9d263ea5df5b1155f", "url": "https://github.com/apache/iceberg/commit/ea532f4ba26f19612eacb7f9d263ea5df5b1155f", "message": "Addressing comments from Ryan.", "committedDate": "2021-01-14T03:33:13Z", "type": "commit"}]}