{"pr_number": 1512, "pr_title": "Spark: Implement equals and hashCode in SparkBatchScan", "pr_createdAt": "2020-09-25T20:41:40Z", "pr_url": "https://github.com/apache/iceberg/pull/1512", "timeline": [{"oid": "090eecde634aa8625c60b3ab8e70bf0e9cc60071", "url": "https://github.com/apache/iceberg/commit/090eecde634aa8625c60b3ab8e70bf0e9cc60071", "message": "Spark: Implement equals and hashCode in SparkBatchScan.", "committedDate": "2020-09-25T20:38:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIyMzUzMw==", "url": "https://github.com/apache/iceberg/pull/1512#discussion_r495223533", "bodyText": "if they aren't case sensitive aren't they equal if the lowercased names are equivelent?\nIe\n(insensitive) == (insenstive) && (TaBle).equals(table)?\nIf we just want to check the fields being equal I understand that too", "author": "RussellSpitzer", "createdAt": "2020-09-25T20:48:20Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java", "diffHunk": "@@ -208,6 +209,38 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    SparkBatchScan that = (SparkBatchScan) o;\n+    // compare Spark schemas to ignore field ids\n+    StructType thisSchema = SparkSchemaUtil.convert(expectedSchema);\n+    StructType thatSchema = SparkSchemaUtil.convert(that.expectedSchema);\n+    return caseSensitive == that.caseSensitive &&", "originalCommit": "090eecde634aa8625c60b3ab8e70bf0e9cc60071", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTI3ODUwNg==", "url": "https://github.com/apache/iceberg/pull/1512#discussion_r495278506", "bodyText": "My initial thought was that filter expression binding depends on whether this is case sensitive or not. But since we get this value from Spark and don't expect it to change (let alone in a single query), it seems unnecessary. I'm also not able to think of a case where a difference would matter without causing an exception.\nI'll remove this.", "author": "rdblue", "createdAt": "2020-09-25T22:20:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTIyMzUzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTI3NzU5OA==", "url": "https://github.com/apache/iceberg/pull/1512#discussion_r495277598", "bodyText": "Oops, I'm also missing an update that uses readSchema() and a lazy variable to avoid converting multiple times.", "author": "rdblue", "createdAt": "2020-09-25T22:18:49Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java", "diffHunk": "@@ -208,6 +209,38 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    SparkBatchScan that = (SparkBatchScan) o;\n+    // compare Spark schemas to ignore field ids\n+    StructType thisSchema = SparkSchemaUtil.convert(expectedSchema);\n+    StructType thatSchema = SparkSchemaUtil.convert(that.expectedSchema);", "originalCommit": "090eecde634aa8625c60b3ab8e70bf0e9cc60071", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "38d865cec7e8cf098fd2ed6889b1f286c84376f0", "url": "https://github.com/apache/iceberg/commit/38d865cec7e8cf098fd2ed6889b1f286c84376f0", "message": "Ignore caseSensitive and make schema conversion lazy.", "committedDate": "2020-09-25T22:59:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2NTY1MA==", "url": "https://github.com/apache/iceberg/pull/1512#discussion_r496165650", "bodyText": "would it make sense to use deepEquals here instead of constructing a string?", "author": "holdenk", "createdAt": "2020-09-28T18:55:56Z", "path": "spark3/src/main/java/org/apache/iceberg/spark/source/SparkBatchScan.java", "diffHunk": "@@ -208,6 +213,33 @@ public Statistics estimateStatistics() {\n     return new Stats(sizeInBytes, numRows);\n   }\n \n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+\n+    SparkBatchScan that = (SparkBatchScan) o;\n+    return table.toString().equals(that.table.toString()) &&\n+        readSchema().equals(that.readSchema()) && // compare Spark schemas to ignore field ids\n+        filterExpressions.toString().equals(that.filterExpressions.toString()) &&", "originalCommit": "38d865cec7e8cf098fd2ed6889b1f286c84376f0", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI0NDYyOA==", "url": "https://github.com/apache/iceberg/pull/1512#discussion_r496244628", "bodyText": "This is definitely something I think we should improve.\nRight now, we don't implement equals because we don't want people to use it to test semantic equivalence. Here, we actually want semantic equivalence, but with no utility to evaluate it the simplest way to check for identical structure is the string representation. A better first solution would be to build a comparison utility to test whether two expressions are structurally identical, and after that to introduce one for semantic equivalence.", "author": "rdblue", "createdAt": "2020-09-28T21:32:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE2NTY1MA=="}], "type": "inlineReview"}]}