{"pr_number": 1346, "pr_title": "Flink: Introduce Flink InputFormat", "pr_createdAt": "2020-08-17T06:22:54Z", "pr_url": "https://github.com/apache/iceberg/pull/1346", "timeline": [{"oid": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "url": "https://github.com/apache/iceberg/commit/decf8f1136d6da4e024f5c91b00ddd4adf92f843", "message": "Flink: Introduce Flink InputFormat", "committedDate": "2020-08-17T08:43:03Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk1OTc1Mw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471959753", "bodyText": "How about renaming this method to openTaskIterator  ?", "author": "openinx", "createdAt": "2020-08-18T07:06:49Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+  private final Schema projectedSchema;\n+  private final int[] fieldsReorder;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema tableSchema,\n+               List<String> projectedFields) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+\n+    this.projectedSchema = FlinkSchemaUtil.pruneWithoutReordering(tableSchema, projectedFields);\n+\n+    // The projected schema is the schema without reordering, but Flink wants its own order, so we need to reorder the\n+    // output row.\n+    List<String> projectedNames = projectedSchema.asStruct().fields().stream()\n+        .map(Types.NestedField::name).collect(Collectors.toList());\n+    this.fieldsReorder = projectedFields == null ?\n+        null : projectedFields.stream().mapToInt(projectedNames::indexOf).toArray();\n+  }\n+\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  int[] fieldsReorder() {\n+    return fieldsReorder;\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        this.fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = nextTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> nextTaskIterator(FileScanTask scanTask) throws IOException;", "originalCommit": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk3MTQ3MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471971470", "bodyText": "nit: is this comment still valuable ? Seems I did not get the point.", "author": "openinx", "createdAt": "2020-08-18T07:29:09Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroReader;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PartitionUtil;\n+\n+class RowDataIterator extends DataIterator<RowData> {\n+\n+  private final String nameMapping;\n+\n+  RowDataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema tableSchema,\n+                  List<String> projectedFields, String nameMapping) {\n+    super(task, fileIo, encryption, tableSchema, projectedFields);\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<RowData> nextTaskIterator(FileScanTask task) {\n+    // schema or rows returned by readers", "originalCommit": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxODg0Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472018847", "bodyText": "We can delete it.", "author": "JingsongLi", "createdAt": "2020-08-18T08:46:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk3MTQ3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471985446", "bodyText": "I'm not quite sure whether flink support complex data type projection, if sure we may need more unit tests to address the projection cases, such as projection by a nested struct, map, list (similar to the spark's TestReadProjection ).", "author": "openinx", "createdAt": "2020-08-18T07:52:24Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,340 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  // before variables\n+  private Configuration conf;\n+  String warehouse;\n+  private HadoopCatalog catalog;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters\n+  public static Object[] parameters() {\n+    // TODO add orc and parquet\n+    return new Object[] {\"avro\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return executeWithOptions(table, null, null, null, null, null, null, null, null);\n+  }\n+\n+  private List<Row> execute(Table table, List<String> projectFields) throws IOException {\n+    return executeWithOptions(table, projectFields, null, null, null, null, null, null, null);\n+  }\n+\n+  protected abstract List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId,\n+      Long startSnapshotId, Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter)\n+      throws IOException;\n+\n+  protected abstract void assertResiduals(List<Row> results, List<Record> writeRecords, List<Record> filteredRecords)\n+      throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {", "originalCommit": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NjQwNQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r471986405", "bodyText": "Another case: Project with a new renamed schema", "author": "openinx", "createdAt": "2020-08-18T07:54:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxOTkxMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472019911", "bodyText": "There is a NestedFieldsProjectableTableSource in Flink, but so far, no connector has implemented it. There may be unknown risks, and I tend not to implement it first.", "author": "JingsongLi", "createdAt": "2020-08-18T08:48:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAyMDM4MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472020381", "bodyText": "Another case: Project with a new renamed schema\n\nYou mean: Create a table, insert some data, rename some fields, insert some data. Then read table using Flink?", "author": "JingsongLi", "createdAt": "2020-08-18T08:49:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjA0MzM2MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472043361", "bodyText": "Not the cases you said,  I read the cases in TestReadProject, and was thought that we may also need a ut to address this one:  write records into a table and then read them from table by another schema with same fields id but different field names. But we flink are prejection with a List<String> (field names),  so we don't have such case.   We could ignore the case for unit test.", "author": "openinx", "createdAt": "2020-08-18T09:27:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY2Mjk5NA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494662994", "bodyText": "Field reordering tests are at the file format level. Each file format has to be able to project columns in the requested order. So any reordered schema should work as long as it is passed down correctly, which is what the new convert method does.", "author": "rdblue", "createdAt": "2020-09-24T23:22:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTk4NTQ0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472011890", "bodyText": "Continue with the question from here. If we could produce a  ordered & projected schema in this method (Saying if this method is pruneWithReordering), then seems we don't have to convert the read RowData to the correct order here ?\nI'd prefer to use the correct projected schema to read the target RowData if possible, rather than reading RowData in a disordered schema and then order them in an iterator transformation.  Because this is in the critical read path and an extra RowData transformation will cost more resources , also make the codes hard to follow.", "author": "openinx", "createdAt": "2020-08-18T08:36:23Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +102,22 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Prune columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   * @throws IllegalArgumentException if the Flink type does not match the Schema\n+   */\n+  public static Schema pruneWithoutReordering(Schema schema, List<String> projectedFields) {\n+    if (projectedFields == null) {\n+      return schema;\n+    }\n+\n+    Map<String, Integer> indexByName = TypeUtil.indexByName(schema.asStruct());\n+    Set<Integer> projectedIds = projectedFields.stream().map(indexByName::get).collect(Collectors.toSet());\n+    return TypeUtil.select(schema, projectedIds);", "originalCommit": "decf8f1136d6da4e024f5c91b00ddd4adf92f843", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAyNjYyNg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472026626", "bodyText": "I'd prefer to use the correct projected schema to read the target RowData if possible\n\nThis is what I want too, but I'm afraid the current format readers do not have this capability. You can take a look to AvroSchemaWithTypeVisitor, the readers order is according to file schema instead of Flink projected/expected schema.\n\nBecause this is in the critical read path and an extra RowData transformation will cost more resources\n\nThe performance is OK, because we just use a lazy projection in ProjectionRowData, Unnecessary projections are omitted.", "author": "JingsongLi", "createdAt": "2020-08-18T08:59:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE4NjQ1Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r472186456", "bodyText": "cc @rdblue , Mind to take a look this ? Thanks.", "author": "openinx", "createdAt": "2020-08-18T13:18:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzQ4MDk2Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r473480966", "bodyText": "You should be able to use the expected/projection schema. All readers should reorder columns to produce the requested column order.\nAvroSchemaWithTypeVisitor is used to traverse the file schema to create the reader structure, but that's because fields in Avro must be read in the file's order. But when that reader adds data columns to records, the values are put in the correct order because the ResolvingDecoder returns the correct position in the projection schema.", "author": "rdblue", "createdAt": "2020-08-20T00:42:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzU4MTMwNA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r473581304", "bodyText": "Thanks @openinx and @rdblue ! I'm very happy to be able to solve my confusion. I will do it.", "author": "JingsongLi", "createdAt": "2020-08-20T04:28:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzU4MjM5MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r473582391", "bodyText": "It doesn't matter if we change the order of the schema, as long as the ID doesn't change.", "author": "JingsongLi", "createdAt": "2020-08-20T04:32:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjAxMTg5MA=="}], "type": "inlineReview"}, {"oid": "d327aa89c54d81a631a172ba10194225c9c06ded", "url": "https://github.com/apache/iceberg/commit/d327aa89c54d81a631a172ba10194225c9c06ded", "message": "Address comments", "committedDate": "2020-08-20T04:40:19Z", "type": "forcePushed"}, {"oid": "0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "url": "https://github.com/apache/iceberg/commit/0b27405879bd2fdcd03a9fcc00236f5fd6ac9209", "message": "Address comments", "committedDate": "2020-08-20T05:24:52Z", "type": "forcePushed"}, {"oid": "4259635402efef3df1c83140723d5968f0b11967", "url": "https://github.com/apache/iceberg/commit/4259635402efef3df1c83140723d5968f0b11967", "message": "Address comments", "committedDate": "2020-08-21T02:04:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4Nzc3MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474587771", "bodyText": "nit : projectedFields == null || projectFields.length ==0", "author": "openinx", "createdAt": "2020-08-21T09:43:28Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   */\n+  public static Schema projectWithReordering(Schema schema, List<String> projectedFields) {\n+    if (projectedFields == null) {", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwMjk4MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475302981", "bodyText": "projectFields.length ==0 means project empty column. But projectedFields == null means project all columns.", "author": "JingsongLi", "createdAt": "2020-08-24T01:53:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU4Nzc3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MzQwMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474593402", "bodyText": "we may need to add a comment to indicate that:  we don't support complex data type projection for flink now.", "author": "openinx", "createdAt": "2020-08-21T09:49:30Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwMzQ3NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475303475", "bodyText": "Add comments: Don't support nested fields projection for Flink now.", "author": "JingsongLi", "createdAt": "2020-08-24T01:55:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MzQwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyNzAyMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476927021", "bodyText": "Why aren't nested fields supported?", "author": "rdblue", "createdAt": "2020-08-26T00:52:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MzQwMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4NTE2NA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476985164", "bodyText": "Flink SQL did not have good support for nested projection before. I'll verify it and try to implement nested fields projection.", "author": "JingsongLi", "createdAt": "2020-08-26T02:18:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5MzQwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474595633", "bodyText": "We've already have a Schema#select, will it fit for your requirement ?\n  /**\n   * Creates a projection schema for a subset of columns, selected by name.\n   * <p>\n   * Names that identify nested fields will select part or all of the field's top-level column.\n   *\n   * @param names a List of String names for selected columns\n   * @return a projection schema from this schema, by name\n   */\n  public Schema select(Collection<String> names) {\n    return internalSelect(names, true);\n  }", "author": "openinx", "createdAt": "2020-08-21T09:51:48Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java", "diffHunk": "@@ -98,4 +101,26 @@ public static TableSchema toSchema(RowType rowType) {\n     }\n     return builder.build();\n   }\n+\n+  /**\n+   * Project columns from a {@link Schema} using a projected fields.\n+   *\n+   * @param schema a Schema\n+   * @param projectedFields projected fields from Flink\n+   * @return a Schema corresponding to the Flink projection\n+   */\n+  public static Schema projectWithReordering(Schema schema, List<String> projectedFields) {", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwMjg0OQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475302849", "bodyText": "You can take a look to Schema.internalSelect, it is a Set<Integer> selected, so actually the interface is: \"selectWithoutReordering\"", "author": "JingsongLi", "createdAt": "2020-08-24T01:52:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMxMTY2MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475311661", "bodyText": "OK, that sounds reasonable.", "author": "openinx", "createdAt": "2020-08-24T02:34:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkyOTkxNg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476929916", "bodyText": "We do this in Spark here: https://github.com/apache/iceberg/blob/master/spark/src/main/java/org/apache/iceberg/spark/SparkSchemaUtil.java#L161-L166\nYou might try a similar approach since we already have Flink to Iceberg conversion. It should just be a matter of reassigning the IDs. You may also need the type fixes, I don't recall if Iceberg to Flink to Iceberg conversion is lossy or not.", "author": "rdblue", "createdAt": "2020-08-26T00:56:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk5OTkxMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476999912", "bodyText": "You may also need the type fixes\n\nYes, we can have.\nAfter thinking about it with nested projection, we can pass a TableSchema requestedSchema parameter, which contains the required projection (Including nested) and the type to be fixed.", "author": "JingsongLi", "createdAt": "2020-08-26T02:46:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwNTUxMA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477005510", "bodyText": "Only UUID need be fixed...", "author": "JingsongLi", "createdAt": "2020-08-26T03:06:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA3NjEzOA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477076138", "bodyText": "Create #1382 for fixup.", "author": "JingsongLi", "createdAt": "2020-08-26T06:58:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5NTYzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5Nzc1Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474597757", "bodyText": "Here, you put all reader related classes inside the source package,  will we also need to put those writer related classes into sink package ?    I don't have strong feeling to do that, keeping consistence is OK for me.", "author": "openinx", "createdAt": "2020-08-21T09:54:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwMzc2MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475303761", "bodyText": "will we also need to put those writer related classes into sink package ?\n\nI think we can.", "author": "JingsongLi", "createdAt": "2020-08-24T01:57:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5Nzc1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDU5OTIyMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474599222", "bodyText": "nit: better to keep the assign order with the arguments ?", "author": "openinx", "createdAt": "2020-08-21T09:57:47Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.fileIo = fileIo;", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwMzQ3MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474603471", "bodyText": "the long value is surely microseconds ?  could we just return (int)((long)value/1_000) ?", "author": "openinx", "createdAt": "2020-08-21T10:06:40Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.fileIo = fileIo;\n+    this.tasks = task.files().iterator();\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {\n+          return ((GenericData.Fixed) value).bytes();\n+        }\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case BINARY: // byte[]\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case TIME: // int instead of long\n+        return (int) (DateTimeUtil.timeFromMicros((Long) value).toNanoOfDay() / 1000_000);", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwNTg5MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474605890", "bodyText": "Em, seems I've missed to close the TableLoader  in IcebergFilesCommitter patch..", "author": "openinx", "createdAt": "2020-08-21T10:11:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,218 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDYwOTA0NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r474609045", "bodyText": "Now the orc reader has been merge into master,  pls add the orc iterable here .", "author": "openinx", "createdAt": "2020-08-21T10:19:01Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/RowDataIterator.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.util.Map;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.avro.Avro;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.flink.data.FlinkAvroReader;\n+import org.apache.iceberg.flink.data.FlinkParquetReaders;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.mapping.NameMappingParser;\n+import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.util.PartitionUtil;\n+\n+class RowDataIterator extends DataIterator<RowData> {\n+\n+  private final Schema projectedSchema;\n+  private final String nameMapping;\n+  private final boolean caseSensitive;\n+\n+  RowDataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption, Schema projectedSchema,\n+                  String nameMapping, boolean caseSensitive) {\n+    super(task, fileIo, encryption);\n+    this.projectedSchema = projectedSchema;\n+    this.nameMapping = nameMapping;\n+    this.caseSensitive = caseSensitive;\n+  }\n+\n+  @Override\n+  protected CloseableIterator<RowData> openTaskIterator(FileScanTask task) {\n+    Schema partitionSchema = TypeUtil.select(projectedSchema, task.spec().identitySourceIds());\n+\n+    Map<Integer, ?> idToConstant = partitionSchema.columns().isEmpty() ? ImmutableMap.of() :\n+        PartitionUtil.constantsMap(task, RowDataIterator::convertConstant);\n+    CloseableIterable<RowData> iterable = newIterable(task, idToConstant);\n+    return iterable.iterator();\n+  }\n+\n+  private CloseableIterable<RowData> newIterable(FileScanTask task, Map<Integer, ?> idToConstant) {\n+    CloseableIterable<RowData> iter;\n+    if (task.isDataTask()) {\n+      throw new UnsupportedOperationException(\"Cannot read data task.\");\n+    } else {\n+      switch (task.file().format()) {\n+        case PARQUET:\n+          iter = newParquetIterable(task, idToConstant);\n+          break;\n+\n+        case AVRO:\n+          iter = newAvroIterable(task, idToConstant);\n+          break;\n+", "originalCommit": "177bc0cb60c0d8a61f0d17b4ec8a5fa49f81f135", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwOTg2MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475309860", "bodyText": "This fixes Flink Orc Reader (with partition) bug.", "author": "JingsongLi", "createdAt": "2020-08-24T02:25:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReaders.java", "diffHunk": "@@ -246,7 +256,7 @@ public void setBatchContext(long batchOffsetInFile) {\n \n     StructReader(List<OrcValueReader<?>> readers, Types.StructType struct, Map<Integer, ?> idToConstant) {\n       super(readers, struct, idToConstant);\n-      this.numFields = readers.size();\n+      this.numFields = struct.fields().size();", "originalCommit": "3cdaad4303f41dd81647806db26acfdad3f615a3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMxNzg0OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r475317848", "bodyText": "Nice catch,  if the schema is a projected read schema,  then the numFields will be mismatched.", "author": "openinx", "createdAt": "2020-08-24T03:05:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTMwOTg2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDUzNw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476930537", "bodyText": "Was this not used anywhere?", "author": "rdblue", "createdAt": "2020-08-26T00:57:20Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReader.java", "diffHunk": "@@ -39,18 +39,14 @@\n public class FlinkOrcReader implements OrcRowReader<RowData> {\n   private final OrcValueReader<?> reader;\n \n-  private FlinkOrcReader(Schema iSchema, TypeDescription readSchema) {\n+  public FlinkOrcReader(Schema iSchema, TypeDescription readSchema) {\n     this(iSchema, readSchema, ImmutableMap.of());\n   }\n \n-  private FlinkOrcReader(Schema iSchema, TypeDescription readSchema, Map<Integer, ?> idToConstant) {\n+  public FlinkOrcReader(Schema iSchema, TypeDescription readSchema, Map<Integer, ?> idToConstant) {\n     this.reader = OrcSchemaWithTypeVisitor.visit(iSchema, readSchema, new ReadBuilder(idToConstant));\n   }\n \n-  public static OrcRowReader<RowData> buildReader(Schema schema, TypeDescription readSchema) {", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk3NDYxMw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476974613", "bodyText": "Because there are two constructors, so we need to have two static helpers, I think we can use constructors directly.", "author": "JingsongLi", "createdAt": "2020-08-26T02:02:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDUzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4Mzg2Mg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476983862", "bodyText": "It is just used by testing code.", "author": "JingsongLi", "createdAt": "2020-08-26T02:16:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDUzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDg3MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476930870", "bodyText": "Does Flink require this?", "author": "rdblue", "createdAt": "2020-08-26T00:57:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/data/FlinkOrcReaders.java", "diffHunk": "@@ -127,6 +128,11 @@ public Integer nonNullRead(ColumnVector vector, int row) {\n     @Override\n     public DecimalData nonNullRead(ColumnVector vector, int row) {\n       HiveDecimalWritable value = ((DecimalColumnVector) vector).vector[row];\n+\n+      // The hive ORC writer may will adjust the scale of decimal data.\n+      Preconditions.checkArgument(value.precision() <= precision,\n+          \"Cannot read value as decimal(%s,%s), too large: %s\", precision, scale, value);", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk3NzYzMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476977632", "bodyText": "I think it is better to add this check to avoid potential precision mismatched bugs.", "author": "JingsongLi", "createdAt": "2020-08-26T02:06:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMDg3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzMzk3Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476933977", "bodyText": "Nit: int in millis?", "author": "rdblue", "createdAt": "2020-08-26T01:02:11Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {\n+          return ((GenericData.Fixed) value).bytes();\n+        }\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case BINARY: // byte[]\n+        return ByteBuffers.toByteArray((ByteBuffer) value);\n+      case TIME: // int instead of long", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNDYxOA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476934618", "bodyText": "Is this needed?", "author": "rdblue", "createdAt": "2020-08-26T01:03:03Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk4MTQ1NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r476981455", "bodyText": "For a stream computing job, I think it's better to have it. If there is a Job running in the cluster, in the future, if user update Iceberg-Flink version, this version modify something that does not affect compatibility, but resulting in a change to serialVersionUID, the user's job will be incompatible after the cluster upgrade. In fact, this situation is compatible.", "author": "JingsongLi", "createdAt": "2020-08-26T02:12:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNDYxOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwMzU1NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487603555", "bodyText": "Is Flink using Java serialization across versions? That seems like a big risk to me. I'd prefer to only use Java serialization between processes running the exact same version of Iceberg. If we need to serialize across versions (like for checkpoint data) then I think we should worry about compatibility a lot more.", "author": "rdblue", "createdAt": "2020-09-14T01:03:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NjkzNDYxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMDM2MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477000361", "bodyText": "Q:  do you think whether there's need to abstract the common options builder sharing between flink and spark (maybe also hive/pig)  to validate and build those properties into a ScanOptions ?  If sure,  we may finish that in a new separate pr.", "author": "openinx", "createdAt": "2020-08-26T02:48:00Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanOptions.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.Configuration;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class ScanOptions implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final ConfigOption<Long> SNAPSHOT_ID =\n+      ConfigOptions.key(\"snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Boolean> CASE_SENSITIVE =\n+      ConfigOptions.key(\"case-sensitive\").booleanType().defaultValue(false);\n+\n+  public static final ConfigOption<Long> AS_OF_TIMESTAMP =\n+      ConfigOptions.key(\"as-of-timestamp\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> START_SNAPSHOT_ID =\n+      ConfigOptions.key(\"start-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> END_SNAPSHOT_ID =\n+      ConfigOptions.key(\"end-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_SIZE =\n+      ConfigOptions.key(\"split-size\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Integer> SPLIT_LOOKBACK =\n+      ConfigOptions.key(\"split-lookback\").intType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_FILE_OPEN_COST =\n+      ConfigOptions.key(\"split-file-open-cost\").longType().defaultValue(null);\n+\n+  private final boolean caseSensitive;\n+  private final Long snapshotId;\n+  private final Long startSnapshotId;\n+  private final Long endSnapshotId;\n+  private final Long asOfTimestamp;\n+  private final Long splitSize;\n+  private final Integer splitLookback;\n+  private final Long splitOpenFileCost;\n+  private final String nameMapping;\n+\n+  public ScanOptions(boolean caseSensitive, Long snapshotId, Long startSnapshotId, Long endSnapshotId,\n+                     Long asOfTimestamp, Long splitSize, Integer splitLookback, Long splitOpenFileCost,\n+                     String nameMapping) {\n+    this.caseSensitive = caseSensitive;\n+    this.snapshotId = snapshotId;\n+    this.startSnapshotId = startSnapshotId;\n+    this.endSnapshotId = endSnapshotId;\n+    this.asOfTimestamp = asOfTimestamp;\n+    this.splitSize = splitSize;\n+    this.splitLookback = splitLookback;\n+    this.splitOpenFileCost = splitOpenFileCost;\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  public boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  public Long getSnapshotId() {\n+    return snapshotId;\n+  }\n+\n+  public Long getStartSnapshotId() {\n+    return startSnapshotId;\n+  }\n+\n+  public Long getEndSnapshotId() {\n+    return endSnapshotId;\n+  }\n+\n+  public Long getAsOfTimestamp() {\n+    return asOfTimestamp;\n+  }\n+\n+  public Long getSplitSize() {\n+    return splitSize;\n+  }\n+\n+  public Integer getSplitLookback() {\n+    return splitLookback;\n+  }\n+\n+  public Long getSplitOpenFileCost() {\n+    return splitOpenFileCost;\n+  }\n+\n+  public String getNameMapping() {\n+    return nameMapping;\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static ScanOptions of(Map<String, String> options) {\n+    return builder().options(options).build();\n+  }\n+\n+  public static final class Builder {\n+    private boolean caseSensitive = CASE_SENSITIVE.defaultValue();\n+    private Long snapshotId = SNAPSHOT_ID.defaultValue();\n+    private Long startSnapshotId = START_SNAPSHOT_ID.defaultValue();\n+    private Long endSnapshotId = END_SNAPSHOT_ID.defaultValue();\n+    private Long asOfTimestamp = AS_OF_TIMESTAMP.defaultValue();\n+    private Long splitSize = SPLIT_SIZE.defaultValue();\n+    private Integer splitLookback = SPLIT_LOOKBACK.defaultValue();\n+    private Long splitOpenFileCost = SPLIT_FILE_OPEN_COST.defaultValue();\n+    private String nameMapping;\n+\n+    private Builder() {\n+    }\n+\n+    public Builder options(Map<String, String> options) {\n+      Configuration config = new Configuration();\n+      options.forEach(config::setString);\n+      this.caseSensitive = config.get(CASE_SENSITIVE);\n+      this.snapshotId = config.get(SNAPSHOT_ID);\n+      this.asOfTimestamp = config.get(AS_OF_TIMESTAMP);\n+      this.startSnapshotId = config.get(START_SNAPSHOT_ID);\n+      this.endSnapshotId = config.get(END_SNAPSHOT_ID);\n+      this.splitSize = config.get(SPLIT_SIZE);\n+      this.splitLookback = config.get(SPLIT_LOOKBACK);\n+      this.splitOpenFileCost = config.get(SPLIT_FILE_OPEN_COST);\n+      this.nameMapping = options.get(DEFAULT_NAME_MAPPING);\n+      return this;\n+    }\n+\n+    public Builder caseSensitive(boolean newCaseSensitive) {\n+      this.caseSensitive = newCaseSensitive;\n+      return this;\n+    }\n+\n+    public Builder snapshotId(Long newSnapshotId) {\n+      this.snapshotId = newSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder startSnapshotId(Long newStartSnapshotId) {\n+      this.startSnapshotId = newStartSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder endSnapshotId(Long newEndSnapshotId) {\n+      this.endSnapshotId = newEndSnapshotId;\n+      return this;\n+    }\n+\n+    public Builder asOfTimestamp(Long newAsOfTimestamp) {\n+      this.asOfTimestamp = newAsOfTimestamp;\n+      return this;\n+    }\n+\n+    public Builder splitSize(Long newSplitSize) {\n+      this.splitSize = newSplitSize;\n+      return this;\n+    }\n+\n+    public Builder splitLookback(Integer newSplitLookback) {\n+      this.splitLookback = newSplitLookback;\n+      return this;\n+    }\n+\n+    public Builder splitOpenFileCost(Long newSplitOpenFileCost) {\n+      this.splitOpenFileCost = newSplitOpenFileCost;\n+      return this;\n+    }\n+\n+    public Builder nameMapping(String newNameMapping) {\n+      this.nameMapping = newNameMapping;\n+      return this;\n+    }\n+\n+    public ScanOptions build() {", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA1NjE4OQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477056189", "bodyText": "I don't know, there are some difference between Flink/Spark and Hive, maybe we can try to do something in Spark.", "author": "JingsongLi", "createdAt": "2020-08-26T06:10:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMDM2MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMzU2OQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477003569", "bodyText": "Q: Do we need more cases to address the other partitioned data type in https://github.com/apache/iceberg/pull/1346/files#diff-84728688cba8556f9ff91f32d3873efcR112 ?", "author": "openinx", "createdAt": "2020-08-26T02:59:55Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,378 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  // before variables\n+  private Configuration conf;\n+  String warehouse;\n+  private HadoopCatalog catalog;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return executeWithOptions(table, null, null, null, null, null, null, null, null);\n+  }\n+\n+  private List<Row> execute(Table table, List<String> projectFields) throws IOException {\n+    return executeWithOptions(table, projectFields, null, null, null, null, null, null, null);\n+  }\n+\n+  protected abstract List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId,\n+      Long startSnapshotId, Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter)\n+      throws IOException;\n+\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> inputRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), inputRecords);\n+    assertRows(execute(table, Collections.singletonList(\"data\")), Row.of(inputRecords.get(0).get(0)));\n+  }\n+\n+  @Test\n+  public void testIdentityPartitionProjections() throws Exception {\n+    Schema logSchema = new Schema(", "originalCommit": "caaf5883be2532a1c5a9911e02609184ed02c289", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA1NTc4NA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477055784", "bodyText": "There is testPartitionTypes for this.", "author": "JingsongLi", "createdAt": "2020-08-26T06:09:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMzU2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE0Njk5OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477146998", "bodyText": "We need to move this line out of the for loop ?", "author": "openinx", "createdAt": "2020-08-26T09:01:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public FlinkInputFormat build() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null.\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      // load required fields by table loader.\n+      if (icebergSchema == null || io == null || encryption == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          Table table = loader.loadTable();\n+          this.icebergSchema = table.schema();\n+          this.io = table.io();\n+          this.encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project.\");\n+      }\n+\n+      TableSchema flinkProjectedSchema = projectedSchema;\n+\n+      if (selectedFields != null) {\n+        TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(() -> new IllegalArgumentException(\n+              \"The fields are illegal in projectedFields: \" + selectedFields));\n+          builder.field(column.getName(), column.getType());\n+          flinkProjectedSchema = builder.build();", "originalCommit": "8466f09738198c472132e4307f471b7c5740f9c3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE2NDg1Mw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477164853", "bodyText": "Yes", "author": "JingsongLi", "createdAt": "2020-08-26T09:27:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE0Njk5OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE0Nzg0Mw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477147843", "bodyText": "I think we need to point out which column is missing in the error message .", "author": "openinx", "createdAt": "2020-08-26T09:02:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,254 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public FlinkInputFormat build() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null.\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      // load required fields by table loader.\n+      if (icebergSchema == null || io == null || encryption == null) {\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          Table table = loader.loadTable();\n+          this.icebergSchema = table.schema();\n+          this.io = table.io();\n+          this.encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project.\");\n+      }\n+\n+      TableSchema flinkProjectedSchema = projectedSchema;\n+\n+      if (selectedFields != null) {\n+        TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(() -> new IllegalArgumentException(\n+              \"The fields are illegal in projectedFields: \" + selectedFields));", "originalCommit": "8466f09738198c472132e4307f471b7c5740f9c3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk3Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477177976", "bodyText": "Do we really need those three methods ?  I saw that we would loadTable and override all of the three if anyone is null, that says setting one or two of them won't work in this builder.", "author": "openinx", "createdAt": "2020-08-26T09:49:39Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,255 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  private FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {\n+    this.iterator = new RowDataIterator(split.getTask(), io, encryption, projectedSchema,\n+                                        options.getNameMapping(), options.isCaseSensitive());\n+  }\n+\n+  @Override\n+  public boolean reachedEnd() {\n+    return !iterator.hasNext();\n+  }\n+\n+  @Override\n+  public RowData nextRecord(RowData reuse) {\n+    return iterator.next();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (iterator != null) {\n+      iterator.close();\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static final class Builder {\n+    private TableLoader tableLoader;\n+    private Schema icebergSchema;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private FileIO io;\n+    private EncryptionManager encryption;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private Builder() {\n+    }\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.icebergSchema = newTable.schema();\n+      this.io = newTable.io();\n+      this.encryption = newTable.encryption();\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder icebergSchema(Schema newSchema) {\n+      this.icebergSchema = newSchema;\n+      return this;\n+    }\n+\n+    public Builder io(FileIO newIO) {\n+      this.io = newIO;\n+      return this;\n+    }\n+\n+    public Builder encryption(EncryptionManager newEncryption) {\n+      this.encryption = newEncryption;\n+      return this;\n+    }", "originalCommit": "20a81915385b70ec3de115e043700e3ee4ed8a1c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3ODUxMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477178511", "bodyText": "For me , the table(Table newTable) and tableLoader(TableLoader newLoader) is enough.", "author": "openinx", "createdAt": "2020-08-26T09:50:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE5MTM2MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r477191361", "bodyText": "OK I can remove them.", "author": "JingsongLi", "createdAt": "2020-08-26T10:13:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzE3Nzk3Ng=="}], "type": "inlineReview"}, {"oid": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "url": "https://github.com/apache/iceberg/commit/72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "message": "Flink: Introduce Flink InputFormat", "committedDate": "2020-08-28T02:47:38Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5MzA5NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480593095", "bodyText": "We have a discussion in #1302 for removing UUID. Is this a temporary solution?", "author": "chenjunjiedada", "createdAt": "2020-09-01T02:06:12Z", "path": "flink/src/main/java/org/apache/iceberg/flink/FlinkFixupTypes.java", "diffHunk": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink;\n+\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.FixupTypes;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n+import org.apache.iceberg.types.Types;\n+\n+/**\n+ * The uuid and fixed are converted to the same Flink type. Conversion back can produce only one,\n+ * which may not be correct.\n+ */\n+class FlinkFixupTypes extends FixupTypes {\n+\n+  private FlinkFixupTypes(Schema referenceSchema) {\n+    super(referenceSchema);\n+  }\n+\n+  static Schema fixup(Schema schema, Schema referenceSchema) {\n+    return new Schema(TypeUtil.visit(schema,\n+        new FlinkFixupTypes(referenceSchema)).asStructType().fields());\n+  }\n+\n+  @Override\n+  protected boolean fixupPrimitive(Type.PrimitiveType type, Type source) {\n+    if (type instanceof Types.FixedType) {\n+      int length = ((Types.FixedType) type).length();\n+      return source.typeId() == Type.TypeID.UUID && length == 16;", "originalCommit": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzNTI3Mg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480635272", "bodyText": "Yes", "author": "JingsongLi", "createdAt": "2020-09-01T02:40:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5MzA5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480596868", "bodyText": "Why we need to exhaust tasks?", "author": "chenjunjiedada", "createdAt": "2020-09-01T02:09:55Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator", "originalCommit": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDYzNzUwMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480637502", "bodyText": "This ensures that it does not return data after it is closed. (hasNext return false).", "author": "JingsongLi", "createdAt": "2020-09-01T02:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwMjkyOA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487602928", "bodyText": "Could it also set tasks to null?", "author": "rdblue", "createdAt": "2020-09-14T00:59:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4NjE4Mg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r493986182", "bodyText": "@JingsongLi, could you take a look at this? Do we need to exhaust the iterator, or can we simply discard the reference to it? If it needs to be closed, then we could do that and then set the reference to null.", "author": "rdblue", "createdAt": "2020-09-24T01:24:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk5OTc3Mg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r493999772", "bodyText": "OH, Sorry, I missed the comment, I am OK to set tasks to null, just this removes the final attribute of tasks.", "author": "JingsongLi", "createdAt": "2020-09-24T02:18:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDU5Njg2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3NTEzNA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r480675134", "bodyText": "nit: typo shcema ->schema.", "author": "chenjunjiedada", "createdAt": "2020-09-01T03:09:54Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Test {@link FlinkInputFormat}.\n+ */\n+public class TestFlinkInputFormat extends TestFlinkScan {\n+\n+  private FlinkInputFormat.Builder builder;\n+\n+  public TestFlinkInputFormat(String fileFormat) {\n+    super(fileFormat);\n+  }\n+\n+  @Override\n+  public void before() throws IOException {\n+    super.before();\n+    builder = FlinkInputFormat.builder().tableLoader(TableLoader.fromHadoopTable(warehouse + \"/default/t\"));\n+  }\n+\n+  @Override\n+  protected List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId, Long startSnapshotId,\n+      Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter) throws IOException {\n+    ScanOptions options = ScanOptions.builder().snapshotId(snapshotId).startSnapshotId(startSnapshotId)\n+        .endSnapshotId(endSnapshotId).asOfTimestamp(asOfTimestamp).build();\n+    if (loader != null) {\n+      builder.tableLoader(TableLoader.fromCatalog(loader, TableIdentifier.of(\"default\", \"t\")));\n+    }\n+\n+    return run(builder.select(projectFields).filters(filters).options(options).build());\n+  }\n+\n+  @Override\n+  protected void assertResiduals(\n+      Schema shcema, List<Row> results, List<Record> writeRecords, List<Record> filteredRecords) {\n+    // can not filter the data.\n+    assertRecords(results, writeRecords, shcema);", "originalCommit": "72d8fe47116364d66cfbf46bc703ce3e6d12a7d3", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4MTgyMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486181821", "bodyText": "Q:  is it possible that  we will step into this if block ?  I saw PartitionData will transform the Utf8 to String ?\n\n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/PartitionData.java\n    \n    \n         Line 142\n      in\n      c28d1c8\n    \n    \n    \n    \n\n        \n          \n           data[pos] = value.toString();", "author": "openinx", "createdAt": "2020-09-10T09:04:04Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2NTk0OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488365948", "bodyText": "I think it is good to keep it safe.", "author": "JingsongLi", "createdAt": "2020-09-15T03:49:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4MTgyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4Mjk3MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486182970", "bodyText": "Same question here,  would it be possible that the value is a GenericData.Fixed or ByteBuffer ?  At least the PartitionData will tranform the ByteBuffer to byte[] ?\n\n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/PartitionData.java\n    \n    \n         Line 148\n      in\n      c28d1c8\n    \n    \n    \n    \n\n        \n          \n           data[pos] = bytes;", "author": "openinx", "createdAt": "2020-09-10T09:05:51Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;\n+  private final EncryptionManager encryption;\n+\n+  private CloseableIterator<T> currentIterator;\n+\n+  DataIterator(CombinedScanTask task, FileIO fileIo, EncryptionManager encryption) {\n+    this.tasks = task.files().iterator();\n+    this.fileIo = fileIo;\n+    this.encryption = encryption;\n+    this.currentIterator = CloseableIterator.empty();\n+  }\n+\n+  InputFile getInputFile(FileScanTask task) {\n+    Preconditions.checkArgument(!task.isDataTask(), \"Invalid task type\");\n+    return encryption.decrypt(EncryptedFiles.encryptedInput(\n+        fileIo.newInputFile(task.file().path().toString()),\n+        task.file().keyMetadata()));\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    updateCurrentIterator();\n+    return currentIterator.hasNext();\n+  }\n+\n+  @Override\n+  public T next() {\n+    updateCurrentIterator();\n+    return currentIterator.next();\n+  }\n+\n+  /**\n+   * Updates the current iterator field to ensure that the current Iterator\n+   * is not exhausted.\n+   */\n+  private void updateCurrentIterator() {\n+    try {\n+      while (!currentIterator.hasNext() && tasks.hasNext()) {\n+        currentIterator.close();\n+        currentIterator = openTaskIterator(tasks.next());\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  abstract CloseableIterator<T> openTaskIterator(FileScanTask scanTask) throws IOException;\n+\n+  @Override\n+  public void close() throws IOException {\n+    // close the current iterator\n+    this.currentIterator.close();\n+\n+    // exhaust the task iterator\n+    while (tasks.hasNext()) {\n+      tasks.next();\n+    }\n+  }\n+\n+  static Object convertConstant(Type type, Object value) {\n+    if (value == null) {\n+      return null;\n+    }\n+\n+    switch (type.typeId()) {\n+      case DECIMAL: // DecimalData\n+        Types.DecimalType decimal = (Types.DecimalType) type;\n+        return DecimalData.fromBigDecimal((BigDecimal) value, decimal.precision(), decimal.scale());\n+      case STRING: // StringData\n+        if (value instanceof Utf8) {\n+          Utf8 utf8 = (Utf8) value;\n+          return StringData.fromBytes(utf8.getBytes(), 0, utf8.getByteLength());\n+        }\n+        return StringData.fromString(value.toString());\n+      case FIXED: // byte[]\n+        if (value instanceof byte[]) {\n+          return value;\n+        } else if (value instanceof GenericData.Fixed) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2NTI3Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488365276", "bodyText": "The PartitionData will convert byte[] back to ByteBuffer..\n\n  \n    \n      iceberg/core/src/main/java/org/apache/iceberg/PartitionData.java\n    \n    \n         Line 131\n      in\n      c28d1c8\n    \n    \n    \n    \n\n        \n          \n           if (data[pos] instanceof byte[]) { \n        \n    \n  \n\n\nI think it is good to keep it safe.", "author": "JingsongLi", "createdAt": "2020-09-15T03:46:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4Mjk3MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4NDI0MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486184240", "bodyText": "nit: could we align the assignment order with the arguments order ?", "author": "openinx", "createdAt": "2020-09-10T09:07:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4ODU5NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486188595", "bodyText": "We could also use this source to read multiple snapshots in bounded mode , right ?  Since it's a public interface/method exposed to user, I'd prefer to provide a more detailed javadoc.", "author": "openinx", "createdAt": "2020-09-10T09:14:29Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjE4OTg5Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r486189896", "bodyText": "nit: how about importing this Configuration explicitly in the import part ?  I did not see the duplicated Configuration classes are used in this file.", "author": "openinx", "createdAt": "2020-09-10T09:16:36Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwMzExNg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487603116", "bodyText": "Nit: we mostly use io for FileIO in other places.", "author": "rdblue", "createdAt": "2020-09-14T01:00:57Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/DataIterator.java", "diffHunk": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.util.Iterator;\n+import org.apache.avro.generic.GenericData;\n+import org.apache.avro.util.Utf8;\n+import org.apache.flink.table.data.DecimalData;\n+import org.apache.flink.table.data.StringData;\n+import org.apache.flink.table.data.TimestampData;\n+import org.apache.iceberg.CombinedScanTask;\n+import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.encryption.EncryptedFiles;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ByteBuffers;\n+import org.apache.iceberg.util.DateTimeUtil;\n+\n+/**\n+ * Base class of Flink iterators.\n+ *\n+ * @param <T> is the Java class returned by this iterator whose objects contain one or more rows.\n+ */\n+abstract class DataIterator<T> implements CloseableIterator<T> {\n+\n+  private final Iterator<FileScanTask> tasks;\n+  private final FileIO fileIo;", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDAyMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604022", "bodyText": "So this class functions as both planner and reader?", "author": "rdblue", "createdAt": "2020-09-14T01:06:53Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.api.common.io.DefaultInputSplitAssigner;\n+import org.apache.flink.api.common.io.InputFormat;\n+import org.apache.flink.api.common.io.RichInputFormat;\n+import org.apache.flink.api.common.io.statistics.BaseStatistics;\n+import org.apache.flink.configuration.Configuration;\n+import org.apache.flink.core.io.InputSplitAssigner;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+\n+/**\n+ * Flink {@link InputFormat} for Iceberg.\n+ */\n+public class FlinkInputFormat extends RichInputFormat<RowData, FlinkInputSplit> {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  private final TableLoader tableLoader;\n+  private final Schema projectedSchema;\n+  private final ScanOptions options;\n+  private final List<Expression> filterExpressions;\n+  private final FileIO io;\n+  private final EncryptionManager encryption;\n+  private final SerializableConfiguration serializableConf;\n+\n+  private transient RowDataIterator iterator;\n+\n+  FlinkInputFormat(\n+      TableLoader tableLoader, Schema projectedSchema, FileIO io, EncryptionManager encryption,\n+      List<Expression> filterExpressions, ScanOptions options, SerializableConfiguration serializableConf) {\n+    this.tableLoader = tableLoader;\n+    this.projectedSchema = projectedSchema;\n+    this.options = options;\n+    this.filterExpressions = filterExpressions;\n+    this.io = io;\n+    this.encryption = encryption;\n+    this.serializableConf = serializableConf;\n+  }\n+\n+  @VisibleForTesting\n+  Schema projectedSchema() {\n+    return projectedSchema;\n+  }\n+\n+  @Override\n+  public BaseStatistics getStatistics(BaseStatistics cachedStatistics) {\n+    // Legacy method, not be used.\n+    return null;\n+  }\n+\n+  @Override\n+  public FlinkInputSplit[] createInputSplits(int minNumSplits) throws IOException {\n+    // Called in Job manager, so it is OK to load table from catalog.\n+    tableLoader.open(serializableConf.get());\n+    try (TableLoader loader = tableLoader) {\n+      Table table = loader.loadTable();\n+      FlinkSplitGenerator generator = new FlinkSplitGenerator(table, projectedSchema, options, filterExpressions);\n+      return generator.createInputSplits();\n+    }\n+  }\n+\n+  @Override\n+  public InputSplitAssigner getInputSplitAssigner(FlinkInputSplit[] inputSplits) {\n+    return new DefaultInputSplitAssigner(inputSplits);\n+  }\n+\n+  @Override\n+  public void configure(Configuration parameters) {\n+  }\n+\n+  @Override\n+  public void open(FlinkInputSplit split) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM2NzAxOQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488367019", "bodyText": "Yes,\n\nPlanner: createInputSplits and getInputSplitAssigner will be called in the JobManager\nReader: open, reachedEnd, nextRecord and close will be called in the TaskManager", "author": "JingsongLi", "createdAt": "2020-09-15T03:53:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDMwNQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604305", "bodyText": "It doesn't seem correct to ignore task in equals. This could lead to bugs in testing. For example, if someone uses this to assert that planned splits are equal to expected splits. Is this required by Flink?", "author": "rdblue", "createdAt": "2020-09-14T01:08:37Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkInputSplit.java", "diffHunk": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import org.apache.flink.core.io.InputSplit;\n+import org.apache.flink.core.io.LocatableInputSplit;\n+import org.apache.iceberg.CombinedScanTask;\n+\n+/**\n+ * TODO Implement {@link LocatableInputSplit}.\n+ */\n+public class FlinkInputSplit implements InputSplit {\n+\n+  private final int splitNumber;\n+  private final CombinedScanTask task;\n+\n+  FlinkInputSplit(int splitNumber, CombinedScanTask task) {\n+    this.splitNumber = splitNumber;\n+    this.task = task;\n+  }\n+\n+  @Override\n+  public int getSplitNumber() {\n+    return splitNumber;\n+  }\n+\n+  CombinedScanTask getTask() {\n+    return task;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null || getClass() != o.getClass()) {\n+      return false;\n+    }\n+    FlinkInputSplit that = (FlinkInputSplit) o;\n+    return splitNumber == that.splitNumber;", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3MzYyOA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488373628", "bodyText": "The equals, hashCode and toString are required by special assigner, but in DefaultInputSplitAssigner, we don't need them.\nI think I can remove them.", "author": "JingsongLi", "createdAt": "2020-09-15T04:20:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDMwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDYyMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604622", "bodyText": "We don't normally include the type returned by the builder in the build method unless it is distinguishing between two options (like build and buildUnchecked). Could this just be build?", "author": "rdblue", "createdAt": "2020-09-14T01:10:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTAzMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487611031", "bodyText": "Nevermind, I see that this is an option after all.", "author": "rdblue", "createdAt": "2020-09-14T01:44:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDYyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDgzMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487604832", "bodyText": "Is this needed? If this delegated to when the scan is built, then the scan would do the check and users would get consistent exception messages.", "author": "rdblue", "createdAt": "2020-09-14T01:11:45Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3OTcyNA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488379724", "bodyText": "See below comments.", "author": "JingsongLi", "createdAt": "2020-09-15T04:43:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNDgzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNTAwNA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487605004", "bodyText": "Why project the Flink schema manually rather than using icebergSchema.select(selectedFIelds) and converting the result?", "author": "rdblue", "createdAt": "2020-09-14T01:12:43Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project\");\n+      }\n+\n+      TableSchema projectedTableSchema = projectedSchema;\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+      if (selectedFields != null) {\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(\n+              () -> new IllegalArgumentException(String.format(\"The field(%s) can not be found in the table schema: %s\",\n+                  field, tableSchema)));\n+          builder.field(column.getName(), column.getType());\n+        }\n+        projectedTableSchema = builder.build();", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3OTM0Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488379347", "bodyText": "I think we remove this select, because what Flink SQL want, is an order changed select, instead of using original iceberg table order.\nWe should provide a unified select, so I think we can provide in Flink side now, and we can use project.", "author": "JingsongLi", "createdAt": "2020-09-15T04:42:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYwNTAwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMDY3MQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487610671", "bodyText": "Will SQL use this or select?", "author": "rdblue", "createdAt": "2020-09-14T01:42:44Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODM3MDQ0Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488370446", "bodyText": "SQL use order changed select, because SQL not supports nested fields push down now.\nBut I think should support nested push down in future, then, should use project.\nBut if we provide an original schema ordered select(Set<String>) like TableScan.select, I think SQL can not use this one.", "author": "JingsongLi", "createdAt": "2020-09-15T04:07:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMDY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTE3MA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487611170", "bodyText": "Since these are intended to be called from child classes, should they be protected?", "author": "rdblue", "createdAt": "2020-09-14T01:45:06Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487611604", "bodyText": "How is this different than an unbounded builder? I don't see anything that passes whether the stream should be bounded or unbounded. It seems like this should pass that information so that the input adapter can plan the current table scan, rather than checking for new data later.", "author": "rdblue", "createdAt": "2020-09-14T01:47:13Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableColumn;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table in bounded mode. Reading a snapshot of the table.\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forBounded() {\n+    return new BoundedBuilder();\n+  }\n+\n+  /**\n+   * Source builder to build {@link DataStream}.\n+   */\n+  public abstract static class Builder {\n+    private StreamExecutionEnvironment env;\n+    private Table table;\n+    private TableLoader tableLoader;\n+    private List<String> selectedFields;\n+    private TableSchema projectedSchema;\n+    private ScanOptions options = ScanOptions.builder().build();\n+    private List<Expression> filterExpressions;\n+    private org.apache.hadoop.conf.Configuration hadoopConf;\n+\n+    private RowDataTypeInfo rowTypeInfo;\n+\n+    // -------------------------- Required options -------------------------------\n+\n+    public Builder tableLoader(TableLoader newLoader) {\n+      this.tableLoader = newLoader;\n+      return this;\n+    }\n+\n+    // -------------------------- Optional options -------------------------------\n+\n+    public Builder table(Table newTable) {\n+      this.table = newTable;\n+      return this;\n+    }\n+\n+    public Builder filters(List<Expression> newFilters) {\n+      this.filterExpressions = newFilters;\n+      return this;\n+    }\n+\n+    public Builder project(TableSchema schema) {\n+      this.projectedSchema = schema;\n+      return this;\n+    }\n+\n+    public Builder select(String... fields) {\n+      this.selectedFields = Lists.newArrayList(fields);\n+      return this;\n+    }\n+\n+    public Builder select(List<String> fields) {\n+      this.selectedFields = fields;\n+      return this;\n+    }\n+\n+    public Builder options(ScanOptions newOptions) {\n+      this.options = newOptions;\n+      return this;\n+    }\n+\n+    public Builder hadoopConf(org.apache.hadoop.conf.Configuration newConf) {\n+      this.hadoopConf = newConf;\n+      return this;\n+    }\n+\n+    public Builder env(StreamExecutionEnvironment newEnv) {\n+      this.env = newEnv;\n+      return this;\n+    }\n+\n+    StreamExecutionEnvironment getEnv() {\n+      return env;\n+    }\n+\n+    RowDataTypeInfo getRowTypeInfo() {\n+      return rowTypeInfo;\n+    }\n+\n+    public FlinkInputFormat buildFormat() {\n+      Preconditions.checkNotNull(tableLoader, \"TableLoader should not be null\");\n+\n+      hadoopConf = hadoopConf == null ? FlinkCatalogFactory.clusterHadoopConf() : hadoopConf;\n+\n+      Schema icebergSchema;\n+      FileIO io;\n+      EncryptionManager encryption;\n+      if (table == null) {\n+        // load required fields by table loader.\n+        tableLoader.open(hadoopConf);\n+        try (TableLoader loader = tableLoader) {\n+          table = loader.loadTable();\n+          icebergSchema = table.schema();\n+          io = table.io();\n+          encryption = table.encryption();\n+        } catch (IOException e) {\n+          throw new UncheckedIOException(e);\n+        }\n+      } else {\n+        icebergSchema = table.schema();\n+        io = table.io();\n+        encryption = table.encryption();\n+      }\n+\n+      if (projectedSchema != null && selectedFields != null) {\n+        throw new IllegalArgumentException(\n+            \"Cannot using both requestedSchema and projectedFields to project\");\n+      }\n+\n+      TableSchema projectedTableSchema = projectedSchema;\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema));\n+      if (selectedFields != null) {\n+        TableSchema.Builder builder = TableSchema.builder();\n+        for (String field : selectedFields) {\n+          TableColumn column = tableSchema.getTableColumn(field).orElseThrow(\n+              () -> new IllegalArgumentException(String.format(\"The field(%s) can not be found in the table schema: %s\",\n+                  field, tableSchema)));\n+          builder.field(column.getName(), column.getType());\n+        }\n+        projectedTableSchema = builder.build();\n+      }\n+\n+      rowTypeInfo = RowDataTypeInfo.of((RowType) (projectedTableSchema == null ? tableSchema : projectedTableSchema)\n+              .toRowDataType().getLogicalType());\n+\n+      Schema expectedSchema = icebergSchema;\n+      if (projectedTableSchema != null) {\n+        expectedSchema = FlinkSchemaUtil.convert(icebergSchema, projectedTableSchema);\n+      }\n+\n+      return new FlinkInputFormat(tableLoader, expectedSchema, io, encryption, filterExpressions, options,\n+          new SerializableConfiguration(hadoopConf));\n+    }\n+\n+    public abstract DataStream<RowData> build();\n+  }\n+\n+  private static final class BoundedBuilder extends Builder {\n+    @Override\n+    public DataStream<RowData> build() {\n+      Preconditions.checkNotNull(getEnv(), \"StreamExecutionEnvironment should not be null\");\n+      FlinkInputFormat format = buildFormat();\n+      return getEnv().createInput(format, getRowTypeInfo());", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMjczMw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487612733", "bodyText": "Not having endSnapshot in ScanOptions seems to imply that the scan is unbounded.", "author": "rdblue", "createdAt": "2020-09-14T01:53:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODQwMzYxNg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488403616", "bodyText": "I was thinking about adding a new config option like bounded is true or not.\nAfter reading your comments, I think it's very good. Because streaming jobs have a high probability of setting the starting snapshot ID or timestamp, the strategy can be:\n\nWithout startSnapshotId: Bounded\nWith startSnapshotId and with endSnapshotId: Bounded\nWith startSnapshotId (-1 means unbounded preceding) and Without endSnapshotId: Unbounded\n\nIn this way, we can have a unify builder.", "author": "JingsongLi", "createdAt": "2020-09-15T05:55:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODgxMzQ2OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488813468", "bodyText": "Sounds like a good way to configure this to me, except that we will want to make sure the default is reasonable. For Flink, should the default be unbounded or bounded?", "author": "rdblue", "createdAt": "2020-09-15T16:47:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTEyNTQxOQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r489125419", "bodyText": "I prefer that default is bounded.\n\nBounded mode is more common in attempt and startup of users.\nIn unbounded mode, users often define startSnapshotId . But in bounded mode, endSnapshotID is rare. If default is unbounded, it is hard to define bounded mode.", "author": "JingsongLi", "createdAt": "2020-09-16T02:33:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMTYwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NzYxMjQzMg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r487612432", "bodyText": "I'm not sure if this class should conform to the typical style used by Flink or Iceberg, but in Iceberg, we omit get from getter names because it doesn't add any helpful context and is awkward in non-Java languages where getter methods are named for fields.", "author": "rdblue", "createdAt": "2020-09-14T01:51:34Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/ScanOptions.java", "diffHunk": "@@ -0,0 +1,221 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.Serializable;\n+import java.util.Map;\n+import org.apache.flink.configuration.ConfigOption;\n+import org.apache.flink.configuration.ConfigOptions;\n+import org.apache.flink.configuration.Configuration;\n+\n+import static org.apache.iceberg.TableProperties.DEFAULT_NAME_MAPPING;\n+\n+public class ScanOptions implements Serializable {\n+\n+  private static final long serialVersionUID = 1L;\n+\n+  public static final ConfigOption<Long> SNAPSHOT_ID =\n+      ConfigOptions.key(\"snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Boolean> CASE_SENSITIVE =\n+      ConfigOptions.key(\"case-sensitive\").booleanType().defaultValue(false);\n+\n+  public static final ConfigOption<Long> AS_OF_TIMESTAMP =\n+      ConfigOptions.key(\"as-of-timestamp\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> START_SNAPSHOT_ID =\n+      ConfigOptions.key(\"start-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> END_SNAPSHOT_ID =\n+      ConfigOptions.key(\"end-snapshot-id\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_SIZE =\n+      ConfigOptions.key(\"split-size\").longType().defaultValue(null);\n+\n+  public static final ConfigOption<Integer> SPLIT_LOOKBACK =\n+      ConfigOptions.key(\"split-lookback\").intType().defaultValue(null);\n+\n+  public static final ConfigOption<Long> SPLIT_FILE_OPEN_COST =\n+      ConfigOptions.key(\"split-file-open-cost\").longType().defaultValue(null);\n+\n+  private final boolean caseSensitive;\n+  private final Long snapshotId;\n+  private final Long startSnapshotId;\n+  private final Long endSnapshotId;\n+  private final Long asOfTimestamp;\n+  private final Long splitSize;\n+  private final Integer splitLookback;\n+  private final Long splitOpenFileCost;\n+  private final String nameMapping;\n+\n+  public ScanOptions(boolean caseSensitive, Long snapshotId, Long startSnapshotId, Long endSnapshotId,\n+                     Long asOfTimestamp, Long splitSize, Integer splitLookback, Long splitOpenFileCost,\n+                     String nameMapping) {\n+    this.caseSensitive = caseSensitive;\n+    this.snapshotId = snapshotId;\n+    this.startSnapshotId = startSnapshotId;\n+    this.endSnapshotId = endSnapshotId;\n+    this.asOfTimestamp = asOfTimestamp;\n+    this.splitSize = splitSize;\n+    this.splitLookback = splitLookback;\n+    this.splitOpenFileCost = splitOpenFileCost;\n+    this.nameMapping = nameMapping;\n+  }\n+\n+  public boolean isCaseSensitive() {\n+    return caseSensitive;\n+  }\n+\n+  public Long getSnapshotId() {", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwNjkzMQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488106931", "bodyText": "I think the tests would be much more readable if the ScanOptions builder were used directly. For example, this is hard to understand:\nassertRecords(executeWithOptions(table, null, null, null, snapshotId1, null, null, null, null), expected1, SCHEMA);\nBut you could rewrite that like this:\nScanOptions options = ScanOptions.builder().startSnapshotId(snapshotId1).build();\nassertRecords(executeWithOptions(table, options), expected1, SCHEMA);\nIn addition, passing null into this leaks the default state within the builder into the tests: test authors need to know that passing null for CatalogLoader is supported. I think it is better to let the test authors use the builder pattern.", "author": "rdblue", "createdAt": "2020-09-14T17:34:27Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkInputFormat.java", "diffHunk": "@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.types.Row;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.CatalogLoader;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+/**\n+ * Test {@link FlinkInputFormat}.\n+ */\n+public class TestFlinkInputFormat extends TestFlinkScan {\n+\n+  private FlinkSource.Builder builder;\n+\n+  public TestFlinkInputFormat(String fileFormat) {\n+    super(fileFormat);\n+  }\n+\n+  @Override\n+  public void before() throws IOException {\n+    super.before();\n+    builder = FlinkSource.forBounded().tableLoader(TableLoader.fromHadoopTable(warehouse + \"/default/t\"));\n+  }\n+\n+  @Override\n+  protected List<Row> executeWithOptions(\n+      Table table, List<String> projectFields, CatalogLoader loader, Long snapshotId, Long startSnapshotId,\n+      Long endSnapshotId, Long asOfTimestamp, List<Expression> filters, String sqlFilter) throws IOException {\n+    ScanOptions options = ScanOptions.builder().snapshotId(snapshotId).startSnapshotId(startSnapshotId)\n+        .endSnapshotId(endSnapshotId).asOfTimestamp(asOfTimestamp).build();", "originalCommit": "43309b1dd72c7bf9ae7eacd46d8b3e3d8add0657", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODQwNjY5Ng==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r488406696", "bodyText": "Agree with you.\nI was thinking about how to make it easy for SQL tests to reuse it. SQL testing can also rebuild SQL strings from ScanOptions.", "author": "JingsongLi", "createdAt": "2020-09-15T06:04:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODEwNjkzMQ=="}], "type": "inlineReview"}, {"oid": "69e753a2cef87c39fe9171d617a185edc13548b3", "url": "https://github.com/apache/iceberg/commit/69e753a2cef87c39fe9171d617a185edc13548b3", "message": "Address comments", "committedDate": "2020-09-15T06:43:55Z", "type": "forcePushed"}, {"oid": "e0fce7f3d8087cd4c527ea1b37163ab24b9a7d78", "url": "https://github.com/apache/iceberg/commit/e0fce7f3d8087cd4c527ea1b37163ab24b9a7d78", "message": "Flink: Introduce Flink InputFormat", "committedDate": "2020-09-24T02:19:26Z", "type": "commit"}, {"oid": "b6e9ecd42ed8831efd95459bf21e502aba09d807", "url": "https://github.com/apache/iceberg/commit/b6e9ecd42ed8831efd95459bf21e502aba09d807", "message": "Address comment", "committedDate": "2020-09-24T02:19:26Z", "type": "commit"}, {"oid": "b13ed9b5183f3a4ad3656a3bd8a5bbbc4a5f4745", "url": "https://github.com/apache/iceberg/commit/b13ed9b5183f3a4ad3656a3bd8a5bbbc4a5f4745", "message": "checkstyles", "committedDate": "2020-09-24T02:19:27Z", "type": "commit"}, {"oid": "8d080ad9c7e2a8a30e4797401552c03abc81de96", "url": "https://github.com/apache/iceberg/commit/8d080ad9c7e2a8a30e4797401552c03abc81de96", "message": "Introduce FlinkSource", "committedDate": "2020-09-24T02:19:27Z", "type": "commit"}, {"oid": "3c727faafb1c097b5ac7248f7f575dabe2f38c95", "url": "https://github.com/apache/iceberg/commit/3c727faafb1c097b5ac7248f7f575dabe2f38c95", "message": "Checkstyle", "committedDate": "2020-09-24T02:19:27Z", "type": "commit"}, {"oid": "6c5f830244961f39f355d26529ce7d923908c121", "url": "https://github.com/apache/iceberg/commit/6c5f830244961f39f355d26529ce7d923908c121", "message": "tmp", "committedDate": "2020-09-24T02:19:27Z", "type": "commit"}, {"oid": "e27e42149acabd483967dad6aae00c097d7ea44e", "url": "https://github.com/apache/iceberg/commit/e27e42149acabd483967dad6aae00c097d7ea44e", "message": "Address comments", "committedDate": "2020-09-24T02:19:27Z", "type": "commit"}, {"oid": "e27e42149acabd483967dad6aae00c097d7ea44e", "url": "https://github.com/apache/iceberg/commit/e27e42149acabd483967dad6aae00c097d7ea44e", "message": "Address comments", "committedDate": "2020-09-24T02:19:27Z", "type": "forcePushed"}, {"oid": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "url": "https://github.com/apache/iceberg/commit/45af8c7b3377c310ea367958cbe63c2aadf50bdf", "message": "Set tasks to null in DataIterator", "committedDate": "2020-09-24T02:21:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY4ODEzMA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494688130", "bodyText": "We usually introduce a spin to avoid sleeping for long durations in lots of tests, like this: https://github.com/apache/iceberg/blob/master/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java#L78", "author": "rdblue", "createdAt": "2020-09-25T00:53:29Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  /**\n+   * Schema: [data, nested[f1, f2, f3], id]\n+   * Projection: [nested.f2, data]\n+   * The Flink SQL output: [f2, data]\n+   * The FlinkInputFormat output: [nested[f2], data].\n+   */\n+  protected abstract void assertNestedProjection(Table table, List<Record> records) throws IOException;\n+\n+  @Test\n+  public void testUnpartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 2, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testPartitionedTable() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    expectedRecords.get(0).set(2, \"2020-03-20\");\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), expectedRecords);\n+    assertRecords(execute(table), expectedRecords, SCHEMA);\n+  }\n+\n+  @Test\n+  public void testProjection() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA, SPEC);\n+    List<Record> inputRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).appendToTable(\n+        org.apache.iceberg.TestHelpers.Row.of(\"2020-03-20\", 0), inputRecords);\n+    assertRows(execute(table, Collections.singletonList(\"data\")), Row.of(inputRecords.get(0).get(0)));\n+  }\n+\n+  @Test\n+  public void testIdentityPartitionProjections() throws Exception {\n+    Schema logSchema = new Schema(\n+        Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+        Types.NestedField.optional(2, \"dt\", Types.StringType.get()),\n+        Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+        Types.NestedField.optional(4, \"message\", Types.StringType.get())\n+    );\n+    PartitionSpec spec =\n+        PartitionSpec.builderFor(logSchema).identity(\"dt\").identity(\"level\").build();\n+\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), logSchema, spec);\n+    List<Record> inputRecords = RandomGenericData.generate(logSchema, 10, 0L);\n+\n+    int idx = 0;\n+    AppendFiles append = table.newAppend();\n+    for (Record record : inputRecords) {\n+      record.set(1, \"2020-03-2\" + idx);\n+      record.set(2, Integer.toString(idx));\n+      append.appendFile(new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER).writeFile(\n+          org.apache.iceberg.TestHelpers.Row.of(\"2020-03-2\" + idx, Integer.toString(idx)), ImmutableList.of(record)));\n+      idx += 1;\n+    }\n+    append.commit();\n+\n+    // individual fields\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Collections.singletonList(\"id\"), inputRecords);\n+    // field pairs\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"level\"), inputRecords);\n+    // out-of-order pairs\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"dt\"), inputRecords);\n+    // out-of-order triplets\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"level\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"dt\", \"message\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"dt\", \"message\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"level\", \"message\", \"dt\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"dt\", \"level\"), inputRecords);\n+    validateIdentityPartitionProjections(table, Arrays.asList(\"message\", \"level\", \"dt\"), inputRecords);\n+  }\n+\n+  private void validateIdentityPartitionProjections(Table table, List<String> projectedFields,\n+      List<Record> inputRecords) throws IOException {\n+    List<Row> rows = execute(table, projectedFields);\n+\n+    for (int pos = 0; pos < inputRecords.size(); pos++) {\n+      Record inputRecord = inputRecords.get(pos);\n+      Row actualRecord = rows.get(pos);\n+\n+      for (int i = 0; i < projectedFields.size(); i++) {\n+        String name = projectedFields.get(i);\n+        Assert.assertEquals(\n+            \"Projected field \" + name + \" should match\", inputRecord.getField(name), actualRecord.getField(i));\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSnapshotReads() throws Exception {\n+    Table table = catalog.createTable(TableIdentifier.of(\"default\", \"t\"), SCHEMA);\n+\n+    GenericAppenderHelper helper = new GenericAppenderHelper(table, fileFormat, TEMPORARY_FOLDER);\n+\n+    List<Record> expectedRecords = RandomGenericData.generate(SCHEMA, 1, 0L);\n+    helper.appendToTable(expectedRecords);\n+    long snapshotId = table.currentSnapshot().snapshotId();\n+\n+    long timestampMillis = table.currentSnapshot().timestampMillis();\n+\n+    // produce another timestamp\n+    Thread.sleep(10);", "originalCommit": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDM2Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494690367", "bodyText": "I find it really strange that this is delegated to a subclass, given that it builds a very specific nested projection.\nWhy not make this use a method like execute(Table, List<String>), but pass in the projection instead of a list of fields?\nThen you could keep all of the schema details in the test method here, rather than delegating this assertion. I think it would be cleaner.", "author": "rdblue", "createdAt": "2020-09-25T01:02:32Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;\n+\n+  /**\n+   * Schema: [data, nested[f1, f2, f3], id]\n+   * Projection: [nested.f2, data]\n+   * The Flink SQL output: [f2, data]\n+   * The FlinkInputFormat output: [nested[f2], data].\n+   */\n+  protected abstract void assertNestedProjection(Table table, List<Record> records) throws IOException;", "originalCommit": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczNTU0Mg==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494735542", "bodyText": "I think we can move testNestedProjection to TestFlinkInputFormat, because only InputFormat supports nested push-down, SQL can not.", "author": "JingsongLi", "createdAt": "2020-09-25T04:07:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDM2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDU5NQ==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494690595", "bodyText": "Why is this implemented by the subclass? Couldn't this just call assertRecords directly?", "author": "rdblue", "createdAt": "2020-09-25T01:03:35Z", "path": "flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkScan.java", "diffHunk": "@@ -0,0 +1,385 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.LocalTime;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Locale;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.data.conversion.DataStructureConverter;\n+import org.apache.flink.table.data.conversion.DataStructureConverters;\n+import org.apache.flink.table.types.utils.TypeConversions;\n+import org.apache.flink.test.util.AbstractTestBase;\n+import org.apache.flink.types.Row;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.GenericAppenderHelper;\n+import org.apache.iceberg.data.RandomGenericData;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.RowDataConverter;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.DateTimeUtil;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestFlinkScan extends AbstractTestBase {\n+\n+  private static final Schema SCHEMA = new Schema(\n+          required(1, \"data\", Types.StringType.get()),\n+          required(2, \"id\", Types.LongType.get()),\n+          required(3, \"dt\", Types.StringType.get()));\n+\n+  private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA)\n+          .identity(\"dt\")\n+          .bucket(\"id\", 1)\n+          .build();\n+\n+  private HadoopCatalog catalog;\n+  protected String warehouse;\n+\n+  // parametrized variables\n+  private final FileFormat fileFormat;\n+\n+  @Parameterized.Parameters(name = \"format={0}\")\n+  public static Object[] parameters() {\n+    return new Object[] {\"avro\", \"parquet\", \"orc\"};\n+  }\n+\n+  TestFlinkScan(String fileFormat) {\n+    this.fileFormat = FileFormat.valueOf(fileFormat.toUpperCase(Locale.ENGLISH));\n+  }\n+\n+  @Before\n+  public void before() throws IOException {\n+    File warehouseFile = TEMPORARY_FOLDER.newFolder();\n+    Assert.assertTrue(warehouseFile.delete());\n+    // before variables\n+    Configuration conf = new Configuration();\n+    warehouse = \"file:\" + warehouseFile;\n+    catalog = new HadoopCatalog(conf, warehouse);\n+  }\n+\n+  private List<Row> execute(Table table) throws IOException {\n+    return execute(table, ScanOptions.builder().build());\n+  }\n+\n+  protected abstract List<Row> execute(Table table, List<String> projectFields) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, ScanOptions options) throws IOException;\n+\n+  protected abstract List<Row> execute(Table table, List<Expression> filters, String sqlFilter) throws IOException;\n+\n+  /**\n+   * The Flink SQL has no residuals, because there will be operator to filter all the data that should be filtered.\n+   * But the FlinkInputFormat can't.\n+   */\n+  protected abstract void assertResiduals(Schema schema, List<Row> results, List<Record> writeRecords,\n+                                          List<Record> filteredRecords) throws IOException;", "originalCommit": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDczNTg2NA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r494735864", "bodyText": "This is also because of the difference between InputFormat and SQL(It can actually filter out the data). I think I should put it into a subclass of SQL.", "author": "JingsongLi", "createdAt": "2020-09-25T04:08:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY5MDU5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MDI3OA==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r507540278", "bodyText": "Hi @JingsongLi , I'm testing Iceberg recently. Since the StreamExecutionEnvironment is a must-have parameter for FlinkSource, would it better to put it in the builder's constructor instead of FlinkSource.forRowData().env(xx)?\njust a minor improvement on user experience.", "author": "Jiayi-Liao", "createdAt": "2020-10-19T07:49:46Z", "path": "flink/src/main/java/org/apache/iceberg/flink/source/FlinkSource.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.flink.source;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.util.List;\n+import org.apache.flink.streaming.api.datastream.DataStream;\n+import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.api.TableSchema;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.runtime.typeutils.RowDataTypeInfo;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableScan;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.expressions.Expression;\n+import org.apache.iceberg.flink.FlinkCatalogFactory;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.flink.TableLoader;\n+import org.apache.iceberg.hadoop.SerializableConfiguration;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class FlinkSource {\n+  private FlinkSource() {\n+  }\n+\n+  /**\n+   * Initialize a {@link Builder} to read the data from iceberg table. Equivalent to {@link TableScan}.\n+   * See more options in {@link ScanOptions}.\n+   * <p>\n+   * The Source can be read static data in bounded mode. It can also continuously check the arrival of new data and\n+   * read records incrementally.\n+   * The Bounded and Unbounded depends on the {@link Builder#options(ScanOptions)}:\n+   * <ul>\n+   *   <li>Without startSnapshotId: Bounded</li>\n+   *   <li>With startSnapshotId and with endSnapshotId: Bounded</li>\n+   *   <li>With startSnapshotId (-1 means unbounded preceding) and Without endSnapshotId: Unbounded</li>\n+   * </ul>\n+   * <p>\n+   *\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRowData() {", "originalCommit": "45af8c7b3377c310ea367958cbe63c2aadf50bdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0Njg0Mw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r507546843", "bodyText": "I think it is better to keep builder pattern. And for now, we can create a FlinkInputFormat without env too.", "author": "JingsongLi", "createdAt": "2020-10-19T08:00:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MDI3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0OTY3Nw==", "url": "https://github.com/apache/iceberg/pull/1346#discussion_r507549677", "bodyText": "I see... The builder is for both InputFormat and DataStreamSource.", "author": "Jiayi-Liao", "createdAt": "2020-10-19T08:05:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNzU0MDI3OA=="}], "type": "inlineReview"}]}