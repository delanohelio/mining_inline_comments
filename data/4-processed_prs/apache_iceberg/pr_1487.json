{"pr_number": 1487, "pr_title": "Spark: add E2E test on partition pruning for filter pushdown", "pr_createdAt": "2020-09-23T04:58:23Z", "pr_url": "https://github.com/apache/iceberg/pull/1487", "timeline": [{"oid": "31c639ce2415eed31dad77b1ff1ce3b8b3e80055", "url": "https://github.com/apache/iceberg/commit/31c639ce2415eed31dad77b1ff1ce3b8b3e80055", "message": "Spark: add E2E test on partition pruning for filter pushdown", "committedDate": "2020-09-23T04:53:14Z", "type": "commit"}, {"oid": "67734e5bd8e2ff13284071af986e326e3b6272b5", "url": "https://github.com/apache/iceberg/commit/67734e5bd8e2ff13284071af986e326e3b6272b5", "message": "Fix checkstyle", "committedDate": "2020-09-23T07:03:01Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4Mzg5Mg==", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r493983892", "bodyText": "I don't think it is a good idea for tests to create Timestamp or Date objects because those representations are tied to a time zone in the JVM implementation. If you have to use them, then pass an instant in milliseconds or microseconds that is produced by Iceberg's literals: Literal.of(\"2020-02-02 01:00:00\").to(TimestampType.withoutZone()).value()", "author": "rdblue", "createdAt": "2020-09-24T01:16:14Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java", "diffHunk": "@@ -0,0 +1,354 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.sql.Timestamp;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.transforms.Transforms;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestPartitionPruning {\n+\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\", false },\n+        new Object[] { \"parquet\", true },\n+        new Object[] { \"avro\", false },\n+        new Object[] { \"orc\", false },\n+        new Object[] { \"orc\", true },\n+    };\n+  }\n+\n+  private final String format;\n+  private final boolean vectorized;\n+\n+  public TestPartitionPruning(String format, boolean vectorized) {\n+    this.format = format;\n+    this.vectorized = vectorized;\n+  }\n+\n+  private static SparkSession spark = null;\n+\n+  private static Transform<Object, Integer> bucketTransform = Transforms.bucket(Types.IntegerType.get(), 3);\n+  private static Transform<Object, Object> truncateTransform = Transforms.truncate(Types.StringType.get(), 5);\n+  private static Transform<Object, Integer> hourTransform = Transforms.hour(Types.TimestampType.withZone());\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestPartitionPruning.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    String optionKey = String.format(\"fs.%s.impl\", CountOpenLocalFileSystem.scheme);\n+\n+    CONF.set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.udf().register(\"bucket3\", (Integer num) -> bucketTransform.apply(num), DataTypes.IntegerType);\n+    spark.udf().register(\"truncate5\", (String str) -> truncateTransform.apply(str), DataTypes.StringType);\n+    // NOTE: date transforms take the type long, not Timestamp\n+    spark.udf().register(\"hour\", (Timestamp ts) -> hourTransform.apply(\n+        (Long) org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaTimestamp(ts)),\n+        DataTypes.IntegerType);\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestPartitionPruning.spark;\n+    TestPartitionPruning.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get()),\n+      Types.NestedField.optional(5, \"timestamp\", Types.TimestampType.withZone())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\", Timestamp.valueOf(\"2020-02-02 00:00:00\")),", "originalCommit": "67734e5bd8e2ff13284071af986e326e3b6272b5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDA0OTc3OA==", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r494049778", "bodyText": "Looks like at least in Spark 2.4, having Java 8 Instant as field directly won't work via Java type inference on Java beans.\nSo if I understand correctly, we should either pass long or Timestamp to Spark on timestamp column, and TestSparkDataFile looks to leverage long. Would it be OK to change this to store long (using your suggestion to get microseconds, and probably convert to milliseconds due to Spark 2.4), and pass the long value to Spark?", "author": "HeartSaVioR", "createdAt": "2020-09-24T05:37:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4Mzg5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDExNzQzNw==", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r494117437", "bodyText": "Just dealt with 7429dc0 - not sure I followed your suggestion properly.", "author": "HeartSaVioR", "createdAt": "2020-09-24T08:04:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Mzk4Mzg5Mg=="}], "type": "inlineReview"}, {"oid": "27617d8affb39284401a15486759495bc65096f6", "url": "https://github.com/apache/iceberg/commit/27617d8affb39284401a15486759495bc65096f6", "message": "Change the test code to be resilient on concurrent test runs", "committedDate": "2020-09-24T03:28:10Z", "type": "commit"}, {"oid": "572ecf1964e871bbafb650ec0acde6a12d92b002", "url": "https://github.com/apache/iceberg/commit/572ecf1964e871bbafb650ec0acde6a12d92b002", "message": "Add some more hints, as I see Travis is complaining on JDK 8", "committedDate": "2020-09-24T04:59:40Z", "type": "commit"}, {"oid": "67adafef46a94de63757ebddf9d763dc7b76ce43", "url": "https://github.com/apache/iceberg/commit/67adafef46a94de63757ebddf9d763dc7b76ce43", "message": "Fix test failures on sequential order of test executions across suites", "committedDate": "2020-09-24T07:04:24Z", "type": "commit"}, {"oid": "7429dc0e4b173686bec097ae4c3eecb9e9c55b66", "url": "https://github.com/apache/iceberg/commit/7429dc0e4b173686bec097ae4c3eecb9e9c55b66", "message": "Use java.time.Instant instead of java.sql.Timestamp", "committedDate": "2020-09-24T08:02:12Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU3NjU4NQ==", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r494576585", "bodyText": "Why does this create its own random number instead of just getting a new temp folder? I don't see much benefit to doing it this way.", "author": "rdblue", "createdAt": "2020-09-24T19:59:21Z", "path": "spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.source;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.sql.Timestamp;\n+import java.time.Instant;\n+import java.util.Arrays;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.expressions.Literal;\n+import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.transforms.Transform;\n+import org.apache.iceberg.transforms.Transforms;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+\n+@RunWith(Parameterized.class)\n+public abstract class TestPartitionPruning {\n+\n+  private static final Configuration CONF = new Configuration();\n+  private static final HadoopTables TABLES = new HadoopTables(CONF);\n+\n+  @Parameterized.Parameters\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+        new Object[] { \"parquet\", false },\n+        new Object[] { \"parquet\", true },\n+        new Object[] { \"avro\", false },\n+        new Object[] { \"orc\", false },\n+        new Object[] { \"orc\", true },\n+    };\n+  }\n+\n+  private final String format;\n+  private final boolean vectorized;\n+\n+  public TestPartitionPruning(String format, boolean vectorized) {\n+    this.format = format;\n+    this.vectorized = vectorized;\n+  }\n+\n+  private static SparkSession spark = null;\n+  private static JavaSparkContext sparkContext = null;\n+\n+  private static Transform<Object, Integer> bucketTransform = Transforms.bucket(Types.IntegerType.get(), 3);\n+  private static Transform<Object, Object> truncateTransform = Transforms.truncate(Types.StringType.get(), 5);\n+  private static Transform<Object, Integer> hourTransform = Transforms.hour(Types.TimestampType.withoutZone());\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestPartitionPruning.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    TestPartitionPruning.sparkContext = new JavaSparkContext(spark.sparkContext());\n+\n+    String optionKey = String.format(\"fs.%s.impl\", CountOpenLocalFileSystem.scheme);\n+    CONF.set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(optionKey, CountOpenLocalFileSystem.class.getName());\n+    spark.conf().set(\"spark.sql.session.timeZone\", \"UTC\");\n+    spark.udf().register(\"bucket3\", (Integer num) -> bucketTransform.apply(num), DataTypes.IntegerType);\n+    spark.udf().register(\"truncate5\", (String str) -> truncateTransform.apply(str), DataTypes.StringType);\n+    // NOTE: date transforms take the type long, not Timestamp\n+    spark.udf().register(\"hour\", (Timestamp ts) -> hourTransform.apply(\n+        org.apache.spark.sql.catalyst.util.DateTimeUtils.fromJavaTimestamp(ts)),\n+        DataTypes.IntegerType);\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestPartitionPruning.spark;\n+    TestPartitionPruning.spark = null;\n+    currentSpark.stop();\n+  }\n+\n+  private static final Schema LOG_SCHEMA = new Schema(\n+      Types.NestedField.optional(1, \"id\", Types.IntegerType.get()),\n+      Types.NestedField.optional(2, \"date\", Types.StringType.get()),\n+      Types.NestedField.optional(3, \"level\", Types.StringType.get()),\n+      Types.NestedField.optional(4, \"message\", Types.StringType.get()),\n+      Types.NestedField.optional(5, \"timestamp\", Types.TimestampType.withZone())\n+  );\n+\n+  private static final List<LogMessage> LOGS = ImmutableList.of(\n+      LogMessage.debug(\"2020-02-02\", \"debug event 1\", getInstant(\"2020-02-02T00:00:00\")),\n+      LogMessage.info(\"2020-02-02\", \"info event 1\", getInstant(\"2020-02-02T01:00:00\")),\n+      LogMessage.debug(\"2020-02-02\", \"debug event 2\", getInstant(\"2020-02-02T02:00:00\")),\n+      LogMessage.info(\"2020-02-03\", \"info event 2\", getInstant(\"2020-02-03T00:00:00\")),\n+      LogMessage.debug(\"2020-02-03\", \"debug event 3\", getInstant(\"2020-02-03T01:00:00\")),\n+      LogMessage.info(\"2020-02-03\", \"info event 3\", getInstant(\"2020-02-03T02:00:00\")),\n+      LogMessage.error(\"2020-02-03\", \"error event 1\", getInstant(\"2020-02-03T03:00:00\")),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 4\", getInstant(\"2020-02-04T01:00:00\")),\n+      LogMessage.warn(\"2020-02-04\", \"warn event 1\", getInstant(\"2020-02-04T02:00:00\")),\n+      LogMessage.debug(\"2020-02-04\", \"debug event 5\", getInstant(\"2020-02-04T03:00:00\"))\n+  );\n+\n+  private static Instant getInstant(String timestampWithoutZone) {\n+    Long epochMicros = (Long) Literal.of(timestampWithoutZone).to(Types.TimestampType.withoutZone()).value();\n+    return Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(epochMicros));\n+  }\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private PartitionSpec spec = PartitionSpec.builderFor(LOG_SCHEMA)\n+      .identity(\"date\")\n+      .identity(\"level\")\n+      .bucket(\"id\", 3)\n+      .truncate(\"message\", 5)\n+      .hour(\"timestamp\")\n+      .build();\n+\n+  private Random random = new Random();\n+\n+  @Test\n+  public void testPartitionPruningIdentityString() {\n+    String filterCond = \"date >= '2020-02-03' AND level = 'DEBUG'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String date = r.getString(0);\n+      String level = r.getString(1);\n+      return date.compareTo(\"2020-02-03\") >= 0 && level.equals(\"DEBUG\");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningBucketingInteger() {\n+    final int[] ids = new int[]{\n+        LOGS.get(3).getId(),\n+        LOGS.get(7).getId()\n+    };\n+    String condForIds = Arrays.stream(ids).mapToObj(String::valueOf)\n+        .collect(Collectors.joining(\",\", \"(\", \")\"));\n+    String filterCond = \"id in \" + condForIds;\n+    Predicate<Row> partCondition = (Row r) -> {\n+      int bucketId = r.getInt(2);\n+      Set<Integer> buckets = Arrays.stream(ids).map(bucketTransform::apply)\n+          .boxed().collect(Collectors.toSet());\n+      return buckets.contains(bucketId);\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningTruncatedString() {\n+    String filterCond = \"message like 'info event%'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String truncatedMessage = r.getString(3);\n+      return truncatedMessage.equals(\"info \");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningTruncatedStringComparingValueShorterThanPartitionValue() {\n+    String filterCond = \"message like 'inf%'\";\n+    Predicate<Row> partCondition = (Row r) -> {\n+      String truncatedMessage = r.getString(3);\n+      return truncatedMessage.startsWith(\"inf\");\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  @Test\n+  public void testPartitionPruningHourlyPartition() {\n+    String filterCond;\n+    if (spark.version().startsWith(\"2\")) {\n+      // Looks like from Spark 2 we need to compare timestamp with timestamp to push down the filter.\n+      filterCond = \"timestamp >= to_timestamp('2020-02-03T01:00:00')\";\n+    } else {\n+      filterCond = \"timestamp >= '2020-02-03T01:00:00'\";\n+    }\n+    Predicate<Row> partCondition = (Row r) -> {\n+      int hourValue = r.getInt(4);\n+      Instant instant = getInstant(\"2020-02-03T01:00:00\");\n+      Integer hourValueToFilter = hourTransform.apply(TimeUnit.MILLISECONDS.toMicros(instant.toEpochMilli()));\n+      return hourValue >= hourValueToFilter;\n+    };\n+\n+    runTest(filterCond, partCondition);\n+  }\n+\n+  private void runTest(String filterCond, Predicate<Row> partCondition) {\n+    File originTableLocation = createTempDir();\n+    Assert.assertTrue(\"Temp folder should exist\", originTableLocation.exists());\n+\n+    Table table = createTable(originTableLocation);\n+    Dataset<Row> logs = createTestDataset();\n+    saveTestDatasetToTable(logs, table);\n+\n+    List<Row> expected = logs\n+        .select(\"id\", \"date\", \"level\", \"message\", \"timestamp\")\n+        .filter(filterCond)\n+        .orderBy(\"id\")\n+        .collectAsList();\n+    Assert.assertFalse(\"Expected rows should be not empty\", expected.isEmpty());\n+\n+    // remove records which may be recorded during storing to table\n+    CountOpenLocalFileSystem.resetRecordsInPathPrefix(originTableLocation.getAbsolutePath());\n+\n+    List<Row> actual = spark.read()\n+        .format(\"iceberg\")\n+        .option(\"vectorization-enabled\", String.valueOf(vectorized))\n+        .load(table.location())\n+        .select(\"id\", \"date\", \"level\", \"message\", \"timestamp\")\n+        .filter(filterCond)\n+        .orderBy(\"id\")\n+        .collectAsList();\n+    Assert.assertFalse(\"Actual rows should not be empty\", actual.isEmpty());\n+\n+    Assert.assertEquals(\"Rows should match\", expected, actual);\n+\n+    assertAccessOnDataFiles(originTableLocation, table, partCondition);\n+  }\n+\n+  private File createTempDir() {\n+    try {\n+      int rand = random.nextInt(1000000);\n+      return temp.newFolder(String.format(\"logs-%d\", rand));", "originalCommit": "7429dc0e4b173686bec097ae4c3eecb9e9c55b66", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDYxNjg5MA==", "url": "https://github.com/apache/iceberg/pull/1487#discussion_r494616890", "bodyText": "I didn't realize temp.newFolder() would just work for creating unique dir. My bad.", "author": "HeartSaVioR", "createdAt": "2020-09-24T21:19:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDU3NjU4NQ=="}], "type": "inlineReview"}, {"oid": "7768df4803b637eb5d1e6c6cfeb2105d947e4582", "url": "https://github.com/apache/iceberg/commit/7768df4803b637eb5d1e6c6cfeb2105d947e4582", "message": "Fix", "committedDate": "2020-09-24T21:18:36Z", "type": "commit"}]}