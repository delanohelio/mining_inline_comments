{"pr_number": 786, "pr_title": "replace SparkDataFile with DataFile", "pr_createdAt": "2020-02-07T13:43:18Z", "pr_url": "https://github.com/apache/iceberg/pull/786", "timeline": [{"oid": "d06c0724d5f02c55f479eafe8dfe21a471272626", "url": "https://github.com/apache/iceberg/commit/d06c0724d5f02c55f479eafe8dfe21a471272626", "message": "replace SparkDataFile with DataFile", "committedDate": "2020-02-07T13:42:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5ODM4MA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377398380", "bodyText": "I think it will be a good idea to have the spec argument before metricsConfig and conf as the last two are optional. I would make it consistent in all touched files (i.e. first spec then config).\n  def listPartition(\n      partition: Map[String, String],\n      uri: String,\n      format: String,\n      spec: PartitionSpec,\n      conf: Configuration = new Configuration(),\n      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[DataFile] = {", "author": "aokolnychyi", "createdAt": "2020-02-11T00:37:23Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -174,22 +175,23 @@ object SparkTableUtil {\n    * @param format partition format, avro or parquet\n    * @param conf a Hadoop conf\n    * @param metricsConfig a metrics conf\n-   * @return a seq of [[SparkDataFile]]\n+   * @return a seq of [[DataFile]]\n    */\n   def listPartition(\n       partition: Map[String, String],\n       uri: String,\n       format: String,\n       conf: Configuration = new Configuration(),\n-      metricsConfig: MetricsConfig = MetricsConfig.getDefault): Seq[SparkDataFile] = {\n+      metricsConfig: MetricsConfig = MetricsConfig.getDefault,\n+      spec: PartitionSpec): Seq[DataFile] = {", "originalCommit": "d06c0724d5f02c55f479eafe8dfe21a471272626", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMTAwNQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377411005", "bodyText": "Make sense, will do.", "author": "chenjunjiedada", "createdAt": "2020-02-11T01:27:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzM5ODM4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMjQ2NA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377402464", "bodyText": "Do we still need these methods?", "author": "aokolnychyi", "createdAt": "2020-02-11T00:52:38Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -200,50 +202,6 @@ object SparkTableUtil {\n    */\n   case class SparkPartition(values: Map[String, String], uri: String, format: String)\n \n-  /**\n-   * Case class representing a data file.\n-   */\n-  case class SparkDataFile(\n-      path: String,\n-      partition: collection.Map[String, String],\n-      format: String,\n-      fileSize: Long,\n-      rowGroupSize: Long,\n-      rowCount: Long,\n-      columnSizes: Array[Long],\n-      valueCounts: Array[Long],\n-      nullValueCounts: Array[Long],\n-      lowerBounds: Seq[Array[Byte]],\n-      upperBounds: Seq[Array[Byte]]\n-    ) {\n-\n-    /**\n-     * Convert this to a [[DataFile]] that can be added to a [[org.apache.iceberg.Table]].\n-     *\n-     * @param spec a [[PartitionSpec]] that will be used to parse the partition key\n-     * @return a [[DataFile]] that can be passed to [[org.apache.iceberg.AppendFiles]]\n-     */\n-    def toDataFile(spec: PartitionSpec): DataFile = {\n-      // values are strings, so pass a path to let the builder coerce to the right types\n-      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n-        s\"$name=${partition(name)}\"\n-      }.mkString(\"/\")\n-\n-      DataFiles.builder(spec)\n-        .withPath(path)\n-        .withFormat(format)\n-        .withFileSizeInBytes(fileSize)\n-        .withMetrics(new Metrics(rowCount,\n-          arrayToMap(columnSizes),\n-          arrayToMap(valueCounts),\n-          arrayToMap(nullValueCounts),\n-          arrayToMap(lowerBounds),\n-          arrayToMap(upperBounds)))\n-        .withPartitionPath(partitionKey)\n-        .build()\n-    }\n-  }\n-\n   private def bytesMapToArray(map: java.util.Map[Integer, ByteBuffer]): Seq[Array[Byte]] = {", "originalCommit": "d06c0724d5f02c55f479eafe8dfe21a471272626", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMzE4OA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377413188", "bodyText": "No, will delete them.", "author": "chenjunjiedada", "createdAt": "2020-02-11T01:38:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMjQ2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMzQwMw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377403403", "bodyText": "What about initializing metrics before as a separate variable?\n    fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))\n      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n        s\"$name=${partitionPath(name)}\"\n      }.mkString(\"/\")\n\n      DataFiles.builder(spec)\n        .withPath(stat.getPath.toString)\n        .withFormat(\"avro\")\n        .withFileSizeInBytes(stat.getLen)\n        .withMetrics(metrics)\n        .withPartitionPath(partitionKey)\n        .build()\n    }", "author": "aokolnychyi", "createdAt": "2020-02-11T00:56:04Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,21 +287,26 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      conf: Configuration,\n+      spec: PartitionSpec): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val partitionKey = spec.fields.asScala.map(_.name).map { name =>\n+        s\"$name=${partitionPath(name)}\"\n+      }.mkString(\"/\")\n+\n+      DataFiles.builder(spec)\n+        .withPath(stat.getPath.toString)\n+        .withFormat(\"avro\")\n+        .withFileSizeInBytes(stat.getLen)\n+        .withMetrics(new Metrics(-1L,", "originalCommit": "d06c0724d5f02c55f479eafe8dfe21a471272626", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQxMjY5Nw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377412697", "bodyText": "Sure, it looks better.", "author": "chenjunjiedada", "createdAt": "2020-02-11T01:35:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwMzQwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3NzQwNDk4OQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r377404989", "bodyText": "What about this?\nval partition = Map.empty[String, String]\nval spec = PartitionSpec.unpartitioned()\nval conf = spark.sessionState.newHadoopConf()\nval metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n\nval files = listPartition(partition, sourceTable.location.toString, format.get, spec, conf, metricsConfig)", "author": "aokolnychyi", "createdAt": "2020-02-11T01:01:40Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -492,10 +454,11 @@ object SparkTableUtil {\n     val conf = spark.sessionState.newHadoopConf()\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n-    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig)\n+    val files = listPartition(Map.empty, sourceTable.location.toString, format.get, conf, metricsConfig,", "originalCommit": "d06c0724d5f02c55f479eafe8dfe21a471272626", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "db85445e56496734bf596a9f49d229424df549a7", "url": "https://github.com/apache/iceberg/commit/db85445e56496734bf596a9f49d229424df549a7", "message": "minor changes", "committedDate": "2020-02-11T01:43:37Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379614197", "bodyText": "Why do we want to remove the sort? I think it is needed to collocate files for the same partition next to each other so that partition skipping is quick.", "author": "aokolnychyi", "createdAt": "2020-02-14T19:48:36Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -527,9 +425,8 @@ object SparkTableUtil {\n     val metricsConfig = MetricsConfig.fromProperties(targetTable.properties)\n \n     val manifests = partitionDS\n-      .flatMap(partition => listPartition(partition, serializableConf, metricsConfig))\n+      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n       .repartition(numShufflePartitions)\n-      .orderBy($\"path\")", "originalCommit": "db85445e56496734bf596a9f49d229424df549a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTc3NTMxMw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379775313", "bodyText": "The GenericDataFile is not a case class so it doesn't have a schema with its fields. It just has one column named value. I tried to construct an extra column with UDF like:\n    val toPath = org.apache.spark.sql.functions.udf {s:DataFile => s.path().toString}\n\n    val manifests = partitionDS\n      .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n      .repartition(numShufflePartitions)\n      .orderBy(toPath($\"value\"))\n      .mapPartitions(buildManifest(serializableConf, spec, stagingDir))\n      .collect()\n\nbut it throws\njava.lang.ClassCastException: [B cannot be cast to org.apache.iceberg.DataFile. So I remove this, do you have any idea on this?", "author": "chenjunjiedada", "createdAt": "2020-02-15T07:03:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk2MTgyMQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379961821", "bodyText": "BTW, even we coalesce files in a manifest for same partition, I think we still have to iterate through all manifest entries in the manifest for partition skipping.  Please correct me if I am wrong.", "author": "chenjunjiedada", "createdAt": "2020-02-17T02:21:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMwNzMzOA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380307338", "bodyText": "What about this? It is a bit uglier than what we had before but should work.\nimplicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\nimplicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\nimplicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING, dataFileEncoder)\n\n....\n\nval manifests = partitionDS\n  .flatMap(partition => listPartition(partition, spec, serializableConf, metricsConfig))\n  .repartition(numShufflePartitions)\n  .map(file => (file.path.toString, file))\n  .orderBy($\"_1\")\n  .mapPartitions(files => buildManifest(serializableConf, spec, stagingDir)(files.map(_._2)))\n  .collect()\n\nThe reason sort is important is because we want to filter out irrelevant manifests when planning a job with a partition predicate. Each manifest keeps lower/upper bounds for partition values. If we place all files for the same partition in one manifest, we will need to read only one manifest to plan a job for that partition. If we don't sort files before creating manifests, we risk having files for one partition scattered across many manifests.", "author": "aokolnychyi", "createdAt": "2020-02-17T17:41:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNzA3Ng==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380407076", "bodyText": "Nice, I tried your way but forgot to define the tuple encoder. Thanks for the detailed explanation.", "author": "chenjunjiedada", "createdAt": "2020-02-18T00:34:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTYxNDE5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379925255", "bodyText": "Shouldn't rowCount be a positive number?", "author": "rdsr", "createdAt": "2020-02-16T19:02:14Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -329,70 +222,74 @@ object SparkTableUtil {\n   private def listAvroPartition(\n       partitionPath: Map[String, String],\n       partitionUri: String,\n-      conf: Configuration): Seq[SparkDataFile] = {\n+      spec: PartitionSpec,\n+      conf: Configuration): Seq[DataFile] = {\n     val partition = new Path(partitionUri)\n     val fs = partition.getFileSystem(conf)\n \n     fs.listStatus(partition, HiddenPathFilter).filter(_.isFile).map { stat =>\n-      SparkDataFile(\n-        stat.getPath.toString,\n-        partitionPath, \"avro\", stat.getLen,\n-        stat.getBlockSize,\n-        -1,\n-        null,\n-        null,\n-        null,\n-        null,\n-        null)\n+      val metrics = new Metrics(-1L, arrayToMap(null), arrayToMap(null), arrayToMap(null))", "originalCommit": "db85445e56496734bf596a9f49d229424df549a7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTk1Njc0NQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r379956745", "bodyText": "I think the metric is not intended to be used so it is set to an invalid value. We might need to read through whole file to get row count, right?", "author": "chenjunjiedada", "createdAt": "2020-02-17T01:45:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDI2ODkxMw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380268913", "bodyText": "Let's try to keep the logic as close to what we had before as possible.", "author": "aokolnychyi", "createdAt": "2020-02-17T16:13:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDI3Mjg2MQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380272861", "bodyText": "I think anything positive should do. keeping it <= 0 may possibly affect some scan planning code to filter out this particular file. e.g see org.apache.iceberg.expressions.InclusiveMetricsEvaluator\n@aokolnychyi , thoughts?", "author": "rdsr", "createdAt": "2020-02-17T16:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyMDcxNA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380320714", "bodyText": "Good catch, @rdsr. This is definitely a problem. Right now, the InclusiveMetricsEvaluator will remove files with negative or 0 row counts.\nI don't think that the solution is to use a positive number here. The reason why this was required is that we want good stats for job planning. Setting this to -1 causes a correctness bug, but setting it to some other constant will introduce bad behavior when using the stats that are provided by Iceberg. I think we should either count the number of records, use a heuristic (file size / est. row size?), or remove support for importing Avro tables. I'm leaning toward counting the number of records.\nWe should also change the check in InclusiveMetricsEvaluator to check for files with 0 rows and allow files with -1 rows through to fix the correctness bug for existing tables that used this path to import Avro data.", "author": "rdblue", "createdAt": "2020-02-17T18:22:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyODY5OA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380328698", "bodyText": "I think the number of records must be correct and precise as we want to answer some data queries with metadata (e.g. give me the number of records per partition). Updating our metrics evaluators to handle -1 seems reasonable to me as well.", "author": "aokolnychyi", "createdAt": "2020-02-17T18:49:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMyOTUzMw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380329533", "bodyText": "@rdsr, could you create follow-up issues so that we don't forget?", "author": "aokolnychyi", "createdAt": "2020-02-17T18:52:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDMzNzA1Mw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380337053", "bodyText": "+1 I'll do that!", "author": "rdsr", "createdAt": "2020-02-17T19:18:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQwNzQ2Mw==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380407463", "bodyText": "Thank you guys for the detail explanation!", "author": "chenjunjiedada", "createdAt": "2020-02-18T00:36:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQxMDk2OA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380410968", "bodyText": "Created #809 to track this.", "author": "rdsr", "createdAt": "2020-02-18T00:58:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM3OTkyNTI1NQ=="}], "type": "inlineReview"}, {"oid": "2e0f4102aa11b6a2be56da23949f9ca81076923b", "url": "https://github.com/apache/iceberg/commit/2e0f4102aa11b6a2be56da23949f9ca81076923b", "message": "sort data file by path", "committedDate": "2020-02-18T00:44:47Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1NjM1OQ==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380456359", "bodyText": "nit: this can fit into 1 line if we reuse dataFileEncoder:\nimplicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING, dataFileEncoder)", "author": "aokolnychyi", "createdAt": "2020-02-18T05:03:03Z", "path": "spark/src/main/scala/org/apache/iceberg/spark/SparkTableUtil.scala", "diffHunk": "@@ -516,6 +413,9 @@ object SparkTableUtil {\n       stagingDir: String): Unit = {\n \n     implicit val manifestFileEncoder: Encoder[ManifestFile] = Encoders.javaSerialization[ManifestFile]\n+    implicit val dataFileEncoder: Encoder[DataFile] = Encoders.javaSerialization[DataFile]\n+    implicit val pathDataFileEncoder: Encoder[(String, DataFile)] = Encoders.tuple(Encoders.STRING,", "originalCommit": "2e0f4102aa11b6a2be56da23949f9ca81076923b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ2Mjg3MA==", "url": "https://github.com/apache/iceberg/pull/786#discussion_r380462870", "bodyText": "Oh, my bad.", "author": "chenjunjiedada", "createdAt": "2020-02-18T05:37:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4MDQ1NjM1OQ=="}], "type": "inlineReview"}, {"oid": "daca7f1725b81bc13c0bacfae5419e2ba323eca6", "url": "https://github.com/apache/iceberg/commit/daca7f1725b81bc13c0bacfae5419e2ba323eca6", "message": "fix nit", "committedDate": "2020-02-18T05:33:22Z", "type": "commit"}]}