{"pr_number": 1854, "pr_title": "Hive: Implement Deserializer for Hive writes", "pr_createdAt": "2020-12-01T15:59:59Z", "pr_url": "https://github.com/apache/iceberg/pull/1854", "timeline": [{"oid": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "url": "https://github.com/apache/iceberg/commit/2a85e723c2043c57c93a6e084c76935fe9dc740d", "message": "Hive: Implement Deserializer for Hive writes", "committedDate": "2020-12-01T15:58:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI0OTI1MA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534249250", "bodyText": "did you mean FieldDeserializer here?", "author": "marton-bod", "createdAt": "2020-12-02T15:19:06Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FiledDeserializer mainDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.mainDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) mainDeserializer.value(data);\n+  }\n+\n+  private interface FiledDeserializer {", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMyNTQ2Ng==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534325466", "bodyText": "Thanks for catching!\nFixed", "author": "pvary", "createdAt": "2020-12-02T16:54:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI0OTI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI4OTc0MA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534289740", "bodyText": "Can't we return an OffsetDateTime object?", "author": "marton-bod", "createdAt": "2020-12-02T16:08:37Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergTimestampObjectInspector.java", "diffHunk": "@@ -22,26 +22,38 @@\n import java.sql.Timestamp;\n import java.time.LocalDateTime;\n import java.time.OffsetDateTime;\n+import java.time.ZoneId;\n import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.AbstractPrimitiveJavaObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n \n public abstract class IcebergTimestampObjectInspector extends AbstractPrimitiveJavaObjectInspector\n-                                                      implements TimestampObjectInspector {\n+    implements TimestampObjectInspector, IcebergWriteObjectInspector {\n \n   private static final IcebergTimestampObjectInspector INSTANCE_WITH_ZONE = new IcebergTimestampObjectInspector() {\n     @Override\n     LocalDateTime toLocalDateTime(Object o) {\n       return ((OffsetDateTime) o).toLocalDateTime();\n     }\n+\n+    @Override\n+    public Object getIcebergObject(Object o) {", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMyNTY2OA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534325668", "bodyText": "Fixed", "author": "pvary", "createdAt": "2020-12-02T16:54:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI4OTc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5MjY1NA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534292654", "bodyText": "nit: unless we have other deserializers, I would just name this field deserializer or fieldDeserializer", "author": "marton-bod", "createdAt": "2020-12-02T16:12:15Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FiledDeserializer mainDeserializer;", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMyNTg1MA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534325850", "bodyText": "Renamed to fieldDeserializer", "author": "pvary", "createdAt": "2020-12-02T16:55:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5MjY1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5Mzc3NQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534293775", "bodyText": "Is there a way to handle the Parquet case? Just wondering what happens if someone tries to write UUID data with parquet", "author": "marton-bod", "createdAt": "2020-12-02T16:13:45Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FiledDeserializer mainDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.mainDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) mainDeserializer.value(data);\n+  }\n+\n+  private interface FiledDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  private static FiledDeserializer deserializer(Type type, ObjectInspector fieldInspector) throws SerDeException {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        return o -> ((BooleanObjectInspector) fieldInspector).get(o);\n+      case INTEGER:\n+        return o -> ((IntObjectInspector) fieldInspector).get(o);\n+      case LONG:\n+        return o -> ((LongObjectInspector) fieldInspector).get(o);\n+      case FLOAT:\n+        return o -> ((FloatObjectInspector) fieldInspector).get(o);\n+      case DOUBLE:\n+        return o -> ((DoubleObjectInspector) fieldInspector).get(o);\n+      case STRING:\n+        return o -> ((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o);\n+      case UUID:\n+        // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ1NDg1Mg==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r535454852", "bodyText": "I think we should find a general way for handling UUID's across different file formats. This is a broader question so I would leave this as it is for this PR, and fix in some upcoming one", "author": "pvary", "createdAt": "2020-12-03T17:54:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5Mzc3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5NjY2Nw==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534296667", "bodyText": "if the fieldValue is null, shouldn't we still set the null in the result record?", "author": "marton-bod", "createdAt": "2020-12-02T16:17:28Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FiledDeserializer mainDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.mainDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) mainDeserializer.value(data);\n+  }\n+\n+  private interface FiledDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  private static FiledDeserializer deserializer(Type type, ObjectInspector fieldInspector) throws SerDeException {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        return o -> ((BooleanObjectInspector) fieldInspector).get(o);\n+      case INTEGER:\n+        return o -> ((IntObjectInspector) fieldInspector).get(o);\n+      case LONG:\n+        return o -> ((LongObjectInspector) fieldInspector).get(o);\n+      case FLOAT:\n+        return o -> ((FloatObjectInspector) fieldInspector).get(o);\n+      case DOUBLE:\n+        return o -> ((DoubleObjectInspector) fieldInspector).get(o);\n+      case STRING:\n+        return o -> ((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o);\n+      case UUID:\n+        // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+        return o -> UUID.fromString(((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o));\n+      case DATE:\n+      case TIMESTAMP:\n+      case FIXED:\n+      case BINARY:\n+      case DECIMAL:\n+        // Iceberg specific conversions\n+        return o -> ((IcebergWriteObjectInspector) fieldInspector).getIcebergObject(o);\n+      case STRUCT:\n+        return new StructDeserializer((Types.StructType) type, (StructObjectInspector) fieldInspector);\n+      case LIST:\n+      case MAP:\n+      case TIME:\n+      default:\n+        throw new SerDeException(\"Unsupported column type: \" + type);\n+    }\n+  }\n+\n+  private static class StructDeserializer implements FiledDeserializer {\n+    private final FiledDeserializer[] filedDeserializers;\n+    private final StructObjectInspector fieldInspector;\n+    private final Types.StructType type;\n+\n+    private StructDeserializer(Types.StructType type, StructObjectInspector fieldInspector) throws SerDeException {\n+      List<? extends StructField> structFields = fieldInspector.getAllStructFieldRefs();\n+      List<Types.NestedField> nestedFields = type.fields();\n+      this.filedDeserializers = new FiledDeserializer[structFields.size()];\n+      this.fieldInspector = fieldInspector;\n+      this.type = type;\n+\n+      for (int i = 0; i < filedDeserializers.length; i++) {\n+        filedDeserializers[i] =\n+            deserializer(nestedFields.get(i).type(), structFields.get(i).getFieldObjectInspector());\n+      }\n+    }\n+\n+    @Override\n+    public Record value(Object object) {\n+      if (object == null) {\n+        return null;\n+      }\n+\n+      List<Object> data = fieldInspector.getStructFieldsDataAsList(object);\n+      Record result = GenericRecord.create(type);\n+\n+      for (int i = 0; i < filedDeserializers.length; i++) {\n+        Object fieldValue = data.get(i);\n+        if (fieldValue != null) {", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMyNjkxNA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534326914", "bodyText": "We created a new Record here, so it should be null by default.\nThat said it should not hurt too much on performance, and easier to read when this is explicit, so added the extra set", "author": "pvary", "createdAt": "2020-12-02T16:56:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDI5NjY2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMwMDQyMg==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534300422", "bodyText": "Can we add a test case where a SerdeException should be thrown?", "author": "marton-bod", "createdAt": "2020-12-02T16:22:16Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestDeserializer.java", "diffHunk": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestDeserializer {\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+      optional(1, \"customer_id\", Types.LongType.get()),\n+      optional(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final StandardStructObjectInspector CUSTOMER_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          Arrays.asList(\"customer_id\", \"first_name\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+          ));\n+\n+", "originalCommit": "2a85e723c2043c57c93a6e084c76935fe9dc740d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMyNzA2Mw==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r534327063", "bodyText": "Added the extra case", "author": "pvary", "createdAt": "2020-12-02T16:56:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDMwMDQyMg=="}], "type": "inlineReview"}, {"oid": "ac3f608c6de396ae0cf4b84a4a4570c00d9f2b60", "url": "https://github.com/apache/iceberg/commit/ac3f608c6de396ae0cf4b84a4a4570c00d9f2b60", "message": "Addressed Marton's comments", "committedDate": "2020-12-02T16:52:41Z", "type": "commit"}, {"oid": "2eea35667e1e157525808a8248302b670a6b2884", "url": "https://github.com/apache/iceberg/commit/2eea35667e1e157525808a8248302b670a6b2884", "message": "Checkstyle", "committedDate": "2020-12-02T19:23:16Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3NjYwMg==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536476602", "bodyText": "List and Map aren't supported?", "author": "rdblue", "createdAt": "2020-12-05T01:41:20Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.fieldDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private interface FieldDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  private static FieldDeserializer deserializer(Type type, ObjectInspector fieldInspector) throws SerDeException {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        return o -> ((BooleanObjectInspector) fieldInspector).get(o);\n+      case INTEGER:\n+        return o -> ((IntObjectInspector) fieldInspector).get(o);\n+      case LONG:\n+        return o -> ((LongObjectInspector) fieldInspector).get(o);\n+      case FLOAT:\n+        return o -> ((FloatObjectInspector) fieldInspector).get(o);\n+      case DOUBLE:\n+        return o -> ((DoubleObjectInspector) fieldInspector).get(o);\n+      case STRING:\n+        return o -> ((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o);\n+      case UUID:\n+        // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+        return o -> UUID.fromString(((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o));\n+      case DATE:\n+      case TIMESTAMP:\n+      case FIXED:\n+      case BINARY:\n+      case DECIMAL:\n+        // Iceberg specific conversions\n+        return o -> ((IcebergWriteObjectInspector) fieldInspector).getIcebergObject(o);\n+      case STRUCT:\n+        return new StructDeserializer((Types.StructType) type, (StructObjectInspector) fieldInspector);\n+      case LIST:\n+      case MAP:\n+      case TIME:\n+      default:\n+        throw new SerDeException(\"Unsupported column type: \" + type);", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEwOTQxMg==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537109412", "bodyText": "Haven't planned for that but it was quite easy with the Visitor, so I have done this.", "author": "pvary", "createdAt": "2020-12-06T19:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3NjYwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3NzQ5MQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536477491", "bodyText": "Most of the other modules use the visitor pattern to traverse a type. That keeps the logic for traversing a schema in just one place so you don't need to mix it in with your domain-specific code.\nIt looks like this method is called from with the StructDeserializer constructor, so there is a recursive traversal of the schema that goes back and forth between object constructors and this method. That's a bit hard to follow, so I think it would be simpler if you took the visitor approach.\nA GenericAvroReader is similar to what you're building here, so take a look at that as an example: https://github.com/apache/iceberg/blob/master/core/src/main/java/org/apache/iceberg/avro/GenericAvroReader.java", "author": "rdblue", "createdAt": "2020-12-05T01:45:48Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.fieldDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private interface FieldDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  private static FieldDeserializer deserializer(Type type, ObjectInspector fieldInspector) throws SerDeException {", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMDYzMQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537120631", "bodyText": "Ok. I moved to the Visitor pattern. The the code is quite nice, and compact \ud83d\ude04\nHive struct names are not matched to the Iceberg struct name fields in case of the main struct which contains the result columns of a query. In this case we have the following col names: 0:col1, 1:col2 and they should be matched by the positions of the fields and not by the names.\nHandling this will add more complexity. I propose to handle this in the next PR. What do you think?", "author": "pvary", "createdAt": "2020-12-06T20:49:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3NzQ5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3Nzc5Ng==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536477796", "bodyText": "As I noted above, I think this would be cleaner if you used a visitor and passed the field deserializers into this constructor instead of calling deserializer here.", "author": "rdblue", "createdAt": "2020-12-05T01:47:08Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.DoubleObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.FloatObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.LongObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergWriteObjectInspector;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  Deserializer(Schema schema, ObjectInspector fieldInspector) throws SerDeException {\n+    this.fieldDeserializer = deserializer(schema.asStruct(), fieldInspector);\n+  }\n+\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private interface FieldDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  private static FieldDeserializer deserializer(Type type, ObjectInspector fieldInspector) throws SerDeException {\n+    switch (type.typeId()) {\n+      case BOOLEAN:\n+        return o -> ((BooleanObjectInspector) fieldInspector).get(o);\n+      case INTEGER:\n+        return o -> ((IntObjectInspector) fieldInspector).get(o);\n+      case LONG:\n+        return o -> ((LongObjectInspector) fieldInspector).get(o);\n+      case FLOAT:\n+        return o -> ((FloatObjectInspector) fieldInspector).get(o);\n+      case DOUBLE:\n+        return o -> ((DoubleObjectInspector) fieldInspector).get(o);\n+      case STRING:\n+        return o -> ((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o);\n+      case UUID:\n+        // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+        return o -> UUID.fromString(((StringObjectInspector) fieldInspector).getPrimitiveJavaObject(o));\n+      case DATE:\n+      case TIMESTAMP:\n+      case FIXED:\n+      case BINARY:\n+      case DECIMAL:\n+        // Iceberg specific conversions\n+        return o -> ((IcebergWriteObjectInspector) fieldInspector).getIcebergObject(o);\n+      case STRUCT:\n+        return new StructDeserializer((Types.StructType) type, (StructObjectInspector) fieldInspector);\n+      case LIST:\n+      case MAP:\n+      case TIME:\n+      default:\n+        throw new SerDeException(\"Unsupported column type: \" + type);\n+    }\n+  }\n+\n+  private static class StructDeserializer implements FieldDeserializer {\n+    private final FieldDeserializer[] fieldDeserializers;\n+    private final StructObjectInspector fieldInspector;\n+    private final Types.StructType type;\n+\n+    private StructDeserializer(Types.StructType type, StructObjectInspector fieldInspector) throws SerDeException {\n+      List<? extends StructField> structFields = fieldInspector.getAllStructFieldRefs();\n+      List<Types.NestedField> nestedFields = type.fields();\n+      this.fieldDeserializers = new FieldDeserializer[structFields.size()];\n+      this.fieldInspector = fieldInspector;\n+      this.type = type;\n+\n+      for (int i = 0; i < fieldDeserializers.length; i++) {\n+        fieldDeserializers[i] =\n+            deserializer(nestedFields.get(i).type(), structFields.get(i).getFieldObjectInspector());", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMDY2OA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537120668", "bodyText": "Done", "author": "pvary", "createdAt": "2020-12-06T20:49:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3Nzc5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3ODY5OQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536478699", "bodyText": "We avoid get in APIs because it doesn't add much value and could be misleading in some cases. I think it is misleading here because this actually converts the object to the value to the Iceberg generic representation rather than just returning it. We also shouldn't need \"Iceberg\" in the name because it's in IcebergWriteObjectInspector.\nHow about naming this convert?\nAlso, where we have similar interfaces elsewhere, we tend to like to use parameterized types. ValueWriters is a good example.", "author": "rdblue", "createdAt": "2020-12-05T01:51:58Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/serde/objectinspector/IcebergWriteObjectInspector.java", "diffHunk": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive.serde.objectinspector;\n+\n+public interface IcebergWriteObjectInspector {\n+  Object getIcebergObject(Object value);", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMzY3MQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537123671", "bodyText": "My reasoning was:\n\nThis is an ObjectInspector only change and adds only a single new method to the Iceberg specific ObjectInspectors\nAll of the ObjectInspector implementations are Overriding get* methods as inherited from Hive\nIt might be reasonable to stick to one naming methods on Object level\n\nThat said, since I am not overly attached to the original naming schema, changed it to the suggested one.\nThanks,\nPeter", "author": "pvary", "createdAt": "2020-12-06T21:07:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3ODY5OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyNTA2MQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537125061", "bodyText": "Oh.. and I plan to collapse the DeserializerVisitor.primitive to the following when we have implemented the handling of every Iceberg primitive type:\n    @Override\n    public FieldDeserializer primitive(PrimitiveType type, ObjectInspector inspector) {\n      if (inspector instanceof WriteObjectInspector) {\n        return o -> ((WriteObjectInspector) inspector).convert(o);\n      } else {\n        return o -> ((PrimitiveObjectInspector) inspector).getPrimitiveJavaObject(o);\n      }\n    }\n\nHaving differently named method for every type would make this impossible.", "author": "pvary", "createdAt": "2020-12-06T21:15:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3ODY5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3ODg1Mw==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536478853", "bodyText": "Can you file a bug for this?", "author": "rdblue", "createdAt": "2020-12-05T01:52:42Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.serde2.io.DateWritable;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.DoubleWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergBinaryObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergDecimalObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.UUIDUtil;\n+import org.junit.Assert;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class HiveIcebergTestUtils {\n+  // TODO: Can this be a constant all around the Iceberg tests?\n+  public static final Schema FULL_SCHEMA = new Schema(\n+      optional(1, \"boolean_type\", Types.BooleanType.get()),\n+      optional(2, \"integer_type\", Types.IntegerType.get()),\n+      optional(3, \"long_type\", Types.LongType.get()),\n+      optional(4, \"float_type\", Types.FloatType.get()),\n+      optional(5, \"double_type\", Types.DoubleType.get()),\n+      optional(6, \"date_type\", Types.DateType.get()),\n+      // TimeType is not supported\n+      // required(7, \"time_type\", Types.TimeType.get()),\n+      optional(7, \"tsTz\", Types.TimestampType.withZone()),\n+      optional(8, \"ts\", Types.TimestampType.withoutZone()),\n+      optional(9, \"string_type\", Types.StringType.get()),\n+      optional(10, \"uuid_type\", Types.UUIDType.get()),\n+      optional(11, \"fixed_type\", Types.FixedType.ofLength(3)),\n+      optional(12, \"binary_type\", Types.BinaryType.get()),\n+      optional(13, \"decimal_type\", Types.DecimalType.of(38, 10)));\n+\n+  public static final StandardStructObjectInspector FULL_SCHEMA_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          // Capitalized `boolean_type` field to check for field case insensitivity.\n+          Arrays.asList(\"Boolean_Type\", \"integer_type\", \"long_type\", \"float_type\", \"double_type\",\n+              \"date_type\", \"tsTz\", \"ts\", \"string_type\", \"uuid_type\", \"fixed_type\", \"binary_type\", \"decimal_type\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableBooleanObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableIntObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableFloatObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableDoubleObjectInspector,\n+              IcebergObjectInspector.DATE_INSPECTOR,\n+              IcebergObjectInspector.TIMESTAMP_INSPECTOR_WITH_TZ,\n+              IcebergObjectInspector.TIMESTAMP_INSPECTOR,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+              IcebergBinaryObjectInspector.byteArray(),\n+              IcebergBinaryObjectInspector.byteBuffer(),\n+              IcebergDecimalObjectInspector.get(38, 10)\n+          ));\n+\n+  private HiveIcebergTestUtils() {\n+    // Empty constructor for the utility class\n+  }\n+\n+  public static Record getTestRecord(boolean uuidAsByte) {\n+    Record record = GenericRecord.create(HiveIcebergTestUtils.FULL_SCHEMA);\n+    record.set(0, true);\n+    record.set(1, 1);\n+    record.set(2, 2L);\n+    record.set(3, 3.1f);\n+    record.set(4, 4.2d);\n+    record.set(5, LocalDate.of(2020, 1, 21));\n+    // TimeType is not supported\n+    // record.set(6, LocalTime.of(11, 33));\n+    // Nano is not supported ?\n+    record.set(6, OffsetDateTime.of(2017, 11, 22, 11, 30, 7, 0, ZoneOffset.ofHours(2)));\n+    record.set(7, LocalDateTime.of(2019, 2, 22, 9, 44, 54));\n+    record.set(8, \"kilenc\");\n+    if (uuidAsByte) {\n+      // TODO: Parquet UUID expect byte[], others are expecting UUID", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyODQ4NQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537128485", "bodyText": "Filed an issue for this: #1881", "author": "pvary", "createdAt": "2020-12-06T21:34:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3ODg1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTA4MA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536479080", "bodyText": "You should be able to use utils in ByteBuffers for this.", "author": "rdblue", "createdAt": "2020-12-05T01:53:56Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.sql.Timestamp;\n+import java.time.LocalDate;\n+import java.time.LocalDateTime;\n+import java.time.OffsetDateTime;\n+import java.time.ZoneOffset;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.serde2.io.DateWritable;\n+import org.apache.hadoop.hive.serde2.io.HiveDecimalWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.BytesWritable;\n+import org.apache.hadoop.io.DoubleWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergBinaryObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergDecimalObjectInspector;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.IcebergObjectInspector;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.UUIDUtil;\n+import org.junit.Assert;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class HiveIcebergTestUtils {\n+  // TODO: Can this be a constant all around the Iceberg tests?\n+  public static final Schema FULL_SCHEMA = new Schema(\n+      optional(1, \"boolean_type\", Types.BooleanType.get()),\n+      optional(2, \"integer_type\", Types.IntegerType.get()),\n+      optional(3, \"long_type\", Types.LongType.get()),\n+      optional(4, \"float_type\", Types.FloatType.get()),\n+      optional(5, \"double_type\", Types.DoubleType.get()),\n+      optional(6, \"date_type\", Types.DateType.get()),\n+      // TimeType is not supported\n+      // required(7, \"time_type\", Types.TimeType.get()),\n+      optional(7, \"tsTz\", Types.TimestampType.withZone()),\n+      optional(8, \"ts\", Types.TimestampType.withoutZone()),\n+      optional(9, \"string_type\", Types.StringType.get()),\n+      optional(10, \"uuid_type\", Types.UUIDType.get()),\n+      optional(11, \"fixed_type\", Types.FixedType.ofLength(3)),\n+      optional(12, \"binary_type\", Types.BinaryType.get()),\n+      optional(13, \"decimal_type\", Types.DecimalType.of(38, 10)));\n+\n+  public static final StandardStructObjectInspector FULL_SCHEMA_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          // Capitalized `boolean_type` field to check for field case insensitivity.\n+          Arrays.asList(\"Boolean_Type\", \"integer_type\", \"long_type\", \"float_type\", \"double_type\",\n+              \"date_type\", \"tsTz\", \"ts\", \"string_type\", \"uuid_type\", \"fixed_type\", \"binary_type\", \"decimal_type\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableBooleanObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableIntObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableFloatObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableDoubleObjectInspector,\n+              IcebergObjectInspector.DATE_INSPECTOR,\n+              IcebergObjectInspector.TIMESTAMP_INSPECTOR_WITH_TZ,\n+              IcebergObjectInspector.TIMESTAMP_INSPECTOR,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+              IcebergBinaryObjectInspector.byteArray(),\n+              IcebergBinaryObjectInspector.byteBuffer(),\n+              IcebergDecimalObjectInspector.get(38, 10)\n+          ));\n+\n+  private HiveIcebergTestUtils() {\n+    // Empty constructor for the utility class\n+  }\n+\n+  public static Record getTestRecord(boolean uuidAsByte) {\n+    Record record = GenericRecord.create(HiveIcebergTestUtils.FULL_SCHEMA);\n+    record.set(0, true);\n+    record.set(1, 1);\n+    record.set(2, 2L);\n+    record.set(3, 3.1f);\n+    record.set(4, 4.2d);\n+    record.set(5, LocalDate.of(2020, 1, 21));\n+    // TimeType is not supported\n+    // record.set(6, LocalTime.of(11, 33));\n+    // Nano is not supported ?\n+    record.set(6, OffsetDateTime.of(2017, 11, 22, 11, 30, 7, 0, ZoneOffset.ofHours(2)));\n+    record.set(7, LocalDateTime.of(2019, 2, 22, 9, 44, 54));\n+    record.set(8, \"kilenc\");\n+    if (uuidAsByte) {\n+      // TODO: Parquet UUID expect byte[], others are expecting UUID\n+      record.set(9, UUIDUtil.convert(UUID.fromString(\"1-2-3-4-5\")));\n+    } else {\n+      record.set(9, UUID.fromString(\"1-2-3-4-5\"));\n+    }\n+    record.set(10, new byte[]{0, 1, 2});\n+    record.set(11, ByteBuffer.wrap(new byte[]{0, 1, 2, 3}));\n+    record.set(12, new BigDecimal(\"0.0000000013\"));\n+\n+    return record;\n+  }\n+\n+  public static Record getNullTestRecord() {\n+    Record record = GenericRecord.create(HiveIcebergTestUtils.FULL_SCHEMA);\n+\n+    for (int i = 0; i < HiveIcebergTestUtils.FULL_SCHEMA.columns().size(); i++) {\n+      record.set(i, null);\n+    }\n+\n+    return record;\n+  }\n+\n+  public static List<Object> valuesForTestRecord(Record record) {\n+    ByteBuffer byteBuffer = record.get(11, ByteBuffer.class);\n+    byte[] bytes = new byte[byteBuffer.remaining()];", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMTkyNw==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537121927", "bodyText": "Done.\nThanks!", "author": "pvary", "createdAt": "2020-12-06T20:57:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTA4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTEzNQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536479135", "bodyText": "Nit: double whitespace.", "author": "rdblue", "createdAt": "2020-12-05T01:54:16Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestDeserializer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestDeserializer {\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+      optional(1, \"customer_id\", Types.LongType.get()),\n+      optional(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final StandardStructObjectInspector CUSTOMER_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          Arrays.asList(\"customer_id\", \"first_name\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+          ));\n+", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMjUwMA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537122500", "bodyText": "Shall we use 2 spaces for Continuation indent as well?", "author": "pvary", "createdAt": "2020-12-06T21:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODczNzI4MQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r538737281", "bodyText": "Continuation indents are 2 indents, which are 4 spaces. What you have here looks correct to me.", "author": "rdblue", "createdAt": "2020-12-08T19:14:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODc0NDQ5NA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r538744494", "bodyText": "Then I am not sure what was the comment about. \ud83d\ude22", "author": "pvary", "createdAt": "2020-12-08T19:25:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTEzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTUzOTU5Mw==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r539539593", "bodyText": "I think it was an extra newline.", "author": "rdblue", "createdAt": "2020-12-09T18:20:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTEzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTMwNQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536479305", "bodyText": "I think that Assume can be passed string context. This would be a good thing to pass in.", "author": "rdblue", "createdAt": "2020-12-05T01:55:01Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestDeserializer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestDeserializer {\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+      optional(1, \"customer_id\", Types.LongType.get()),\n+      optional(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final StandardStructObjectInspector CUSTOMER_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          Arrays.asList(\"customer_id\", \"first_name\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+          ));\n+\n+\n+  @Test\n+  public void testSimpleDeserialize() throws SerDeException {\n+    Deserializer deserializer = new Deserializer(CUSTOMER_SCHEMA, CUSTOMER_OBJECT_INSPECTOR);\n+\n+    Record expected = GenericRecord.create(CUSTOMER_SCHEMA);\n+    expected.set(0, 1L);\n+    expected.set(1, \"Bob\");\n+\n+    Record actual = deserializer.deserialize(new Object[] { new LongWritable(1L), new Text(\"Bob\") });\n+\n+    Assert.assertEquals(expected, actual);\n+  }\n+\n+  @Test\n+  public void testDeserializeEverySupportedType() throws SerDeException {\n+    // No test yet for Hive3 (Date/Timestamp creation)", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMjU0OQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537122549", "bodyText": "Thanks for noticing!\nDone", "author": "pvary", "createdAt": "2020-12-06T21:01:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTMwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTUyNQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536479525", "bodyText": "We typically prefer to use AssertHelpers.assertThrows rather than expected. When we're testing other places, we can continue making assertions after the exception is thrown that way. Here it probably doesn't matter much, but it is a good habit to be in.", "author": "rdblue", "createdAt": "2020-12-05T01:56:20Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestDeserializer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestDeserializer {\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+      optional(1, \"customer_id\", Types.LongType.get()),\n+      optional(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final StandardStructObjectInspector CUSTOMER_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          Arrays.asList(\"customer_id\", \"first_name\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+          ));\n+\n+\n+  @Test\n+  public void testSimpleDeserialize() throws SerDeException {\n+    Deserializer deserializer = new Deserializer(CUSTOMER_SCHEMA, CUSTOMER_OBJECT_INSPECTOR);\n+\n+    Record expected = GenericRecord.create(CUSTOMER_SCHEMA);\n+    expected.set(0, 1L);\n+    expected.set(1, \"Bob\");\n+\n+    Record actual = deserializer.deserialize(new Object[] { new LongWritable(1L), new Text(\"Bob\") });\n+\n+    Assert.assertEquals(expected, actual);\n+  }\n+\n+  @Test\n+  public void testDeserializeEverySupportedType() throws SerDeException {\n+    // No test yet for Hive3 (Date/Timestamp creation)\n+    Assume.assumeFalse(MetastoreUtil.hive3PresentOnClasspath());\n+\n+    Deserializer deserializer = new Deserializer(HiveIcebergTestUtils.FULL_SCHEMA,\n+        HiveIcebergTestUtils.FULL_SCHEMA_OBJECT_INSPECTOR);\n+\n+    Record expected = HiveIcebergTestUtils.getTestRecord(false);\n+    Record actual = deserializer.deserialize(HiveIcebergTestUtils.valuesForTestRecord(expected));\n+\n+    HiveIcebergTestUtils.assertEquals(expected, actual);\n+  }\n+\n+  @Test\n+  public void testNullDeserialize() throws SerDeException {\n+    Deserializer deserializer = new Deserializer(HiveIcebergTestUtils.FULL_SCHEMA,\n+        HiveIcebergTestUtils.FULL_SCHEMA_OBJECT_INSPECTOR);\n+\n+    Record expected = HiveIcebergTestUtils.getNullTestRecord();\n+\n+    Object[] nulls = new Object[HiveIcebergTestUtils.FULL_SCHEMA.columns().size()];\n+    Arrays.fill(nulls, null);\n+\n+    Record actual = deserializer.deserialize(nulls);\n+\n+    Assert.assertEquals(expected, actual);\n+\n+    // Check null record as well\n+    Assert.assertNull(deserializer.deserialize(null));\n+  }\n+\n+  @Test(expected = SerDeException.class)", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMjkwMA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537122900", "bodyText": "SerDeException was not a runtime exception and would have to do some extra magic to use AssertHelpers.assertThrows.\nOn the other hand I think we can convert any exception in the SerDe to SerDeException, we can throw IllegalArgumentExceptions instead and handle those outside.\nFixed", "author": "pvary", "createdAt": "2020-12-06T21:03:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTUyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTYyMQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r536479621", "bodyText": "Why did you choose to use SerDeException? Is that expected by Hive?", "author": "rdblue", "createdAt": "2020-12-05T01:56:35Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/TestDeserializer.java", "diffHunk": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.Arrays;\n+import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hive.MetastoreUtil;\n+import org.apache.iceberg.types.Types;\n+import org.junit.Assert;\n+import org.junit.Assume;\n+import org.junit.Test;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+\n+public class TestDeserializer {\n+  private static final Schema CUSTOMER_SCHEMA = new Schema(\n+      optional(1, \"customer_id\", Types.LongType.get()),\n+      optional(2, \"first_name\", Types.StringType.get())\n+  );\n+\n+  private static final StandardStructObjectInspector CUSTOMER_OBJECT_INSPECTOR =\n+      ObjectInspectorFactory.getStandardStructObjectInspector(\n+          Arrays.asList(\"customer_id\", \"first_name\"),\n+          Arrays.asList(\n+              PrimitiveObjectInspectorFactory.writableLongObjectInspector,\n+              PrimitiveObjectInspectorFactory.writableStringObjectInspector\n+          ));\n+\n+\n+  @Test\n+  public void testSimpleDeserialize() throws SerDeException {\n+    Deserializer deserializer = new Deserializer(CUSTOMER_SCHEMA, CUSTOMER_OBJECT_INSPECTOR);\n+\n+    Record expected = GenericRecord.create(CUSTOMER_SCHEMA);\n+    expected.set(0, 1L);\n+    expected.set(1, \"Bob\");\n+\n+    Record actual = deserializer.deserialize(new Object[] { new LongWritable(1L), new Text(\"Bob\") });\n+\n+    Assert.assertEquals(expected, actual);\n+  }\n+\n+  @Test\n+  public void testDeserializeEverySupportedType() throws SerDeException {\n+    // No test yet for Hive3 (Date/Timestamp creation)\n+    Assume.assumeFalse(MetastoreUtil.hive3PresentOnClasspath());\n+\n+    Deserializer deserializer = new Deserializer(HiveIcebergTestUtils.FULL_SCHEMA,\n+        HiveIcebergTestUtils.FULL_SCHEMA_OBJECT_INSPECTOR);\n+\n+    Record expected = HiveIcebergTestUtils.getTestRecord(false);\n+    Record actual = deserializer.deserialize(HiveIcebergTestUtils.valuesForTestRecord(expected));\n+\n+    HiveIcebergTestUtils.assertEquals(expected, actual);\n+  }\n+\n+  @Test\n+  public void testNullDeserialize() throws SerDeException {\n+    Deserializer deserializer = new Deserializer(HiveIcebergTestUtils.FULL_SCHEMA,\n+        HiveIcebergTestUtils.FULL_SCHEMA_OBJECT_INSPECTOR);\n+\n+    Record expected = HiveIcebergTestUtils.getNullTestRecord();\n+\n+    Object[] nulls = new Object[HiveIcebergTestUtils.FULL_SCHEMA.columns().size()];\n+    Arrays.fill(nulls, null);\n+\n+    Record actual = deserializer.deserialize(nulls);\n+\n+    Assert.assertEquals(expected, actual);\n+\n+    // Check null record as well\n+    Assert.assertNull(deserializer.deserialize(null));\n+  }\n+\n+  @Test(expected = SerDeException.class)\n+  public void testSerDeException() throws SerDeException {", "originalCommit": "2eea35667e1e157525808a8248302b670a6b2884", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzEyMzA0NA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r537123044", "bodyText": "Yeah. The SerDe should only throw SerDeException.\nBut we can throw whatever we want here, and wrap it later to SerDeException.\nFixed", "author": "pvary", "createdAt": "2020-12-06T21:04:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ3OTYyMQ=="}], "type": "inlineReview"}, {"oid": "294abb0921525db40b07826a0fd98b46b7177383", "url": "https://github.com/apache/iceberg/commit/294abb0921525db40b07826a0fd98b46b7177383", "message": "Using the Visitor for going through the Schema", "committedDate": "2020-12-06T19:43:02Z", "type": "commit"}, {"oid": "37fd4fd86badf1d234bc03859b7b480332f49375", "url": "https://github.com/apache/iceberg/commit/37fd4fd86badf1d234bc03859b7b480332f49375", "message": "Renamed IcebergWriteObjectInspector to WriteObjectInspector", "committedDate": "2020-12-06T21:05:55Z", "type": "commit"}, {"oid": "b8d5a64ea1d1a3fca6b635118dea32ab2ee9e9da", "url": "https://github.com/apache/iceberg/commit/b8d5a64ea1d1a3fca6b635118dea32ab2ee9e9da", "message": "Handling Hive Schema problem and some javadoc", "committedDate": "2020-12-08T19:02:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDIyMA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r538820220", "bodyText": "Nit: prefer Lists.newArrayList().", "author": "rdblue", "createdAt": "2020-12-08T21:28:05Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.WriteObjectInspector;\n+import org.apache.iceberg.schema.SchemaWithPartnerVisitor;\n+import org.apache.iceberg.types.Type.PrimitiveType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  /**\n+   * Builder to create a Deserializer instance.\n+   * Requires an Iceberg Schema and the Hive ObjectInspector for converting the data.\n+   */\n+  static class Builder {\n+    private Schema schema;\n+    private ObjectInspector inspector;\n+\n+    Builder schema(Schema mainSchema) {\n+      this.schema = mainSchema;\n+      return this;\n+    }\n+\n+    Builder inspector(ObjectInspector mainInspector) {\n+      this.inspector = mainInspector;\n+      return this;\n+    }\n+\n+    Deserializer build() {\n+      return new Deserializer(schema, inspector);\n+    }\n+  }\n+\n+  /**\n+   * Deserializes the Hive result object to an Iceberg record using the provided ObjectInspectors.\n+   * @param data The Hive data to deserialize\n+   * @return The resulting Iceberg Record\n+   */\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private Deserializer(Schema schema, ObjectInspector fieldInspector) {\n+    this.fieldDeserializer = DeserializerVisitor.visit(schema, fieldInspector);\n+  }\n+\n+  private static class DeserializerVisitor extends SchemaWithPartnerVisitor<ObjectInspector, FieldDeserializer> {\n+\n+    public static FieldDeserializer visit(Schema schema, ObjectInspector objectInspector) {\n+      return visit(schema, new FixNameMappingObjectInspector(schema, objectInspector), new DeserializerVisitor(),\n+          new PartnerObjectInspectorByNameAccessors());\n+    }\n+\n+    @Override\n+    public FieldDeserializer schema(Schema schema, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer field(NestedField field, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer primitive(PrimitiveType type, ObjectInspector inspector) {\n+      switch (type.typeId()) {\n+        case BOOLEAN:\n+        case INTEGER:\n+        case LONG:\n+        case FLOAT:\n+        case DOUBLE:\n+        case STRING:\n+          // Generic conversions where Iceberg and Hive are using the same java object\n+          return o -> ((PrimitiveObjectInspector) inspector).getPrimitiveJavaObject(o);\n+        case UUID:\n+          // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+          return o -> UUID.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(o));\n+        case DATE:\n+        case TIMESTAMP:\n+        case FIXED:\n+        case BINARY:\n+        case DECIMAL:\n+          // Iceberg specific conversions\n+          return o -> ((WriteObjectInspector) inspector).convert(o);\n+        case TIME:\n+        default:\n+          throw new IllegalArgumentException(\"Unsupported column type: \" + type);\n+      }\n+    }\n+\n+    @Override\n+    public FieldDeserializer struct(StructType type, ObjectInspector inspector, List<FieldDeserializer> deserializers) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> data = ((StructObjectInspector) inspector).getStructFieldsDataAsList(o);\n+        Record result = GenericRecord.create(type);\n+\n+        for (int i = 0; i < deserializers.size(); i++) {\n+          Object fieldValue = data.get(i);\n+          if (fieldValue != null) {\n+            result.set(i, deserializers.get(i).value(fieldValue));\n+          } else {\n+            result.set(i, null);\n+          }\n+        }\n+\n+        return result;\n+      };\n+    }\n+\n+    @Override\n+    public FieldDeserializer list(ListType listTypeInfo, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> result = new ArrayList<>();", "originalCommit": "b8d5a64ea1d1a3fca6b635118dea32ab2ee9e9da", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTA3NzA1NA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r539077054", "bodyText": "I remember on some other PR got a review from someone else to avoid unnecessary guava uses.\nI am find with both, but I think we should stick to one or the other. Shall it be Lists.newArrayList then?", "author": "pvary", "createdAt": "2020-12-09T07:44:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDIyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTU0MDY4Ng==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r539540686", "bodyText": "We don't want to bring in any additional Guava classes if possible, but using the ones that are already there is a good thing. For this case, we can easily replace map class implementations with an import change later.", "author": "rdblue", "createdAt": "2020-12-09T18:21:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDIyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODgyMDgwMQ==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r538820801", "bodyText": "Minor: This could probably be a singleton.", "author": "rdblue", "createdAt": "2020-12-08T21:29:07Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.WriteObjectInspector;\n+import org.apache.iceberg.schema.SchemaWithPartnerVisitor;\n+import org.apache.iceberg.types.Type.PrimitiveType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  /**\n+   * Builder to create a Deserializer instance.\n+   * Requires an Iceberg Schema and the Hive ObjectInspector for converting the data.\n+   */\n+  static class Builder {\n+    private Schema schema;\n+    private ObjectInspector inspector;\n+\n+    Builder schema(Schema mainSchema) {\n+      this.schema = mainSchema;\n+      return this;\n+    }\n+\n+    Builder inspector(ObjectInspector mainInspector) {\n+      this.inspector = mainInspector;\n+      return this;\n+    }\n+\n+    Deserializer build() {\n+      return new Deserializer(schema, inspector);\n+    }\n+  }\n+\n+  /**\n+   * Deserializes the Hive result object to an Iceberg record using the provided ObjectInspectors.\n+   * @param data The Hive data to deserialize\n+   * @return The resulting Iceberg Record\n+   */\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private Deserializer(Schema schema, ObjectInspector fieldInspector) {\n+    this.fieldDeserializer = DeserializerVisitor.visit(schema, fieldInspector);\n+  }\n+\n+  private static class DeserializerVisitor extends SchemaWithPartnerVisitor<ObjectInspector, FieldDeserializer> {\n+\n+    public static FieldDeserializer visit(Schema schema, ObjectInspector objectInspector) {\n+      return visit(schema, new FixNameMappingObjectInspector(schema, objectInspector), new DeserializerVisitor(),\n+          new PartnerObjectInspectorByNameAccessors());\n+    }\n+\n+    @Override\n+    public FieldDeserializer schema(Schema schema, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer field(NestedField field, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer primitive(PrimitiveType type, ObjectInspector inspector) {\n+      switch (type.typeId()) {\n+        case BOOLEAN:\n+        case INTEGER:\n+        case LONG:\n+        case FLOAT:\n+        case DOUBLE:\n+        case STRING:\n+          // Generic conversions where Iceberg and Hive are using the same java object\n+          return o -> ((PrimitiveObjectInspector) inspector).getPrimitiveJavaObject(o);\n+        case UUID:\n+          // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+          return o -> UUID.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(o));\n+        case DATE:\n+        case TIMESTAMP:\n+        case FIXED:\n+        case BINARY:\n+        case DECIMAL:\n+          // Iceberg specific conversions\n+          return o -> ((WriteObjectInspector) inspector).convert(o);\n+        case TIME:\n+        default:\n+          throw new IllegalArgumentException(\"Unsupported column type: \" + type);\n+      }\n+    }\n+\n+    @Override\n+    public FieldDeserializer struct(StructType type, ObjectInspector inspector, List<FieldDeserializer> deserializers) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> data = ((StructObjectInspector) inspector).getStructFieldsDataAsList(o);\n+        Record result = GenericRecord.create(type);\n+\n+        for (int i = 0; i < deserializers.size(); i++) {\n+          Object fieldValue = data.get(i);\n+          if (fieldValue != null) {\n+            result.set(i, deserializers.get(i).value(fieldValue));\n+          } else {\n+            result.set(i, null);\n+          }\n+        }\n+\n+        return result;\n+      };\n+    }\n+\n+    @Override\n+    public FieldDeserializer list(ListType listTypeInfo, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> result = new ArrayList<>();\n+        ListObjectInspector listInspector = (ListObjectInspector) inspector;\n+\n+        for (Object val : listInspector.getList(o)) {\n+          result.add(deserializer.value(val));\n+        }\n+\n+        return result;\n+      };\n+    }\n+\n+    @Override\n+    public FieldDeserializer map(MapType mapType, ObjectInspector inspector, FieldDeserializer keyDeserializer,\n+                                 FieldDeserializer valueDeserializer) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        Map<Object, Object> result = new HashMap<>();\n+        MapObjectInspector mapObjectInspector = (MapObjectInspector) inspector;\n+\n+        for (Map.Entry<?, ?> entry : mapObjectInspector.getMap(o).entrySet()) {\n+          result.put(keyDeserializer.value(entry.getKey()), valueDeserializer.value(entry.getValue()));\n+        }\n+        return result;\n+      };\n+    }\n+  }\n+\n+  private static class PartnerObjectInspectorByNameAccessors\n+      implements SchemaWithPartnerVisitor.PartnerAccessors<ObjectInspector> {", "originalCommit": "b8d5a64ea1d1a3fca6b635118dea32ab2ee9e9da", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODg5NTk1NA==", "url": "https://github.com/apache/iceberg/pull/1854#discussion_r538895954", "bodyText": "Don't all of the structs need to be wrapped by this?", "author": "rdblue", "createdAt": "2020-12-08T23:48:06Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/Deserializer.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.mr.hive.serde.objectinspector.WriteObjectInspector;\n+import org.apache.iceberg.schema.SchemaWithPartnerVisitor;\n+import org.apache.iceberg.types.Type.PrimitiveType;\n+import org.apache.iceberg.types.Types.ListType;\n+import org.apache.iceberg.types.Types.MapType;\n+import org.apache.iceberg.types.Types.NestedField;\n+import org.apache.iceberg.types.Types.StructType;\n+\n+\n+class Deserializer {\n+  private FieldDeserializer fieldDeserializer;\n+\n+  /**\n+   * Builder to create a Deserializer instance.\n+   * Requires an Iceberg Schema and the Hive ObjectInspector for converting the data.\n+   */\n+  static class Builder {\n+    private Schema schema;\n+    private ObjectInspector inspector;\n+\n+    Builder schema(Schema mainSchema) {\n+      this.schema = mainSchema;\n+      return this;\n+    }\n+\n+    Builder inspector(ObjectInspector mainInspector) {\n+      this.inspector = mainInspector;\n+      return this;\n+    }\n+\n+    Deserializer build() {\n+      return new Deserializer(schema, inspector);\n+    }\n+  }\n+\n+  /**\n+   * Deserializes the Hive result object to an Iceberg record using the provided ObjectInspectors.\n+   * @param data The Hive data to deserialize\n+   * @return The resulting Iceberg Record\n+   */\n+  Record deserialize(Object data) {\n+    return (Record) fieldDeserializer.value(data);\n+  }\n+\n+  private Deserializer(Schema schema, ObjectInspector fieldInspector) {\n+    this.fieldDeserializer = DeserializerVisitor.visit(schema, fieldInspector);\n+  }\n+\n+  private static class DeserializerVisitor extends SchemaWithPartnerVisitor<ObjectInspector, FieldDeserializer> {\n+\n+    public static FieldDeserializer visit(Schema schema, ObjectInspector objectInspector) {\n+      return visit(schema, new FixNameMappingObjectInspector(schema, objectInspector), new DeserializerVisitor(),\n+          new PartnerObjectInspectorByNameAccessors());\n+    }\n+\n+    @Override\n+    public FieldDeserializer schema(Schema schema, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer field(NestedField field, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return deserializer;\n+    }\n+\n+    @Override\n+    public FieldDeserializer primitive(PrimitiveType type, ObjectInspector inspector) {\n+      switch (type.typeId()) {\n+        case BOOLEAN:\n+        case INTEGER:\n+        case LONG:\n+        case FLOAT:\n+        case DOUBLE:\n+        case STRING:\n+          // Generic conversions where Iceberg and Hive are using the same java object\n+          return o -> ((PrimitiveObjectInspector) inspector).getPrimitiveJavaObject(o);\n+        case UUID:\n+          // TODO: This will not work with Parquet. Parquet UUID expect byte[], others are expecting UUID\n+          return o -> UUID.fromString(((StringObjectInspector) inspector).getPrimitiveJavaObject(o));\n+        case DATE:\n+        case TIMESTAMP:\n+        case FIXED:\n+        case BINARY:\n+        case DECIMAL:\n+          // Iceberg specific conversions\n+          return o -> ((WriteObjectInspector) inspector).convert(o);\n+        case TIME:\n+        default:\n+          throw new IllegalArgumentException(\"Unsupported column type: \" + type);\n+      }\n+    }\n+\n+    @Override\n+    public FieldDeserializer struct(StructType type, ObjectInspector inspector, List<FieldDeserializer> deserializers) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> data = ((StructObjectInspector) inspector).getStructFieldsDataAsList(o);\n+        Record result = GenericRecord.create(type);\n+\n+        for (int i = 0; i < deserializers.size(); i++) {\n+          Object fieldValue = data.get(i);\n+          if (fieldValue != null) {\n+            result.set(i, deserializers.get(i).value(fieldValue));\n+          } else {\n+            result.set(i, null);\n+          }\n+        }\n+\n+        return result;\n+      };\n+    }\n+\n+    @Override\n+    public FieldDeserializer list(ListType listTypeInfo, ObjectInspector inspector, FieldDeserializer deserializer) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        List<Object> result = new ArrayList<>();\n+        ListObjectInspector listInspector = (ListObjectInspector) inspector;\n+\n+        for (Object val : listInspector.getList(o)) {\n+          result.add(deserializer.value(val));\n+        }\n+\n+        return result;\n+      };\n+    }\n+\n+    @Override\n+    public FieldDeserializer map(MapType mapType, ObjectInspector inspector, FieldDeserializer keyDeserializer,\n+                                 FieldDeserializer valueDeserializer) {\n+      return o -> {\n+        if (o == null) {\n+          return null;\n+        }\n+\n+        Map<Object, Object> result = new HashMap<>();\n+        MapObjectInspector mapObjectInspector = (MapObjectInspector) inspector;\n+\n+        for (Map.Entry<?, ?> entry : mapObjectInspector.getMap(o).entrySet()) {\n+          result.put(keyDeserializer.value(entry.getKey()), valueDeserializer.value(entry.getValue()));\n+        }\n+        return result;\n+      };\n+    }\n+  }\n+\n+  private static class PartnerObjectInspectorByNameAccessors\n+      implements SchemaWithPartnerVisitor.PartnerAccessors<ObjectInspector> {\n+\n+    @Override\n+    public ObjectInspector fieldPartner(ObjectInspector inspector, int fieldId, String name) {\n+      StructObjectInspector fieldInspector  = (StructObjectInspector) inspector;\n+      return fieldInspector.getStructFieldRef(name).getFieldObjectInspector();\n+    }\n+\n+    @Override\n+    public ObjectInspector mapKeyPartner(ObjectInspector inspector) {\n+      MapObjectInspector fieldInspector  = (MapObjectInspector) inspector;\n+      return fieldInspector.getMapKeyObjectInspector();\n+    }\n+\n+    @Override\n+    public ObjectInspector mapValuePartner(ObjectInspector inspector) {\n+      MapObjectInspector fieldInspector  = (MapObjectInspector) inspector;\n+      return fieldInspector.getMapValueObjectInspector();\n+    }\n+\n+    @Override\n+    public ObjectInspector listElementPartner(ObjectInspector inspector) {\n+      ListObjectInspector fieldInspector  = (ListObjectInspector) inspector;\n+      return fieldInspector.getListElementObjectInspector();\n+    }\n+  }\n+\n+  private interface FieldDeserializer {\n+    Object value(Object object);\n+  }\n+\n+  /**\n+   * Hive query results schema column names do not match the target Iceberg column names.\n+   * Instead we have to rely on the column order. To keep the other parts of the code generic we fix this with a\n+   * wrapper around the ObjectInspector. This wrapper uses the Iceberg schema column names instead of the Hive column\n+   * names for {@link #getStructFieldRef(String) getStructFieldRef}\n+   */\n+  private static class FixNameMappingObjectInspector extends StructObjectInspector {", "originalCommit": "b8d5a64ea1d1a3fca6b635118dea32ab2ee9e9da", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}