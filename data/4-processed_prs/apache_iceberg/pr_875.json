{"pr_number": 875, "pr_title": "Spark: Implement an action to rewrite manifests", "pr_createdAt": "2020-03-26T19:35:05Z", "pr_url": "https://github.com/apache/iceberg/pull/875", "timeline": [{"oid": "1ae293ed593542597a95cc44cc734f7f6be11efb", "url": "https://github.com/apache/iceberg/commit/1ae293ed593542597a95cc44cc734f7f6be11efb", "message": "Spark: Implement an action to rewrite manifests", "committedDate": "2020-04-06T14:47:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2OTUzMw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404469533", "bodyText": "Minor: I usually like to add a constant for the check interval so it can be configured easily.", "author": "rdblue", "createdAt": "2020-04-07T00:36:50Z", "path": "core/src/main/java/org/apache/iceberg/ManifestsWriter.java", "diffHunk": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.collect.Lists;\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.io.OutputFile;\n+\n+/**\n+ * A manifest writer that splits entries into multiple files in order to produce files of the configured size.\n+ */\n+public class ManifestsWriter implements Closeable {\n+  private final PartitionSpec spec;\n+  private final Long snapshotId;\n+  private final Supplier<OutputFile> outputFileSupplier;\n+  private final long targetManifestSizeBytes;\n+  private final List<ManifestFile> manifests;\n+  private ManifestWriter writer;\n+  private long currentNumEntries;\n+\n+  public ManifestsWriter(PartitionSpec spec, Long snapshotId,\n+                         Supplier<OutputFile> outputFileSupplier,\n+                         long targetManifestSizeBytes) {\n+    this.spec = spec;\n+    this.snapshotId = snapshotId;\n+    this.outputFileSupplier = outputFileSupplier;\n+    this.targetManifestSizeBytes = targetManifestSizeBytes;\n+    this.manifests = Lists.newArrayList();\n+    this.currentNumEntries = 0;\n+  }\n+\n+  public ManifestsWriter(PartitionSpec spec,\n+                         Supplier<OutputFile> outputFileSupplier,\n+                         long targetManifestSizeBytes) {\n+    this(spec, null, outputFileSupplier, targetManifestSizeBytes);\n+  }\n+\n+  public void add(DataFile addedFile) {\n+    lazyWriter().add(addedFile);\n+    currentNumEntries++;\n+  }\n+\n+  void add(ManifestEntry entry) {\n+    lazyWriter().add(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void existing(DataFile existingFile, long fileSnapshotId) {\n+    lazyWriter().existing(existingFile, fileSnapshotId);\n+    currentNumEntries++;\n+  }\n+\n+  void existing(ManifestEntry entry) {\n+    lazyWriter().existing(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void delete(DataFile deletedFile) {\n+    lazyWriter().delete(deletedFile);\n+    currentNumEntries++;\n+  }\n+\n+  void delete(ManifestEntry entry) {\n+    lazyWriter().delete(entry);\n+    currentNumEntries++;\n+  }\n+\n+  public void close() {\n+    if (writer != null) {\n+      try {\n+        writer.close();\n+        manifests.add(writer.toManifestFile());\n+        currentNumEntries = 0;\n+      } catch (IOException e) {\n+        throw new RuntimeIOException(e);\n+      }\n+    }\n+  }\n+\n+  public List<ManifestFile> manifests() {\n+    return manifests;\n+  }\n+\n+  private ManifestWriter lazyWriter() {\n+    // verify the size of the current manifest every 25 entries to avoid calling writer.length() every time\n+    if (writer == null) {\n+      writer = new ManifestWriter(spec, outputFileSupplier.get(), snapshotId);\n+    } else if (currentNumEntries % 25 == 0 && writer.length() >= targetManifestSizeBytes) {", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MDUxMg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404470512", "bodyText": "This bit can be tricky if snapshot id inheritance is enabled concurrently.", "author": "aokolnychyi", "createdAt": "2020-04-07T00:40:10Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);\n+\n+      // the size of bins is estimated to be roughly targetManifestSizeBytes\n+      // we allow the actual size of manifests to be 5% higher to avoid closing\n+      // manifests near the end of bins if the estimation is not precise enough\n+      // it is better to have slightly bigger manifests rather than manifests with a couple of entries\n+      long manifestSizeBytes = (long) (1.05 * targetManifestSizeBytes);\n+      List<ManifestFile> newManifests = manifestEntryDS\n+          .groupByKey(toBin(bins), binEncoder)\n+          .flatMapGroups(toManifest(io, manifestSizeBytes, stagingLocation, specs), manifestEncoder)\n+          .collectAsList();\n+\n+      replaceManifests(matchingManifests, newManifests);\n+\n+      return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+    } finally {\n+      manifestEntryDS.unpersist(false);\n+    }\n+  }\n+\n+  private List<ManifestFile> findMatchingManifests() {\n+    Snapshot snapshot = table.currentSnapshot();\n+    if (snapshot == null) {\n+      return ImmutableList.of();\n+    }\n+    return snapshot.manifests().stream().filter(predicate).collect(Collectors.toList());\n+  }\n+\n+  // computes the average manifest entry size based on available stats for manifests\n+  private Long computeManifestEntrySizeBytes(List<ManifestFile> manifests) {\n+    long totalSize = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile m : manifests) {\n+      ValidationException.check(\n+          m.addedFilesCount() != null && m.existingFilesCount() != null && m.deletedFilesCount() != null,\n+          \"No file counts in manifest: \" + m.path());\n+\n+      totalSize += m.length();\n+      numEntries += m.addedFilesCount() + m.existingFilesCount() + m.deletedFilesCount();\n+    }\n+\n+    ValidationException.check(totalSize > 0L, \"Total size of manifests must be greater than 0\");\n+    ValidationException.check(numEntries > 0, \"Number of manifest entries must be greater than 0\");\n+\n+    return totalSize / numEntries;\n+  }\n+\n+  // computes the estimated metadata size per spec and partition\n+  private Map<Integer, List<PartitionMetadata>> computeMetadataSizeSummary(\n+      Dataset<Entry> manifestEntryDS,\n+      long manifestEntrySizeBytes) {\n+\n+    MetadataSizeAggregator agg = new MetadataSizeAggregator(specs, manifestEntrySizeBytes);\n+    TypedColumn<Entry, Map<Integer, List<PartitionMetadata>>> column = agg.toColumn().name(\"result\");\n+    return manifestEntryDS.select(column).collectAsList().get(0);\n+  }\n+\n+  // groups smaller partitions into bins of the target size\n+  private Map<Integer, Map<StructLike, Integer>> computeBins(Map<Integer, List<PartitionMetadata>> sizeSummary) {\n+    Map<Integer, Map<StructLike, Integer>> binMap = Maps.newHashMap();\n+\n+    sizeSummary.forEach((specId, sizes) -> {\n+      BinPacking.ListPacker<PartitionMetadata> packer = new BinPacking.ListPacker<>(targetManifestSizeBytes, 1, false);\n+      List<List<PartitionMetadata>> bins = packer.pack(sizes, PartitionMetadata::getMetadataSizeBytes);\n+\n+      for (int binIndex = 0; binIndex < bins.size(); binIndex++) {\n+        List<PartitionMetadata> bin = bins.get(binIndex);\n+\n+        for (PartitionMetadata binEntry : bin) {\n+          Map<StructLike, Integer> map = binMap.computeIfAbsent(binEntry.specId, key -> Maps.newHashMap());\n+          map.put(binEntry.partition, binIndex);\n+        }\n+      }\n+    });\n+\n+    return binMap;\n+  }\n+\n+  private void replaceManifests(Iterable<ManifestFile> deletedManifests, Iterable<ManifestFile> addedManifests) {\n+    try {\n+      RewriteManifests rewriteManifests = table.rewriteManifests();\n+      deletedManifests.forEach(rewriteManifests::deleteManifest);\n+      addedManifests.forEach(rewriteManifests::addManifest);\n+      summary.forEach(rewriteManifests::set);\n+      rewriteManifests.commit();\n+\n+      if (!snapshotIdInheritanceEnabled) {", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk5ODQzMw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404998433", "bodyText": "I think we can pick up settings at the start of a commit and use the for the duration. There's no need to try to handle updates like that in the middle of a commit.", "author": "rdblue", "createdAt": "2020-04-07T17:47:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MDUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA2OTM5Mw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405069393", "bodyText": "I also don't want to overcomplicate this. The only problem is that the clean-up logic of staged manifests in this action depends on the value before the commit while the actual commit logic that decides whether to rewrite manifests will depend on calling ops.current().properties() in BaseRewriteManifests. If snapshot id inheritance is enabled concurrently, there can be a case when BaseRewriteManifests appends manifests without rewriting them but we still delete them after the commit. The probability of this is low as we need to reuse the same Table instance to enable snapshot id inheritance. It is just a point I wanted to bring attention to.", "author": "aokolnychyi", "createdAt": "2020-04-07T19:48:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MDUxMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkyMTEwMw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413921103", "bodyText": "I kept it simple for now.", "author": "aokolnychyi", "createdAt": "2020-04-23T15:58:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MDUxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MTUxMw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r404471513", "bodyText": "If snapshot id inheritance is not enabled, newManifests will be rewritten during the commit. However, the action will still return the original staged manifests.", "author": "aokolnychyi", "createdAt": "2020-04-07T00:43:47Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);\n+\n+      // the size of bins is estimated to be roughly targetManifestSizeBytes\n+      // we allow the actual size of manifests to be 5% higher to avoid closing\n+      // manifests near the end of bins if the estimation is not precise enough\n+      // it is better to have slightly bigger manifests rather than manifests with a couple of entries\n+      long manifestSizeBytes = (long) (1.05 * targetManifestSizeBytes);\n+      List<ManifestFile> newManifests = manifestEntryDS\n+          .groupByKey(toBin(bins), binEncoder)\n+          .flatMapGroups(toManifest(io, manifestSizeBytes, stagingLocation, specs), manifestEncoder)\n+          .collectAsList();\n+\n+      replaceManifests(matchingManifests, newManifests);\n+\n+      return new RewriteManifestsActionResult(matchingManifests, newManifests);", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwMTI3Mg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405101272", "bodyText": "Why does the action return something? Shouldn't this clean up any temporary files if it needs to?", "author": "rdblue", "createdAt": "2020-04-07T20:45:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MTUxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5OTExNg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r406999116", "bodyText": "Initially, this action did not return anything. Then I started to think about returning a summary of what was done. In this case, it is a list of manifests that were replaced and added. Now, it seems a bit awkward. Maybe, we can drop this idea. Users can assign an id to the snapshot and play around with that to get some summary.", "author": "aokolnychyi", "createdAt": "2020-04-11T01:21:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MTUxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA4MDMwOQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407080309", "bodyText": "I think it should handle temp cleanup, but returning something like stats is a good idea.", "author": "rdblue", "createdAt": "2020-04-11T16:03:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ3MTUxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODUyMg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405028522", "bodyText": "I think it would simplify the implementation if you added a way to configure the spec (or spec id) you want to rewrite and default to the current spec.\nI think it's a reasonable trade-off to process just one at a time. I can imagine a few cases where a table has multiple specs:\n\nTransitioning from an old spec to a new one: we probably only need to process the old spec's files once since new data will be written with the new one.\nWhen data is written using one spec and then transitioned to another for long-term storage. Like the first case, I think you'd only want to process one spec, the one that data is written with.\n\nFocusing on a single spec allows you to simplify this.", "author": "rdblue", "createdAt": "2020-04-07T18:36:57Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1OTM5OA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405059398", "bodyText": "I agree processing one spec a time is a reasonable trade-off. Even if users need to rewrite all metadata, they can call this action for each spec. The latter can even be done in parallel.", "author": "aokolnychyi", "createdAt": "2020-04-07T19:29:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAyODUyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405035252", "bodyText": "I think I can produce basically the same result as this step with this SQL query:\nwith\n  manifests as (select * from db.table.manifests where partition_spec_id = 0),\n  entries as (select input_file_name() as manifest, * from db.table.entries where status < 2),\n  matching_entry_counts as (\n      select count(1) as entry_count, e.data_file.partition\n      from entries e join manifests m on m.path = e.manifest\n      group by e.data_file.partition)\nselect * from matching_entry_counts\nNote that this assumes we're only processing one spec at a time.", "author": "rdblue", "createdAt": "2020-04-07T18:48:05Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1MDkzMg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405050932", "bodyText": "Yes, we can definitely compute this part using metadata tables. However, we need to migrate the second part of this action (i.e. writing new manifests) to metadata tables as well. Otherwise, we will read the metadata twice and I think we want to avoid that. We need a typed Dataset and actual DataFile instances with StructLike objects as partitions. Assuming we are processing one partition spec at a time will probably help us as the partition struct will have a fixed type.", "author": "aokolnychyi", "createdAt": "2020-04-07T19:14:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA1MjQxMA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405052410", "bodyText": "I mention a couple of challenges in the TODO part of the PR description. I think we can try to map Spark's Row into DataFile or StructLike manually if we fix processing to one partition spec at a time. Any other ideas there, @rdblue?", "author": "aokolnychyi", "createdAt": "2020-04-07T19:17:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwMDQ1NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405100454", "bodyText": "Sorry, this is a big one, so I've been trying to understand the parts of it that require using MapFunction and FlatMapFunction.\nI think that we can use the entries table using a query like the one above to get the data, a global sort to do the bin packing, and then we just need to convert back to DataFile to write in a task. For that, I'd build a bean to get the data that implements DataFile, and an adapter to IndexedRecord that depends on the interface. Here's an incomplete example of the wrapper that can be used to write any DataFile implementation to Avro:\n  static class IndexedDataFile implements DataFile, IndexedRecord {\n    private final Types.StructType partitionType;\n    private DataFile wrapped = null;\n\n    IndexedDataFile(Types.StructType partitionType) {\n      this.partitionType = partitionType;\n    }\n\n    public IndexedDataFile wrap(DataFile bean) {\n      this.wrapped = bean;\n      return this;\n    }\n\n    @Override\n    public Object get(int pos) {\n      switch (pos) {\n        case 0:\n          return wrapped.path();\n        case 1:\n          return wrapped.format();\n        case 2:\n          return wrapped.partition();\n        case 3:\n          return wrapped.recordCount();\n        ...\n      }\n      throw new IllegalArgumentException(\"Unknown field position: \" + pos);\n    }\n\n    @Override\n    public void put(int i, Object v) {\n      throw new UnsupportedOperationException(\"Cannot read into IndexedDataFile\");\n    }\n\n    @Override\n    public Schema getSchema() {\n      return AvroSchemaUtil.convert(DataFile.getType(partitionType));\n    }\n\n    @Override\n    public String path() {\n      return wrapped.path();\n    }\n\n    @Override\n    public FileFormat format() {\n      return wrapped.format();\n    }\n\n    @Override\n    public PartitionData partition() {\n      return wrapped.partition();\n    }\n\n    @Override\n    public long recordCount() {\n      return wrapped.recordCount();\n    }\n\n    ...\n  }\nI'd do the same wrapping for the StructLike partition. Then for each task, you just need to get the bean, wrap it, and write it to your ManifestsWriter.", "author": "rdblue", "createdAt": "2020-04-07T20:43:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwNDc2NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405104764", "bodyText": "Also, we can keep the wrapper class package-private. The writer can wrap every incoming DataFile.", "author": "rdblue", "createdAt": "2020-04-07T20:51:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA4MDIxNA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407080214", "bodyText": "I added the IndexedDataFile wrapper in #901, so I'll resolve this thread.", "author": "rdblue", "createdAt": "2020-04-11T16:02:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAzNTI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405043553", "bodyText": "I'm not sure that computing bins is necessary. This handles skew so that the stage runtime is reasonable, but that doesn't matter for the outputs because ManifestsWriter will keep manifests to a reasonable size. So this is a runtime optimization only.\nAlso, because we're using a constant times the number of entries, manifestEntrySizeBytes, this is basically identical to using a count of entries and dividing the target size by the constant: targetEntries = targetManifestSizeBytes / manifestEntrySizeBytes. That means we're aiming for a some number of entries in each bin, which could be thought of as aiming for some number of bins given a total number of entries.\nSince we're trying to divide the input data into some number of bins, cluster by partition, and handle skew, that's something that Spark can do for us automatically. By adding a global sort and a target number of tasks, we can get the same thing without doing our own bin packing calculation.", "author": "rdblue", "createdAt": "2020-04-07T19:01:38Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();\n+\n+      long manifestEntrySizeBytes = computeManifestEntrySizeBytes(matchingManifests);\n+      Map<Integer, List<PartitionMetadata>> metadataSizeSummary = computeMetadataSizeSummary(\n+          manifestEntryDS,\n+          manifestEntrySizeBytes);\n+\n+      Map<Integer, Map<StructLike, Integer>> bins = computeBins(metadataSizeSummary);", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjk5ODU3OA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r406998578", "bodyText": "One point to consider is about the precision we can get. Users most likely won\u2019t play around with the number of samples that Spark will use to determine ranges for range partitioning. As a consequence, this can lead to a worse precision which is essential in this case. ManifestsWriter, introduced in this commit, is currently used only in cases when one partition has too much data for one partition. If we rely on Spark alone, we can get tasks that are let\u2019s say 5 MB while we want to write manifests with 4 MB. ManifestsWriter will only harm here as it would split 5 MB into 4 MB and 1 MB files. At the same time, if we get rid of ManifestsWriter and write all data in a task to one manifest, we risk having large manifests. For example, if one task gets 8MB of metadata, it will produce one large manifest even though we wanted to have manifests of 4 MB. Moreover, the ranges might not be precise and a couple of partitions can be put together. All of that will degrade the performance and job planning might take more time.", "author": "aokolnychyi", "createdAt": "2020-04-11T01:15:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA4MDEzMg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407080132", "bodyText": "I'm not sure that the other approach would do much better on precision since the size of DataFile can vary and both ways assume that it is constant. And if we need manifests to be under 4MB, then I think the right way to do that is to set the size limit to 4MB and not to rely on packing.\nI'd probably go with the simpler approach until we know that it doesn't achieve the precision necessary. The sort sampling defaults work for us almost all the time, and this is a small dataset because it is just metadata.", "author": "rdblue", "createdAt": "2020-04-11T16:01:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODM5NTQ0OA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r408395448", "bodyText": "I ran a quick test using the code I just pushed which leverages Spark for bin packing on 500 and 2600 manifests with a target size of 4 MB per manifest. As a result, I got 450 manifests where 25% of them were in 1-2 MB range and 25% were greater than 4 MB. The size of metadata per entry was pretty stable in this example. Increasing the number of samples did not help much. With manual bin-packing, I did not have any manifests larger than 4MB and most of them were bigger than 3 MB. I'll think about this more. I'll try to prototype how manual bin-packing can look like with the new approach and how it will impact the overall performance. If it makes the logic too complicated or degrades the performance, I'll consider writing whatever we have in one task to one manifest as it should be uncommon to have individual partitions that need a couple of manifests.", "author": "aokolnychyi", "createdAt": "2020-04-14T19:53:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQzMTgzOQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r408431839", "bodyText": "Thanks for running the test. I think I'd prefer the simpler approach for maintainability, but I'm okay with either one going in.", "author": "rdblue", "createdAt": "2020-04-14T20:59:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkyMjU5NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413922594", "bodyText": "I went for automatic bin-packing using Spark but I have an additional step where I collect all manifest entries I got in a task before writing manifests. This means we will iterate through entries twice but it allows us to split the metadata more evenly.", "author": "aokolnychyi", "createdAt": "2020-04-23T16:00:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTA0MzU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwMzU2NQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r405103565", "bodyText": "We never recommend caching and instead recommend using the shuffle system. Basically all you need to do is add a repartition(parallelism) and then run a map that does nothing. Then future executions will pick up the repartition's shuffle data.\nThat's much better than caching for clusters that use dynamic allocation:\n\nIt doesn't depend on the application being scaled up to keep cache balanced\nIt doesn't keep executors around for longer than needed\nIt has a smaller memory requirement (which we can't control)", "author": "rdblue", "createdAt": "2020-04-07T20:49:13Z", "path": "spark/src/main/java/org/apache/iceberg/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,490 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import com.google.common.collect.Maps;\n+import java.io.Serializable;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.function.Predicate;\n+import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.util.BinPacking;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.FlatMapGroupsFunction;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.TypedColumn;\n+import org.apache.spark.sql.expressions.Aggregator;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+// TODO: concurrent modification of snapshotIdInheritanceEnabled or specs?\n+public class RewriteManifestsAction\n+    implements SnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final Map<Integer, PartitionSpec> specs;\n+  private final Map<String, String> summary;\n+  private final int defaultParallelism;\n+  private final boolean snapshotIdInheritanceEnabled;\n+  private final long targetManifestSizeBytes;\n+\n+  private final Encoder<ManifestFile> manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+  private final Encoder<Entry> entryEncoder = Encoders.javaSerialization(Entry.class);\n+  private final Encoder<Bin> binEncoder = Encoders.bean(Bin.class);\n+\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.table = table;\n+    this.specs = table.specs();\n+    this.summary = Maps.newHashMap();\n+    this.defaultParallelism = Integer.parseInt(\n+        spark.conf().get(\"spark.default.parallelism\", \"200\"));\n+    this.snapshotIdInheritanceEnabled = PropertyUtil.propertyAsBoolean(\n+        table.properties(),\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED,\n+        TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED_DEFAULT);\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      fileIO = table.io();\n+    }\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsAction set(String property, String value) {\n+    summary.put(property, value);\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return null;\n+    }\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+\n+    int parallelism = Math.min(matchingManifests.size(), defaultParallelism);\n+    JavaRDD<ManifestFile> manifestRDD = sparkContext.parallelize(matchingManifests, parallelism);\n+    Dataset<ManifestFile> manifestDS = spark.createDataset(manifestRDD.rdd(), manifestEncoder);\n+    Dataset<Entry> manifestEntryDS = manifestDS.flatMap(toEntries(io, specs), entryEncoder);\n+\n+    try {\n+      manifestEntryDS.cache();", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzAwMDY4OA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407000688", "bodyText": "I think both approaches make sense depending on circumstances. The data set will be relatively small and caching might be not a bad option without dynamic allocation as it won't require shuffling and writing/reading from disk. I would expose a config option so that people can pick whatever is more reasonable for their case. One point is clear: we should read metadata once.", "author": "aokolnychyi", "createdAt": "2020-04-11T01:36:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTEwMzU2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MzM4NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407653384", "bodyText": "This API is useful and extendable for other usecases. Would it be a good idea to add this to the Iceberg Spec? (In a separate followup issue maybe)", "author": "prodeezy", "createdAt": "2020-04-13T18:57:54Z", "path": "spark/src/main/java/org/apache/iceberg/Actions.java", "diffHunk": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg;\n+\n+import org.apache.spark.sql.SparkSession;\n+\n+public class Actions {", "originalCommit": "aa8ec21506acfa0f69e34024b795f953f91a0531", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1NjI3MA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r407656270", "bodyText": "I think it's good to have this API (which was actually added in #873), but I wouldn't make it part of the spec. The spec covers at-rest data tracking, not how the implementations work.\nThat said, I think it would be great to add some documentation for this on the site! Both for people to know what actions will be available in 0.8.0 and to document how to build actions to include in Iceberg. I'd like to get our data warehouse team to build their planned actions this way so we can easily share them.", "author": "rdblue", "createdAt": "2020-04-13T19:02:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MzM4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwODQxMjIyNQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r408412225", "bodyText": "Yes, we would definitely want to document that before the release. I did not do that in the first place as we still might change some things.", "author": "aokolnychyi", "createdAt": "2020-04-14T20:23:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY1MzM4NA=="}], "type": "inlineReview"}, {"oid": "bb797f1421a681311704e634e4552dfa847a59dd", "url": "https://github.com/apache/iceberg/commit/bb797f1421a681311704e634e4552dfa847a59dd", "message": "Switch to metadata tables", "committedDate": "2020-04-14T19:41:52Z", "type": "forcePushed"}, {"oid": "05d7d2399557691686b07c561e8d2907d27f4c81", "url": "https://github.com/apache/iceberg/commit/05d7d2399557691686b07c561e8d2907d27f4c81", "message": "Spark: Implement an action to rewrite manifests", "committedDate": "2020-04-16T17:51:50Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MTQ2Mw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409751463", "bodyText": "This part requires further discussion. Right now, we accept a Java Predicate that we run on manifests locally and then we have a broadcast join between the matching manifest names and all manifest entries. The problem is that we read all manifest entries no matter what. However, this does not seem to be a huge problem as the metadata size is small. In addition, the current snapshot might change between the time we iterate through manifests locally and the time when we query the metadata table. It should work correctly, though, as the commit would fail if a manifest we are trying to replace is missing.\nAs an alternative, we could accept a Spark filter instead and run it on top of the metadata table. In that case, we would need to run one more job to collect matching manifest file names and construct GenericManifestFiles from those names so that we can pass them to the commit operation.", "author": "aokolnychyi", "createdAt": "2020-04-16T18:07:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,342 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return RewriteManifestsActionResult.empty();\n+    }\n+\n+    long totalSizeBytes = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile manifest : matchingManifests) {\n+      ValidationException.check(hasFileCounts(manifest), \"No file counts in manifest: \" + manifest.path());\n+\n+      totalSizeBytes += manifest.length();\n+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();\n+    }\n+\n+    int targetNumManifests = targetNumManifests(totalSizeBytes);\n+    int targetNumManifestEntries = targetNumManifestEntries(numEntries, targetNumManifests);\n+\n+    Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);\n+\n+    List<ManifestFile> newManifests;\n+    if (spec.fields().size() < 1) {\n+      newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);\n+    } else {\n+      newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests, targetNumManifestEntries);\n+    }\n+\n+    replaceManifests(matchingManifests, newManifests);\n+\n+    return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+  }\n+\n+  private Dataset<Row> buildManifestEntryDF(List<ManifestFile> manifests) {", "originalCommit": "05d7d2399557691686b07c561e8d2907d27f4c81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc5ODM3MQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409798371", "bodyText": "I think it's fine the way it works right now. I'm not concerned about loading the manifest list contents into memory since it is the top-level metadata and is done whenever you plan a scan.", "author": "rdblue", "createdAt": "2020-04-16T19:30:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MTQ2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MjA4NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409752084", "bodyText": "I am still not sure about this. Any ideas are welcome.", "author": "aokolnychyi", "createdAt": "2020-04-16T18:08:25Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsActionResult.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.collect.ImmutableList;\n+import java.util.List;\n+import org.apache.iceberg.ManifestFile;\n+\n+public class RewriteManifestsActionResult {", "originalCommit": "05d7d2399557691686b07c561e8d2907d27f4c81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc5ODQyNw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409798427", "bodyText": "Looks fine to me.", "author": "rdblue", "createdAt": "2020-04-16T19:30:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc1MjA4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc3OTgzMQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409779831", "bodyText": "You might consider using a bean that implements DataFile instead. If you did that, then Spark would construct the record for you by calling setX methods instead of you needing to keep track of the offsets. I think it would be more reliable.", "author": "rdblue", "createdAt": "2020-04-16T18:56:09Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {", "originalCommit": "05d7d2399557691686b07c561e8d2907d27f4c81", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg0NjU0MA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409846540", "bodyText": "How can I represent a Row for DataFile as a bean? I'll have to represent partition StructLike as a bean too. I am not sure there is a way to do that as the partition type varies from table to table.", "author": "aokolnychyi", "createdAt": "2020-04-16T21:03:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc3OTgzMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTg4NDA3Ng==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409884076", "bodyText": "Sorry, I forgot that was the problem. You're right about this.", "author": "rdblue", "createdAt": "2020-04-16T22:25:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc3OTgzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTc4MDk2Mw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r409780963", "bodyText": "Is this needed?", "author": "rdblue", "createdAt": "2020-04-16T18:58:15Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -0,0 +1,133 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark;\n+\n+import java.nio.ByteBuffer;\n+import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.Row;\n+\n+public class SparkDataFile implements DataFile {\n+\n+  private final Type lowerBoundsType;\n+  private final Type upperBoundsType;\n+  private final int fieldShift;\n+  private final SparkStructLike wrappedPartition;\n+  private Row wrapped;\n+\n+  public SparkDataFile(Types.StructType type) {\n+    this.lowerBoundsType = type.fieldType(\"lower_bounds\");\n+    this.upperBoundsType = type.fieldType(\"upper_bounds\");\n+    this.wrappedPartition = new SparkStructLike(type.fieldType(\"partition\").asStructType());\n+    // the partition field is absent for unpartitioned tables\n+    this.fieldShift = wrappedPartition.size() != 0 ? 1 : 0;\n+  }\n+\n+  public SparkDataFile wrap(Row row) {\n+    this.wrapped = row;\n+    if (wrappedPartition.size() > 0) {\n+      this.wrappedPartition.wrap(row.getAs(2));\n+    }\n+    return this;\n+  }\n+\n+  @Override\n+  public CharSequence path() {\n+    return wrapped.getAs(0);\n+  }\n+\n+  @Override\n+  public FileFormat format() {\n+    String formatAsString = wrapped.<String>getAs(1).toUpperCase(Locale.ROOT);\n+    return FileFormat.valueOf(formatAsString);\n+  }\n+\n+  @Override\n+  public StructLike partition() {\n+    return wrappedPartition;\n+  }\n+\n+  @Override\n+  public long recordCount() {\n+    return wrapped.getAs(fieldShift + 2);\n+  }\n+\n+  @Override\n+  public long fileSizeInBytes() {\n+    return wrapped.getAs(fieldShift + 3);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> columnSizes() {\n+    return wrapped.getJavaMap(fieldShift + 5);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> valueCounts() {\n+    return wrapped.getJavaMap(fieldShift + 6);\n+  }\n+\n+  @Override\n+  public Map<Integer, Long> nullValueCounts() {\n+    return wrapped.getJavaMap(fieldShift + 7);\n+  }\n+\n+  @Override\n+  public Map<Integer, ByteBuffer> lowerBounds() {\n+    return convert(lowerBoundsType, wrapped.getJavaMap(fieldShift + 8));\n+  }\n+\n+  @Override\n+  public Map<Integer, ByteBuffer> upperBounds() {\n+    return convert(upperBoundsType, wrapped.getJavaMap(fieldShift + 9));\n+  }\n+\n+  @Override\n+  public ByteBuffer keyMetadata() {\n+    byte[] bytes = (byte[]) wrapped.get(fieldShift + 10);\n+    return bytes != null ? ByteBuffer.wrap(bytes) : null;\n+  }\n+\n+  @Override\n+  public DataFile copy() {\n+    throw new UnsupportedOperationException(\"Not implemented: copy\");\n+  }\n+\n+  @Override\n+  public DataFile copyWithoutStats() {\n+    throw new UnsupportedOperationException(\"Not implemented: copyWithoutStats\");\n+  }\n+\n+  @Override\n+  public List<Long> splitOffsets() {\n+    return wrapped.getList(fieldShift + 11);\n+  }\n+\n+  @SuppressWarnings(\"unchecked\")\n+  private <T> T convert(Type valueType, Object value) {", "originalCommit": "05d7d2399557691686b07c561e8d2907d27f4c81", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "96ce82db28057f719a25415a5475fb3374a336aa", "url": "https://github.com/apache/iceberg/commit/96ce82db28057f719a25415a5475fb3374a336aa", "message": "Spark: Implement an action to rewrite manifests", "committedDate": "2020-04-23T15:46:30Z", "type": "commit"}, {"oid": "aa8ec21506acfa0f69e34024b795f953f91a0531", "url": "https://github.com/apache/iceberg/commit/aa8ec21506acfa0f69e34024b795f953f91a0531", "message": "Add action result validation", "committedDate": "2020-04-06T19:39:04Z", "type": "forcePushed"}, {"oid": "96ce82db28057f719a25415a5475fb3374a336aa", "url": "https://github.com/apache/iceberg/commit/96ce82db28057f719a25415a5475fb3374a336aa", "message": "Spark: Implement an action to rewrite manifests", "committedDate": "2020-04-23T15:46:30Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxNTA0NA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413915044", "bodyText": "We apply the user predicate after the predicate on the partition spec.", "author": "aokolnychyi", "createdAt": "2020-04-23T15:51:29Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.\n+ * <p>\n+ * By default, this action rewrites all manifests for the current partition spec. The behavior can\n+ * be modified by passing a custom predicate to {@link #rewriteIf(Predicate)} and a custom spec id\n+ * to {@link #specId(int)}. In addition, this action requires a staging location for new manifests\n+ * that should be configured via {@link #stagingLocation}.\n+ */\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Rewrites only manifests that match the given predicate.\n+   *\n+   * @param newPredicate a predicate\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes a location where the manifests should be written.\n+   *\n+   * @param newStagingLocation a staging location\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Configures whether the action should cache manifest entries used in multiple jobs.\n+   *\n+   * @param newUseCaching a flag whether to use caching\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");\n+\n+    List<ManifestFile> matchingManifests = findMatchingManifests();\n+    if (matchingManifests.isEmpty()) {\n+      return RewriteManifestsActionResult.empty();\n+    }\n+\n+    long totalSizeBytes = 0L;\n+    int numEntries = 0;\n+\n+    for (ManifestFile manifest : matchingManifests) {\n+      ValidationException.check(hasFileCounts(manifest), \"No file counts in manifest: \" + manifest.path());\n+\n+      totalSizeBytes += manifest.length();\n+      numEntries += manifest.addedFilesCount() + manifest.existingFilesCount() + manifest.deletedFilesCount();\n+    }\n+\n+    int targetNumManifests = targetNumManifests(totalSizeBytes);\n+    int targetNumManifestEntries = targetNumManifestEntries(numEntries, targetNumManifests);\n+\n+    Dataset<Row> manifestEntryDF = buildManifestEntryDF(matchingManifests);\n+\n+    List<ManifestFile> newManifests;\n+    if (spec.fields().size() < 1) {\n+      newManifests = writeManifestsForUnpartitionedTable(manifestEntryDF, targetNumManifests);\n+    } else {\n+      newManifests = writeManifestsForPartitionedTable(manifestEntryDF, targetNumManifests, targetNumManifestEntries);\n+    }\n+\n+    replaceManifests(matchingManifests, newManifests);\n+\n+    return new RewriteManifestsActionResult(matchingManifests, newManifests);\n+  }\n+\n+  private Dataset<Row> buildManifestEntryDF(List<ManifestFile> manifests) {\n+    Dataset<Row> manifestDF = spark\n+        .createDataset(Lists.transform(manifests, ManifestFile::path), Encoders.STRING())\n+        .toDF(\"manifest\");\n+\n+    String entriesMetadataTable = metadataTableName(MetadataTableType.ENTRIES);\n+    Dataset<Row> manifestEntryDF = spark.read().format(\"iceberg\")\n+        .load(entriesMetadataTable)\n+        .filter(\"status < 2\") // select only live entries\n+        .selectExpr(\"input_file_name() as manifest\", \"snapshot_id\", \"sequence_number\", \"data_file\");\n+\n+    Column joinCond = manifestDF.col(\"manifest\").equalTo(manifestEntryDF.col(\"manifest\"));\n+    return manifestEntryDF\n+        .join(manifestDF, joinCond, \"left_semi\")\n+        .select(\"snapshot_id\", \"sequence_number\", \"data_file\");\n+  }\n+\n+  private List<ManifestFile> writeManifestsForUnpartitionedTable(Dataset<Row> manifestEntryDF, int numManifests) {\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    StructType sparkType = (StructType) manifestEntryDF.schema().apply(\"data_file\").dataType();\n+\n+    // we rely only on the target number of manifests for unpartitioned tables\n+    // as we should not worry about having too much metadata per partition\n+    long maxNumManifestEntries = Long.MAX_VALUE;\n+\n+    return manifestEntryDF\n+        .repartition(numManifests)\n+        .mapPartitions(toManifests(io, maxNumManifestEntries, stagingLocation, spec, sparkType), manifestEncoder)\n+        .collectAsList();\n+  }\n+\n+  private List<ManifestFile> writeManifestsForPartitionedTable(\n+      Dataset<Row> manifestEntryDF, int numManifests,\n+      int targetNumManifestEntries) {\n+\n+    Broadcast<FileIO> io = sparkContext.broadcast(fileIO);\n+    StructType sparkType = (StructType) manifestEntryDF.schema().apply(\"data_file\").dataType();\n+\n+    // we allow the actual size of manifests to be 10% higher if the estimation is not precise enough\n+    long maxNumManifestEntries = (long) (1.1 * targetNumManifestEntries);\n+\n+    return withReusableDS(manifestEntryDF, df -> {\n+      Column partitionColumn = df.col(\"data_file.partition\");\n+      return df.repartitionByRange(numManifests, partitionColumn)\n+          .sortWithinPartitions(partitionColumn)\n+          .mapPartitions(toManifests(io, maxNumManifestEntries, stagingLocation, spec, sparkType), manifestEncoder)\n+          .collectAsList();\n+    });\n+  }\n+\n+  private <T, U> U withReusableDS(Dataset<T> ds, Function<Dataset<T>, U> func) {\n+    Dataset<T> reusableDS;\n+    if (useCaching) {\n+      reusableDS = ds.cache();\n+    } else {\n+      int parallelism = SQLConf.get().numShufflePartitions();\n+      reusableDS = ds.repartition(parallelism).map((MapFunction<T, T>) value -> value, ds.exprEnc());\n+    }\n+\n+    try {\n+      return func.apply(reusableDS);\n+    } finally {\n+      if (useCaching) {\n+        reusableDS.unpersist(false);\n+      }\n+    }\n+  }\n+\n+  private List<ManifestFile> findMatchingManifests() {\n+    Snapshot currentSnapshot = table.currentSnapshot();\n+\n+    if (currentSnapshot == null) {\n+      return ImmutableList.of();\n+    }\n+\n+    return currentSnapshot.manifests().stream()\n+        .filter(manifest -> manifest.partitionSpecId() == spec.specId() && predicate.test(manifest))", "originalCommit": "96ce82db28057f719a25415a5475fb3374a336aa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxNzEyMQ==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413917121", "bodyText": "This is a debatable decision. It felt safer not to rely on the order of the fields in DataFile in case if something changes.", "author": "aokolnychyi", "createdAt": "2020-04-23T15:54:06Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -33,37 +34,67 @@\n \n public class SparkDataFile implements DataFile {\n \n+  private final int filePathPosition;\n+  private final int fileFormatPosition;\n+  private final int partitionPosition;", "originalCommit": "96ce82db28057f719a25415a5475fb3374a336aa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkzOTk0Ng==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413939946", "bodyText": "I like this.", "author": "rdblue", "createdAt": "2020-04-23T16:23:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxNzEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkxODU3Nw==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413918577", "bodyText": "Unfortunately, Spark does not handle nulls in getJavaList and getJavaMap correctly. Internally, it calls .asJava on null that causes a NPE. I hit that for cases when offsets are not present.", "author": "aokolnychyi", "createdAt": "2020-04-23T15:56:00Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -74,42 +105,44 @@ public StructLike partition() {\n \n   @Override\n   public long recordCount() {\n-    return wrapped.getAs(fieldPositions[3]);\n+    return wrapped.getAs(recordCountPosition);\n   }\n \n   @Override\n   public long fileSizeInBytes() {\n-    return wrapped.getAs(fieldPositions[4]);\n+    return wrapped.getAs(fileSizeInBytesPosition);\n   }\n \n   @Override\n   public Map<Integer, Long> columnSizes() {\n-    return wrapped.getJavaMap(fieldPositions[6]);\n+    return !wrapped.isNullAt(columnSizesPosition) ? wrapped.getJavaMap(columnSizesPosition) : null;", "originalCommit": "96ce82db28057f719a25415a5475fb3374a336aa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzkzODQ3Mg==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413938472", "bodyText": "Nit: I generally prefer reversing the order instead of using !:\n\nIt's usually easier to understand the positive: if isNullAt ? null : ...\nChanging these later with a second condition is easier if the condition is positive", "author": "rdblue", "createdAt": "2020-04-23T16:21:19Z", "path": "spark/src/main/java/org/apache/iceberg/spark/SparkDataFile.java", "diffHunk": "@@ -74,42 +105,44 @@ public StructLike partition() {\n \n   @Override\n   public long recordCount() {\n-    return wrapped.getAs(fieldPositions[3]);\n+    return wrapped.getAs(recordCountPosition);\n   }\n \n   @Override\n   public long fileSizeInBytes() {\n-    return wrapped.getAs(fieldPositions[4]);\n+    return wrapped.getAs(fileSizeInBytesPosition);\n   }\n \n   @Override\n   public Map<Integer, Long> columnSizes() {\n-    return wrapped.getJavaMap(fieldPositions[6]);\n+    return !wrapped.isNullAt(columnSizesPosition) ? wrapped.getJavaMap(columnSizesPosition) : null;\n   }\n \n   @Override\n   public Map<Integer, Long> valueCounts() {\n-    return wrapped.getJavaMap(fieldPositions[7]);\n+    return !wrapped.isNullAt(valueCountsPosition) ? wrapped.getJavaMap(valueCountsPosition) : null;\n   }\n \n   @Override\n   public Map<Integer, Long> nullValueCounts() {\n-    return wrapped.getJavaMap(fieldPositions[8]);\n+    return !wrapped.isNullAt(nullValueCountsPosition) ? wrapped.getJavaMap(nullValueCountsPosition) : null;", "originalCommit": "96ce82db28057f719a25415a5475fb3374a336aa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxMzk0NjgwOA==", "url": "https://github.com/apache/iceberg/pull/875#discussion_r413946808", "bodyText": "Could we default to the equivalent of TableOperations.metadataFileLocation(...).parent()?", "author": "rdblue", "createdAt": "2020-04-23T16:32:00Z", "path": "spark/src/main/java/org/apache/iceberg/actions/RewriteManifestsAction.java", "diffHunk": "@@ -0,0 +1,375 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.actions;\n+\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Iterables;\n+import com.google.common.collect.Lists;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RewriteManifests;\n+import org.apache.iceberg.Snapshot;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.spark.SparkDataFile;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.apache.iceberg.util.Tasks;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.api.java.function.MapFunction;\n+import org.apache.spark.api.java.function.MapPartitionsFunction;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.sql.Column;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.util.SerializableConfiguration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An action that rewrites manifests in a distributed manner and co-locates metadata for partitions.\n+ * <p>\n+ * By default, this action rewrites all manifests for the current partition spec. The behavior can\n+ * be modified by passing a custom predicate to {@link #rewriteIf(Predicate)} and a custom spec id\n+ * to {@link #specId(int)}. In addition, this action requires a staging location for new manifests\n+ * that should be configured via {@link #stagingLocation}.\n+ */\n+public class RewriteManifestsAction\n+    extends BaseSnapshotUpdateAction<RewriteManifestsAction, RewriteManifestsActionResult> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(RewriteManifestsAction.class);\n+\n+  private final SparkSession spark;\n+  private final JavaSparkContext sparkContext;\n+  private final Encoder<ManifestFile> manifestEncoder;\n+  private final Table table;\n+  private final FileIO fileIO;\n+  private final long targetManifestSizeBytes;\n+\n+  private PartitionSpec spec = null;\n+  private Predicate<ManifestFile> predicate = manifest -> true;\n+  private String stagingLocation = null;\n+  private boolean useCaching = true;\n+\n+  RewriteManifestsAction(SparkSession spark, Table table) {\n+    this.spark = spark;\n+    this.sparkContext = new JavaSparkContext(spark.sparkContext());\n+    this.manifestEncoder = Encoders.javaSerialization(ManifestFile.class);\n+    this.table = table;\n+    this.spec = table.spec();\n+    this.targetManifestSizeBytes = PropertyUtil.propertyAsLong(\n+        table.properties(),\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES,\n+        TableProperties.MANIFEST_TARGET_SIZE_BYTES_DEFAULT);\n+\n+    if (table.io() instanceof HadoopFileIO) {\n+      // we need to use Spark's SerializableConfiguration to avoid issues with Kryo serialization\n+      SerializableConfiguration conf = new SerializableConfiguration(((HadoopFileIO) table.io()).conf());\n+      this.fileIO = new HadoopFileIO(conf::value);\n+    } else {\n+      this.fileIO = table.io();\n+    }\n+  }\n+\n+  @Override\n+  protected RewriteManifestsAction self() {\n+    return this;\n+  }\n+\n+  @Override\n+  protected Table table() {\n+    return table;\n+  }\n+\n+  public RewriteManifestsAction specId(int specId) {\n+    Preconditions.checkArgument(table.specs().containsKey(specId), \"Invalid spec id %d\", specId);\n+    this.spec = table.specs().get(specId);\n+    return this;\n+  }\n+\n+  /**\n+   * Rewrites only manifests that match the given predicate.\n+   *\n+   * @param newPredicate a predicate\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction rewriteIf(Predicate<ManifestFile> newPredicate) {\n+    this.predicate = newPredicate;\n+    return this;\n+  }\n+\n+  /**\n+   * Passes a location where the manifests should be written.\n+   *\n+   * @param newStagingLocation a staging location\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction stagingLocation(String newStagingLocation) {\n+    this.stagingLocation = newStagingLocation;\n+    return this;\n+  }\n+\n+  /**\n+   * Configures whether the action should cache manifest entries used in multiple jobs.\n+   *\n+   * @param newUseCaching a flag whether to use caching\n+   * @return this for method chaining\n+   */\n+  public RewriteManifestsAction useCaching(boolean newUseCaching) {\n+    this.useCaching = newUseCaching;\n+    return this;\n+  }\n+\n+  @Override\n+  public RewriteManifestsActionResult execute() {\n+    Preconditions.checkArgument(stagingLocation != null, \"Staging location must be set\");", "originalCommit": "96ce82db28057f719a25415a5475fb3374a336aa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}