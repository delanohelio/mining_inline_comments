{"pr_number": 1861, "pr_title": "Hive: OutputCommitter implementation for Hive writes", "pr_createdAt": "2020-12-02T14:44:49Z", "pr_url": "https://github.com/apache/iceberg/pull/1861", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMxMDYwMQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535310601", "bodyText": "nit: typo in word Iceberg", "author": "marton-bod", "createdAt": "2020-12-03T15:04:11Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/ClosedFileData.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+/**\n+ * Class for storing the data file properties which are needed for an Icebreg commit.", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2MjU0NQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535462545", "bodyText": "Fixed", "author": "pvary", "createdAt": "2020-12-03T18:06:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMxMDYwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMxMjI1MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535312251", "bodyText": "can we rename this to fileSize() to align with the javadoc and the above two methods? Also maybe a short javadoc comment on what unit we use here (I'm assuming bytes)", "author": "marton-bod", "createdAt": "2020-12-03T15:06:09Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/ClosedFileData.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+/**\n+ * Class for storing the data file properties which are needed for an Icebreg commit.\n+ * <ul>\n+ *   <li>Partition key\n+ *   <li>File name\n+ *   <li>File format\n+ *   <li>File size\n+ *   <li>Metrics\n+ * </ul>\n+ */\n+final class ClosedFileData implements Serializable {\n+  private final PartitionKey partitionKey;\n+  private final String fileName;\n+  private final FileFormat fileFormat;\n+  private final Long length;\n+  private final Metrics metrics;\n+\n+  ClosedFileData(PartitionKey partitionKey, String fileName, FileFormat fileFormat, Long length, Metrics metrics) {\n+    this.partitionKey = partitionKey;\n+    this.fileName = fileName;\n+    this.fileFormat = fileFormat;\n+    this.length = length;\n+    this.metrics = metrics;\n+  }\n+\n+  PartitionKey partitionKey() {\n+    return partitionKey;\n+  }\n+\n+  String fileName() {\n+    return fileName;\n+  }\n+\n+  FileFormat fileFormat() {\n+    return fileFormat;\n+  }\n+\n+  Long length() {", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2MzE0MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535463141", "bodyText": "Done. For the record the name was inherited from the appender length(), but fileSize is better.\nRenamed", "author": "pvary", "createdAt": "2020-12-03T18:07:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMxMjI1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMzNzg4Ng==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535337886", "bodyText": "can we use try-with-resources here to make sure the stream is closed?", "author": "marton-bod", "createdAt": "2020-12-03T15:34:37Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateToCommitFileLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readToCommitFile(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.length())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with file(s) {}\",\n+            System.currentTimeMillis() - startTime, table, addedFiles);\n+      } else {\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with no new files\",\n+            System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    // Remove the result directory for the failed job\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on abort job\", file, exc))\n+        .run(file -> {\n+          Path toDelete = new Path(file);\n+          FileSystem fs = Util.getFs(toDelete, jobContext.getJobConf());\n+          try {\n+            fs.delete(toDelete, true /* recursive */);\n+          } catch (IOException e) {\n+            throw new RuntimeIOException(e, \"Failed to delete job directory: %s\", file);\n+          }\n+        });\n+    cleanupJob(jobContext);\n+  }\n+\n+  private static void createToCommitFile(Set<ClosedFileData> closedFiles, String location, FileIO io)\n+      throws IOException {\n+\n+    OutputFile commitFile = io.newOutputFile(location);\n+    ObjectOutputStream oos = new ObjectOutputStream(commitFile.createOrOverwrite());", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2MzM0NQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535463345", "bodyText": "Good catch!\nThanks", "author": "pvary", "createdAt": "2020-12-03T18:07:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMzNzg4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMzOTkwMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535339903", "bodyText": "I know this is named ToCommit because of the file extension introduced in the Writer. But I personally find this wording a little confusing here (especially before scrolling down to the Writer code) - what do you think about naming this something like createFileForCommit? similarly for the read", "author": "marton-bod", "createdAt": "2020-12-03T15:36:29Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateToCommitFileLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readToCommitFile(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.length())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with file(s) {}\",\n+            System.currentTimeMillis() - startTime, table, addedFiles);\n+      } else {\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with no new files\",\n+            System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    // Remove the result directory for the failed job\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on abort job\", file, exc))\n+        .run(file -> {\n+          Path toDelete = new Path(file);\n+          FileSystem fs = Util.getFs(toDelete, jobContext.getJobConf());\n+          try {\n+            fs.delete(toDelete, true /* recursive */);\n+          } catch (IOException e) {\n+            throw new RuntimeIOException(e, \"Failed to delete job directory: %s\", file);\n+          }\n+        });\n+    cleanupJob(jobContext);\n+  }\n+\n+  private static void createToCommitFile(Set<ClosedFileData> closedFiles, String location, FileIO io)", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2NDE5Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535464193", "bodyText": "That was a long debate between me and myself \ud83d\ude04\nIf you say FileForCommit is better, then I am happy to change it.\nTry to change all occurrences :)", "author": "pvary", "createdAt": "2020-12-03T18:08:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTMzOTkwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0MzM4Mg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535343382", "bodyText": "just to clarify: Tasks is only used here for the retry feature? location seems to be a single string, so I guess there'd be no parallel execution here", "author": "marton-bod", "createdAt": "2020-12-03T15:39:44Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateToCommitFileLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readToCommitFile(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.length())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with file(s) {}\",\n+            System.currentTimeMillis() - startTime, table, addedFiles);\n+      } else {\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with no new files\",\n+            System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    // Remove the result directory for the failed job\n+    Tasks.foreach(location)", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTUyMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535479523", "bodyText": "Yes. Tasks are used for the retry here.\nUpdated the comment so it is easier to understand", "author": "pvary", "createdAt": "2020-12-03T18:27:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0MzM4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0NTcyNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535345726", "bodyText": "nit phrasing: \"Commit took {} ms for table: {} with file(s): {}\"", "author": "marton-bod", "createdAt": "2020-12-03T15:41:56Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateToCommitFileLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readToCommitFile(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.length())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit for Iceberg write taken {} ms for {} with file(s) {}\",", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2NDMwMQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535464301", "bodyText": "Done.", "author": "pvary", "createdAt": "2020-12-03T18:08:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0NTcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0NzM3NA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535347374", "bodyText": "do we want to make the retry count configurable?", "author": "marton-bod", "createdAt": "2020-12-03T15:43:30Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2NTIyOA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535465228", "bodyText": "I do not see too much point adding a new configuration for this.\nIf we see some problems we can introduce the config. Especially now, that S3 supposed to have consistent listing.", "author": "pvary", "createdAt": "2020-12-03T18:10:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0NzM3NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwOTE1Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536909153", "bodyText": "+1 for avoiding extra configuration unless we have a use case for it later.", "author": "rdblue", "createdAt": "2020-12-05T22:59:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0NzM3NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0ODk4Ng==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535348986", "bodyText": "nit: it might help to include labels for the placeholders\ne.g. \"Committing job has started for table: {}, using location: {}, expecting {} file(s).\"", "author": "marton-bod", "createdAt": "2020-12-03T15:44:54Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String commitFileLocation = LocationHelper.generateToCommitFileLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Create the committed file for the task\n+    createToCommitFile(closedFiles, commitFileLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job is started for {} using {} expecting {} file(s)\", table,", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2NTM1MA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535465350", "bodyText": "Done", "author": "pvary", "createdAt": "2020-12-03T18:10:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM0ODk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM1NzMzNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535357336", "bodyText": "can you add a short comment on why this needs to be a Container instance?", "author": "marton-bod", "createdAt": "2020-12-03T15:53:10Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,\n+                          Schema schema, PartitionSpec spec) {\n+    this.io = new HadoopFileIO(conf);\n+    this.location = location;\n+    this.fileFormat = fileFormat;\n+    this.appenderFactory = new GenericAppenderFactory(schema);\n+    this.currentKey = new PartitionKey(spec, schema);\n+    writers.put(taskAttemptID, this);\n+    LOG.info(\"IcebergRecordWriter is created in {} with {}\", location, fileFormat);\n+  }\n+\n+  @Override\n+  public void write(Writable row) {\n+    Preconditions.checkArgument(row instanceof Container);", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3OTA2MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535479061", "bodyText": "Because in the next line we try to cast it to a Container? \ud83d\ude04\nMore seriously: Previous MR patches introduced Container as a way to convert Iceberg Records to Writables and this is needed to push them through the MR framework. HiveIcebergRecordWriter only able to handle Containers and to be even more precise Container objects.\nAfter you made me think more, I got rid of this method call as the next line should throw an exception anyway and this check did not provide any more information above a ClassCastException. Also this is called for every record so saving anything small here could result in big additional performance gains.", "author": "pvary", "createdAt": "2020-12-03T18:26:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM1NzMzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM2MTAxOA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535361018", "bodyText": "did you mean closedFileData.size() or you wanted to list the map contents?", "author": "marton-bod", "createdAt": "2020-12-03T15:57:26Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,178 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,\n+                          Schema schema, PartitionSpec spec) {\n+    this.io = new HadoopFileIO(conf);\n+    this.location = location;\n+    this.fileFormat = fileFormat;\n+    this.appenderFactory = new GenericAppenderFactory(schema);\n+    this.currentKey = new PartitionKey(spec, schema);\n+    writers.put(taskAttemptID, this);\n+    LOG.info(\"IcebergRecordWriter is created in {} with {}\", location, fileFormat);\n+  }\n+\n+  @Override\n+  public void write(Writable row) {\n+    Preconditions.checkArgument(row instanceof Container);\n+\n+    Record record = ((Container<Record>) row).get();\n+\n+    currentKey.partition(record);\n+\n+    AppenderWrapper currentAppender = openAppenders.get(currentKey);\n+    if (currentAppender == null) {\n+      currentAppender = getAppender();\n+      openAppenders.put(currentKey.copy(), currentAppender);\n+    }\n+\n+    currentAppender.appender.add(record);\n+  }\n+\n+  @Override\n+  public void write(NullWritable key, Container value) {\n+    write(value);\n+  }\n+\n+  @Override\n+  public void close(boolean abort) throws IOException {\n+    // Close the open appenders and store the closed file data\n+    for (PartitionKey key : openAppenders.keySet()) {\n+      AppenderWrapper wrapper = openAppenders.get(key);\n+      wrapper.close();\n+      closedFileData.put(key,\n+          new ClosedFileData(key, wrapper.location, fileFormat, wrapper.length(), wrapper.metrics()));\n+    }\n+\n+    openAppenders.clear();\n+\n+    // If abort then remove the unnecessary files\n+    if (abort) {\n+      Tasks.foreach(closedFileData.values().stream().map(ClosedFileData::fileName).iterator())\n+          .retry(3)\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exception) -> LOG.debug(\"Failed on to remove file {} on abort\", file, exception))\n+          .run(io::deleteFile);\n+    }\n+    LOG.info(\"IcebergRecordWriter is closed. Created {} files\", closedFileData);", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ2OTc1Nw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535469757", "bodyText": "First I wanted to print the size, later decided to print the map.\nAfter a second look I have realized that the keys in the map are already contained in the values in the map. So ended up changing it to printing the values only", "author": "pvary", "createdAt": "2020-12-03T18:15:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM2MTAxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MjEzNA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535372134", "bodyText": "shouldn't this be task-[0..numTasks).toCommit? (open interval on the right)", "author": "marton-bod", "createdAt": "2020-12-03T16:11:30Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/LocationHelper.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+class LocationHelper {\n+  private static final String TO_COMMIT_EXTENSION = \".toCommit\";\n+\n+  private LocationHelper() {\n+  }\n+\n+  /**\n+   * Generates query directory location based on the configuration.\n+   * Currently it uses tableLocation/queryId\n+   * @param conf The job's configuration\n+   * @return The directory to store the query result files\n+   */\n+  static String generateQueryLocation(Configuration conf) {\n+    String tableLocation = conf.get(InputFormatConfig.TABLE_LOCATION);\n+    String queryId = conf.get(HiveConf.ConfVars.HIVEQUERYID.varname);\n+    return tableLocation + \"/\" + queryId;\n+  }\n+\n+  /**\n+   * Generates the job temp location based on the job configuration.\n+   * Currently it uses QUERY_LOCATION/jobId.\n+   * @param conf The job's configuration\n+   * @param jobId The JobID for the task\n+   * @return The file to store the results\n+   */\n+  static String generateJobLocation(Configuration conf, JobID jobId) {\n+    return generateQueryLocation(conf) + \"/\" + jobId;\n+  }\n+\n+  /**\n+   * Generates datafile location based on the task configuration.\n+   * Currently it uses QUERY_LOCATION/jobId/taskAttemptId.\n+   * @param conf The job's configuration\n+   * @param taskAttemptId The TaskAttemptID for the task\n+   * @return The file to store the results\n+   */\n+  static String generateDataFileLocation(Configuration conf, TaskAttemptID taskAttemptId) {\n+    return generateJobLocation(conf, taskAttemptId.getJobID()) + \"/\" + taskAttemptId.toString();\n+  }\n+\n+  /**\n+   * Generates file location based on the task configuration and a specific task id.\n+   * This file will be used to store the data required to generate the Iceberg commit.\n+   * Currently it uses QUERY_LOCATION/jobId/task-[0..numTasks].toCommit.", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3MDI4MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535470281", "bodyText": "I did not meant to be so mathematically correct, but you are absolutely right \ud83d\ude04", "author": "pvary", "createdAt": "2020-12-03T18:16:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM3MjEzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MTM3OQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535381379", "bodyText": "Just to check: will this be merged with the same named new class from #1854?", "author": "marton-bod", "createdAt": "2020-12-03T16:23:14Z", "path": "mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java", "diffHunk": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.time.OffsetDateTime;\n+import java.util.ArrayList;\n+import java.util.Comparator;\n+import java.util.List;\n+import org.apache.hadoop.mapred.JobID;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.IcebergGenerics;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.CloseableIterable;\n+import org.junit.Assert;\n+\n+public class HiveIcebergTestUtils {", "originalCommit": "d9d68fd4569fe039f9f6157c512d8ac37b3bd3c4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTQ3MDc0MA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r535470740", "bodyText": "I think it would be a good idea to have them merged.", "author": "pvary", "createdAt": "2020-12-03T18:16:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTM4MTM3OQ=="}], "type": "inlineReview"}, {"oid": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "url": "https://github.com/apache/iceberg/commit/9c6451c46ef708f0e84bf53b5960fa1ce700f359", "message": "Removed the Precondition since it does not have an added value anyway", "committedDate": "2020-12-03T18:37:52Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwNzczNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536907736", "bodyText": "Is this needed? The GenericDataFile and GenericDeleteFile classes are already Serializable. In Spark, the task result class has just a DataFile array.", "author": "rdblue", "createdAt": "2020-12-05T22:49:00Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/ClosedFileData.java", "diffHunk": "@@ -0,0 +1,87 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.Serializable;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+\n+/**\n+ * Class for storing the data file properties which are needed for an Iceberg commit.\n+ * <ul>\n+ *   <li>Partition key\n+ *   <li>File name\n+ *   <li>File format\n+ *   <li>File size\n+ *   <li>Metrics\n+ * </ul>\n+ */\n+final class ClosedFileData implements Serializable {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzNzU1Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537537553", "bodyText": "Serializable DataFile[] is enough.\nThanks for pointing out!", "author": "pvary", "createdAt": "2020-12-07T14:13:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwNzczNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwODU2OA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536908568", "bodyText": "In Spark, we just use DataFile[].", "author": "rdblue", "createdAt": "2020-12-05T22:54:53Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzNzcyOQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537537729", "bodyText": "Changed", "author": "pvary", "createdAt": "2020-12-07T14:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwODU2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwOTAyMg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536909022", "bodyText": "I don't think that we expect DataFile instances to be duplicated anywhere, so the set is probably unnecessary compared to using a List. Also, DataFile doesn't require that equals is implemented, so there is no guarantee that this would even deduplicate if there were duplicate data files.", "author": "rdblue", "createdAt": "2020-12-05T22:57:55Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzNzg3MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537537871", "bodyText": "Changed", "author": "pvary", "createdAt": "2020-12-07T14:13:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwOTAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwOTczNw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536909737", "bodyText": "I also like to explicitly set the failure behavior. This should probably use throwFailureWhenFinished() and stopOnFailure() because this can't continue if any task fails.\nNot using stopOnFailure is for tasks like cleaning up files, where each task should at least attempt.", "author": "rdblue", "createdAt": "2020-12-05T23:02:50Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTAwNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537539006", "bodyText": "Added throwFailureWhenFinished, but did not used stopOnFailure. This way I was able to reuse the code for abort and for commit. Since this is only for the exception handing I think this could be an acceptable compromise", "author": "pvary", "createdAt": "2020-12-07T14:15:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkwOTczNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDA5MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536910091", "bodyText": "Minor: I would recommend not adding this method because it should be obvious in the committer when paths are the same. By having this convenience method, it looks like two different signatures and a bit more magic.", "author": "rdblue", "createdAt": "2020-12-05T23:05:32Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/LocationHelper.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+class LocationHelper {\n+  private static final String FOR_COMMIT_EXTENSION = \".forCommit\";\n+\n+  private LocationHelper() {\n+  }\n+\n+  /**\n+   * Generates query directory location based on the configuration.\n+   * Currently it uses tableLocation/queryId\n+   * @param conf The job's configuration\n+   * @return The directory to store the query result files\n+   */\n+  static String generateQueryLocation(Configuration conf) {\n+    String tableLocation = conf.get(InputFormatConfig.TABLE_LOCATION);\n+    String queryId = conf.get(HiveConf.ConfVars.HIVEQUERYID.varname);\n+    return tableLocation + \"/\" + queryId;\n+  }\n+\n+  /**\n+   * Generates the job temp location based on the job configuration.\n+   * Currently it uses QUERY_LOCATION/jobId.\n+   * @param conf The job's configuration\n+   * @param jobId The JobID for the task\n+   * @return The file to store the results\n+   */\n+  static String generateJobLocation(Configuration conf, JobID jobId) {\n+    return generateQueryLocation(conf) + \"/\" + jobId;\n+  }\n+\n+  /**\n+   * Generates datafile location based on the task configuration.\n+   * Currently it uses QUERY_LOCATION/jobId/taskAttemptId.\n+   * @param conf The job's configuration\n+   * @param taskAttemptId The TaskAttemptID for the task\n+   * @return The file to store the results\n+   */\n+  static String generateDataFileLocation(Configuration conf, TaskAttemptID taskAttemptId) {\n+    return generateJobLocation(conf, taskAttemptId.getJobID()) + \"/\" + taskAttemptId.toString();\n+  }\n+\n+  /**\n+   * Generates file location based on the task configuration and a specific task id.\n+   * This file will be used to store the data required to generate the Iceberg commit.\n+   * Currently it uses QUERY_LOCATION/jobId/task-[0..numTasks).forCommit.\n+   * @param conf The job's configuration\n+   * @param jobId The jobId for the task\n+   * @param taskId The taskId for the commit file\n+   * @return The file to store the results\n+   */\n+  static String generateFileForCommitLocation(Configuration conf, JobID jobId, int taskId) {\n+    return generateJobLocation(conf, jobId) + \"/task-\" + taskId + FOR_COMMIT_EXTENSION;\n+  }\n+\n+  /**\n+   * Generates file location location based on the task configuration.\n+   * This file will be used to store the data required to generate the Iceberg commit.\n+   * Currently it uses QUERY_LOCATION/jobId/task-[0..numTasks).forCommit.\n+   * @param conf The job's configuration\n+   * @param taskAttemptId The TaskAttemptID for the task\n+   * @return The file to store the results\n+   */\n+  static String generateFileForCommitLocation(Configuration conf, TaskAttemptID taskAttemptId) {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTE1Ng==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537539156", "bodyText": "Removed", "author": "pvary", "createdAt": "2020-12-07T14:15:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDA5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDI3Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536910273", "bodyText": "You might consider moving the builder call into the ClosedFileData class as toDataFile, if you don't simply serialize DataFile instead of ClosedFileData.", "author": "rdblue", "createdAt": "2020-12-05T23:06:53Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTU4MA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537539580", "bodyText": "Removed ClosedFileData and started using DataFile[]", "author": "pvary", "createdAt": "2020-12-07T14:16:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDI3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDQ3MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536910471", "bodyText": "I would probably remove added files from the log. Maybe a debug log if you think they would be helpful.", "author": "rdblue", "createdAt": "2020-12-05T23:07:56Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.fileSize())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit took {} ms for table: {} with file(s): {}\", System.currentTimeMillis() - startTime, table,\n+            addedFiles);", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzUzOTgwNA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537539804", "bodyText": "Added specific debug log for the added files", "author": "pvary", "createdAt": "2020-12-07T14:16:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDQ3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDcwMA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536910700", "bodyText": "Is it possible to serialize the table's FileIO to tasks instead of using a new HadoopFileIO here?", "author": "rdblue", "createdAt": "2020-12-05T23:09:42Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0MDMzNw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537540337", "bodyText": "Serialized FileIO/LocationProvider/EncryptionManager", "author": "pvary", "createdAt": "2020-12-07T14:17:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDcwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDgyMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536910823", "bodyText": "Not a blocker since this is scoped to appends only, but can we detect when the user called INSERT OVERWRITE at least?", "author": "rdblue", "createdAt": "2020-12-05T23:10:32Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.fileSize())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0MjMxNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537542316", "bodyText": "I prefer to do it in another PR. I do not see how specific writes could be handled with SerDe-s. This might be trivial, but since I have too many things in progress I prefer to not to delve into a new issue before closing up some already open tasks.", "author": "pvary", "createdAt": "2020-12-07T14:19:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMDgyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTEyMQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536911121", "bodyText": "It would be nice to do this with the FileIO API rather than using Hadoop. Since each task generates just one commit file, this could generate the commit file location and use io.deleteFile(commitFile) right?", "author": "rdblue", "createdAt": "2020-12-05T23:12:29Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.fileSize())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit took {} ms for table: {} with file(s): {}\", System.currentTimeMillis() - startTime, table,\n+            addedFiles);\n+      } else {\n+        LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    // Remove the result directory for the failed job.\n+    // Intentionally used foreach on a single item. Using the Tasks API here only for the retry capability.\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on abort job\", file, exc))\n+        .run(file -> {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0MjUwMg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537542502", "bodyText": "Serialized FileIO and started to use this", "author": "pvary", "createdAt": "2020-12-07T14:19:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTEyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTc1NA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536911754", "bodyText": "I think the implementation of this is actual correct for cleanupJob, which is not implemented.\nBoth commitJob and abortJob should clean up the committer's data files, which are the files created by each task in commitTask that hold the DataFile list. Both functions should call cleanupJob to do that, and that's what this function appears to do.\nBefore abortJob calls cleanupJob, I think it should delete all of the data files that were written but will not be committed. So I think this function should actually call readFileForCommit via Tasks to generate a list of data files, then call Tasks again to delete each data file (that way both operations are independently retried).", "author": "rdblue", "createdAt": "2020-12-05T23:17:07Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.fileSize())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit took {} ms for table: {} with file(s): {}\", System.currentTimeMillis() - startTime, table,\n+            addedFiles);\n+      } else {\n+        LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0MzMyNQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537543325", "bodyText": "Refactored out the way of getting the dataFiles, and used it for removing the data files when the job is aborted", "author": "pvary", "createdAt": "2020-12-07T14:21:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTc1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTg1Mg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536911852", "bodyText": "Can we add a level between table location and query ID, like \"temp\"?", "author": "rdblue", "createdAt": "2020-12-05T23:17:52Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/LocationHelper.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+\n+class LocationHelper {\n+  private static final String FOR_COMMIT_EXTENSION = \".forCommit\";\n+\n+  private LocationHelper() {\n+  }\n+\n+  /**\n+   * Generates query directory location based on the configuration.\n+   * Currently it uses tableLocation/queryId\n+   * @param conf The job's configuration\n+   * @return The directory to store the query result files\n+   */\n+  static String generateQueryLocation(Configuration conf) {\n+    String tableLocation = conf.get(InputFormatConfig.TABLE_LOCATION);\n+    String queryId = conf.get(HiveConf.ConfVars.HIVEQUERYID.varname);\n+    return tableLocation + \"/\" + queryId;", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0MzYwNw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537543607", "bodyText": "Added temp to the table location", "author": "pvary", "createdAt": "2020-12-07T14:21:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMTg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMjExOQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536912119", "bodyText": "Minor: Since this lives in Iceberg, we can probably remove Iceberg from the class names.", "author": "rdblue", "createdAt": "2020-12-05T23:19:53Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0NDQzOA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537544438", "bodyText": "I prefer to have it in the name of the classes. When we have to debug code in Hive it could be very helpful to see that we are looking at an Iceberg class without examining the package first.", "author": "pvary", "createdAt": "2020-12-07T14:22:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMjExOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMjU2NA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536912564", "bodyText": "Ideally, LocationProvider and FileIO should be passed in and should come from the table implementation. Those are required to be Serializable.", "author": "rdblue", "createdAt": "2020-12-05T23:23:03Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,\n+                          Schema schema, PartitionSpec spec) {\n+    this.io = new HadoopFileIO(conf);\n+    this.location = location;", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0NDc3MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537544771", "bodyText": "Added LocationProvider serialization", "author": "pvary", "createdAt": "2020-12-07T14:22:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMjU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzA0OA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536913048", "bodyText": "I think it would be quite a bit easier to implement this if you based it on the existing class shared by Flink and Spark, BaseTaskWriter and its children, PartitionedWriter and UnpartitionedWriter. Since this needs to extend the mapreduce writer class, you probably can't do that directly. But, you could create an inner class that extends, for example, UnpartitionedWriter that is actually used for the write.\nUsing that as a base would make it easier to reach feature parity between Hive and the other engines. By using the existing FileOutputFactory, this would have encryption support. And it would have support for rolling new files at a target size. And also support for paths from LocationProvider.", "author": "rdblue", "createdAt": "2020-12-05T23:26:52Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0ODQwMA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537548400", "bodyText": "Hive uses the mapred hadoop interface. Extended the mapreduce writer only to try to keep parity with the mapreduce package as well, but since it complicates stuff too much simply changed to extend PartitionedFanoutWriter instead.\nMoved to PartitionedFanoutWriter because without extra settings it is not guaranteed that the data is grouped by the partition key.", "author": "pvary", "createdAt": "2020-12-07T14:27:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzA0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzI5Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536913293", "bodyText": "This won't fail if the path doesn't exist, but if you delete just the file directly, you'd want to check that it exists first, since some tasks may not have finished and committed and have aborted instead.", "author": "rdblue", "createdAt": "2020-12-05T23:28:50Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.Set;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(), attemptID);\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    Set<ClosedFileData> closedFiles = Collections.emptySet();\n+    if (writer != null) {\n+      closedFiles = writer.closedFileData();\n+    }\n+\n+    // Creating the file containing the descriptor(s) for the file(s) written by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, new HadoopFileIO(context.getJobConf()));\n+  }\n+\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    // If there are reducers, then every reducer will generate a result file.\n+    // If this is a map only task, then every mapper will generate a result file.\n+    int expectedFiles = conf.getNumReduceTasks() != 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}, expecting {} file(s).\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()), expectedFiles);\n+\n+    ExecutorService executor = null;\n+    try {\n+      // Creating executor service for parallel handling of file reads\n+      executor = Executors.newFixedThreadPool(\n+          conf.getInt(InputFormatConfig.COMMIT_THREAD_POOL_SIZE, InputFormatConfig.COMMIT_THREAD_POOL_SIZE_DEFAULT),\n+          new ThreadFactoryBuilder()\n+              .setDaemon(true)\n+              .setPriority(Thread.NORM_PRIORITY)\n+              .setNameFormat(\"iceberg-commit-pool-%d\")\n+              .build());\n+\n+      Set<DataFile> dataFiles = new ConcurrentHashMap<>().newKeySet();\n+\n+      // Reading the committed files. The assumption here is that the taskIds are generated in sequential order\n+      // starting from 0.\n+      Tasks.range(expectedFiles)\n+          .executeWith(executor)\n+          .retry(3)\n+          .run(taskId -> {\n+            String taskFileName = LocationHelper.generateFileForCommitLocation(conf, jobContext.getJobID(), taskId);\n+            Set<ClosedFileData> closedFiles = readFileForCommit(taskFileName, table.io());\n+\n+            // If the data is not empty add to the table\n+            if (!closedFiles.isEmpty()) {\n+              closedFiles.forEach(file -> {\n+                DataFiles.Builder builder = DataFiles.builder(table.spec())\n+                    .withPath(file.fileName())\n+                    .withFormat(file.fileFormat())\n+                    .withFileSizeInBytes(file.fileSize())\n+                    .withPartition(file.partitionKey())\n+                    .withMetrics(file.metrics());\n+                dataFiles.add(builder.build());\n+              });\n+            }\n+          });\n+\n+      if (dataFiles.size() > 0) {\n+        // Appending data files to the table\n+        AppendFiles append = table.newAppend();\n+        Set<String> addedFiles = new HashSet<>(dataFiles.size());\n+        dataFiles.forEach(dataFile -> {\n+          append.appendFile(dataFile);\n+          addedFiles.add(dataFile.path().toString());\n+        });\n+        append.commit();\n+        LOG.info(\"Commit took {} ms for table: {} with file(s): {}\", System.currentTimeMillis() - startTime, table,\n+            addedFiles);\n+      } else {\n+        LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+      }\n+\n+      // Calling super to cleanupJob if something more is needed\n+      cleanupJob(jobContext);\n+\n+    } finally {\n+      if (executor != null) {\n+        executor.shutdown();\n+      }\n+    }\n+  }\n+\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    // Remove the result directory for the failed job.\n+    // Intentionally used foreach on a single item. Using the Tasks API here only for the retry capability.\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on abort job\", file, exc))\n+        .run(file -> {\n+          Path toDelete = new Path(file);\n+          FileSystem fs = Util.getFs(toDelete, jobContext.getJobConf());\n+          try {\n+            fs.delete(toDelete, true /* recursive */);", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0OTQ3MA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537549470", "bodyText": "If there are runaway tasks they might write into the temp directory even after the job is finished.\nSo I feel that it is ok to clean this as a best effort. Do you agree?", "author": "pvary", "createdAt": "2020-12-07T14:29:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzI5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzQ3Ng==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536913476", "bodyText": "I would expect a log message for aborting this task, not just the close message below with how many files were created.", "author": "rdblue", "createdAt": "2020-12-05T23:30:05Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,\n+                          Schema schema, PartitionSpec spec) {\n+    this.io = new HadoopFileIO(conf);\n+    this.location = location;\n+    this.fileFormat = fileFormat;\n+    this.appenderFactory = new GenericAppenderFactory(schema);\n+    this.currentKey = new PartitionKey(spec, schema);\n+    writers.put(taskAttemptID, this);\n+    LOG.info(\"IcebergRecordWriter is created in {} with {}\", location, fileFormat);\n+  }\n+\n+  @Override\n+  public void write(Writable row) {\n+    Record record = ((Container<Record>) row).get();\n+\n+    // Update the current key with the record, so we do not create a new object for every record\n+    currentKey.partition(record);\n+\n+    AppenderWrapper currentAppender = openAppenders.get(currentKey);\n+    if (currentAppender == null) {\n+      currentAppender = getAppender();\n+      openAppenders.put(currentKey.copy(), currentAppender);\n+    }\n+\n+    currentAppender.appender.add(record);\n+  }\n+\n+  @Override\n+  public void write(NullWritable key, Container value) {\n+    write(value);\n+  }\n+\n+  @Override\n+  public void close(boolean abort) throws IOException {\n+    // Close the open appenders and store the closed file data\n+    for (PartitionKey key : openAppenders.keySet()) {\n+      AppenderWrapper wrapper = openAppenders.get(key);\n+      wrapper.close();\n+      closedFileData.put(key,\n+          new ClosedFileData(key, wrapper.location, fileFormat, wrapper.length(), wrapper.metrics()));\n+    }\n+\n+    openAppenders.clear();\n+\n+    // If abort then remove the unnecessary files\n+    if (abort) {", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0OTcyOQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537549729", "bodyText": "Yeah, somehow missed that.\nThanks for pointing out!\nAdded the log message.", "author": "pvary", "createdAt": "2020-12-07T14:29:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzQ3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzU0Mw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r536913543", "bodyText": "Can you move this static method up to the top near the static writers map? Seems like that should all be kept together.", "author": "rdblue", "createdAt": "2020-12-05T23:30:47Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Metrics;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.GenericAppenderFactory;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.hadoop.HadoopFileIO;\n+import org.apache.iceberg.io.FileAppender;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends org.apache.hadoop.mapreduce.RecordWriter<NullWritable, Container>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  private final FileIO io;\n+  private final String location;\n+  private final FileFormat fileFormat;\n+  private final GenericAppenderFactory appenderFactory;\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  // Data for every partition is written to a different appender\n+  // This map stores the open appenders for the given partition key\n+  private final Map<PartitionKey, AppenderWrapper> openAppenders = new HashMap<>();\n+  // When the appenders are closed the file data needed for the Iceberg commit is stored and accessible through\n+  // this map\n+  private final Map<PartitionKey, ClosedFileData> closedFileData = new HashMap<>();\n+\n+  HiveIcebergRecordWriter(TaskAttemptID taskAttemptID, Configuration conf, String location, FileFormat fileFormat,\n+                          Schema schema, PartitionSpec spec) {\n+    this.io = new HadoopFileIO(conf);\n+    this.location = location;\n+    this.fileFormat = fileFormat;\n+    this.appenderFactory = new GenericAppenderFactory(schema);\n+    this.currentKey = new PartitionKey(spec, schema);\n+    writers.put(taskAttemptID, this);\n+    LOG.info(\"IcebergRecordWriter is created in {} with {}\", location, fileFormat);\n+  }\n+\n+  @Override\n+  public void write(Writable row) {\n+    Record record = ((Container<Record>) row).get();\n+\n+    // Update the current key with the record, so we do not create a new object for every record\n+    currentKey.partition(record);\n+\n+    AppenderWrapper currentAppender = openAppenders.get(currentKey);\n+    if (currentAppender == null) {\n+      currentAppender = getAppender();\n+      openAppenders.put(currentKey.copy(), currentAppender);\n+    }\n+\n+    currentAppender.appender.add(record);\n+  }\n+\n+  @Override\n+  public void write(NullWritable key, Container value) {\n+    write(value);\n+  }\n+\n+  @Override\n+  public void close(boolean abort) throws IOException {\n+    // Close the open appenders and store the closed file data\n+    for (PartitionKey key : openAppenders.keySet()) {\n+      AppenderWrapper wrapper = openAppenders.get(key);\n+      wrapper.close();\n+      closedFileData.put(key,\n+          new ClosedFileData(key, wrapper.location, fileFormat, wrapper.length(), wrapper.metrics()));\n+    }\n+\n+    openAppenders.clear();\n+\n+    // If abort then remove the unnecessary files\n+    if (abort) {\n+      Tasks.foreach(closedFileData.values().stream().map(ClosedFileData::fileName).iterator())\n+          .retry(3)\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exception) -> LOG.debug(\"Failed on to remove file {} on abort\", file, exception))\n+          .run(io::deleteFile);\n+    }\n+    LOG.info(\"IcebergRecordWriter is closed. Created {} files\", closedFileData.values());\n+  }\n+\n+  @Override\n+  public void close(org.apache.hadoop.mapreduce.TaskAttemptContext context) throws IOException {\n+    close(false);\n+  }\n+\n+  @Override\n+  public void close(Reporter reporter) throws IOException {\n+    close(false);\n+  }\n+\n+  public Set<ClosedFileData> closedFileData() {\n+    return new HashSet<>(closedFileData.values());\n+  }\n+\n+  static HiveIcebergRecordWriter removeWriter(TaskAttemptID taskAttemptID) {\n+    return writers.remove(taskAttemptID);\n+  }", "originalCommit": "9c6451c46ef708f0e84bf53b5960fa1ce700f359", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzU0OTgxNA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537549814", "bodyText": "Done", "author": "pvary", "createdAt": "2020-12-07T14:29:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjkxMzU0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5MDEzNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537790136", "bodyText": "You can use run(file -> {...}, IOException.class) to be able to throw IOException from the callable here instead of catching it. That way you don't need the try/catch.", "author": "rdblue", "createdAt": "2020-12-07T19:56:42Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+  private FileIO io;\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  /**\n+   * Collects the generated data files and creates a commit file storing the data file list.\n+   * @param context The job context\n+   * @throws IOException Thrown if there is an error writing the commit file\n+   */\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(),\n+        attemptID.getJobID(), attemptID.getTaskID().getId());\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    DataFile[] closedFiles;\n+    if (writer != null) {\n+      closedFiles = writer.complete();\n+    } else {\n+      closedFiles = new DataFile[0];\n+    }\n+\n+    // Creating the file containing the data files generated by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, io(context.getJobConf()));\n+  }\n+\n+  /**\n+   * Removes files generated by this task.\n+   * @param context The task context\n+   * @throws IOException Thrown if there is an error closing the writer\n+   */\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  /**\n+   * Reads the commit files stored in the temp directory and collects the generated committed data files.\n+   * Appends the data files to the table. At the end removes the temporary directory.\n+   * @param jobContext The job context\n+   */\n+  @Override\n+  public void commitJob(JobContext jobContext) {\n+    JobConf conf = jobContext.getJobConf();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()));\n+\n+    List<DataFile> dataFiles = dataFiles(jobContext, io(jobContext.getJobConf()), true);\n+\n+    if (dataFiles.size() > 0) {\n+      // Appending data files to the table\n+      AppendFiles append = table.newAppend();\n+      dataFiles.forEach(append::appendFile);\n+      append.commit();\n+      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n+          dataFiles.size());\n+      LOG.debug(\"Added files {}\", dataFiles);\n+    } else {\n+      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+    }\n+\n+    cleanup(jobContext);\n+  }\n+\n+  /**\n+   * Removes the generated data files, if there is a commit file already generated for them.\n+   * The cleanup at the end removes the temporary directory as well.\n+   * @param jobContext The job context\n+   * @param status The status of the job\n+   */\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    List<DataFile> dataFiles = dataFiles(jobContext, io(jobContext.getJobConf()), false);\n+\n+    // Check if we have files already committed and remove data files if there are any\n+    if (dataFiles.size() > 0) {\n+      Tasks.foreach(dataFiles)\n+          .retry(3)\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n+          .run(file -> io(jobContext.getJobConf()).deleteFile(file.path().toString()));\n+    }\n+\n+    cleanup(jobContext);\n+  }\n+\n+  /**\n+   * Cleans up the jobs temporary location.\n+   * @param jobContext The job context\n+   */\n+  private void cleanup(JobContext jobContext) {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Cleaning for job: {} on location: {}\", jobContext.getJobID(), location);\n+\n+    // Remove the job's temp directory recursively.\n+    // Intentionally used foreach on a single item. Using the Tasks API here only for the retry capability.\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on cleanup job\", file, exc))\n+        .run(file -> {\n+          Path toDelete = new Path(file);\n+          FileSystem fs = Util.getFs(toDelete, jobContext.getJobConf());\n+          try {\n+            fs.delete(toDelete, true);\n+          } catch (IOException e) {\n+            throw new UncheckedIOException(String.format(\"Failed to delete job directory: %s\", file), e);", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNjE4MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537816181", "bodyText": "Thanks!\nDone", "author": "pvary", "createdAt": "2020-12-07T20:39:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5MDEzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5MTY0OA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537791648", "bodyText": "Minor: io stores the FileIO after it is loaded, but it still seems odd to call the method and pass in the configuration each time. I think it would be cleaner to have FileIO = io(jobContext.getJobConf()); at the top of this method. That keeps the handler here simpler.", "author": "rdblue", "createdAt": "2020-12-07T19:59:00Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,267 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.io.UncheckedIOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+  private FileIO io;\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  /**\n+   * Collects the generated data files and creates a commit file storing the data file list.\n+   * @param context The job context\n+   * @throws IOException Thrown if there is an error writing the commit file\n+   */\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = LocationHelper.generateFileForCommitLocation(context.getJobConf(),\n+        attemptID.getJobID(), attemptID.getTaskID().getId());\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    DataFile[] closedFiles;\n+    if (writer != null) {\n+      closedFiles = writer.complete();\n+    } else {\n+      closedFiles = new DataFile[0];\n+    }\n+\n+    // Creating the file containing the data files generated by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, io(context.getJobConf()));\n+  }\n+\n+  /**\n+   * Removes files generated by this task.\n+   * @param context The task context\n+   * @throws IOException Thrown if there is an error closing the writer\n+   */\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  /**\n+   * Reads the commit files stored in the temp directory and collects the generated committed data files.\n+   * Appends the data files to the table. At the end removes the temporary directory.\n+   * @param jobContext The job context\n+   */\n+  @Override\n+  public void commitJob(JobContext jobContext) {\n+    JobConf conf = jobContext.getJobConf();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n+        LocationHelper.generateJobLocation(conf, jobContext.getJobID()));\n+\n+    List<DataFile> dataFiles = dataFiles(jobContext, io(jobContext.getJobConf()), true);\n+\n+    if (dataFiles.size() > 0) {\n+      // Appending data files to the table\n+      AppendFiles append = table.newAppend();\n+      dataFiles.forEach(append::appendFile);\n+      append.commit();\n+      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n+          dataFiles.size());\n+      LOG.debug(\"Added files {}\", dataFiles);\n+    } else {\n+      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+    }\n+\n+    cleanup(jobContext);\n+  }\n+\n+  /**\n+   * Removes the generated data files, if there is a commit file already generated for them.\n+   * The cleanup at the end removes the temporary directory as well.\n+   * @param jobContext The job context\n+   * @param status The status of the job\n+   */\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) {\n+    String location = LocationHelper.generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    List<DataFile> dataFiles = dataFiles(jobContext, io(jobContext.getJobConf()), false);\n+\n+    // Check if we have files already committed and remove data files if there are any\n+    if (dataFiles.size() > 0) {\n+      Tasks.foreach(dataFiles)\n+          .retry(3)\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n+          .run(file -> io(jobContext.getJobConf()).deleteFile(file.path().toString()));", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNjcxMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537816713", "bodyText": "Removed the io(Configuration configuration) method. Easier to understand and we do not really reuse the io.\nDone", "author": "pvary", "createdAt": "2020-12-07T20:39:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5MTY0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NDIxNg==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537794216", "bodyText": "This was just changed in #1867. I think you just need to update it to super.dataFiles().", "author": "rdblue", "createdAt": "2020-12-07T20:03:07Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.io.PartitionedFanoutWriter;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends PartitionedFanoutWriter<Record>\n+    implements FileSinkOperator.RecordWriter, org.apache.hadoop.mapred.RecordWriter<NullWritable, Container<Record>> {\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergRecordWriter.class);\n+\n+  // The current key is reused at every write to avoid unnecessary object creation\n+  private final PartitionKey currentKey;\n+  private final FileIO io;\n+\n+  // <TaskAttemptId, HiveIcebergRecordWriter> map to store the active writers\n+  // Stored in concurrent map, since some executor engines can share containers\n+  private static final Map<TaskAttemptID, HiveIcebergRecordWriter> writers = new ConcurrentHashMap<>();\n+\n+  static HiveIcebergRecordWriter removeWriter(TaskAttemptID taskAttemptID) {\n+    return writers.remove(taskAttemptID);\n+  }\n+\n+  HiveIcebergRecordWriter(Schema schema, PartitionSpec spec, FileFormat format,\n+      FileAppenderFactory<Record> appenderFactory, OutputFileFactory fileFactory, FileIO io, long targetFileSize,\n+      TaskAttemptID taskAttemptID) {\n+    super(spec, format, appenderFactory, fileFactory, io, targetFileSize);\n+    this.io = io;\n+    this.currentKey = new PartitionKey(spec, schema);\n+    writers.put(taskAttemptID, this);\n+  }\n+\n+  @Override\n+  protected PartitionKey partition(Record row) {\n+    currentKey.partition(row);\n+    return currentKey;\n+  }\n+\n+  @Override\n+  public void write(Writable row) throws IOException {\n+    super.write(((Container<Record>) row).get());\n+  }\n+\n+  @Override\n+  public void write(NullWritable key, Container value) throws IOException {\n+    write(value);\n+  }\n+\n+  @Override\n+  public void close(boolean abort) throws IOException {\n+    DataFile[] dataFiles = super.complete();", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNjg1OQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537816859", "bodyText": "Fixed\nThanks for the pointer!", "author": "pvary", "createdAt": "2020-12-07T20:40:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NDIxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NDk2NQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537794965", "bodyText": "We generally don't recommend using the fanout writer because it takes so much memory and could easily produce a ton of tiny files. Are you sure you want to use it instead of the normal partitioned writer?", "author": "rdblue", "createdAt": "2020-12-07T20:04:22Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergRecordWriter.java", "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n+import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Writable;\n+import org.apache.hadoop.mapred.Reporter;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionKey;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.io.FileAppenderFactory;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.io.PartitionedFanoutWriter;\n+import org.apache.iceberg.mr.mapred.Container;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+class HiveIcebergRecordWriter extends PartitionedFanoutWriter<Record>", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODI4NQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537818285", "bodyText": "How much is per writer?\nMy approach is: I would like to see the writes working. We can optimize later.\nWhat I see as a problem here is that the data should be sorted, or at least grouped by partition key, which I think is a non-trivial task without changing the query.", "author": "pvary", "createdAt": "2020-12-07T20:42:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NDk2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODc3MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537848771", "bodyText": "What I see as a problem here is that the data should be sorted, or at least grouped by partition key, which I think is a non-trivial task without changing the query.\n\nYes, and we still don't have this working in Spark yet. But we've documented how to write to tables.\nI'm okay with this for initial development, but I don't think it is something that we should ship with by default if we consider Hive writes production-ready.", "author": "rdblue", "createdAt": "2020-12-07T21:33:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NDk2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjU0NA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537796544", "bodyText": "Could this class delegate to SerializationUtil.serializeToBase64 and SerializationUtil.deserializeFromBase64?", "author": "rdblue", "createdAt": "2020-12-07T20:07:05Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java", "diffHunk": "@@ -133,4 +146,93 @@ public DecomposedPredicate decomposePredicate(JobConf jobConf, Deserializer dese\n     predicate.pushedPredicate = (ExprNodeGenericFuncDesc) exprNodeDesc;\n     return predicate;\n   }\n+\n+  /**\n+   * Returns the Table FileIO serialized to the configuration.\n+   * @param configuration The configuration used to get the data from\n+   * @return The Table FileIO object\n+   */\n+  public static FileIO io(Configuration configuration) {\n+    return (FileIO) get(configuration, InputFormatConfig.FILE_IO);\n+  }\n+\n+  /**\n+   * Returns the Table LocationProvider serialized to the configuration.\n+   * @param configuration The configuration used to get the data from\n+   * @return The Table LocationProvider object\n+   */\n+  public static LocationProvider location(Configuration configuration) {\n+    return (LocationProvider) get(configuration, InputFormatConfig.LOCATION_PROVIDER);\n+  }\n+\n+  /**\n+   * Returns the Table EncryptionManager serialized to the configuration.\n+   * @param configuration The configuration used to get the data from\n+   * @return The Table EncryptionManager object\n+   */\n+  public static EncryptionManager encryption(Configuration configuration) {\n+    return (EncryptionManager) get(configuration, InputFormatConfig.ENCRYPTION_MANAGER);\n+  }\n+\n+  /**\n+   * Returns the Table Schema serialized to the configuration.\n+   * @param configuration The configuration used to get the data from\n+   * @return The Table Schema object\n+   */\n+  public static Schema schema(Configuration configuration) {\n+    return SchemaParser.fromJson(configuration.get(InputFormatConfig.TABLE_SCHEMA));\n+  }\n+\n+  /**\n+   * Returns the Table PartitionSpec serialized to the configuration.\n+   * @param configuration The configuration used to get the data from\n+   * @return The Table PartitionSpec object\n+   */\n+  public static PartitionSpec spec(Configuration configuration) {\n+    return PartitionSpecParser.fromJson(schema(configuration), configuration.get(InputFormatConfig.PARTITION_SPEC));\n+  }\n+\n+  /**\n+   * Stores the serializable table data in the configuration.\n+   * Currently the following is handled:\n+   * <ul>\n+   *   <li>- Location</li>\n+   *   <li>- Schema</li>\n+   *   <li>- Partition specification</li>\n+   *   <li>- FileIO for handling table files</li>\n+   *   <li>- Location provider used for file generation</li>\n+   *   <li>- Encryption manager for encryption handling</li>\n+   * </ul>\n+   * @param configuration The target configuration to store to\n+   * @param table The table which we want to store to the configuration\n+   */\n+  @VisibleForTesting\n+  static void put(Configuration configuration, Table table) {\n+    configuration.set(InputFormatConfig.TABLE_LOCATION, table.location());\n+    configuration.set(InputFormatConfig.TABLE_SCHEMA, SchemaParser.toJson(table.schema()));\n+    configuration.set(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(table.spec()));\n+\n+    put(configuration, InputFormatConfig.FILE_IO, table.io());\n+    put(configuration, InputFormatConfig.LOCATION_PROVIDER, table.locationProvider());\n+    put(configuration, InputFormatConfig.ENCRYPTION_MANAGER, table.encryption());\n+  }\n+\n+  private static void put(Configuration configuration, String key, Serializable object) {\n+    try (ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+         ObjectOutputStream oos = new ObjectOutputStream(baos)) {\n+      oos.writeObject(object);\n+      configuration.set(key, Base64.getEncoder().encodeToString(baos.toByteArray()));\n+    } catch (IOException ioe) {\n+      throw new RuntimeException(String.format(\"Error serializing %s to configuration\", object), ioe);\n+    }\n+  }\n+\n+  private static Object get(Configuration configuration, String key) {", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxODUxNA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537818514", "bodyText": "Definitely!\nRemoved the methods for serializing/deserializing! :)", "author": "pvary", "createdAt": "2020-12-07T20:43:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzc5NjU0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMDgyOQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537800829", "bodyText": "The first number is typically the task ID so we can trace back to the task that produced a file easily.\nThe job ID is constant and will be 1-to-1 with a write UUID, so I'm not sure that it is needed. If not, then this could use the existing factory and pass taskAttemptId.getTaskID() and taskAttemptId.getId() to the constructor (instead of 0, 0).\nI think I'd prefer using the existing one and not creating a new subclass here.", "author": "rdblue", "createdAt": "2020-12-07T20:14:02Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveOutputFileFactory.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.encryption.EncryptionManager;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.LocationProvider;\n+import org.apache.iceberg.io.OutputFileFactory;\n+\n+public class HiveOutputFileFactory extends OutputFileFactory {\n+  private final TaskAttemptID taskAttemptID;\n+\n+  public HiveOutputFileFactory(PartitionSpec spec, FileFormat fileFormat, LocationProvider locationProvider, FileIO io,\n+                               EncryptionManager encryptionManager, TaskAttemptID taskAttemptID) {\n+    super(spec, fileFormat, locationProvider, io, encryptionManager, 0, 0);\n+    this.taskAttemptID = taskAttemptID;\n+  }\n+\n+  /**\n+   * Override the filename generation so it contains jobId, taskId, taskAttemptId. Kept the UUID and the fileCount so\n+   * the filenames are similar for other writers.\n+   * @return The generated file name\n+   */\n+  @Override\n+  protected String generateFilename() {\n+    return format().addExtension(\n+        String.format(\"%05d-%d-%d-%s-%05d\", taskAttemptID.getJobID().getId(), taskAttemptID.getTaskID().getId(),", "originalCommit": "88459ba6ba717ed9639bb7ca5ef6ba55ea46d594", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgyNDMzMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537824333", "bodyText": "The uuid is generated by the OutputFileFactory - this will be generated on every mapper-reducer. So different tasks / even task attempts will have a different uuid. The only thing connecting together the files generated by the same query would be the jobID (and even that can restart).\nI was thinking about changing the fileName format to something like:\n<queryId>-<jobId>-<taskId>-<attemptId>-<fileCount>.<fileFormat>\nBut decided to keep something more like the current names for the familiarities shake.", "author": "pvary", "createdAt": "2020-12-07T20:52:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMDgyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0NzQxMw==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r537847413", "bodyText": "The UUID was supposed to be a UUID for the write, not for each task. Looks like that was broken at some point by refactoring.\nIn that case, What do you think about adding a constructor that accepts a UUID string? Then you could call it with an identifier that works for your job, whether that's a job-level UUID from config or a Hadoop Job ID. And we can follow up in the other engines and generate the UUID correctly.", "author": "rdblue", "createdAt": "2020-12-07T21:31:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMDgyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODExMjI1MQ==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r538112251", "bodyText": "Removed HiveOutputFileFactory - added new cosntructor to OutputFileFactory instead\nAlso since the locations are generated by the OutputFileFactory, we do not need LocationHelper anymore, so I removed it as well.", "author": "pvary", "createdAt": "2020-12-08T07:54:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMDgyOQ=="}], "type": "inlineReview"}, {"oid": "80dd89da87bded83d8811da5001112808bb160c4", "url": "https://github.com/apache/iceberg/commit/80dd89da87bded83d8811da5001112808bb160c4", "message": "Rebase and some minor changes", "committedDate": "2020-12-07T21:43:53Z", "type": "forcePushed"}, {"oid": "2f310b0b4ab92398907c5f6f6b2c84378cf9d263", "url": "https://github.com/apache/iceberg/commit/2f310b0b4ab92398907c5f6f6b2c84378cf9d263", "message": "Hive: OutputCommitter implementation for Hive writes", "committedDate": "2020-12-09T07:19:42Z", "type": "commit"}, {"oid": "fa43aef90c9bd95ff4dbf5859e90acd9b7e052e2", "url": "https://github.com/apache/iceberg/commit/fa43aef90c9bd95ff4dbf5859e90acd9b7e052e2", "message": "Addressing Marton's comments", "committedDate": "2020-12-09T07:19:46Z", "type": "commit"}, {"oid": "a12fd5ec8a1948ef3d099d66e6049e154179fc02", "url": "https://github.com/apache/iceberg/commit/a12fd5ec8a1948ef3d099d66e6049e154179fc02", "message": "Removed the Precondition since it does not have an added value anyway", "committedDate": "2020-12-09T07:19:46Z", "type": "commit"}, {"oid": "97c1016bde16cd30b676d795546f0d3cf473ca42", "url": "https://github.com/apache/iceberg/commit/97c1016bde16cd30b676d795546f0d3cf473ca42", "message": "Moved to PartitionedFanoutWriter\nAddressed other review comments as well.", "committedDate": "2020-12-09T07:22:18Z", "type": "commit"}, {"oid": "7ab921c9e2382b5dae1e5cbafb66b77d11c7ea80", "url": "https://github.com/apache/iceberg/commit/7ab921c9e2382b5dae1e5cbafb66b77d11c7ea80", "message": "Checkstyle", "committedDate": "2020-12-09T07:22:20Z", "type": "commit"}, {"oid": "55b9223902bb1140a5be63dcc6044f2693fc2e1e", "url": "https://github.com/apache/iceberg/commit/55b9223902bb1140a5be63dcc6044f2693fc2e1e", "message": "Removing crc files", "committedDate": "2020-12-09T07:22:20Z", "type": "commit"}, {"oid": "16dbfb8355fa0673bb586d157a457e018abe2322", "url": "https://github.com/apache/iceberg/commit/16dbfb8355fa0673bb586d157a457e018abe2322", "message": "Rebase and some minor changes", "committedDate": "2020-12-09T07:22:20Z", "type": "commit"}, {"oid": "f1eddc4a3b8e5b883e8c2458ddb24fc36de0bccb", "url": "https://github.com/apache/iceberg/commit/f1eddc4a3b8e5b883e8c2458ddb24fc36de0bccb", "message": "Removed HiveOutputFileFactory - added new cosntructor to OutputFileFactory instead\nRemoved LocationHelper - added the methods to HiveIcebergOutputCommitter instead since the methods are not used elsewhere anymore", "committedDate": "2020-12-09T07:22:58Z", "type": "commit"}, {"oid": "1741c3793842496b07e32db505270f5dc8b451fa", "url": "https://github.com/apache/iceberg/commit/1741c3793842496b07e32db505270f5dc8b451fa", "message": "Rebased", "committedDate": "2020-12-09T07:51:57Z", "type": "commit"}, {"oid": "1741c3793842496b07e32db505270f5dc8b451fa", "url": "https://github.com/apache/iceberg/commit/1741c3793842496b07e32db505270f5dc8b451fa", "message": "Rebased", "committedDate": "2020-12-09T07:51:57Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDQwMjY1NA==", "url": "https://github.com/apache/iceberg/pull/1861#discussion_r540402654", "bodyText": "Nit: I would prefer if cleanup happened by removing the expected task commit files one-by-one rather than deleting a directory because it could use FileIO. I understand that this is intended to drop the folder as well for stores that track folders. Maybe a follow-up to add a deletePrefix to FileIO would fix it.", "author": "rdblue", "createdAt": "2020-12-10T18:31:50Z", "path": "mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java", "diffHunk": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.mr.hive;\n+\n+import java.io.IOException;\n+import java.io.ObjectInputStream;\n+import java.io.ObjectOutputStream;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.mapred.JobConf;\n+import org.apache.hadoop.mapred.JobContext;\n+import org.apache.hadoop.mapred.OutputCommitter;\n+import org.apache.hadoop.mapred.TaskAttemptContext;\n+import org.apache.hadoop.mapred.TaskAttemptID;\n+import org.apache.hadoop.mapreduce.JobID;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.iceberg.AppendFiles;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.exceptions.NotFoundException;\n+import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.io.FileIO;\n+import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.mr.Catalogs;\n+import org.apache.iceberg.mr.InputFormatConfig;\n+import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.util.Tasks;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * An Iceberg table committer for adding data files to the Iceberg tables.\n+ * Currently independent of the Hive ACID transactions.\n+ */\n+public final class HiveIcebergOutputCommitter extends OutputCommitter {\n+  private static final String FOR_COMMIT_EXTENSION = \".forCommit\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(HiveIcebergOutputCommitter.class);\n+\n+  @Override\n+  public void setupJob(JobContext jobContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public void setupTask(TaskAttemptContext taskAttemptContext) {\n+    // do nothing.\n+  }\n+\n+  @Override\n+  public boolean needsTaskCommit(TaskAttemptContext context) {\n+    // We need to commit if this is the last phase of a MapReduce process\n+    return TaskType.REDUCE.equals(context.getTaskAttemptID().getTaskID().getTaskType()) ||\n+        context.getJobConf().getNumReduceTasks() == 0;\n+  }\n+\n+  /**\n+   * Collects the generated data files and creates a commit file storing the data file list.\n+   * @param context The job context\n+   * @throws IOException Thrown if there is an error writing the commit file\n+   */\n+  @Override\n+  public void commitTask(TaskAttemptContext context) throws IOException {\n+    TaskAttemptID attemptID = context.getTaskAttemptID();\n+    String fileForCommitLocation = generateFileForCommitLocation(context.getJobConf(),\n+        attemptID.getJobID(), attemptID.getTaskID().getId());\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(attemptID);\n+\n+    DataFile[] closedFiles;\n+    if (writer != null) {\n+      closedFiles = writer.dataFiles();\n+    } else {\n+      closedFiles = new DataFile[0];\n+    }\n+\n+    // Creating the file containing the data files generated by this task\n+    createFileForCommit(closedFiles, fileForCommitLocation, HiveIcebergStorageHandler.io(context.getJobConf()));\n+  }\n+\n+  /**\n+   * Removes files generated by this task.\n+   * @param context The task context\n+   * @throws IOException Thrown if there is an error closing the writer\n+   */\n+  @Override\n+  public void abortTask(TaskAttemptContext context) throws IOException {\n+    // Clean up writer data from the local store\n+    HiveIcebergRecordWriter writer = HiveIcebergRecordWriter.removeWriter(context.getTaskAttemptID());\n+\n+    // Remove files if it was not done already\n+    writer.close(true);\n+  }\n+\n+  /**\n+   * Reads the commit files stored in the temp directory and collects the generated committed data files.\n+   * Appends the data files to the table. At the end removes the temporary directory.\n+   * @param jobContext The job context\n+   * @throws IOException if there is a failure deleting the files\n+   */\n+  @Override\n+  public void commitJob(JobContext jobContext) throws IOException {\n+    JobConf conf = jobContext.getJobConf();\n+    Table table = Catalogs.loadTable(conf);\n+\n+    long startTime = System.currentTimeMillis();\n+    LOG.info(\"Committing job has started for table: {}, using location: {}\", table,\n+        generateJobLocation(conf, jobContext.getJobID()));\n+\n+    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n+    List<DataFile> dataFiles = dataFiles(jobContext, io, true);\n+\n+    if (dataFiles.size() > 0) {\n+      // Appending data files to the table\n+      AppendFiles append = table.newAppend();\n+      dataFiles.forEach(append::appendFile);\n+      append.commit();\n+      LOG.info(\"Commit took {} ms for table: {} with {} file(s)\", System.currentTimeMillis() - startTime, table,\n+          dataFiles.size());\n+      LOG.debug(\"Added files {}\", dataFiles);\n+    } else {\n+      LOG.info(\"Commit took {} ms for table: {} with no new files\", System.currentTimeMillis() - startTime, table);\n+    }\n+\n+    cleanup(jobContext);\n+  }\n+\n+  /**\n+   * Removes the generated data files, if there is a commit file already generated for them.\n+   * The cleanup at the end removes the temporary directory as well.\n+   * @param jobContext The job context\n+   * @param status The status of the job\n+   * @throws IOException if there is a failure deleting the files\n+   */\n+  @Override\n+  public void abortJob(JobContext jobContext, int status) throws IOException {\n+    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Job {} is aborted. Cleaning job location {}\", jobContext.getJobID(), location);\n+\n+    FileIO io = HiveIcebergStorageHandler.io(jobContext.getJobConf());\n+    List<DataFile> dataFiles = dataFiles(jobContext, io, false);\n+\n+    // Check if we have files already committed and remove data files if there are any\n+    if (dataFiles.size() > 0) {\n+      Tasks.foreach(dataFiles)\n+          .retry(3)\n+          .suppressFailureWhenFinished()\n+          .onFailure((file, exc) -> LOG.debug(\"Failed on to remove data file {} on abort job\", file.path(), exc))\n+          .run(file -> io.deleteFile(file.path().toString()));\n+    }\n+\n+    cleanup(jobContext);\n+  }\n+\n+  /**\n+   * Cleans up the jobs temporary location.\n+   * @param jobContext The job context\n+   * @throws IOException if there is a failure deleting the files\n+   */\n+  private void cleanup(JobContext jobContext) throws IOException {\n+    String location = generateJobLocation(jobContext.getJobConf(), jobContext.getJobID());\n+    LOG.info(\"Cleaning for job: {} on location: {}\", jobContext.getJobID(), location);\n+\n+    // Remove the job's temp directory recursively.\n+    // Intentionally used foreach on a single item. Using the Tasks API here only for the retry capability.\n+    Tasks.foreach(location)\n+        .retry(3)\n+        .suppressFailureWhenFinished()\n+        .onFailure((file, exc) -> LOG.debug(\"Failed on to remove directory {} on cleanup job\", file, exc))\n+        .run(file -> {\n+          Path toDelete = new Path(file);\n+          FileSystem fs = Util.getFs(toDelete, jobContext.getJobConf());", "originalCommit": "1741c3793842496b07e32db505270f5dc8b451fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}