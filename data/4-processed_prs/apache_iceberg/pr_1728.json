{"pr_number": 1728, "pr_title": "Spark: Add Spark3 extensions module", "pr_createdAt": "2020-11-05T20:35:14Z", "pr_url": "https://github.com/apache/iceberg/pull/1728", "timeline": [{"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "url": "https://github.com/apache/iceberg/commit/c982ca0d9899bce0aff547b630f29d70f79f41d5", "message": "Spark: Add Spark3 extensions module", "committedDate": "2020-11-05T20:39:37Z", "type": "commit"}, {"oid": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "url": "https://github.com/apache/iceberg/commit/c982ca0d9899bce0aff547b630f29d70f79f41d5", "message": "Spark: Add Spark3 extensions module", "committedDate": "2020-11-05T20:39:37Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518357940", "bodyText": "It is debatable whether we should have Iceberg in names. We will add MIGRATE and SNAPSHOT here later too. However, CALL will go away once it is available in Spark.", "author": "aokolnychyi", "createdAt": "2020-11-05T20:53:43Z", "path": "spark3-extensions/src/main/antlr/org.apache.spark.sql.catalyst.parser.extensions/IcebergSqlExtensions.g4", "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ *\n+ * This file is an adaptation of Presto's and Spark's grammar files.\n+ */\n+\n+grammar IcebergSqlExtensions;", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM2MzAwOA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518363008", "bodyText": "I'd also like to add ALTER TABLE ... [ADD|DROP] PARTITION FIELD ...\nI like having Iceberg in the name because it will be more clear whether something is coming from Iceberg or Spark.", "author": "rdblue", "createdAt": "2020-11-05T21:03:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODU4NzIyMQ==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518587221", "bodyText": "It would be great if we can come up with a list of commands that Iceberg would like to add. If possible, I am planning to add VACUUM, OPTIMIZE and RESTORE after this module is added.", "author": "jackye1995", "createdAt": "2020-11-06T08:16:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1Njg4Nw==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518856887", "bodyText": "We have a design doc on which commands/stored procedures we are going to support. In addition, we have a design doc on how data compaction procedures will look like (i.e. OPTIMIZE). I need to update the latter one.", "author": "aokolnychyi", "createdAt": "2020-11-06T16:18:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg1ODQwMA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518858400", "bodyText": "While I think we want to support VACUUM in a way that is close to Redshift, the design of the command requires further consideration. I'd also represent OPTIMIZE as stored procedures.", "author": "aokolnychyi", "createdAt": "2020-11-06T16:21:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM1Nzk0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM5NzkyOA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518397928", "bodyText": "Could we put this in a different file? I'd also like to separate the parts that are inherited from Spark from the ones that are Iceberg-specific. What do you think about creating an abstract CatalystAstBuilder and extending it in IcebergAstBuilder?", "author": "rdblue", "createdAt": "2020-11-05T22:05:07Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTE4OA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891188", "bodyText": "Done. I did not introduce CatalystAstBuilder since this class became pretty small right now.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:16:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODM5NzkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMDQyOA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518400428", "bodyText": "The rules in Spark all use withOrigin(ctx). Should we do the same?", "author": "rdblue", "createdAt": "2020-11-05T22:10:35Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {\n+      case plan: LogicalPlan => plan\n+      case _ => delegate.parsePlan(sqlText)\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(PostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+// literal parsing is taken from Spark's AstBuilder\n+class IcebergSqlExtensionsAstBuilder extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = {", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTI5MA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891290", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:16:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMDQyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMTI1Nw==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518401257", "bodyText": "Spark performs variable substitution before parsing:\n  private val substitutor = new VariableSubstitution(conf)\n\n  protected override def parse[T](command: String)(toResult: SqlBaseParser => T): T = {\n    super.parse(substitutor.substitute(command))(toResult)\n  }\nIf possible, I think we should do the same here.", "author": "rdblue", "createdAt": "2020-11-05T22:12:25Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,417 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import java.util.Locale\n+import javax.xml.bind.DatatypeConverter\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.{Interval, ParseCancellationException}\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNodeImpl}\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.{Expression, Literal}\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface}\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.catalyst.util.DateTimeUtils.{getZoneId, stringToDate, stringToTimestamp}\n+import org.apache.spark.sql.catalyst.util.IntervalUtils\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.sql.types.{ByteType, CalendarIntervalType, DataType, DateType, DoubleType, FloatType, LongType, ShortType, StructType, TimestampType}\n+import org.apache.spark.unsafe.types.UTF8String\n+import scala.collection.JavaConverters._\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private val astBuilder = new IcebergSqlExtensionsAstBuilder()\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = parse(sqlText) { parser =>\n+    astBuilder.visit(parser.singleStatement()) match {", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTY0Mg==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891642", "bodyText": "I had to introduce a dependency on SQLConf and make sure the initialization is lazy. Seems to work, added tests.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:17:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMTI1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMjg0OQ==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518402849", "bodyText": "It would be good to have some documentation on these classes.", "author": "rdblue", "createdAt": "2020-11-05T22:15:43Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/statements.scala", "diffHunk": "@@ -0,0 +1,32 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.plans.logical\n+\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+\n+case class CallStatement(name: Seq[String], args: Seq[CallArgument]) extends ParsedStatement", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MTk5Mg==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518891992", "bodyText": "Let me add docs before the release when we know how this is going to look like.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwMjg0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNjE1NQ==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518406155", "bodyText": "Minor: there are a few casts in these tests that aren't checked. What about adding a checkCast method?\nprivate <T> T checkCast(Object value, Class<T> expectedClass) {\n  Assert.assertTrue(\"Expected instance of \" + expectedClass.getName(), expectedClass.isInstance(value));\n  return expectedClass.cast(value);\n}", "author": "rdblue", "createdAt": "2020-11-05T22:23:05Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1);\n+    checkArg(call, 1, \"c2\", \"2\");\n+    checkArg(call, 2, \"c3\", true);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, Object expectedValue) {\n+    checkArg(call, index, null, expectedValue);\n+  }\n+\n+  private void checkArg(CallStatement call, int index, String expectedName, Object expectedValue) {\n+    if (expectedName != null) {\n+      NamedArgument arg = (NamedArgument) call.args().apply(index);", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MjA1OA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892058", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:18:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNjE1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNzIwMA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518407200", "bodyText": "Can you also add testMixedArgs() that validates order doesn't change when positional and named arguments are mixed together? Even if this isn't allowed by a later rule, the parser accepts it.", "author": "rdblue", "createdAt": "2020-11-05T22:25:25Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestIcebergSparkSqlExtensionsParser.java", "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestIcebergSparkSqlExtensionsParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestIcebergSparkSqlExtensionsParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .getOrCreate();\n+    TestIcebergSparkSqlExtensionsParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestIcebergSparkSqlExtensionsParser.spark;\n+    TestIcebergSparkSqlExtensionsParser.spark = null;\n+    TestIcebergSparkSqlExtensionsParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1);\n+    checkArg(call, 1, \"2\");\n+    checkArg(call, 2, 3L);\n+    checkArg(call, 3, true);\n+    checkArg(call, 4, 1.0D);\n+    checkArg(call, 5, 9.0e1);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"));\n+  }\n+\n+  @Test\n+  public void testNamedArgs() throws ParseException {", "originalCommit": "c982ca0d9899bce0aff547b630f29d70f79f41d5", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MjUwMg==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892502", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:19:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQwNzIwMA=="}], "type": "inlineReview"}, {"oid": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "url": "https://github.com/apache/iceberg/commit/889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "message": "Rewort extensions", "committedDate": "2020-11-06T17:15:18Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5Mjg4MQ==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518892881", "bodyText": "This is the same version Spark uses.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:19:40Z", "path": "build.gradle", "diffHunk": "@@ -884,6 +884,49 @@ project(':iceberg-spark3') {\n   }\n }\n \n+project(\":iceberg-spark3-extensions\") {\n+  apply plugin: 'java'\n+  apply plugin: 'scala'\n+  apply plugin: 'antlr'\n+\n+  configurations {\n+    /*\n+     The Gradle Antlr plugin erroneously adds both antlr-build and runtime dependencies to the runtime path. This\n+     bug https://github.com/gradle/gradle/issues/820 exists because older versions of Antlr do not have separate\n+     runtime and compile dependencies and they do not want to break backwards compatibility. So to only end up with\n+     the runtime dependency on the runtime classpath we remove the dependencies added by the plugin here. Then add\n+     the runtime dependency back to only the runtime configuration manually.\n+    */\n+    compile {\n+      extendsFrom = extendsFrom.findAll { it != configurations.antlr }\n+    }\n+  }\n+\n+  dependencies {\n+    compileOnly project(':iceberg-spark3')\n+\n+    compileOnly \"org.scala-lang:scala-library\"\n+    compileOnly(\"org.apache.spark:spark-hive_2.12\") {\n+      exclude group: 'org.apache.avro', module: 'avro'\n+      exclude group: 'org.apache.arrow'\n+    }\n+\n+    testCompile project(path: ':iceberg-api', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-hive-metastore', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-spark', configuration: 'testArtifacts')\n+    testCompile project(path: ':iceberg-spark3', configuration: 'testArtifacts')\n+\n+    // Required because we remove antlr plugin dependencies from the compile configuration, see note above\n+    runtime \"org.antlr:antlr4-runtime:4.7.1\"", "originalCommit": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518893286", "bodyText": "@rdblue, this is the part I was talking about.", "author": "aokolnychyi", "createdAt": "2020-11-06T17:20:22Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.tree.{ParseTree, TerminalNode}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.ParserInterface\n+import org.apache.spark.sql.catalyst.parser.ParserUtils._\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.{CallArgument, CallStatement, LogicalPlan, NamedArgument, PositionalArgument}\n+import scala.collection.JavaConverters._\n+\n+class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergSqlExtensionsBaseVisitor[AnyRef] {\n+\n+  override def visitCall(ctx: CallContext): LogicalPlan = {\n+    val name = ctx.multipartIdentifier.parts.asScala.map(_.getText)\n+    val args = ctx.callArgument.asScala.map(typedVisit[CallArgument])\n+    CallStatement(name, args)\n+  }\n+\n+  override def visitPositionalArgument(ctx: PositionalArgumentContext): CallArgument = withOrigin(ctx) {\n+    val expr = typedVisit[Expression](ctx.expression)\n+    PositionalArgument(expr)\n+  }\n+\n+  override def visitNamedArgument(ctx: NamedArgumentContext): CallArgument = withOrigin(ctx) {\n+    val name = ctx.identifier.getText\n+    val expr = typedVisit[Expression](ctx.expression)\n+    NamedArgument(name, expr)\n+  }\n+\n+  // return null for any statement we cannot handle so it can be parsed with the main Spark parser\n+  override def visitNonIcebergCommand(ctx: NonIcebergCommandContext): LogicalPlan = null\n+\n+  override def visitSingleStatement(ctx: SingleStatementContext): LogicalPlan = withOrigin(ctx) {\n+    visit(ctx.statement).asInstanceOf[LogicalPlan]\n+  }\n+\n+  override def visitExpression(ctx: ExpressionContext): Expression = {\n+    // reconstruct the SQL string and parse it using the main Spark parser\n+    // while we can avoid having the logic to build Spark expressions, we still have to parse them\n+    val sqlString = reconstructSqlString(ctx)", "originalCommit": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0MDcxOA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518940718", "bodyText": "ParserRuleContext extends RuleContext, which is where getText is defined. Why not just call getText on any context? It doesn't seem like the reconstructSqlString method is needed.", "author": "rdblue", "createdAt": "2020-11-06T18:50:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4MTk4Ng==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518981986", "bodyText": "Unfortunately, calling getText on a node with multiple children messes up spaces.\nFor example, we will get back TIMESTAMP'2020-01-01 00:00:00' (without spaces).\nThat's why we need to recurse in reconstructSqlString.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:12:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDg1Mg==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518984852", "bodyText": "Let me update the comment.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:18:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NTc4OA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518985788", "bodyText": "Updated.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:20:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODg5MzI4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518937686", "bodyText": "I think it's quite a bit broader than this. How about \"portions of the extensions parser\"?", "author": "rdblue", "createdAt": "2020-11-06T18:44:42Z", "path": "LICENSE", "diffHunk": "@@ -279,6 +280,9 @@ This product includes code from Apache Spark.\n \n * dev/check-license script\n * vectorized reading of definition levels in BaseVectorizedParquetValuesReader.java\n+* SQL grammar rules in IcebergSqlExtensions.g4\n+* SQL parsing logic in IcebergSparkSqlExtensionsParser.java\n+* parser post processing logic in IcebergSqlExtensionsPostProcessor.java", "originalCommit": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4Mjk0MA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518982940", "bodyText": "Sounds good to me.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:14:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NjI0NA==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518986244", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:22:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzNzY4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODkzODIwOQ==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518938209", "bodyText": "Thanks for updating the comment.", "author": "rdblue", "createdAt": "2020-11-06T18:45:47Z", "path": "spark3-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSparkSqlExtensionsParser.scala", "diffHunk": "@@ -0,0 +1,180 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.spark.sql.catalyst.parser.extensions\n+\n+import org.antlr.v4.runtime._\n+import org.antlr.v4.runtime.atn.PredictionMode\n+import org.antlr.v4.runtime.misc.ParseCancellationException\n+import org.antlr.v4.runtime.tree.TerminalNodeImpl\n+import org.apache.spark.sql.AnalysisException\n+import org.apache.spark.sql.catalyst.{FunctionIdentifier, TableIdentifier}\n+import org.apache.spark.sql.catalyst.expressions.Expression\n+import org.apache.spark.sql.catalyst.parser.{ParseErrorListener, ParseException, ParserInterface, UpperCaseCharStream}\n+import org.apache.spark.sql.catalyst.parser.extensions.IcebergSqlExtensionsParser._\n+import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan\n+import org.apache.spark.sql.catalyst.trees.Origin\n+import org.apache.spark.sql.internal.{SQLConf, VariableSubstitution}\n+import org.apache.spark.sql.types.{DataType, StructType}\n+\n+class IcebergSparkSqlExtensionsParser(delegate: ParserInterface) extends ParserInterface {\n+\n+  private lazy val substitutor = new VariableSubstitution(SQLConf.get)\n+  private lazy val astBuilder = new IcebergSqlExtensionsAstBuilder(delegate)\n+\n+  /**\n+   * Parse a string to a DataType.\n+   */\n+  override def parseDataType(sqlText: String): DataType = {\n+    delegate.parseDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a raw DataType without CHAR/VARCHAR replacement.\n+   */\n+  override def parseRawDataType(sqlText: String): DataType = {\n+    delegate.parseRawDataType(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to an Expression.\n+   */\n+  override def parseExpression(sqlText: String): Expression = {\n+    delegate.parseExpression(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a TableIdentifier.\n+   */\n+  override def parseTableIdentifier(sqlText: String): TableIdentifier = {\n+    delegate.parseTableIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a FunctionIdentifier.\n+   */\n+  override def parseFunctionIdentifier(sqlText: String): FunctionIdentifier = {\n+    delegate.parseFunctionIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a multi-part identifier.\n+   */\n+  override def parseMultipartIdentifier(sqlText: String): Seq[String] = {\n+    delegate.parseMultipartIdentifier(sqlText)\n+  }\n+\n+  /**\n+   * Creates StructType for a given SQL string, which is a comma separated list of field\n+   * definitions which will preserve the correct Hive metadata.\n+   */\n+  override def parseTableSchema(sqlText: String): StructType = {\n+    delegate.parseTableSchema(sqlText)\n+  }\n+\n+  /**\n+   * Parse a string to a LogicalPlan.\n+   */\n+  override def parsePlan(sqlText: String): LogicalPlan = {\n+    val sqlTextAfterSubstitution = substitutor.substitute(sqlText)\n+    parse(sqlTextAfterSubstitution) { parser =>\n+      astBuilder.visit(parser.singleStatement()) match {\n+        case plan: LogicalPlan => plan\n+        case _ => delegate.parsePlan(sqlText)\n+      }\n+    }\n+  }\n+\n+  protected def parse[T](command: String)(toResult: IcebergSqlExtensionsParser => T): T = {\n+    val lexer = new IcebergSqlExtensionsLexer(new UpperCaseCharStream(CharStreams.fromString(command)))\n+    lexer.removeErrorListeners()\n+    lexer.addErrorListener(ParseErrorListener)\n+\n+    val tokenStream = new CommonTokenStream(lexer)\n+    val parser = new IcebergSqlExtensionsParser(tokenStream)\n+    parser.addParseListener(IcebergSqlExtensionsPostProcessor)\n+    parser.removeErrorListeners()\n+    parser.addErrorListener(ParseErrorListener)\n+\n+    try {\n+      try {\n+        // first, try parsing with potentially faster SLL mode\n+        parser.getInterpreter.setPredictionMode(PredictionMode.SLL)\n+        toResult(parser)\n+      }\n+      catch {\n+        case _: ParseCancellationException =>\n+          // if we fail, parse with LL mode\n+          tokenStream.seek(0) // rewind input stream\n+          parser.reset()\n+\n+          // Try Again.\n+          parser.getInterpreter.setPredictionMode(PredictionMode.LL)\n+          toResult(parser)\n+      }\n+    }\n+    catch {\n+      case e: ParseException if e.command.isDefined =>\n+        throw e\n+      case e: ParseException =>\n+        throw e.withCommand(command)\n+      case e: AnalysisException =>\n+        val position = Origin(e.line, e.startPosition)\n+        throw new ParseException(Option(command), e.message, position, position)\n+    }\n+  }\n+}\n+\n+/**\n+ * The post-processor validates & cleans-up the parse tree during the parse process.\n+ */\n+// while we reuse ParseErrorListener and ParseException, we have to copy and modify PostProcessor\n+// as it directly depends on classes generated from the extensions grammar file", "originalCommit": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0Mjg2Ng==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518942866", "bodyText": "For test portability, it's always better to construct Timestamp using an instant: Timestamp.from(Instant.parse(\"2020-01-01 00:00:00\")).", "author": "rdblue", "createdAt": "2020-11-06T18:54:53Z", "path": "spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java", "diffHunk": "@@ -0,0 +1,159 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.iceberg.spark.extensions;\n+\n+import java.math.BigDecimal;\n+import java.sql.Timestamp;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.expressions.Expression;\n+import org.apache.spark.sql.catalyst.expressions.Literal;\n+import org.apache.spark.sql.catalyst.expressions.Literal$;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.catalyst.parser.ParserInterface;\n+import org.apache.spark.sql.catalyst.plans.logical.CallArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.CallStatement;\n+import org.apache.spark.sql.catalyst.plans.logical.NamedArgument;\n+import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n+import org.apache.spark.sql.types.DataType;\n+import org.apache.spark.sql.types.DataTypes;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n+import scala.collection.JavaConverters;\n+\n+public class TestCallStatementParser {\n+\n+  @Rule\n+  public TemporaryFolder temp = new TemporaryFolder();\n+\n+  private static SparkSession spark = null;\n+  private static ParserInterface parser = null;\n+\n+  @BeforeClass\n+  public static void startSpark() {\n+    TestCallStatementParser.spark = SparkSession.builder()\n+        .master(\"local[2]\")\n+        .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n+        .config(\"spark.extra.prop\", \"value\")\n+        .getOrCreate();\n+    TestCallStatementParser.parser = spark.sessionState().sqlParser();\n+  }\n+\n+  @AfterClass\n+  public static void stopSpark() {\n+    SparkSession currentSpark = TestCallStatementParser.spark;\n+    TestCallStatementParser.spark = null;\n+    TestCallStatementParser.parser = null;\n+    currentSpark.stop();\n+  }\n+\n+  @Test\n+  public void testCallWithPositionalArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL c.n.func(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n+    Assert.assertEquals(ImmutableList.of(\"c\", \"n\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(7, call.args().size());\n+\n+    checkArg(call, 0, 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+    checkArg(call, 2, 3L, DataTypes.LongType);\n+    checkArg(call, 3, true, DataTypes.BooleanType);\n+    checkArg(call, 4, 1.0D, DataTypes.DoubleType);\n+    checkArg(call, 5, 9.0e1, DataTypes.DoubleType);\n+    checkArg(call, 6, new BigDecimal(\"900e-1\"), DataTypes.createDecimalType(3, 1));\n+  }\n+\n+  @Test\n+  public void testCallWithNamedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, c2 => '2', c3 => true)\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(3, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"c2\", \"2\", DataTypes.StringType);\n+    checkArg(call, 2, \"c3\", true, DataTypes.BooleanType);\n+  }\n+\n+  @Test\n+  public void testCallWithMixedArgs() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(c1 => 1, '2')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(2, call.args().size());\n+\n+    checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n+    checkArg(call, 1, \"2\", DataTypes.StringType);\n+  }\n+\n+  @Test\n+  public void testCallWithTimestampArg() throws ParseException {\n+    CallStatement call = (CallStatement) parser.parsePlan(\"CALL cat.system.func(TIMESTAMP '2020-01-01 00:00:00')\");\n+    Assert.assertEquals(ImmutableList.of(\"cat\", \"system\", \"func\"), JavaConverters.seqAsJavaList(call.name()));\n+\n+    Assert.assertEquals(1, call.args().size());\n+\n+    checkArg(call, 0, Timestamp.valueOf(\"2020-01-01 00:00:00\"), DataTypes.TimestampType);", "originalCommit": "889d172d20c0efbd1f41f23e18ee4ae6e5d4d9af", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NjEzNg==", "url": "https://github.com/apache/iceberg/pull/1728#discussion_r518986136", "bodyText": "Done.", "author": "aokolnychyi", "createdAt": "2020-11-06T20:21:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk0Mjg2Ng=="}], "type": "inlineReview"}, {"oid": "a16346beeac286fad907103b1e3e0d7740983f31", "url": "https://github.com/apache/iceberg/commit/a16346beeac286fad907103b1e3e0d7740983f31", "message": "Minor fixes", "committedDate": "2020-11-06T20:19:58Z", "type": "commit"}, {"oid": "9553da0595914551f6c5361d2b9bc76ec3cb64ee", "url": "https://github.com/apache/iceberg/commit/9553da0595914551f6c5361d2b9bc76ec3cb64ee", "message": "Fix test", "committedDate": "2020-11-06T20:52:17Z", "type": "commit"}]}