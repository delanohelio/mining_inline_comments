{"pr_number": 1334, "pr_title": "Fix Error Prone ByteBuffer warnings", "pr_createdAt": "2020-08-13T12:54:02Z", "pr_url": "https://github.com/apache/iceberg/pull/1334", "timeline": [{"oid": "81e93c4c6b20d77c1d83b3c5a3ab13317c2dc19c", "url": "https://github.com/apache/iceberg/commit/81e93c4c6b20d77c1d83b3c5a3ab13317c2dc19c", "message": "Add in basic autolabeler github action", "committedDate": "2020-08-04T06:26:43Z", "type": "commit"}, {"oid": "bc92e2a0f6ffee3ed670a1bab1a183456e0011aa", "url": "https://github.com/apache/iceberg/commit/bc92e2a0f6ffee3ed670a1bab1a183456e0011aa", "message": "Add in missing quotes from yaml - we should definitely consider the yaml linter", "committedDate": "2020-08-04T06:35:51Z", "type": "commit"}, {"oid": "d0d7895b189dd474368ad35c9ef15d020d96d86f", "url": "https://github.com/apache/iceberg/commit/d0d7895b189dd474368ad35c9ef15d020d96d86f", "message": "Give credit to spark project for the top level doc comment", "committedDate": "2020-08-04T06:38:19Z", "type": "commit"}, {"oid": "f157fb06b9bee7c0f6b90555092a5b80e7f1ea2f", "url": "https://github.com/apache/iceberg/commit/f157fb06b9bee7c0f6b90555092a5b80e7f1ea2f", "message": "Sentence syntax for the comment crediting spark project", "committedDate": "2020-08-04T06:41:11Z", "type": "commit"}, {"oid": "b2368bbf4a44cfb8de90a975f8a724225289a1d9", "url": "https://github.com/apache/iceberg/commit/b2368bbf4a44cfb8de90a975f8a724225289a1d9", "message": "address some pr feedback", "committedDate": "2020-08-04T21:30:40Z", "type": "commit"}, {"oid": "ce99c1981b6efd3482f06af1e8ce180632ce31c3", "url": "https://github.com/apache/iceberg/commit/ce99c1981b6efd3482f06af1e8ce180632ce31c3", "message": "add the asf license file header", "committedDate": "2020-08-05T07:11:36Z", "type": "commit"}, {"oid": "3c373bd0686c5a0bf23e5a6d1bcb538543c74f2c", "url": "https://github.com/apache/iceberg/commit/3c373bd0686c5a0bf23e5a6d1bcb538543c74f2c", "message": "address some more PR feedback on keeping mkdocs.yaml in docs and out of infrastructure", "committedDate": "2020-08-05T07:28:18Z", "type": "commit"}, {"oid": "e9619651ab68b5fb25254f968ea9b97c8aa30937", "url": "https://github.com/apache/iceberg/commit/e9619651ab68b5fb25254f968ea9b97c8aa30937", "message": "Only match on top level directories, not subdirectories", "committedDate": "2020-08-05T07:31:50Z", "type": "commit"}, {"oid": "35a171f26885a4d72dfb84d62d042c825acd0577", "url": "https://github.com/apache/iceberg/commit/35a171f26885a4d72dfb84d62d042c825acd0577", "message": "label any file with gradle in the name as BUILD and include the gradle version dependencies properties file", "committedDate": "2020-08-05T07:38:36Z", "type": "commit"}, {"oid": "51c6d9c5f35cb85565fcba602368aa0762ec9688", "url": "https://github.com/apache/iceberg/commit/51c6d9c5f35cb85565fcba602368aa0762ec9688", "message": "theres no need for the top level gradle folder rule as it will be matched by the all files or directories containing gradle rule", "committedDate": "2020-08-05T07:40:46Z", "type": "commit"}, {"oid": "b5c1ea2893ad51fe01ea20441484720061b0f97a", "url": "https://github.com/apache/iceberg/commit/b5c1ea2893ad51fe01ea20441484720061b0f97a", "message": "small whitespac change to merge this in my personal fork for testing", "committedDate": "2020-08-05T07:42:17Z", "type": "commit"}, {"oid": "56b544fe63e2ddf57a8375cabca2748a76af7bff", "url": "https://github.com/apache/iceberg/commit/56b544fe63e2ddf57a8375cabca2748a76af7bff", "message": "Add in an autolabeler github action\n\n* Add in an autolabeler github action. This will be used to then test that the autolabeler action works as expected before merging in the apache/iceberg repo.", "committedDate": "2020-08-05T07:44:12Z", "type": "commit"}, {"oid": "799c4e8f3bdedaa6767567232679557d9466585a", "url": "https://github.com/apache/iceberg/commit/799c4e8f3bdedaa6767567232679557d9466585a", "message": "add leading slash", "committedDate": "2020-08-05T08:26:28Z", "type": "commit"}, {"oid": "b8bd352105f1cbc4c366b344984ea3210074ec7f", "url": "https://github.com/apache/iceberg/commit/b8bd352105f1cbc4c366b344984ea3210074ec7f", "message": "make sure we match infinite depth from root folder", "committedDate": "2020-08-05T08:32:38Z", "type": "commit"}, {"oid": "68b7302a416c8ca3e31dec63aa6c9b43afe983d2", "url": "https://github.com/apache/iceberg/commit/68b7302a416c8ca3e31dec63aa6c9b43afe983d2", "message": "allow for matching files and not just directories", "committedDate": "2020-08-05T08:39:51Z", "type": "commit"}, {"oid": "d5ddfd9ed13ccfec5f46eb6efaadb2389bab8501", "url": "https://github.com/apache/iceberg/commit/d5ddfd9ed13ccfec5f46eb6efaadb2389bab8501", "message": "add leading slash to labeler config now that the bot is installed\n\nadd leading slash to labeler config now that the bot is installed", "committedDate": "2020-08-05T08:40:47Z", "type": "commit"}, {"oid": "11de827df9fc3a90b08669ad61aa3f282e73df4f", "url": "https://github.com/apache/iceberg/commit/11de827df9fc3a90b08669ad61aa3f282e73df4f", "message": "Make autlolabeler.yml top level comments valid yaml comments", "committedDate": "2020-08-05T08:56:10Z", "type": "commit"}, {"oid": "c1e99acc691c611d71f446c75a2409a79f19dcc1", "url": "https://github.com/apache/iceberg/commit/c1e99acc691c611d71f446c75a2409a79f19dcc1", "message": "Make the license comment a valid YAML comment and then use a leading slash to match files and directories below top level folders", "committedDate": "2020-08-05T09:20:04Z", "type": "commit"}, {"oid": "6702e7489d81640534099a20eff3f30ee27c8cc0", "url": "https://github.com/apache/iceberg/commit/6702e7489d81640534099a20eff3f30ee27c8cc0", "message": "Update autolabeler.yml", "committedDate": "2020-08-05T09:20:55Z", "type": "commit"}, {"oid": "5d628fff2e0399511351bc535a3ad7b811d1e709", "url": "https://github.com/apache/iceberg/commit/5d628fff2e0399511351bc535a3ad7b811d1e709", "message": "Leave an empty line between the license header and file contents", "committedDate": "2020-08-05T09:30:39Z", "type": "commit"}, {"oid": "1644adb2256268eef0fe9bb24b6f09ca56a1f5d5", "url": "https://github.com/apache/iceberg/commit/1644adb2256268eef0fe9bb24b6f09ca56a1f5d5", "message": "Remove bash label", "committedDate": "2020-08-05T22:42:52Z", "type": "commit"}, {"oid": "827599ea679938315931fe2c799a3cd2ab4f66fa", "url": "https://github.com/apache/iceberg/commit/827599ea679938315931fe2c799a3cd2ab4f66fa", "message": "Merge branch 'autolabeler-github-action' of github.com:kbendick/iceberg into autolabeler-github-action", "committedDate": "2020-08-05T22:43:05Z", "type": "commit"}, {"oid": "1c77e39815b7b7d58bc670abb260562f39695084", "url": "https://github.com/apache/iceberg/commit/1c77e39815b7b7d58bc670abb260562f39695084", "message": "move dev directory from build to infra", "committedDate": "2020-08-05T22:46:54Z", "type": "commit"}, {"oid": "7a82421b119251abc8c222d4fbddbb5a57cfc689", "url": "https://github.com/apache/iceberg/commit/7a82421b119251abc8c222d4fbddbb5a57cfc689", "message": "use leading slash syntax for top level . folders", "committedDate": "2020-08-05T22:52:15Z", "type": "commit"}, {"oid": "a97dab051a680138f58aa2ea805cb840a3adee1e", "url": "https://github.com/apache/iceberg/commit/a97dab051a680138f58aa2ea805cb840a3adee1e", "message": "Merge branch 'master' into autolabeler-v2-test", "committedDate": "2020-08-05T22:57:15Z", "type": "commit"}, {"oid": "c75b14f41e9c4f13e98ddfb6c288b6745aa7a666", "url": "https://github.com/apache/iceberg/commit/c75b14f41e9c4f13e98ddfb6c288b6745aa7a666", "message": "Autolabeler v2 test\n\n- Uses leading slash syntax for top level folders that start with a .", "committedDate": "2020-08-05T22:58:22Z", "type": "commit"}, {"oid": "845e4ffbda59fe877edde69d37a7f0a5c28dfa08", "url": "https://github.com/apache/iceberg/commit/845e4ffbda59fe877edde69d37a7f0a5c28dfa08", "message": "merge upstream master", "committedDate": "2020-08-13T10:15:37Z", "type": "commit"}, {"oid": "cd4fd45df432e65227db3df22c05329ab40688dc", "url": "https://github.com/apache/iceberg/commit/cd4fd45df432e65227db3df22c05329ab40688dc", "message": "Suprress and remove the one potentially unspressable reference - will check if anybody with more knowledge of Orc knows but I think it would be needed for one person wo have all knowledge of systes", "committedDate": "2020-08-13T12:43:52Z", "type": "commit"}, {"oid": "37b7b618a2ab4dd3042529656f9f2aa618bc1332", "url": "https://github.com/apache/iceberg/commit/37b7b618a2ab4dd3042529656f9f2aa618bc1332", "message": "Make whitespace the sane on untouched PR portions", "committedDate": "2020-08-13T12:57:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470022792", "bodyText": "I believe the write can only come from org.apache.orc.storage.ql.exec.vector.BytesColumnVector, which would be heap array backed. Unless there is another implementation I don't know about.\nSee\nhttps://orc.apache.org/api/hive-storage-api/index.html?org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.html\nand\nhttps://github.com/apache/hive/blob/master/storage-api/src/java/org/apache/hadoop/hive/ql/exec/vector/BytesColumnVector.java", "author": "RussellSpitzer", "createdAt": "2020-08-13T15:07:19Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?", "originalCommit": "37b7b618a2ab4dd3042529656f9f2aa618bc1332", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MDE1Nw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470760157", "bodyText": "Isn't this writing into a BytesColumnVector? If so, I don't think that the values are coming from one.\nI think it is correct to updating the handling here.\nIt would be great to get an opinion from @shardulm94 and @edgarRd as well.", "author": "rdblue", "createdAt": "2020-08-14T17:31:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MDUxOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470760519", "bodyText": "Ah that is true, I was thinking about it the wrong direction", "author": "RussellSpitzer", "createdAt": "2020-08-14T17:32:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyMjg4MQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470922881", "bodyText": "I went looking into spark's implementation of off-heap byte arrays (for the spark-native version) and it somewhat lead me to believe that it could be reading in values as off-heap byte buffers when using spark.memory.offHeap.enabled=true and some value for  --conf spark.memory.offHeap.size.\nI'm working on a demo to see if that's true, but of note when I started up a spark-shell with those parameters specified (4G for the off heap size), I got the following error which leads me to believe that ByteBuffers were being created in an off-heap way.\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.0.0/libexec/jars/spark-unsafe_2.12-3.0.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n\nI am trying to work on a demo to test writing specifically off heap byte arrays to format(\"orc\") and then try reading it in and see what happens, but I'm not 100% sure when I'll have the time.\nBut if the ByteBuffers are off heap and we pass them to this function without \"sanitizing\" them, we need the check. The check is pretty inexpensive as well fwiw, it just checks if a primitive array element if equal to null or not.", "author": "kbendick", "createdAt": "2020-08-15T01:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY0OTgwNQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471649805", "bodyText": "Right, I agree that when writing this ByteBuffer to a BytesColumnVector there was an assumption that it was backed by an array, but to support any ByteBuffer we need this handling here.", "author": "edgarRd", "createdAt": "2020-08-17T17:32:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDAyMjc5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470758731", "bodyText": "This is not a correct use of ByteBuffer because it doesn't use arrayOffset or remaining. I think the current version must work because Spark returns new arrays that we wrap in ByteBuffer, but if the goal here is to make this accept any ByteBuffer then we should account for cases where the buffer is not simply the entire backing array.", "author": "rdblue", "createdAt": "2020-08-14T17:28:50Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,24 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // We technically can't be sure if the ByteBuffer coming in is on or off\n+      // heap so we cannot safely call `.array()` on it without first checking\n+      // via the method ByteBuffer.hasArray().\n+      // See: https://errorprone.info/bugpattern/ByteBufferBackingArray\n+      //\n+      // When there is a backing heap based byte array, we avoided the overhead of\n+      // copying, which is especially important for small byte buffers.\n+      //\n+      // TODO - This copy slows it down, perhap unnecessarily. Is there any other way to tell, or no?\n+      //        My guess is no, if I consider things like VectorizedOrcReaders on Spark.\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);", "originalCommit": "37b7b618a2ab4dd3042529656f9f2aa618bc1332", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyMzA2OA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470923068", "bodyText": "I'd like to point out that this is the original version of nonNullWrite, unless you're referring to the else block lines 246-251 just below your comment, in which case that's my addition.  https://github.com/apache/iceberg/pull/1334/files#diff-b1b07b15f036000a3f2bed76fdd9f961R246-R251\nI believe my addition (in the else block) is correct as it does call remaining. If we use hasArray, I just left it as the original function. I'd be happy to update anything that needs updating though.", "author": "kbendick", "createdAt": "2020-08-15T01:46:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA1NTA5Mg==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471055092", "bodyText": "Ok I see what you're referring to now. I will handle the fix in this PR as well. \ud83d\udc4d", "author": "kbendick", "createdAt": "2020-08-16T02:20:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2MzIxMg==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471063212", "bodyText": "Should we consume the buffer up until that point or reset it to its original position?", "author": "kbendick", "createdAt": "2020-08-16T04:21:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMjczOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471712739", "bodyText": "If we are accessing the backing array, there is no need to worry about the state of the ByteBuffer. But the actual starting offset in the array is data.arrayOffset() + data.position() and the length is data.remaining(). Have a look at the copy methods in our ByteBuffers class to see examples.", "author": "rdblue", "createdAt": "2020-08-17T18:57:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc1ODczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470762678", "bodyText": "I don't think this is correct. This calls buffer.get(new byte[...]), which returns the ByteBuffer that get was called on. It is no different than returning new DataByteArray(buffer.array()). Calling get shows that the intent was to read the bytes into a new array and pass that to create a DataByteArray.\nThe correct implementation is this:\nByteBuffer buffer = (ByteBuffer) value;\nbyte[] bytes = new byte[buffer.remaining()];\nbuffer.get(bytes);\nreturn new DataByteArray(bytes);", "author": "rdblue", "createdAt": "2020-08-14T17:36:56Z", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -243,6 +243,7 @@ private boolean advance() throws IOException {\n       return true;\n     }\n \n+    @SuppressWarnings(\"ByteBufferBackingArray\")", "originalCommit": "37b7b618a2ab4dd3042529656f9f2aa618bc1332", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxODk1NQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470918955", "bodyText": "Ok. Thanks for the insight. This was one admittedly that I suppressed but wasn't sure about later on. I'm in agreement though, due to the same arguments I've given elsewhere. I know much less about Pig, but I think it's better to be safe than sorry here if we don't know about the exact behavior of the input source, not or in the future.\nShould I update it in this PR or another PR?", "author": "kbendick", "createdAt": "2020-08-15T01:07:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyNDA5Nw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470924097", "bodyText": "And do I need to restore the buffer position as well? In my implementation above that uses remaining above in GenericOrcWriters that we're also discussing, I returned the original buffer (value here) to its original position. IIUC, they'd be backed by the same data as the casted buffer.", "author": "kbendick", "createdAt": "2020-08-15T01:58:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkyNDQzNA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r470924434", "bodyText": "It's worth noting that the java doc of ByteBuffer states that the underlying buffer does get moved forward.\n     * Unrelated stuff about BufferUnderflowException....\n     *\n     * <p> Otherwise, this method copies <tt>length</tt> bytes from this\n     * buffer into the given array, starting at the current position of this\n     * buffer and at the given offset in the array.  The position of this\n     * buffer is then incremented by <tt>length</tt>.\n     * ....", "author": "kbendick", "createdAt": "2020-08-15T02:02:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODkyMQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471068921", "bodyText": "I updated this. I also moved the position of value (or buffer as it were) back to remaining before returning the copy. Is that necessary / will it lead to issues? I'm not sure what's needed in this case.", "author": "kbendick", "createdAt": "2020-08-16T05:43:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxMzMwNw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471713307", "bodyText": "We usually duplicate the buffer before calling get to avoid changing the original buffer's state.\nAnd this can be updated in this PR, since the scope here is to fix the errors and suppress any other warnings.", "author": "rdblue", "createdAt": "2020-08-17T18:58:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5MTkzNQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472791935", "bodyText": "I see. That makes sense. Thanks for helping me to understand better. And you're right, fixing the issues is definitely in scope. That's why I started this endeavor \ud83d\ude05", "author": "kbendick", "createdAt": "2020-08-19T07:14:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDc2MjY3OA=="}], "type": "inlineReview"}, {"oid": "00921bfc114fd3e117ce3e1ae82f8d996e036330", "url": "https://github.com/apache/iceberg/commit/00921bfc114fd3e117ce3e1ae82f8d996e036330", "message": "Use slice to take a shallow copy of the remaining data in the backing array of data", "committedDate": "2020-08-16T04:54:26Z", "type": "commit"}, {"oid": "8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23", "url": "https://github.com/apache/iceberg/commit/8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23", "message": "Fix issue in IcebergPigInputFormat", "committedDate": "2020-08-16T04:59:39Z", "type": "commit"}, {"oid": "ed800998af3e0c06e96b107afe605d87fea2ae42", "url": "https://github.com/apache/iceberg/commit/ed800998af3e0c06e96b107afe605d87fea2ae42", "message": "Use the simpler relative consuming get as we've already called .remaining on the byte buffer", "committedDate": "2020-08-16T05:02:38Z", "type": "commit"}, {"oid": "00f9e46044b270aca0d29179e2405a9c312455e6", "url": "https://github.com/apache/iceberg/commit/00f9e46044b270aca0d29179e2405a9c312455e6", "message": "Small fixes", "committedDate": "2020-08-16T05:04:45Z", "type": "commit"}, {"oid": "11828085e8ab5d5d63e394053495ff46128a3571", "url": "https://github.com/apache/iceberg/commit/11828085e8ab5d5d63e394053495ff46128a3571", "message": "Merge remote-tracking branch 'upstream/master' into fix-errorprone-bytebuffer-warnings", "committedDate": "2020-08-16T05:23:52Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471065804", "bodyText": "I updated this one as you requested @rdblue, however I wasn't sure if I should return the input value to its original position (vs consuming the remainder of the byte buffer and moving the position to the end).", "author": "kbendick", "createdAt": "2020-08-16T05:01:04Z", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -246,7 +246,11 @@ private boolean advance() throws IOException {\n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n         ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        byte[] bytes = new byte[buffer.remaining()];\n+        buffer.get(bytes);\n+        // Return the input buffer back to its original position.\n+        buffer.position(buffer.position() - bytes.length);\n+        return new DataByteArray(bytes);", "originalCommit": "8fe4ecdb2a522c419791c57567c6ae0cbf6ecf23", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTg5OQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715899", "bodyText": "Looks good, but we should use slice or duplicate to read to avoid needing to restore the original buffer's state.", "author": "rdblue", "createdAt": "2020-08-17T19:03:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5OTQ4OQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471899489", "bodyText": "Ok. I will update this one to use slice to avoid having to restore the original buffer's state.", "author": "kbendick", "createdAt": "2020-08-18T03:58:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2NTgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471068779", "bodyText": "I updated this line to not assume that the entire backing array of data is what we're reading in. Via the use of slice, a shallow copy takes place (which mimics the previous shallow copy). Does this resolve your previous concern?\nI can clean up the comments etc once I know if this resolves any on going issues or not @rdblue . Thanks so much for your thorough review!", "author": "kbendick", "createdAt": "2020-08-16T05:42:00Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);", "originalCommit": "00f9e46044b270aca0d29179e2405a9c312455e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTE3Mjc0OA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471172748", "bodyText": "Actually, my solution with slice causes a ByteBufferBackingArray warning, which could be suppressed as we called .hasArray before. But at this point suppression seems silly given the work that went in to correcting this.\nIf anybody has any solutions which doesn't involve a deep copy, I'm open to suggestions.", "author": "kbendick", "createdAt": "2020-08-16T23:28:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTM1NQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715355", "bodyText": "This isn't a correct use of slice. Slice is like duplicate, except that it guarantees that position will be 0 and capacity is equal to limit. It still uses the same backing bytes and will not copy, which we would not want even if it did. Because this still uses the same backing array, we still need to use arrayOffset.\nThere isn't any benefit to using slice here since accessing the backing byte array means that we don't modify the original buffer.", "author": "rdblue", "createdAt": "2020-08-17T19:02:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5MzQzNw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472793437", "bodyText": "I've factored out a duplicate and then reused that in both portions, on heap and off, and changed the position to be arrayOffset when on heap. Thanks for your review.", "author": "kbendick", "createdAt": "2020-08-19T07:16:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTA2ODc3OQ=="}], "type": "inlineReview"}, {"oid": "1684ac45736a7ea92cd1bd06a0b64d7a503a605b", "url": "https://github.com/apache/iceberg/commit/1684ac45736a7ea92cd1bd06a0b64d7a503a605b", "message": "wip but copy over a function from ParquetValueWriters", "committedDate": "2020-08-17T00:21:17Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471651416", "bodyText": "Could you avoid this by using slice too? Maybe factoring the slice out of the if...else?", "author": "edgarRd", "createdAt": "2020-08-17T17:35:22Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,23 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // When there is a backing heap based byte array, we avoid the overhead of\n+      // copying it.\n+      if (data.hasArray()) {\n+        // Don't assume the we're reading in the entire backing array.\n+        // Using slice as any mutations to the data at rowId of output\n+        // would also be visible in `data`.\n+        ByteBuffer slice = data.slice();\n+        ((BytesColumnVector) output).setRef(rowId, slice.array(), 0, slice.array().length);\n+      } else {\n+        // Consume the remaining contents of the input data\n+        byte[] bytes = new byte[data.remaining()];\n+        data.get(bytes);\n+        // Restores the buffer position\n+        // TODO - Is this necessary?\n+        data.position(data.position() - bytes.length);", "originalCommit": "00f9e46044b270aca0d29179e2405a9c312455e6", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTcxNTYyNA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471715624", "bodyText": "I agree. By using either slice or duplicate, there is no need to restore the state of the incoming buffer.", "author": "rdblue", "createdAt": "2020-08-17T19:03:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc3OTEyOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472779129", "bodyText": "Ok. I will prefer duplicate here. I'll still need to use a Suppression, but it will be the correct solution if I understand correctly.", "author": "kbendick", "createdAt": "2020-08-19T06:58:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc4OTcxNQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472789715", "bodyText": "I actually didn't have to use the suppression once I realized why I needed to use arrayOffset in addition to hasArray. Thanks guys! Let me know if this solution won't do.", "author": "kbendick", "createdAt": "2020-08-19T07:12:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTY1MTQxNg=="}], "type": "inlineReview"}, {"oid": "9c6c190f1cc07bbc4f498a1d5967f284deaac34f", "url": "https://github.com/apache/iceberg/commit/9c6c190f1cc07bbc4f498a1d5967f284deaac34f", "message": "Using logic from ParquetValueWriters, remove ByteBuffer warnings from GenericOrcWriters nonNullWrite method", "committedDate": "2020-08-18T03:38:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5NTM3Mw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r471895373", "bodyText": "I took the logic for copying from following org.apache.iceberg.io.ParquetValueWriters into org.apache.parquet.io.api.Binary, specifically the need to update both the limit and the position in order to return the original ByteBuffer, data, back to its original position as ByteBuffer offers no API to do that.\nSince I was no longer going to use any optimizations if the buffer is on heap or not (such as the usage of .slice() or .duplicate() or .array() I chose to skip entirely the check for data.hasArray().\nI somewhat copied emulated the logic to copy and reset the original byte buffer data's position (which could be invalidated by moving the position passed the limit), as demonstrated in org.apache.parquet.io.api.Binary", "author": "kbendick", "createdAt": "2020-08-18T03:40:58Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,33 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      // Don't assume the we're reading in the entire backing array.\n+      //\n+      // We use the same logic for on heap vs off heap buffers as we don't assume\n+      // that the position of the byte buffer received for the write is at the beginning\n+      // position, such an assumption is needed to make use of methods like `.slice()`\n+      // or any methods that would be useful here if we checked if there is an on-heap\n+      // backing array and that the buffer is at the beginning position. (aka we don't check\n+      // that one can use if `data.hasArray()` is true as we couldn't make any optimizations\n+      // in that case).\n+      int position = data.position();\n+      int limit = data.limit();\n+      int curIndex = data.arrayOffset() + data.position();\n+      int endIndex = curIndex + data.remaining();\n+\n+      // Prep for copy into bytes\n+      byte[] bytes = new byte[data.remaining()];\n+      data.limit(curIndex + limit);\n+      data.position(curIndex);\n+\n+      // Perform copy into bytes of remainder of byte buffer.\n+      data.get(bytes, curIndex, endIndex - curIndex);\n+\n+      // Reset the byte buffer.\n+      data.limit(limit);\n+      data.position(position);", "originalCommit": "9c6c190f1cc07bbc4f498a1d5967f284deaac34f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Mjc5NDc4NQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r472794785", "bodyText": "This wasn't needed and I rolled back", "author": "kbendick", "createdAt": "2020-08-19T07:18:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg5NTM3Mw=="}], "type": "inlineReview"}, {"oid": "d1f9506f39948022b9aca54c6197167877c73136", "url": "https://github.com/apache/iceberg/commit/d1f9506f39948022b9aca54c6197167877c73136", "message": "simplify the logic of copying the byte buffer", "committedDate": "2020-08-18T03:52:31Z", "type": "commit"}, {"oid": "d57d81ad92c1997190ab4ca656033e5c3deecb40", "url": "https://github.com/apache/iceberg/commit/d57d81ad92c1997190ab4ca656033e5c3deecb40", "message": "Use duplicate to avoid having to reposition the original byte buffer", "committedDate": "2020-08-19T06:28:07Z", "type": "commit"}, {"oid": "c91503c02d799c007d6c2fe5046da3bc7a8ad0e4", "url": "https://github.com/apache/iceberg/commit/c91503c02d799c007d6c2fe5046da3bc7a8ad0e4", "message": "roll back a few commits and then factor out to use duplicate with arrayOffset for on heap buffers with orc", "committedDate": "2020-08-19T07:03:46Z", "type": "commit"}, {"oid": "f39802c8913abd778b4802c0b6a611592448a942", "url": "https://github.com/apache/iceberg/commit/f39802c8913abd778b4802c0b6a611592448a942", "message": "use dupe throughout once it's created", "committedDate": "2020-08-19T07:05:42Z", "type": "commit"}, {"oid": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134", "url": "https://github.com/apache/iceberg/commit/60d3ef03e6905257ede7a7d7caf6d07bce5a3134", "message": "duplicate directly after cast so nobody is tempted to use value", "committedDate": "2020-08-19T07:20:00Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDEyMw==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473124123", "bodyText": "The offset should be dupe.arrayOffset() + dupe.position().", "author": "rdblue", "createdAt": "2020-08-19T15:38:26Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,14 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      ByteBuffer dupe = data.duplicate();\n+      if (dupe.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, dupe.array(), dupe.arrayOffset(), dupe.remaining());", "originalCommit": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzEyOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479623129", "bodyText": "Ok. I have updated \ud83d\udc4d", "author": "kbendick", "createdAt": "2020-08-29T07:57:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDEyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNDYxNA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473124614", "bodyText": "There is no need to duplicate the ByteBuffer when accessing the backing on-heap byte array, so this should only be done in the case where we read from the ByteBuffer using get.", "author": "rdblue", "createdAt": "2020-08-19T15:39:07Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +231,14 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      ByteBuffer dupe = data.duplicate();", "originalCommit": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNjgwMQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r473126801", "bodyText": "I don't think that we need to check hasArray here. I think the reason why this didn't previously check hasArray is that the array passed to DataByteArray must start at offset 0 and be valid through the array length, so a copy was needed in almost every case.\nIt may be simpler to change this to use ByteBuffers.toByteArray and pass the result to create DataByteArray.", "author": "rdblue", "createdAt": "2020-08-19T15:42:17Z", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +245,14 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        ByteBuffer dupe = ((ByteBuffer) value).duplicate();\n+        if (dupe.hasArray()) {\n+          return new DataByteArray(dupe.array());", "originalCommit": "60d3ef03e6905257ede7a7d7caf6d07bce5a3134", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMjAyOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479622029", "bodyText": "Let me know if I used this wrong but this is a very useful utility function!", "author": "kbendick", "createdAt": "2020-08-29T07:43:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzEyNjgwMQ=="}], "type": "inlineReview"}, {"oid": "27f3d855cf33202b737e55dc8b044432dd247697", "url": "https://github.com/apache/iceberg/commit/27f3d855cf33202b737e55dc8b044432dd247697", "message": "use utility function in IcebergPigInputFormat for ByteBuffer.toByteArray", "committedDate": "2020-08-29T07:43:23Z", "type": "commit"}, {"oid": "2882f9cd957639c10e24e34a85fe72a4f11e4f0f", "url": "https://github.com/apache/iceberg/commit/2882f9cd957639c10e24e34a85fe72a4f11e4f0f", "message": "Remove unnecessary duplicate call on ByteBuffer as it's handled in the utility", "committedDate": "2020-08-29T07:46:29Z", "type": "commit"}, {"oid": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4", "url": "https://github.com/apache/iceberg/commit/fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4", "message": "Cleanup GenericOrcWriters and use utility copy function for off-heap", "committedDate": "2020-08-29T07:57:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479623220", "bodyText": "@rdblue please let me know if this is an appropriate use of ByteBuffers.toByteArray (since we know this is off-heap, I figure this brings it on heap in the way I was somewhat before).\nAnd is the null check necessary? I saw that toByteArray returned null if null was passed in, and I didn't see any @NonNull kind of tags so I wasn't sure.", "author": "kbendick", "createdAt": "2020-08-29T07:59:21Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());\n+      } else {\n+        byte[] rawData = ByteBuffers.toByteArray(data);\n+        int length = rawData == null ? 0 : rawData.length;\n+        ((BytesColumnVector) output).setRef(rowId, rawData, 0, length);", "originalCommit": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY1NA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673654", "bodyText": "The null check isn't needed because this is nonNullWrite so nulls have already been handled at this point.", "author": "rdblue", "createdAt": "2020-08-29T17:53:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5Nzk5MQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479697991", "bodyText": "Haha didn't realize that. That's what I get for not looking at this PR for over a week \ud83d\ude04", "author": "kbendick", "createdAt": "2020-08-29T22:55:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTYyMzIyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY3MQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673671", "bodyText": "Looks good.", "author": "rdblue", "createdAt": "2020-08-29T17:53:25Z", "path": "pig/src/main/java/org/apache/iceberg/pig/IcebergPigInputFormat.java", "diffHunk": "@@ -245,8 +246,7 @@ private boolean advance() throws IOException {\n \n     private Object convertPartitionValue(Type type, Object value) {\n       if (type.typeId() == Types.BinaryType.get().typeId()) {\n-        ByteBuffer buffer = (ByteBuffer) value;\n-        return new DataByteArray(buffer.get(new byte[buffer.remaining()]).array());\n+        return new DataByteArray(ByteBuffers.toByteArray(((ByteBuffer) value));", "originalCommit": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwMDU1MA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479700550", "bodyText": "Fixed a linter error. Wasn't aware we had any breaking linting in the repo. Would love to help enable more of that!", "author": "kbendick", "createdAt": "2020-08-29T23:33:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY4MA==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479673680", "bodyText": "Looks good.", "author": "rdblue", "createdAt": "2020-08-29T17:53:36Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +234,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data != null && data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(), data.arrayOffset() + data.position(), data.remaining());", "originalCommit": "fbd6021bdf0a5f71b7756786d993dd2a7d2c29e4", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY5ODIxOQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479698219", "bodyText": "I removed the null check above since as mentioned we're in nonNullWrite \ud83d\ude05", "author": "kbendick", "createdAt": "2020-08-29T22:58:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTY3MzY4MA=="}], "type": "inlineReview"}, {"oid": "8521649d671a94ba71febe17f18a46898171ebbf", "url": "https://github.com/apache/iceberg/commit/8521649d671a94ba71febe17f18a46898171ebbf", "message": "Remove unneceessary null checks in nonNullWrite", "committedDate": "2020-08-29T22:57:15Z", "type": "commit"}, {"oid": "c8515ba47bbc997979b7f680f072dde2a1a1797b", "url": "https://github.com/apache/iceberg/commit/c8515ba47bbc997979b7f680f072dde2a1a1797b", "message": "Adds in missing closing parens", "committedDate": "2020-08-29T23:03:18Z", "type": "commit"}, {"oid": "737b8afa1fd84d1156c8aedd7f9ae1e41430ff1f", "url": "https://github.com/apache/iceberg/commit/737b8afa1fd84d1156c8aedd7f9ae1e41430ff1f", "message": "Cleans up linter failing errors in GenericOrcWriters", "committedDate": "2020-08-29T23:22:46Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTcwMDY2MQ==", "url": "https://github.com/apache/iceberg/pull/1334#discussion_r479700661", "bodyText": "There was a line length breaking linter error here. Not sure what the expected way to break up the argument list is, so I chose something that seemed to make sense and that passed the linter.", "author": "kbendick", "createdAt": "2020-08-29T23:34:59Z", "path": "data/src/main/java/org/apache/iceberg/data/orc/GenericOrcWriters.java", "diffHunk": "@@ -231,7 +232,13 @@ public void nonNullWrite(int rowId, String data, ColumnVector output) {\n \n     @Override\n     public void nonNullWrite(int rowId, ByteBuffer data, ColumnVector output) {\n-      ((BytesColumnVector) output).setRef(rowId, data.array(), 0, data.array().length);\n+      if (data.hasArray()) {\n+        ((BytesColumnVector) output).setRef(rowId, data.array(),\n+                data.arrayOffset() + data.position(), data.remaining());", "originalCommit": "737b8afa1fd84d1156c8aedd7f9ae1e41430ff1f", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}