{"pr_number": 1727, "pr_title": "[python] Adding parquet package and the classes for reading parquet", "pr_createdAt": "2020-11-05T19:01:17Z", "pr_url": "https://github.com/apache/iceberg/pull/1727", "timeline": [{"oid": "5976811c354be139b875a51ca7c8bad1ab5225f8", "url": "https://github.com/apache/iceberg/commit/5976811c354be139b875a51ca7c8bad1ab5225f8", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-05T19:04:05Z", "type": "forcePushed"}, {"oid": "7c6864f20018e7f513ddc4942fe3de43845cb40e", "url": "https://github.com/apache/iceberg/commit/7c6864f20018e7f513ddc4942fe3de43845cb40e", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-05T19:06:13Z", "type": "forcePushed"}, {"oid": "b88e778a3ad70c53474bac4e8771e2402e88002f", "url": "https://github.com/apache/iceberg/commit/b88e778a3ad70c53474bac4e8771e2402e88002f", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-05T19:21:53Z", "type": "forcePushed"}, {"oid": "1eda220360d954ee9c67c72a9d17b276d7588191", "url": "https://github.com/apache/iceberg/commit/1eda220360d954ee9c67c72a9d17b276d7588191", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-06T00:28:07Z", "type": "forcePushed"}, {"oid": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "url": "https://github.com/apache/iceberg/commit/00cdc50c58a68a5b2467ff99437cdb126f768fbb", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-09T19:04:25Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTgzMzQzNg==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r519833436", "bodyText": "just curious why we have get_dataset_filter and get_expr. Maybe it will become clear as I review the rest but from here it seems redundnat", "author": "rymurr", "createdAt": "2020-11-09T13:58:54Z", "path": "python/iceberg/parquet/dataset_utils.py", "diffHunk": "@@ -0,0 +1,177 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from iceberg.api.expressions import Expression, Operation, Predicate\n+import pyarrow.dataset as ds\n+\n+\n+def get_dataset_filter(expr: Expression, expected_to_file_map: dict) -> ds.Expression:", "originalCommit": "1eda220360d954ee9c67c72a9d17b276d7588191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5NzQ0NQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520697445", "bodyText": "you're right this is redundant.  It's an artifact of a previous implementation that I should have cleaned up.  I'll update.", "author": "TGooch44", "createdAt": "2020-11-10T16:27:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTgzMzQzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg0MTQyMQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r519841421", "bodyText": "probably want this to be _logger.debug", "author": "rymurr", "createdAt": "2020-11-09T14:10:57Z", "path": "python/iceberg/core/util/profile.py", "diffHunk": "@@ -0,0 +1,36 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from contextlib import contextmanager\n+import logging\n+import time\n+\n+_logger = logging.getLogger(__name__)\n+\n+\n+@contextmanager\n+def profile(label, stats_dict=None):\n+    if stats_dict is None:\n+        print('PROFILE: %s starting' % label)", "originalCommit": "1eda220360d954ee9c67c72a9d17b276d7588191", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5NzU4Mg==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520697582", "bodyText": "agree.", "author": "TGooch44", "createdAt": "2020-11-10T16:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTg0MTQyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyMTA3MQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520521071", "bodyText": "Is there any way to do this w/o first going to pandas? Pandas is the most expensive part of this operation and if we could avoid it (say get tuples direct from pyarrow) performance would significantly improve", "author": "rymurr", "createdAt": "2020-11-10T12:22:47Z", "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],\n+                                                            schema=self._arrow_file.schema_arrow,\n+                                                            format=ds.ParquetFileFormat(),\n+                                                            filesystem=fs.LocalFileSystem())\n+\n+            arrow_table = arrow_dataset.to_table(columns=cols_to_read, filter=self._filter)\n+\n+        # process schema evolution if needed\n+        with profile(\"schema_evol_proc\", self._stats):\n+            processed_tbl = self.migrate_schema(arrow_table)\n+            for i, field in self.get_missing_fields():\n+                dtype_func = DTYPE_MAP.get(field.type.type_id)\n+                if dtype_func is None:\n+                    raise RuntimeError(\"Unable to create null column for type %s\" % field.type.type_id)\n+\n+                dtype = dtype_func(field)\n+                processed_tbl = (processed_tbl.add_column(i,\n+                                                          pa.field(field.name, dtype[0], True, None),\n+                                                          ParquetReader.create_null_column(processed_tbl[0],\n+                                                                                           dtype)))\n+        self._table = processed_tbl\n+        self.materialized_table = True\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        if self.curr_iterator is None:\n+            if not self.materialized_table:", "originalCommit": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDY5OTQxMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520699413", "bodyText": "Let me revisit this.  Internally(at NFLX), we generally end up in a pandas DF, so that was the motivation here, but as arrow has matured, it does seem like it's more viable to leave things in an arrow buffer, and let clients deal with how they want to post-process.", "author": "TGooch44", "createdAt": "2020-11-10T16:30:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyMTA3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ5MzIzOQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r524493239", "bodyText": "@rymurr I updated this, although I'm wondering if it makes more sense to remove this iterator logic and to_pandas from the parquet reader and move it to a higher level class because above the parquet reader, we will need something that combines all the various file scans into a single arrow table/pandas df.", "author": "TGooch44", "createdAt": "2020-11-16T18:44:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyMTA3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjExNDA4MA==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r526114080", "bodyText": "I agree. I think keeping this at the arrow level is best and having a layer above which concats the arrow tables together before handing back to the client.\nThis of course could lead to memory difficulties so a future enhancement might be to do some sort of chunked reads (moving to arrow's datasets is preferable to writing our own tho :-)", "author": "rymurr", "createdAt": "2020-11-18T14:08:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyMTA3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520525224", "bodyText": "Its a shame that we can't do this for a batch of files. Seems like a common pattern that pyarrow should be able to handle. Have you asked on user@arrow.apache.org if there is a plan to support this?\nDo you have a sense of the performance hit of reading files individually as opposed to in one batch? I am worried it could be significant.\nFinally, what is the path for reading/iterating multi-file iceberg tables? Do we read 1 parrquet file at a time or all at once or all at once as a pandas df? One of the key features of Datasets (for me) was being able to control memory when doing large reads.", "author": "rymurr", "createdAt": "2020-11-10T12:30:01Z", "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],", "originalCommit": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwODM1MQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520708351", "bodyText": "I agree this is the biggest problem with this implementation. I can start a discussion on the arrow dev list and see if there are any ideas here.", "author": "TGooch44", "createdAt": "2020-11-10T16:42:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc0MjczMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520742733", "bodyText": "Here is the thread on the user list:\nhttps://lists.apache.org/thread.html/r7d82d38d1e9a1b9bc99d64a0779de8886cf0bf7216d64d576a6c18e9%40%3Cuser.arrow.apache.org%3E", "author": "TGooch44", "createdAt": "2020-11-10T17:30:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDQ5NjA0OQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r524496049", "bodyText": "currently, the way that I do it is to downloaded the needed files in parallel using an internal lib, then do a single threaded read of the local files into an arrow table then combine the per file tables using concat_tables.  I can set-up some benchmarks to comapre the two approaches, but it seems like from the answer on the user list that it's not currently possible if there is schema evolution across the files.", "author": "TGooch44", "createdAt": "2020-11-16T18:49:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjExMjgzNA==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r526112834", "bodyText": "I would be curious to see perf impact if we just made 10 copies of a single parquet file and read it in one go vs sequentially w/ a concat at the end.\nIts too bad that the support for this use case in Arrow seems to be a future enhancement. Should be relatively easy to take it up once Arrow supports it.", "author": "rymurr", "createdAt": "2020-11-18T14:06:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI0NjEzMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r526246133", "bodyText": "I just did some initial testing and I'm not seeing a significant difference between passing a list of files to Datasets vs. using a python threadper file and then concating the resulting tables. Let me come up with some more test cases and see if there are some where the arrow threading does better.  Using some internal data I read about ~90M rows in about 9 secs both ways. Test was run on a r4.16xlarge instance with EBS storage.  I'll try and setup something reproducible for you to try out", "author": "TGooch44", "createdAt": "2020-11-18T16:53:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjk1MjU1Nw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r526952557", "bodyText": "Ok, thats good. I just wanted to get a broad idea to ensure we understand where the python reader will work/fail. So the bottleneck is really comign down to memory then as there is currently no way to page or dynamically load these per-file arrow tables", "author": "rymurr", "createdAt": "2020-11-19T14:58:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODQzMTQzMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528431433", "bodyText": "Check this out and let me know what you think:\nhttps://www.kaggle.com/tedgooch/notebook2b16380982", "author": "TGooch44", "createdAt": "2020-11-23T00:42:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc1NjUwNQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528756505", "bodyText": "thanks for the through investigation! Really interesting how close they are in a lot of cases.", "author": "rymurr", "createdAt": "2020-11-23T14:49:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc3NzUwNQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528777505", "bodyText": "yeah just wondering if this is primarily IO bound, so the diff between C threads and python threads is negligible.  Maybe in cases where there is more cpu intense work going on this won't track so closely.   In any case, I think this is an ok design until we can get better native support for schema normalization in arrow itself.  What do you think?  Any other outstanding items before this is ready for merge?", "author": "TGooch44", "createdAt": "2020-11-23T15:16:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc3OTI5Mg==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528779292", "bodyText": "Yeah, I will do another quick pass now but I think its ready to go", "author": "rymurr", "createdAt": "2020-11-23T15:19:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTIyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTYwMA==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520525600", "bodyText": "Is LocalFileSystem correct here? It caught my eye and wasn't sure how we would read eg from S3 or hadoop", "author": "rymurr", "createdAt": "2020-11-10T12:30:38Z", "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,240 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from datetime import datetime\n+import decimal\n+import logging\n+from typing import Any, Callable, Dict, List, Tuple, Union\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: Dict[TypeID,\n+                Callable[[NestedField], Tuple[pa.Field, Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> Union[pd.Series, pd.DataFrame]:\n+        if not self.materialized_table:\n+            self._read_data()\n+            with profile(\"to_pandas\", self._stats):\n+                if self._table is not None:\n+                    self._df = self._table.to_pandas(use_threads=True)\n+                else:\n+                    self._df = None\n+\n+        return self._df\n+\n+    def to_arrow_table(self) -> pa.Table:\n+        if not self.materialized_table:\n+            self._read_data()\n+\n+        return self._table\n+\n+    def _read_data(self) -> None:\n+        _logger.debug(\"Starting data read\")\n+\n+        # only scan the columns projected and in our file\n+        cols_to_read = prune_columns(self._file_schema, self._expected_schema)\n+\n+        with profile(\"read data\", self._stats):\n+            arrow_dataset = ds.FileSystemDataset.from_paths([self._input.location()],\n+                                                            schema=self._arrow_file.schema_arrow,\n+                                                            format=ds.ParquetFileFormat(),\n+                                                            filesystem=fs.LocalFileSystem())", "originalCommit": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwODI0MQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520708241", "bodyText": "Let me change this to map from the filesystem type of the input files.  We always bring files local before reading because we have some very fast libs for sucking down s3 data. Much better than throughput than reading via boto, alhough I haven't benchmarked against what pyarrow is doing. We typically do not see a lot of row-group elimination. so the mileage on doing partial s3 reads is not that significant.  This may not be the general-case though, so it should probably pick a filesystem based on the input file.", "author": "TGooch44", "createdAt": "2020-11-10T16:42:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNTYwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNjIzMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520526233", "bodyText": "why not pyarrow 2.0.0?", "author": "rymurr", "createdAt": "2020-11-10T12:31:42Z", "path": "python/setup.py", "diffHunk": "@@ -38,7 +37,7 @@\n                       'requests',\n                       'retrying',\n                       'pandas',\n-                      'pyarrow'\n+                      'pyarrow>=0.17.0'", "originalCommit": "00cdc50c58a68a5b2467ff99437cdb126f768fbb", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDcwODc1MQ==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r520708751", "bodyText": "Good catch.", "author": "TGooch44", "createdAt": "2020-11-10T16:43:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDUyNjIzMw=="}], "type": "inlineReview"}, {"oid": "c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "url": "https://github.com/apache/iceberg/commit/c2cd2e277c1d5ff2b6b18581ff1b511673cc0613", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-16T16:50:23Z", "type": "forcePushed"}, {"oid": "0eca8f40cff2669627263ad38a93224e8e989058", "url": "https://github.com/apache/iceberg/commit/0eca8f40cff2669627263ad38a93224e8e989058", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-16T16:51:51Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4MDYyMw==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528780623", "bodyText": "I think the import for the S3 filesystem should be down here right?", "author": "rymurr", "createdAt": "2020-11-23T15:21:03Z", "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,264 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from collections import namedtuple\n+from datetime import datetime\n+import decimal\n+import logging\n+import typing\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.filesystem import FileSystem, LocalFileSystem, S3FileSystem\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import FileSystemNotFound, InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: typing.Dict[TypeID,\n+                       typing.Callable[[NestedField], typing.Tuple[pa.Field, typing.Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+FS_MAP: typing.Dict[typing.Type[FileSystem], typing.Type[fs.FileSystem]] = {LocalFileSystem: fs.LocalFileSystem}\n+\n+try:\n+    FS_MAP[S3FileSystem] = fs.S3FileSystem", "originalCommit": "0eca8f40cff2669627263ad38a93224e8e989058", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MTQwMg==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r535781402", "bodyText": "what happens is the import succeeds for fs, but when you actually do fs.S3FileSystem it throws the import error. It might be clearer to do it all in the try...except though...let me just change that", "author": "TGooch44", "createdAt": "2020-12-04T02:04:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4MDYyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4Mjc3MA==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r528782770", "bodyText": "were you still going to remove the pandas and iterator stuff? I was kinda thinking that we would have a class 1 level higher that would read across all tables and pandas would be accessed there", "author": "rymurr", "createdAt": "2020-11-23T15:23:56Z", "path": "python/iceberg/parquet/parquet_reader.py", "diffHunk": "@@ -0,0 +1,264 @@\n+# Licensed to the Apache Software Foundation (ASF) under one\n+# or more contributor license agreements.  See the NOTICE file\n+# distributed with this work for additional information\n+# regarding copyright ownership.  The ASF licenses this file\n+# to you under the Apache License, Version 2.0 (the\n+# \"License\"); you may not use this file except in compliance\n+# with the License.  You may obtain a copy of the License at\n+#\n+#   http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing,\n+# software distributed under the License is distributed on an\n+# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+# KIND, either express or implied.  See the License for the\n+# specific language governing permissions and limitations\n+# under the License.\n+\n+\n+from collections import namedtuple\n+from datetime import datetime\n+import decimal\n+import logging\n+import typing\n+\n+from iceberg.api import Schema\n+from iceberg.api.expressions import Expression\n+from iceberg.api.io import InputFile\n+from iceberg.api.types import NestedField, Type, TypeID\n+from iceberg.core.filesystem import FileSystem, LocalFileSystem, S3FileSystem\n+from iceberg.core.util.profile import profile\n+from iceberg.exceptions import FileSystemNotFound, InvalidCastException\n+import numpy as np\n+import pandas as pd\n+import pyarrow as pa\n+from pyarrow import fs\n+import pyarrow.dataset as ds\n+import pyarrow.parquet as pq\n+\n+from .dataset_utils import get_dataset_filter\n+from .parquet_schema_utils import prune_columns\n+from .parquet_to_iceberg import convert_parquet_to_iceberg\n+\n+_logger = logging.getLogger(__name__)\n+\n+DTYPE_MAP: typing.Dict[TypeID,\n+                       typing.Callable[[NestedField], typing.Tuple[pa.Field, typing.Any]]] = \\\n+    {TypeID.BINARY: lambda field: pa.binary(),\n+     TypeID.BOOLEAN: lambda field: (pa.bool_(), False),\n+     TypeID.DATE: lambda field: (pa.date32(), datetime.now()),\n+     TypeID.DECIMAL: lambda field: (pa.decimal128(field.type.precision, field.type.scale),\n+                                    decimal.Decimal()),\n+     TypeID.DOUBLE: lambda field: (pa.float64(), np.nan),\n+     TypeID.FIXED: lambda field: pa.binary(field.length),\n+     TypeID.FLOAT: lambda field: (pa.float32(), np.nan),\n+     TypeID.INTEGER: lambda field: (pa.int32(), np.nan),\n+     TypeID.LIST: lambda field: (pa.list_(pa.field(\"element\",\n+                                                   DTYPE_MAP[field.type.element_type.type_id](field.type)[0])),\n+                                 None),\n+     TypeID.LONG: lambda field: (pa.int64(), np.nan),\n+     # To-Do: update to support reading map fields\n+     # TypeID.MAP: lambda field: (,),\n+     TypeID.STRING: lambda field: (pa.string(), \"\"),\n+     TypeID.STRUCT: lambda field: (pa.struct([(nested_field.name,\n+                                               DTYPE_MAP[nested_field.type.type_id](nested_field.type)[0])\n+                                              for nested_field in field.type.fields]), {}),\n+     TypeID.TIMESTAMP: lambda field: (pa.timestamp(\"us\"), datetime.now()),\n+     # not used in SPARK, so not implementing for now\n+     # TypeID.TIME: pa.time64(None)\n+     }\n+\n+FS_MAP: typing.Dict[typing.Type[FileSystem], typing.Type[fs.FileSystem]] = {LocalFileSystem: fs.LocalFileSystem}\n+\n+try:\n+    FS_MAP[S3FileSystem] = fs.S3FileSystem\n+except ImportError:\n+    _logger.warning(\"S3 Filesystem not available to arrow\")\n+\n+\n+class ParquetReader(object):\n+\n+    def __init__(self, input: InputFile, expected_schema: Schema, options, filter_expr: Expression,\n+                 case_sensitive: bool, start: int = None, end: int = None):\n+        self._stats: typing.Dict[str, int] = dict()\n+\n+        self._input = input\n+        self._input_fo = input.new_fo()\n+\n+        self._arrow_file = pq.ParquetFile(self._input_fo)\n+        self._file_schema = convert_parquet_to_iceberg(self._arrow_file)\n+        self._expected_schema = expected_schema\n+        self._file_to_expected_name_map = ParquetReader.get_field_map(self._file_schema,\n+                                                                      self._expected_schema)\n+        self._options = options\n+        self._filter = get_dataset_filter(filter_expr, ParquetReader.get_reverse_field_map(self._file_schema,\n+                                                                                           self._expected_schema))\n+\n+        self._case_sensitive = case_sensitive\n+        if start is not None or end is not None:\n+            raise NotImplementedError(\"Partial file reads are not yet supported\")\n+            # self.start = start\n+            # self.end = end\n+\n+        self.materialized_table = False\n+        self.curr_iterator = None\n+        self._table = None\n+        self._df = None\n+        self._batches = None\n+        self._row_tuple = None\n+\n+        _logger.debug(\"Reader initialized for %s\" % self._input.path)\n+\n+    @property\n+    def stats(self) -> dict:\n+        return dict(self._stats)\n+\n+    def to_pandas(self) -> typing.Union[pd.Series, pd.DataFrame]:", "originalCommit": "0eca8f40cff2669627263ad38a93224e8e989058", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc4MjQxMg==", "url": "https://github.com/apache/iceberg/pull/1727#discussion_r535782412", "bodyText": "It's removed in the latest commit, and the to_arrow_table method has been renamed to read().  Yes, there should be a something between TableScan and ParquetReader that combines the results of all the ScanTasks.", "author": "TGooch44", "createdAt": "2020-12-04T02:07:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODc4Mjc3MA=="}], "type": "inlineReview"}, {"oid": "544b90f4505e3292cda84d4c942caec8086a8c00", "url": "https://github.com/apache/iceberg/commit/544b90f4505e3292cda84d4c942caec8086a8c00", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-11-27T22:46:12Z", "type": "forcePushed"}, {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-12-04T02:01:35Z", "type": "commit"}, {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-12-04T02:01:35Z", "type": "forcePushed"}, {"oid": "9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "url": "https://github.com/apache/iceberg/commit/9d5d82ebed0b08771f52ba43166b6f1eb4c0614f", "message": "[python] Adding parquet package and the classes for reading parquet files and converting between arrow and iceberg schemas", "committedDate": "2020-12-04T02:01:35Z", "type": "forcePushed"}]}