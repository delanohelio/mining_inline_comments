{"pr_number": 1338, "pr_title": "Add file stats range optimizations for DeleteFileIndex", "pr_createdAt": "2020-08-13T22:02:24Z", "pr_url": "https://github.com/apache/iceberg/pull/1338", "timeline": [{"oid": "85128bb066a9d95e4e847ead937f8ba704949dce", "url": "https://github.com/apache/iceberg/commit/85128bb066a9d95e4e847ead937f8ba704949dce", "message": "Add file stats range optimizations for DeleteFileIndex.", "committedDate": "2020-08-13T22:03:18Z", "type": "commit"}, {"oid": "85128bb066a9d95e4e847ead937f8ba704949dce", "url": "https://github.com/apache/iceberg/commit/85128bb066a9d95e4e847ead937f8ba704949dce", "message": "Add file stats range optimizations for DeleteFileIndex.", "committedDate": "2020-08-13T22:03:18Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3NDg2Mw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470374863", "bodyText": "Q:    Is it possible that the dataLowers.get(id) or dataUppers.get(id) could be null ?  Assume the case,  we have a table with two columns. (a, b):\n\ntxn1:  insert a (1,2), then insert file1 will have (1,2);\ntxn2:  delete a (1,2) with equality fields (a,b),  then delete file2 will have (1,2) ;\ntxn3:  add an optional column c in schema;\ntxn4:  insert a (1,2,5) then insert file3 will have (1,2,5);\ntxn5:  delete a (1,2,5) with equality fields (a,b,c), then delete file4 will have (1,2,5).\n\nFinally, the  data file1 will check to decide wether is overlap with file4.  but delete file4 have equalityFieldSet (a,b,c) and file1 don't have the column c.  So seems the dataLowers.get(id)  could be null.  The following Comparator will throw NPE I guess.", "author": "openinx", "createdAt": "2020-08-14T02:24:22Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +100,101 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, schema))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        // check that the delete file can contain the data file's file_path\n+        Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+        if (lowers == null || uppers == null) {\n+          return true;\n+        }\n+\n+        Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+        int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+        ByteBuffer lower = lowers.get(pathId);\n+        if (lower != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+          return false;\n+        }\n+\n+        ByteBuffer upper = uppers.get(pathId);\n+        if (upper != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+          return false;\n+        }\n+\n+        break;\n+\n+      case EQUALITY_DELETES:\n+        if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+            deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+          return true;\n+        }\n+\n+        Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+        Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+        Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+        for (int id : deleteFile.equalityFieldIds()) {\n+          Type type = schema.findType(id);\n+          if (!type.isPrimitiveType()) {\n+            return true;\n+          }\n+\n+          if (!rangesOverlap(type.asPrimitiveType(),\n+              dataLowers.get(id), dataUppers.get(id), deleteLowers.get(id), deleteUppers.get(id))) {", "originalCommit": "85128bb066a9d95e4e847ead937f8ba704949dce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDcyODkwNQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470728905", "bodyText": "Yes, the stats may be missing. Looks like I missed that case here, but it is handled in the check for position deletes.\nI'll update this. Good catch!", "author": "rdblue", "createdAt": "2020-08-14T16:29:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3NDg2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNjA3Mw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471806073", "bodyText": "Fixed. I also added cases for null values, along with tests.", "author": "rdblue", "createdAt": "2020-08-17T22:19:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDM3NDg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQzMjk5NA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470432994", "bodyText": "How about the case:  data file1 have a range ['3', '5'],  delete file2 have a range ['2', '8'] ?   I think it should also be overlap but here we will return false ?\nIMO, we should return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(deleteUpper, dataLower) >=0  .    That should handle all overlap cases.", "author": "openinx", "createdAt": "2020-08-14T06:24:06Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +100,101 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, schema))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        // check that the delete file can contain the data file's file_path\n+        Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+        if (lowers == null || uppers == null) {\n+          return true;\n+        }\n+\n+        Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+        int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+        ByteBuffer lower = lowers.get(pathId);\n+        if (lower != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+          return false;\n+        }\n+\n+        ByteBuffer upper = uppers.get(pathId);\n+        if (upper != null &&\n+            Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+          return false;\n+        }\n+\n+        break;\n+\n+      case EQUALITY_DELETES:\n+        if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+            deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+          return true;\n+        }\n+\n+        Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+        Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+        Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+        Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+        for (int id : deleteFile.equalityFieldIds()) {\n+          Type type = schema.findType(id);\n+          if (!type.isPrimitiveType()) {\n+            return true;\n+          }\n+\n+          if (!rangesOverlap(type.asPrimitiveType(),\n+              dataLowers.get(id), dataUppers.get(id), deleteLowers.get(id), deleteUppers.get(id))) {\n+            return false;\n+          }\n+        }\n+        break;\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLower, ByteBuffer dataUpper,\n+                                           ByteBuffer deleteLower, ByteBuffer deleteUpper) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T low = Conversions.fromByteBuffer(type, dataLower);\n+    T high = Conversions.fromByteBuffer(type, dataUpper);\n+\n+    if (contains(comparator, low, high, Conversions.fromByteBuffer(type, deleteLower))) {\n+      return true;\n+    }\n+\n+    if (contains(comparator, low, high, Conversions.fromByteBuffer(type, deleteUpper))) {\n+      return true;", "originalCommit": "85128bb066a9d95e4e847ead937f8ba704949dce", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDczMDA5Mw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r470730093", "bodyText": "You're right, I'll take a look at this.", "author": "rdblue", "createdAt": "2020-08-14T16:31:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQzMjk5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNjExNg==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471806116", "bodyText": "Fixed.", "author": "rdblue", "createdAt": "2020-08-17T22:19:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQzMjk5NA=="}], "type": "inlineReview"}, {"oid": "95ccd5e272c9457c6b9780a02b8a45f57dd6a877", "url": "https://github.com/apache/iceberg/commit/95ccd5e272c9457c6b9780a02b8a45f57dd6a877", "message": "Fix range overlap check and add null check.", "committedDate": "2020-08-17T22:12:49Z", "type": "commit"}, {"oid": "b4e177b7c8fbd979b9afe0a00e8adde3cb6d9280", "url": "https://github.com/apache/iceberg/commit/b4e177b7c8fbd979b9afe0a00e8adde3cb6d9280", "message": "Add tests for DataFileIndex stats filters.", "committedDate": "2020-08-17T22:12:49Z", "type": "commit"}, {"oid": "721a484a2c908e2e0f58022a3e78f4b5334e1928", "url": "https://github.com/apache/iceberg/commit/721a484a2c908e2e0f58022a3e78f4b5334e1928", "message": "Minor fixes for delete files.", "committedDate": "2020-08-17T22:16:50Z", "type": "commit"}, {"oid": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "url": "https://github.com/apache/iceberg/commit/c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "message": "Fix FilterIterator with reused values.", "committedDate": "2020-08-17T22:17:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNzI1Ng==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471807256", "bodyText": "This was needed to ensure the stats columns are projected for data files when there are delete files, even if the stats columns were not requested by the caller.", "author": "rdblue", "createdAt": "2020-08-17T22:23:02Z", "path": "core/src/main/java/org/apache/iceberg/ManifestGroup.java", "diffHunk": "@@ -163,6 +166,9 @@ ManifestGroup planWith(ExecutorService newExecutorService) {\n     DeleteFileIndex deleteFiles = deleteIndexBuilder.build();\n \n     boolean dropStats = ManifestReader.dropStats(dataFilter, columns);\n+    if (!deleteFiles.isEmpty()) {\n+      select(Streams.concat(columns.stream(), ManifestReader.STATS_COLUMNS.stream()).collect(Collectors.toList()));", "originalCommit": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwNzc1Nw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471807757", "bodyText": "This fixes the filter with reused containers, like GenericRecord. The advance call in next would replace values in a reused row, which would in effect return the next matching row.", "author": "rdblue", "createdAt": "2020-08-17T22:24:27Z", "path": "core/src/main/java/org/apache/iceberg/util/FilterIterator.java", "diffHunk": "@@ -34,21 +34,21 @@\n public abstract class FilterIterator<T> implements CloseableIterator<T> {\n   private final Iterator<T> items;\n   private boolean closed;\n-  private boolean hasNext;\n+  private boolean nextReady;", "originalCommit": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgwODI3MQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r471808271", "bodyText": "Checking for createWriterFunc here is needed because forTable sets the row schema. So rows can be included in position deletes when the writer func is added and when there is a row schema.", "author": "rdblue", "createdAt": "2020-08-17T22:25:37Z", "path": "parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java", "diffHunk": "@@ -383,10 +383,7 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n \n       meta(\"delete-type\", \"position\");\n \n-      if (rowSchema != null) {\n-        Preconditions.checkState(createWriterFunc != null,\n-            \"Cannot create delete file with deletes rows unless createWriterFunc is set\");\n-\n+      if (rowSchema != null && createWriterFunc != null) {", "originalCommit": "c6d8b7fcce666607c54a2d25abcf752ee9cb8719", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "b32e087a782e34fa1660e045b057a2f448997a12", "url": "https://github.com/apache/iceberg/commit/b32e087a782e34fa1660e045b057a2f448997a12", "message": "Fix checkstyle issues.", "committedDate": "2020-08-17T22:27:26Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjExOTM3NQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472119375", "bodyText": "Nice catch.", "author": "openinx", "createdAt": "2020-08-18T11:51:56Z", "path": "core/src/main/java/org/apache/iceberg/BaseFile.java", "diffHunk": "@@ -175,6 +175,7 @@ public PartitionData copy() {\n     this.keyMetadata = toCopy.keyMetadata == null ? null : Arrays.copyOf(toCopy.keyMetadata, toCopy.keyMetadata.length);\n     this.splitOffsets = toCopy.splitOffsets == null ? null :\n         Arrays.copyOf(toCopy.splitOffsets, toCopy.splitOffsets.length);\n+    this.equalityIds = toCopy.equalityIds != null ? Arrays.copyOf(toCopy.equalityIds, toCopy.equalityIds.length) : null;", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEzNzI5Nw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472137297", "bodyText": "Why change this ? IMO, if rowSchema is not null and createWriteFunc is null, we should throw exception, rather than going to the delete files without row path ?", "author": "openinx", "createdAt": "2020-08-18T12:24:52Z", "path": "core/src/main/java/org/apache/iceberg/avro/Avro.java", "diffHunk": "@@ -300,10 +300,7 @@ public DeleteWriteBuilder equalityFieldIds(int... fieldIds) {\n \n       meta(\"delete-type\", \"position\");\n \n-      if (rowSchema != null) {\n-        Preconditions.checkState(createWriterFunc != null,\n-            \"Cannot create delete file with deletes rows unless createWriterFunc is set\");\n-\n+      if (rowSchema != null && createWriterFunc != null) {", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM1NzkxNQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472357915", "bodyText": "Using forTable sets the row schema automatically, so we can't determine that the user intended to write rows that way.", "author": "rdblue", "createdAt": "2020-08-18T17:19:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjEzNzI5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE2MzExMw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472163113", "bodyText": "Q:  What's the case that nullValueCount or valueCount will be null ?  Does that mean there's no null item in this file ?\nIt's safe enough to return false here (Because the canContainEqDeletesForFile won't skip the files to join , while if return true in this method then it possible that we will skip files to join),  but once return true we must ensure that it's surely the right case.", "author": "openinx", "createdAt": "2020-08-18T12:55:14Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {\n+        return false;\n+      }\n+\n+      if (allNull(deleteNullCounts, deleteValueCounts, field) && allNonNull(dataNullCounts, field)) {\n+        return false;\n+      }\n+\n+      ByteBuffer dataLower = dataLowers.get(id);\n+      ByteBuffer dataUpper = dataUppers.get(id);\n+      ByteBuffer deleteLower = deleteLowers.get(id);\n+      ByteBuffer deleteUpper = deleteUppers.get(id);\n+      if (dataLower == null || dataUpper == null || deleteLower == null || deleteUpper == null) {\n+        return true;\n+      }\n+\n+      if (!rangesOverlap(field.type().asPrimitiveType(), dataLower, dataUpper, deleteLower, deleteUpper)) {\n+        return false;\n+      }\n+    }\n+\n+    return true;\n+  }\n+\n+  private static <T> boolean rangesOverlap(Type.PrimitiveType type,\n+                                           ByteBuffer dataLowerBuf, ByteBuffer dataUpperBuf,\n+                                           ByteBuffer deleteLowerBuf, ByteBuffer deleteUpperBuf) {\n+    Comparator<T> comparator = Comparators.forType(type);\n+    T dataLower = Conversions.fromByteBuffer(type, dataLowerBuf);\n+    T dataUpper = Conversions.fromByteBuffer(type, dataUpperBuf);\n+    T deleteLower = Conversions.fromByteBuffer(type, deleteLowerBuf);\n+    T deleteUpper = Conversions.fromByteBuffer(type, deleteUpperBuf);\n+\n+    return comparator.compare(deleteLower, dataUpper) <= 0 && comparator.compare(dataLower, deleteUpper) <= 0;\n+  }\n+\n+  private static boolean allNonNull(Map<Integer, Long> nullValueCounts, Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return true;\n+    }\n+\n+    if (nullValueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    if (nullValueCount == null) {\n+      return false;\n+    }\n+\n+    return nullValueCount <= 0;\n+  }\n+\n+  private static boolean allNull(Map<Integer, Long> nullValueCounts, Map<Integer, Long> valueCounts,\n+                                 Types.NestedField field) {\n+    if (field.isRequired()) {\n+      return false;\n+    }\n+\n+    if (nullValueCounts == null || valueCounts == null) {\n+      return false;\n+    }\n+\n+    Long nullValueCount = nullValueCounts.get(field.fieldId());\n+    Long valueCount = valueCounts.get(field.fieldId());\n+    if (nullValueCount == null || valueCount == null) {\n+      return true;", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM2MDcwMQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472360701", "bodyText": "Stats can be configured for each column, so that you can avoid keeping stats in table metadata when they will not be useful. If the table has column-level stats, but a column is configured to none, then these would be null. That indicates that the values are unknown.\nI think you're right that this is not the correct return value. It should be false because we do not know that all values are null if there are no counts.", "author": "rdblue", "createdAt": "2020-08-18T17:24:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE2MzExMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQwMjQxMw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472402413", "bodyText": "Fixed.", "author": "rdblue", "createdAt": "2020-08-18T18:37:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjE2MzExMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4ODc0MA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472288740", "bodyText": "nit: why expected size is 1?", "author": "aokolnychyi", "createdAt": "2020-08-18T15:33:43Z", "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM1ODIyMA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472358220", "bodyText": "Accident.", "author": "rdblue", "createdAt": "2020-08-18T17:20:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4ODc0MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI4ODg1NQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472288855", "bodyText": "nit: same here", "author": "aokolnychyi", "createdAt": "2020-08-18T15:33:52Z", "path": "api/src/main/java/org/apache/iceberg/data/Record.java", "diffHunk": "@@ -35,4 +36,26 @@\n   Record copy();\n \n   Record copy(Map<String, Object> overwriteValues);\n+\n+  default Record copy(String field, Object value) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field, value);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);\n+    overwriteValues.put(field1, value1);\n+    overwriteValues.put(field2, value2);\n+    return copy(overwriteValues);\n+  }\n+\n+  default Record copy(String field1, Object value1, String field2, Object value2, String field3, Object value3) {\n+    Map<String, Object> overwriteValues = Maps.newHashMapWithExpectedSize(1);", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5NjQxNA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472296414", "bodyText": "nit: will it fit on one line if we define a comparator as a separate variable?", "author": "aokolnychyi", "createdAt": "2020-08-18T15:44:46Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjM2NTg0OQ==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472365849", "bodyText": "Fixed.", "author": "rdblue", "createdAt": "2020-08-18T17:32:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5NjQxNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5NjQ0Mw==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472296443", "bodyText": "same here", "author": "aokolnychyi", "createdAt": "2020-08-18T15:44:50Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5OTQ3MA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472299470", "bodyText": "A few comments would be helpful here for devs who will work on this later", "author": "aokolnychyi", "createdAt": "2020-08-18T15:49:12Z", "path": "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java", "diffHunk": "@@ -96,21 +104,157 @@ private StructLikeWrapper newWrapper(int specId) {\n     Pair<Integer, StructLikeWrapper> partition = partition(file.specId(), file.partition());\n     Pair<long[], DeleteFile[]> partitionDeletes = sortedDeletesByPartition.get(partition);\n \n+    Stream<DeleteFile> matchingDeletes;\n     if (partitionDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes);\n     } else if (globalDeletes == null) {\n-      return limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n+      matchingDeletes = limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second());\n     } else {\n-      return Stream.concat(\n-          Stream.of(limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes)),\n-          Stream.of(limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()))\n-      ).toArray(DeleteFile[]::new);\n+      matchingDeletes = Stream.concat(\n+          limitBySequenceNumber(sequenceNumber, globalSeqs, globalDeletes),\n+          limitBySequenceNumber(sequenceNumber, partitionDeletes.first(), partitionDeletes.second()));\n+    }\n+\n+    return matchingDeletes\n+        .filter(deleteFile -> canContainDeletesForFile(file, deleteFile, specsById.get(file.specId()).schema()))\n+        .toArray(DeleteFile[]::new);\n+  }\n+\n+  private static boolean canContainDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    switch (deleteFile.content()) {\n+      case POSITION_DELETES:\n+        return canContainPosDeletesForFile(dataFile, deleteFile);\n+\n+      case EQUALITY_DELETES:\n+        return canContainEqDeletesForFile(dataFile, deleteFile, schema);\n+    }\n+\n+    return true;\n+  }\n+\n+  private static boolean canContainPosDeletesForFile(DataFile dataFile, DeleteFile deleteFile) {\n+    // check that the delete file can contain the data file's file_path\n+    Map<Integer, ByteBuffer> lowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> uppers = deleteFile.upperBounds();\n+    if (lowers == null || uppers == null) {\n+      return true;\n+    }\n+\n+    Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n+    int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n+    ByteBuffer lower = lowers.get(pathId);\n+    if (lower != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, lower)) < 0) {\n+      return false;\n     }\n+\n+    ByteBuffer upper = uppers.get(pathId);\n+    if (upper != null &&\n+        Comparators.charSequences().compare(dataFile.path(), Conversions.fromByteBuffer(pathType, upper)) > 0) {\n+      return false;\n+    }\n+\n+    return true;\n+  }\n+\n+  @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n+  private static boolean canContainEqDeletesForFile(DataFile dataFile, DeleteFile deleteFile, Schema schema) {\n+    if (dataFile.lowerBounds() == null || dataFile.upperBounds() == null ||\n+        deleteFile.lowerBounds() == null || deleteFile.upperBounds() == null) {\n+      return true;\n+    }\n+\n+    Map<Integer, ByteBuffer> dataLowers = dataFile.lowerBounds();\n+    Map<Integer, ByteBuffer> dataUppers = dataFile.upperBounds();\n+    Map<Integer, ByteBuffer> deleteLowers = deleteFile.lowerBounds();\n+    Map<Integer, ByteBuffer> deleteUppers = deleteFile.upperBounds();\n+\n+    Map<Integer, Long> dataNullCounts = dataFile.nullValueCounts();\n+    Map<Integer, Long> dataValueCounts = dataFile.valueCounts();\n+    Map<Integer, Long> deleteNullCounts = deleteFile.nullValueCounts();\n+    Map<Integer, Long> deleteValueCounts = deleteFile.valueCounts();\n+\n+    for (int id : deleteFile.equalityFieldIds()) {\n+      Types.NestedField field = schema.findField(id);\n+      if (!field.type().isPrimitiveType()) {\n+        return true;\n+      }\n+\n+      if (allNull(dataNullCounts, dataValueCounts, field) && allNonNull(deleteNullCounts, field)) {", "originalCommit": "b32e087a782e34fa1660e045b057a2f448997a12", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjQxMDI2NA==", "url": "https://github.com/apache/iceberg/pull/1338#discussion_r472410264", "bodyText": "Added a comment for each case.", "author": "rdblue", "createdAt": "2020-08-18T18:51:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MjI5OTQ3MA=="}], "type": "inlineReview"}, {"oid": "591f7808d80d91fd975a6d59d1cbdd06c8e6250b", "url": "https://github.com/apache/iceberg/commit/591f7808d80d91fd975a6d59d1cbdd06c8e6250b", "message": "Fix checkstyle and review comments.", "committedDate": "2020-08-18T18:35:04Z", "type": "commit"}, {"oid": "52db59f151be59f4a9c9260f11e505c5903f7a9d", "url": "https://github.com/apache/iceberg/commit/52db59f151be59f4a9c9260f11e505c5903f7a9d", "message": "Fix allNull in DeleteFileIndex when counts are unknown.", "committedDate": "2020-08-18T18:36:41Z", "type": "commit"}, {"oid": "690da6ad40d01aacbd864218f7335922616eee6b", "url": "https://github.com/apache/iceberg/commit/690da6ad40d01aacbd864218f7335922616eee6b", "message": "Add comments to canContain for equality, update null handling.", "committedDate": "2020-08-18T18:47:52Z", "type": "commit"}, {"oid": "3b994df9140e4f3dc5a78faa11da5d9f41e61ac2", "url": "https://github.com/apache/iceberg/commit/3b994df9140e4f3dc5a78faa11da5d9f41e61ac2", "message": "Add a test for null values without value range overlap.", "committedDate": "2020-08-18T18:50:46Z", "type": "commit"}, {"oid": "8c0b01a643dc69e1ee5a704bfbd6785343192dbd", "url": "https://github.com/apache/iceberg/commit/8c0b01a643dc69e1ee5a704bfbd6785343192dbd", "message": "Apply equality count checks even if value ranges are missing.", "committedDate": "2020-08-18T18:55:36Z", "type": "commit"}]}