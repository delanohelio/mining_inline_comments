{"pr_number": 5433, "pr_title": "Improves maintainability of SingleBrokerDataDeletionTest integration test", "pr_createdAt": "2020-09-25T15:54:01Z", "pr_url": "https://github.com/camunda-cloud/zeebe/pull/5433", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw==", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495807037", "bodyText": "Fix the rawtypes warning.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              @SuppressWarnings(\"rawtypes\")\n          \n          \n            \n              private Stream<Record> newRecordStream(final LogStreamReader reader) {\n          \n          \n            \n                final Spliterator<LoggedEvent> spliterator =\n          \n          \n            \n                    Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n          \n          \n            \n                return StreamSupport.stream(spliterator, false)\n          \n          \n            \n                    .map(event -> CopiedRecords.createCopiedRecord(1, event));\n          \n          \n            \n              }\n          \n          \n            \n              private Stream<Record<?>> newRecordStream(final LogStreamReader reader) {\n          \n          \n            \n                final Spliterator<LoggedEvent> spliterator =\n          \n          \n            \n                    Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n          \n          \n            \n                return StreamSupport.stream(spliterator, false)\n          \n          \n            \n                    .map(event -> (CopiedRecord<?>) CopiedRecords.createCopiedRecord(1, event));\n          \n          \n            \n              }", "author": "saig0", "createdAt": "2020-09-28T09:29:23Z", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n-    writeSegments(broker, 2);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and a single deployment\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // grab the first log position, and the position of the last unacknowledged event\n+    // memorize first position pre compaction to compare later on\n     reader.seekToFirstEvent();\n-    final long firstPosition = reader.getPosition();\n-    long lastUnacknowledgedPosition = -1;\n-    while (reader.hasNext()) {\n-      final LoggedEvent event = reader.next();\n-      event.readMetadata(metadata);\n-      if (metadata.getValueType() == ValueType.DEPLOYMENT) {\n-        lastUnacknowledgedPosition = event.getPosition();\n-        break;\n-      }\n-    }\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    // then - enforce compaction and ensure the accepted deployment is still present on the log\n+    // after compaction\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-\n-    // then\n-    assertThat(lastUnacknowledgedPosition).isGreaterThan(-1L);\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n-    reader.seekToFirstEvent();\n-    assertThat(reader.getPosition())\n-        .isGreaterThan(firstPosition)\n-        .isLessThanOrEqualTo(lastUnacknowledgedPosition);\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactNotExportedEvents() {\n     // given\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     final Broker broker = clusteringRule.getBroker(0);\n-\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // - write records and update the exporter position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n \n-    // - write more records but don't update the exporter position\n+    // when - filling the log with messages (updating the position), then a single deployment\n+    // command, and more messages (all of which do not update the position)\n+    publishEnoughMessagesForCompaction();\n     ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n+    publishEnoughMessagesForCompaction();\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    fillSegments(broker, filledSegmentCount);\n-\n-    // - trigger a snapshot creation\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    long firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(filledSegmentCount));\n-\n-    // then verify that the log still contains the records that are not exported\n-    final var firstNonExportedPosition =\n-        ControllableExporter.NOT_EXPORTED_RECORDS.get(0).getPosition();\n-\n-    assertThat(hasRecordWithPosition(reader, firstNonExportedPosition))\n-        .describedAs(\"Expected first non-exported record to be present in the log but not found.\")\n-        .isTrue();\n-\n-    // - write more records and update the exporter position again\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n \n+    // when - re-enabling updating the position\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, segmentsBeforeSnapshot + 1);\n+    publishEnoughMessagesForCompaction();\n \n-    // - trigger the next snapshot creation\n+    // then - ensure we can still compact\n+    reader.seekToFirstEvent();\n+    firstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n     clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n-\n-    // then verify that the log is now compacted after the exporter position was updated\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n   }\n \n   @Test\n   public void shouldCompactWhenExporterHasBeenRemoved() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n+    // given - an exporter which updates its position and accepts all records\n+    final int nodeId = 0;\n+    LogStreamReader reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final Broker broker = clusteringRule.getBroker(nodeId);\n     ControllableExporter.updatePosition(true);\n-    fillSegments(broker, SEGMENT_COUNT);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    // create first snapshot with exporter positions\n-    final var firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n-\n-    // restart with no exporter\n-    final var brokerCfg = clusteringRule.getBrokerCfg(0);\n-    brokerCfg.setExporters(Map.of());\n-    clusteringRule.stopBroker(0);\n-    clusteringRule.startBroker(0);\n \n-    final var filledSegmentCount = SEGMENT_COUNT * 2;\n-    writeSegments(broker, filledSegmentCount);\n+    // when - filling the log with messages, and a single deployment command for which we will not\n+    // update the position\n+    publishEnoughMessagesForCompaction();\n+    ControllableExporter.updatePosition(false);\n+    deployDummyProcess();\n \n-    // when triggering new snapshot creation\n-    final var segmentsCount = getSegmentsCount(broker);\n+    // then - force compaction and ensure we compacted only things before our sentinel command\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    final FileBasedSnapshotMetadata firstSnapshot = clusteringRule.waitForSnapshotAtBroker(broker);\n+    awaitUntilCompaction(reader, firstPositionPreCompaction);\n+    assertContainsDeploymentCommand(reader);\n+\n+    // when - restarting without the exporter\n+    final var brokerCfg = clusteringRule.getBrokerCfg(nodeId);\n+    brokerCfg.setExporters(Collections.emptyMap());\n+    clusteringRule.stopBroker(nodeId);\n+    clusteringRule.startBroker(nodeId);\n+    publishEnoughMessagesForCompaction();\n+\n+    // then - force compaction, and expect the deployment command to have been removed\n+    reader = clusteringRule.getLogStream(1).newLogStreamReader().join();\n+    final long newFirstPositionPreCompaction = reader.getPosition();\n     clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n-    final var secondSnapshot = clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    clusteringRule.waitForNewSnapshotAtBroker(broker, firstSnapshot);\n+    assertThat(newFirstPositionPreCompaction).isGreaterThan(firstPositionPreCompaction);\n+    awaitUntilCompaction(reader, newFirstPositionPreCompaction);\n+    assertDoesNotContainDeploymentCommand(reader);\n+  }\n \n-    // then\n-    assertThat(firstSnapshot).isNotEqualTo(secondSnapshot);\n-    await()\n+  private void awaitUntilCompaction(\n+      final LogStreamReader reader, final long firstPositionPreCompaction) {\n+    await(\"until some data was compacted\")\n+        .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsCount));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+            });\n   }\n \n-  private void fillSegments(final Broker broker, final int segmentCount) {\n-    writeSegments(broker, segmentCount);\n-\n-    await()\n-        .untilAsserted(\n-            () ->\n-                assertThat(ControllableExporter.EXPORTED_RECORDS.get())\n-                    .describedAs(\"Expected all written records to be exported\")\n-                    .isGreaterThanOrEqualTo(writtenRecords.get()));\n+  private void deployDummyProcess() {\n+    clusteringRule\n+        .getClient()\n+        .newDeployCommand()\n+        .addWorkflowModel(\n+            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n+        .send()\n+        .join();\n   }\n \n-  private void writeSegments(final Broker broker, final int segmentCount) {\n-    while (getSegmentsCount(broker) <= segmentCount) {\n-      writeToLog();\n-      writtenRecords.incrementAndGet();\n-    }\n+  private void configureBroker(final BrokerCfg brokerCfg) {\n+    final DataCfg data = brokerCfg.getData();\n+    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n+    data.setLogSegmentSize(LOG_SEGMENT_SIZE);\n+    data.setLogIndexDensity(5);\n+    brokerCfg.getNetwork().setMaxMessageSize(MAX_MESSAGE_SIZE);\n+\n+    final ExporterCfg exporterCfg = new ExporterCfg();\n+    exporterCfg.setClassName(ControllableExporter.class.getName());\n+    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n   }\n \n-  private void writeToLog() {\n+  private void publishEnoughMessagesForCompaction() {\n+    final int requiredMessageCount = (int) LOG_SEGMENT_SIZE.toBytes() / LARGE_VARIABLE_SIZE;\n+    IntStream.range(0, requiredMessageCount + 1).forEach(this::publishMaxMessageSizeMessage);\n+  }\n \n+  private void publishMaxMessageSizeMessage(final int key) {\n     clusteringRule\n         .getClient()\n         .newPublishMessageCommand()\n         .messageName(\"msg\")\n-        .correlationKey(\"key\")\n+        .correlationKey(\"msg-\" + key)\n+        .variables(Map.of(\"foo\", MAX_MESSAGE_SIZE_VARIABLE))\n         .send()\n         .join();\n   }\n \n-  private int getSegmentsCount(final Broker broker) {\n-    return getSegments(broker).size();\n+  @SuppressWarnings(\"rawtypes\")\n+  private Stream<Record> newRecordStream(final LogStreamReader reader) {\n+    final Spliterator<LoggedEvent> spliterator =\n+        Spliterators.spliteratorUnknownSize(reader, Spliterator.ORDERED);\n+    return StreamSupport.stream(spliterator, false)\n+        .map(event -> CopiedRecords.createCopiedRecord(1, event));\n   }", "originalCommit": "2549378f6d9803bdffbca6f9c5ceffd364782774", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg4MTA0Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495881042", "bodyText": "Did it make sense why I'm using CopiedRecord here? Wasn't sure if it would be obvious - so that we can use RecordAssert and get nicer error messages mostly. I was worried it would seem over the top for someone reading it later, but the errors are really much nicer.", "author": "npepinpe", "createdAt": "2020-09-28T11:51:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTg4NDM0Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495884342", "bodyText": "Make sense \ud83d\udc4d", "author": "saig0", "createdAt": "2020-09-28T11:58:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwNzAzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTgwODUxNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/5433#discussion_r495808516", "bodyText": "Replace the magic number with a constant. Or, extract clusteringRule.getLogStream(1).newLogStreamReader().join() into a new method.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                final LogStream logStream = clusteringRule.getLogStream(1);\n          \n          \n            \n                final LogStream logStream = clusteringRule.getLogStream(PARTITION_ID);", "author": "saig0", "createdAt": "2020-09-28T09:31:50Z", "path": "qa/integration-tests/src/test/java/io/zeebe/broker/it/clustering/SingleBrokerDataDeletionTest.java", "diffHunk": "@@ -47,284 +47,247 @@\n public class SingleBrokerDataDeletionTest {\n \n   private static final Duration SNAPSHOT_PERIOD = Duration.ofMinutes(5);\n-  private static final int SEGMENT_COUNT = 5;\n+  private static final DataSize LOG_SEGMENT_SIZE = DataSize.ofKilobytes(8);\n+  private static final DataSize MAX_MESSAGE_SIZE = DataSize.ofKilobytes(4);\n+  // variable has to be a bit smaller than max message size otherwise it will be rejected\n+  private static final int LARGE_VARIABLE_SIZE = (int) MAX_MESSAGE_SIZE.toBytes() / 2;\n+  private static final String MAX_MESSAGE_SIZE_VARIABLE = \"x\".repeat(LARGE_VARIABLE_SIZE);\n \n   @Rule\n-  public final ClusteringRule clusteringRule =\n-      new ClusteringRule(1, 1, 1, this::configureCustomExporter);\n+  public final ClusteringRule clusteringRule = new ClusteringRule(1, 1, 1, this::configureBroker);\n \n-  private final AtomicLong writtenRecords = new AtomicLong(0);\n-\n-  private void configureCustomExporter(final BrokerCfg brokerCfg) {\n-    final DataCfg data = brokerCfg.getData();\n-    data.setSnapshotPeriod(SNAPSHOT_PERIOD);\n-    data.setLogSegmentSize(DataSize.ofKilobytes(8));\n-    data.setLogIndexDensity(5);\n-    brokerCfg.getNetwork().setMaxMessageSize(DataSize.ofKilobytes(8));\n-\n-    final ExporterCfg exporterCfg = new ExporterCfg();\n-    exporterCfg.setClassName(ControllableExporter.class.getName());\n-    brokerCfg.setExporters(Collections.singletonMap(\"snapshot-test-exporter\", exporterCfg));\n+  @After\n+  public void cleanUp() {\n+    ControllableExporter.updatePosition(true);\n+    ControllableExporter.EXPORTED_RECORDS.set(0);\n+    ControllableExporter.RECORD_TYPE_FILTER.set(r -> true);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(r -> true);\n   }\n \n   @Test\n-  public void shouldCompactEvenIfSkippingAllRecords() {\n-    // given\n-    final Broker broker = clusteringRule.getBroker(0);\n-\n-    // when\n+  public void shouldCompactEvenIfSkippingAllRecordsInitially() {\n+    // given - an exporter which does not update its own position and filters everything but\n+    // deployment commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);\n+    final LogStreamReader reader = logStream.newLogStreamReader().join();\n     ControllableExporter.updatePosition(false);\n-    ControllableExporter.RECORD_TYPE_FILTER.set(r -> r == RecordType.COMMAND);\n-    ControllableExporter.VALUE_TYPE_FILTER.set(r -> r == ValueType.DEPLOYMENT);\n-    writeSegments(broker, 2);\n-    clusteringRule\n-        .getClient()\n-        .newDeployCommand()\n-        .addWorkflowModel(\n-            Bpmn.createExecutableProcess(\"process\").startEvent().done(), \"process.bpmn\")\n-        .send()\n-        .join();\n+    ControllableExporter.RECORD_TYPE_FILTER.set(t -> t == RecordType.COMMAND);\n+    ControllableExporter.VALUE_TYPE_FILTER.set(t -> t == ValueType.DEPLOYMENT);\n+\n+    // when - filling up the log with messages and NO deployments\n+    publishEnoughMessagesForCompaction();\n+    deployDummyProcess();\n     await(\"until at least one record is exported\")\n         .atMost(Duration.ofSeconds(5))\n         .untilAsserted(\n             () -> assertThat(ControllableExporter.EXPORTED_RECORDS).hasValueGreaterThan(0));\n \n-    // enforce compaction\n-    final var segmentsBeforeSnapshot = getSegmentsCount(broker);\n-    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    // memorize first position pre compaction to compare later on\n+    reader.seekToFirstEvent();\n+    final long firstPositionPreCompaction = reader.getPosition();\n \n-    // then\n-    assertThat(clusteringRule.waitForSnapshotAtBroker(broker)).isNotNull();\n-    await()\n+    // then - enforce compaction and make sure we have less records than we previously did\n+    clusteringRule.getClock().addTime(SNAPSHOT_PERIOD);\n+    clusteringRule.waitForSnapshotAtBroker(clusteringRule.getBroker(0));\n+    await(\"until some data before the deployment command was compacted\")\n         .untilAsserted(\n-            () ->\n-                assertThat(getSegmentsCount(broker))\n-                    .describedAs(\"Expected less segments after a snapshot is taken\")\n-                    .isLessThan(segmentsBeforeSnapshot));\n+            () -> {\n+              reader.seekToFirstEvent();\n+              final long firstPositionPostCompaction = reader.getPosition();\n+              assertThat(firstPositionPostCompaction).isGreaterThan(firstPositionPreCompaction);\n+              assertContainsDeploymentCommand(reader);\n+            });\n   }\n \n   @Test\n   public void shouldNotCompactUnacknowledgedEventsEvenIfSkipping() {\n-    // given\n-    final RecordMetadata metadata = new RecordMetadata();\n-    final Broker broker = clusteringRule.getBroker(0);\n-    final var logstream = clusteringRule.getLogStream(1);\n-    final var reader = logstream.newLogStreamReader().join();\n-\n-    // when\n+    // given - an exporter which does not update its own position and only accepts deployment\n+    // commands\n+    final LogStream logStream = clusteringRule.getLogStream(1);", "originalCommit": "2549378f6d9803bdffbca6f9c5ceffd364782774", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2d5139e879c3a818618f1854e703f94b7c914176", "url": "https://github.com/camunda-cloud/zeebe/commit/2d5139e879c3a818618f1854e703f94b7c914176", "message": "chore(qa): refactor test to improve maintainability\n\n- refactors tests to decouple from internal details (such as segments,\n  segment count, etc.)\n- refactors tests for better readability/maintainability", "committedDate": "2020-09-30T09:23:05Z", "type": "commit"}, {"oid": "2d5139e879c3a818618f1854e703f94b7c914176", "url": "https://github.com/camunda-cloud/zeebe/commit/2d5139e879c3a818618f1854e703f94b7c914176", "message": "chore(qa): refactor test to improve maintainability\n\n- refactors tests to decouple from internal details (such as segments,\n  segment count, etc.)\n- refactors tests for better readability/maintainability", "committedDate": "2020-09-30T09:23:05Z", "type": "forcePushed"}]}