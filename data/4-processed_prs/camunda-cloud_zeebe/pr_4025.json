{"pr_number": 4025, "pr_title": "docs(appendix): Update docs with new info on configuration", "pr_createdAt": "2020-03-10T16:53:48Z", "pr_url": "https://github.com/camunda-cloud/zeebe/pull/4025", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NDYxMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390854610", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            docker run --name zeebe -p 26500-26502:2600-26502 camunda/zeebe:latest\n          \n          \n            \n            docker run --name zeebe -p 26500-26502:26500-26502 camunda/zeebe:latest", "author": "npepinpe", "createdAt": "2020-03-11T09:52:44Z", "path": "docs/src/introduction/install.md", "diffHunk": "@@ -33,7 +33,7 @@ Docker configurations for starting a single Zeebe broker using `docker-compose`,\n You can run Zeebe with Docker:\n \n ```bash\n-docker run --name zeebe -p 26500:26500 camunda/zeebe:latest\n+docker run --name zeebe -p 26500-26502:2600-26502 camunda/zeebe:latest", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk5NTYyNQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r391995625", "bodyText": "I think we need to have the \"legacy\" configuration in the docs. My assessment is grounded in our statement that 0.20 was production ready - and I know of several users who have in fact gone into production with pre-0.23 brokers. They will need documentation until they are ready to do the work to migrate to a later version in production.", "author": "jwulf", "createdAt": "2020-03-13T02:16:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NDYxMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NDc0OQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390854749", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            variables. The configuration template file also contains information an the environment\n          \n          \n            \n            variables. The configuration template file also contains information on the environment", "author": "npepinpe", "createdAt": "2020-03-11T09:52:59Z", "path": "docs/src/introduction/install.md", "diffHunk": "@@ -53,16 +53,14 @@ The Zeebe configuration is located at `/usr/local/zeebe/config/zeebe.cfg.yaml`.\n The logging configuration is located at `/usr/local/zeebe/config/log4j2.xml`.\n \n The configuration of the docker image can also be changed by using environment\n-variables.\n+variables. The configuration template file also contains information an the environment", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NTQyMg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390855422", "bodyText": "Maybe adding a link to their docs would be useful?", "author": "npepinpe", "createdAt": "2020-03-11T09:54:07Z", "path": "docs/src/operations/configuration.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Configuration\n+\n+Zeebe can be configured through configuration files, environment variables or a mix of both. If both configuration files and environment variables are used, then the latter take precedence over configuration files.\n+\n+The configuration will be applied during startup of Zeebe. It is not possible to change the configuration at runtime.\n+\n+## Configuration file templates\n+We provide templates that contain all possible configuration settings, along with explanations for each setting:\n+* [Broker Configuration Template](/appendix/broker-config-template.md)\n+* [Gateway Configuration Template](/appendix/gateway-config-template.md)\n+\n+Note that these templates also include the corresponding environment variables to use for every setting.\n+\n+## Editing the configuration\n+You can either start from scratch or start from the configuration templates listed above.\n+\n+If you use the configuration template and want to uncomment certain lines, make sure to also uncomment their parent elements:\n+\n+```yaml\n+Valid Configuration\n+\n+    zeebe:\n+      gateway:\n+        network:\n+          # host: 0.0.0.0\n+          port: 26500\n+\n+Invalid configuration\n+\n+    # zeebe:\n+      # gateway:\n+        # network:\n+          # host: 0.0.0.0\n+          port: 26500\n+```\n+\n+Uncommenting individual lines is a bit finicky, because YAML is sensitive to indentation. The best way to do it is to position the cursor before the `#` character and delete two characters (the dash and the space). Doing this consistently will give you a valid YAML file.\n+\n+When it comes to editing individual settings two data types are worth mentioning:\n+* Data Sizes (e.g. `logSegmentSize`)\n+  * Human friendly format: `500MB` (or `KB, GB`)\n+  * Machine friendly format: size in bytes as long\n+* Timeouts/Intervals (e.g. `requestTimeout`)\n+  * Human friendly format: `15s` (or `m, h`)\n+  * Machine friendly format: either duration in milliseconds as long, or [ISO-8601 Duration](ttps://en.wikipedia.org/wiki/ISO_8601#Durations) format (e.g. `PT15S`)\n+\n+## Passing configuration file to Zeebe\n+The configuration file can be specified through an environment variable, as a command line argument when launching Zeebe, or via conventions.\n+\n+*Environment Variable*\n+```shell script\n+export SPRING_CONFIG_LOCATION='file:./[path to config file]'\n+```\n+\n+*Command line argument*\n+```shell script\n+./bin/broker --spring.config.location=file:./[path to config file]\n+ \n+or \n+\n+./bin/gateway --spring.config.location=file:./[path to config file]\n+```\n+\n+*Conventions*\n+\n+Rename the configuration file to `application.yml` and place it in the following location:\n+```shell script\n+./config/application.yml\n+``` \n+\n+*Misc*\n+\n+Zeebe uses Spring Boot for its configuration parsing. So all other ways to configure a Spring Boot application should also work.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQxMjg0NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394412844", "bodyText": "done", "author": "pihme", "createdAt": "2020-03-18T15:00:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NTQyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NTg2NQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390855865", "bodyText": "Maybe adding a note/link for pre 0.23.x users to point to the deprecated configuration mechanism?", "author": "npepinpe", "createdAt": "2020-03-11T09:54:57Z", "path": "docs/src/operations/configuration.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Configuration\n+", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ1NjgxMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396456810", "bodyText": "I will bring this up with Sebastian or in the team meeting, and create a new issue for this, Here I would like to close it for now, so I can merge the canges", "author": "pihme", "createdAt": "2020-03-23T13:39:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1NTg2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1ODU1Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390858557", "bodyText": "I like this a lot - I wish we could simplify it to some extent, but it's a great first step since we previously provided nothing.\nOne thing missing is the pending snapshots - so we have to account for a slight deduplication while you're receiving snapshots. Let's say you have maxSnapshots 3, and you have 3 followers partitions - all of these with already 3 snapshots. Well technically you might have 4 snapshots for each partition if you include the one currently being replicated (so there's some time where really 4 are present). Additionally sometimes pending snapshots fail, so you have a partially replicated snapshot, but I guess this goes into reserveForPartialSystemFailure", "author": "npepinpe", "createdAt": "2020-03-11T09:59:19Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] ", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTk5NTg3NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r391995874", "bodyText": "I heard that we are fixing snapshots at 1? Simple, sane defaults are best.", "author": "jwulf", "createdAt": "2020-03-13T02:17:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1ODU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQyOTk5Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394429997", "bodyText": "please look at this section again. I already removed the maxSnapshot setting in anticipation of @korthout PR. Still I am unsure about the formulas.", "author": "pihme", "createdAt": "2020-03-18T15:22:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1ODU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ5NTE5Ng==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394495196", "bodyText": "looks good to me \ud83d\udc4d", "author": "npepinpe", "createdAt": "2020-03-18T16:51:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1ODU1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1OTczMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390859730", "bodyText": "We can also add a warning to monitor closely the exporter endpoints, as failure to export can also lead quickly to a saturated disk", "author": "npepinpe", "createdAt": "2020-03-11T10:01:11Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] \n+```\n+\n+Some observations on the scaling of the factors above:\n+- `eventLogSize`: This factor scales with the throughput of your system \n+- `singleSnapshotSize`: This factor scales with the number of in-flight workflows\n+- `reserveForPartialSystemFailure`: This factor is supposed to be a reserve to account for partial system failure (e.g. loss of quorum inside Zeebe cluster, or loss of connection to external system). See the remainder of this document for a further discussion on the effects of partial system failure on Zeebe cluster and disk space provisioning.\n+\n+Many of the factors influencing above formula can be fine-tuned in the [configuration](/appendix/broker-config-template.md). The relevant configuration settings are:\n+```\n+Config file\n+    zeebe:\n+      broker:\n+        data:\n+          logSegmentSize: 512MB\n+          snapshotPeriod: 15m\n+          maxSnapshots: 3\n+        cluster:\n+          partitionsCount: 1\n+          replicationFactor: 1\n+          clusterSize: 1\n+\n+Environment Variables\n+  ZEEBE_BROKER_DATA_LOGSEGMENTSIZE = 512MB\n+  ZEEBE_BROKER_DATA_SNAPSHOTPERIOD = 15m\n+  ZEEBE_BROKER_DATA_MAXSNAPSHOTS = 3\n+  ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT = 1\n+  ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR = 1\n+  ZEEBE_BROKER_CLUSTER_CLUSTERSIZE = 1\n+```\n+\n+Other factors can be observed in a productive system with representative throughput.\n+\n+If you want to know where to look, by default this data is stored in \n \n - `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n - `snapshot` - the data of a state at a certain point in time\n \n-If you want a recipe to explode your disk space usage, here are a few ways to do it:\n+> **Pitfalls** \n+> \n+> If you want to avoid exploding your disk space usage, here are a few pitfalls to avoid:\n+> - Do not create a high number of snapshots with a long period between them.\n+> - Do not configure an exporter which does not not advance its record position (such as the Debug Exporter)", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MDk1Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394460952", "bodyText": "done, please look at this section again", "author": "pihme", "createdAt": "2020-03-18T16:01:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDg1OTczMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDU0Ng==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r390950546", "bodyText": "If quorum is lost for too long, then the leader will also step down, resulting in no new events being queued, so it wouldn't grow forever. Not sure if this is useful for users to know, however \ud83e\udd14", "author": "npepinpe", "createdAt": "2020-03-11T12:57:06Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] \n+```\n+\n+Some observations on the scaling of the factors above:\n+- `eventLogSize`: This factor scales with the throughput of your system \n+- `singleSnapshotSize`: This factor scales with the number of in-flight workflows\n+- `reserveForPartialSystemFailure`: This factor is supposed to be a reserve to account for partial system failure (e.g. loss of quorum inside Zeebe cluster, or loss of connection to external system). See the remainder of this document for a further discussion on the effects of partial system failure on Zeebe cluster and disk space provisioning.\n+\n+Many of the factors influencing above formula can be fine-tuned in the [configuration](/appendix/broker-config-template.md). The relevant configuration settings are:\n+```\n+Config file\n+    zeebe:\n+      broker:\n+        data:\n+          logSegmentSize: 512MB\n+          snapshotPeriod: 15m\n+          maxSnapshots: 3\n+        cluster:\n+          partitionsCount: 1\n+          replicationFactor: 1\n+          clusterSize: 1\n+\n+Environment Variables\n+  ZEEBE_BROKER_DATA_LOGSEGMENTSIZE = 512MB\n+  ZEEBE_BROKER_DATA_SNAPSHOTPERIOD = 15m\n+  ZEEBE_BROKER_DATA_MAXSNAPSHOTS = 3\n+  ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT = 1\n+  ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR = 1\n+  ZEEBE_BROKER_CLUSTER_CLUSTERSIZE = 1\n+```\n+\n+Other factors can be observed in a productive system with representative throughput.\n+\n+If you want to know where to look, by default this data is stored in \n \n - `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n - `snapshot` - the data of a state at a certain point in time\n \n-If you want a recipe to explode your disk space usage, here are a few ways to do it:\n+> **Pitfalls** \n+> \n+> If you want to avoid exploding your disk space usage, here are a few pitfalls to avoid:\n+> - Do not create a high number of snapshots with a long period between them.\n+> - Do not configure an exporter which does not not advance its record position (such as the Debug Exporter)\n \n-- Create a high number of snapshots with a long period between them.\n-- Load an exporter, such as the Debug Exporter, that does not advance its record position.\n+### Event Log\n \n-## Event Log\n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the [configuration](/appendix/broker-config-template.md) with the setting `zeebe.broker.data.logSegmentSize`. \n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log will grow over time, unless and until individual event log segments are deleted.\n \n-An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n+An event log segment can be deleted once:\n+ - all the events it contains have been processed by exporters, \n+ - all the events it contains have been replicated to other brokers, \n+ - all the events it contains have been processed, and\n+ - the maximum number of snapshots has been reached. \n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n-- An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+The following conditions inhibit the automatic deletion of event log segments:\n+- A cluster loses its quorum. In this case events are queued but not processed. Once a quorum is reestablished, events will be replicated and eventually event log segments will be deleted.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2MzAwOQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394463009", "bodyText": "I don't know how to describe this well. Maybe this is also not so much a topic for ordinary resource planning, but more a topic for how to deal with cascading failures.", "author": "pihme", "createdAt": "2020-03-18T16:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDU0Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ5NzU5NQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394497595", "bodyText": "Let's leave it as is for now then", "author": "npepinpe", "createdAt": "2020-03-18T16:54:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDk1MDU0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTc5MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392045791", "bodyText": "Should this not be Zeebe Broker?", "author": "jwulf", "createdAt": "2020-03-13T06:09:56Z", "path": "docs/src/appendix/deprecated-features.md", "diffHunk": "@@ -0,0 +1,53 @@\n+# Deprecated Features\n+\n+This section lists deprecated features.\n+\n+## Deprecated in 0.23.0-alpha2\n+- TOML configuration - deprecated and removed in 0.23.0-alpha2\n+- Legacy environment variables - deprecated in 0.23.0-alpha2, to be removed in 0.25.0\n+\n+### TOML Configuration files\n+TOML configuration files are no longer supported. Existing configuration files need to be changed into YAML configuration files.\n+\n+The structure of the configuration files has not changed - with one exception: The exporters are now configured via a map instead of a list.\n+\n+Old configuration:\n+```toml\n+ [[exporters]]\n+- id = \"elasticsearch\"\n+  className = \"io.zeebe.exporter.ElasticsearchExporter\"\n+- id = \"debug-http\"\n+  className = \"io.zeebe.broker.exporter.debug.DebugHttpExporter\"\n+```\n+\n+New configuration:\n+```yaml\n+exporters:\n+  elasticsearch:\n+    className: io.zeebe.exporter.ElasticsearchExporter\n+  debughttp:\n+    className: io.zeebe.broker.exporter.debug.DebugHttpExporter\n+```\n+\n+In terms of specifying values, there were two minor changes:\n+- Memory sizes are now specified like this: `512MB` (old way: `512M`)\n+- Durations, e.g. timeouts, can now also be given in ISO-8601 Durations format. However you can still use the established way and specify a timeout of `30s`  \n+\n+### Legacy Environment Variables\n+With the changes to the configuration, legacy environment variables were also deprecated.\n+\n+They are still supported during the backwards compatibility window and will be internally mapped to the new environment variables. If legacy environment variables are detected, there will be a warning in the log:\n+\n+```\n+17:20:00.591 [] [main] WARN  io.zeebe.legacy - Found use of legacy system environment variable 'ZEEBE_HOST'. Please use 'ZEEBE_BROKER_NETWORK_HOST' instead.\n+17:20:00.602 [] [main] INFO  io.zeebe.legacy - The old environment variable is currently supported as part of our backwards compatibility goals.\n+17:20:00.602 [] [main] INFO  io.zeebe.legacy - However, please note that support for the old environment variable is scheduled to be removed for release 0.25.0.\n+__  ___  ___  __   ___  ___     __   __   __        ___  __", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2NTY4OQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394465689", "bodyText": "Don't know. I also see many cases where Zeebe is written in lowercase. Who would know such a thing?", "author": "pihme", "createdAt": "2020-03-18T16:08:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQwMjc0OQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396402749", "bodyText": "At the moment it says ZEEBEE with four e's", "author": "jwulf", "createdAt": "2020-03-23T12:07:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTc5MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQ1NDc1OA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396454758", "bodyText": "There is a dedicated issue for this (which sparked some discussion regarding font) #4111", "author": "pihme", "createdAt": "2020-03-23T13:36:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTc5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTkxOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392045918", "bodyText": "The configuration can be provided as a file or through environment variables.", "author": "jwulf", "createdAt": "2020-03-13T06:10:24Z", "path": "docs/src/appendix/environment-variables.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Environment Variables\n+\n+## Environment Variables for Configuration\n+The configuration can be provided as a file and/or through environment variables. Mixing both sources is also possible. In that case environment variables have precedence over the configuration settings in the configuration file. ", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0Nzk4NQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392047985", "bodyText": "Doesn't need and/or. And is qualified in the next line.", "author": "jwulf", "createdAt": "2020-03-13T06:18:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0NTkxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0OTIwMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392049201", "bodyText": "All available environment variables are documented", "author": "jwulf", "createdAt": "2020-03-13T06:23:19Z", "path": "docs/src/appendix/environment-variables.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Environment Variables\n+\n+## Environment Variables for Configuration\n+The configuration can be provided as a file and/or through environment variables. Mixing both sources is also possible. In that case environment variables have precedence over the configuration settings in the configuration file. \n+\n+The respective environment variables are documented in the configuration file templates:", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0OTM1Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392049352", "bodyText": "Sets the log level of the Zeebe Broker Logger", "author": "jwulf", "createdAt": "2020-03-13T06:23:59Z", "path": "docs/src/appendix/environment-variables.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Environment Variables\n+\n+## Environment Variables for Configuration\n+The configuration can be provided as a file and/or through environment variables. Mixing both sources is also possible. In that case environment variables have precedence over the configuration settings in the configuration file. \n+\n+The respective environment variables are documented in the configuration file templates:\n+* [Broker Configuration Template](broker-config-template.md)\n+* [Gateway Configuration Template](gateway-config-template.md)\n+\n+## Environment Variables for Operators\n+The following environment variables are intended for operators:\n+  - `ZEEBE_LOG_LEVEL`: Sets the log level of the Zeebe Logger (default: `info`).", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2NzA5OQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394467099", "bodyText": "Would like to keep this. Will work for broker + gateway, afaik", "author": "pihme", "createdAt": "2020-03-18T16:10:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0OTM1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0OTUyMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392049520", "bodyText": "Note: It is not recommended to use these settings in production.", "author": "jwulf", "createdAt": "2020-03-13T06:24:44Z", "path": "docs/src/appendix/environment-variables.md", "diffHunk": "@@ -0,0 +1,19 @@\n+# Environment Variables\n+\n+## Environment Variables for Configuration\n+The configuration can be provided as a file and/or through environment variables. Mixing both sources is also possible. In that case environment variables have precedence over the configuration settings in the configuration file. \n+\n+The respective environment variables are documented in the configuration file templates:\n+* [Broker Configuration Template](broker-config-template.md)\n+* [Gateway Configuration Template](gateway-config-template.md)\n+\n+## Environment Variables for Operators\n+The following environment variables are intended for operators:\n+  - `ZEEBE_LOG_LEVEL`: Sets the log level of the Zeebe Logger (default: `info`).\n+ \n+ ## Environment Variables for Developers \n+The following environment variables are intended for developers:\n+ - `SPRING_PROFILES_ACTIVE=dev`: If this is set, the broker will start in a temporary folder and all data will be cleaned up upon exit\n+ - `ZEEBE_DEBUG=true/false`: Activates a `DebugLogExporter` with default settings. The value of the environment variable toggles pretty printing\n+\n+ > Note: It is not recommended to use these environment variables in production.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA0OTYwMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392049601", "bodyText": "The following snippet", "author": "jwulf", "createdAt": "2020-03-13T06:25:07Z", "path": "docs/src/appendix/gateway-config-template.md", "diffHunk": "@@ -0,0 +1,10 @@\n+# Gateway Configuration Template\n+\n+The following snipped represents the default Zeebe gateway configuration, which is shipped with the distribution. It can be found inside the `config` folder (`config/gateway.cfg.yaml`) and can be used to adjust Zeebe to your needs.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDMzNw==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392054337", "bodyText": "look at the broker log", "author": "jwulf", "createdAt": "2020-03-13T06:44:31Z", "path": "docs/src/operations/configuration.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Configuration\n+\n+Zeebe can be configured through configuration files, environment variables or a mix of both. If both configuration files and environment variables are used, then the latter take precedence over configuration files.\n+\n+The configuration will be applied during startup of Zeebe. It is not possible to change the configuration at runtime.\n+\n+## Configuration file templates\n+We provide templates that contain all possible configuration settings, along with explanations for each setting:\n+* [Broker Configuration Template](/appendix/broker-config-template.md)\n+* [Gateway Configuration Template](/appendix/gateway-config-template.md)\n+\n+Note that these templates also include the corresponding environment variables to use for every setting.\n+\n+## Editing the configuration\n+You can either start from scratch or start from the configuration templates listed above.\n+\n+If you use the configuration template and want to uncomment certain lines, make sure to also uncomment their parent elements:\n+\n+```yaml\n+Valid Configuration\n+\n+    zeebe:\n+      gateway:\n+        network:\n+          # host: 0.0.0.0\n+          port: 26500\n+\n+Invalid configuration\n+\n+    # zeebe:\n+      # gateway:\n+        # network:\n+          # host: 0.0.0.0\n+          port: 26500\n+```\n+\n+Uncommenting individual lines is a bit finicky, because YAML is sensitive to indentation. The best way to do it is to position the cursor before the `#` character and delete two characters (the dash and the space). Doing this consistently will give you a valid YAML file.\n+\n+When it comes to editing individual settings two data types are worth mentioning:\n+* Data Sizes (e.g. `logSegmentSize`)\n+  * Human friendly format: `500MB` (or `KB, GB`)\n+  * Machine friendly format: size in bytes as long\n+* Timeouts/Intervals (e.g. `requestTimeout`)\n+  * Human friendly format: `15s` (or `m, h`)\n+  * Machine friendly format: either duration in milliseconds as long, or [ISO-8601 Duration](ttps://en.wikipedia.org/wiki/ISO_8601#Durations) format (e.g. `PT15S`)\n+\n+## Passing configuration file to Zeebe\n+The configuration file can be specified through an environment variable, as a command line argument when launching Zeebe, or via conventions.\n+\n+*Environment Variable*\n+```shell script\n+export SPRING_CONFIG_LOCATION='file:./[path to config file]'\n+```\n+\n+*Command line argument*\n+```shell script\n+./bin/broker --spring.config.location=file:./[path to config file]\n+ \n+or \n+\n+./bin/gateway --spring.config.location=file:./[path to config file]\n+```\n+\n+*Conventions*\n+\n+Rename the configuration file to `application.yml` and place it in the following location:\n+```shell script\n+./config/application.yml\n+``` \n+\n+*Misc*\n+\n+Zeebe uses Spring Boot for its configuration parsing. So all other ways to configure a Spring Boot application should also work.\n+\n+## Verifying that configuration was applied\n+To verify that the configuration was applied, start the application and look at the applications log.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2ODU3Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394468577", "bodyText": "Again, would like to keep it this way. It pertains to Broker and Gateway alike.", "author": "pihme", "createdAt": "2020-03-18T16:12:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDMzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQwMzgxMQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396403811", "bodyText": "In that case, maybe:\n\"To verify that the configuration was applied, start Zeebe and look at the log\"", "author": "jwulf", "createdAt": "2020-03-23T12:09:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDMzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDQ4NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392054484", "bodyText": "the broker will log the effective configuration", "author": "jwulf", "createdAt": "2020-03-13T06:45:08Z", "path": "docs/src/operations/configuration.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Configuration\n+\n+Zeebe can be configured through configuration files, environment variables or a mix of both. If both configuration files and environment variables are used, then the latter take precedence over configuration files.\n+\n+The configuration will be applied during startup of Zeebe. It is not possible to change the configuration at runtime.\n+\n+## Configuration file templates\n+We provide templates that contain all possible configuration settings, along with explanations for each setting:\n+* [Broker Configuration Template](/appendix/broker-config-template.md)\n+* [Gateway Configuration Template](/appendix/gateway-config-template.md)\n+\n+Note that these templates also include the corresponding environment variables to use for every setting.\n+\n+## Editing the configuration\n+You can either start from scratch or start from the configuration templates listed above.\n+\n+If you use the configuration template and want to uncomment certain lines, make sure to also uncomment their parent elements:\n+\n+```yaml\n+Valid Configuration\n+\n+    zeebe:\n+      gateway:\n+        network:\n+          # host: 0.0.0.0\n+          port: 26500\n+\n+Invalid configuration\n+\n+    # zeebe:\n+      # gateway:\n+        # network:\n+          # host: 0.0.0.0\n+          port: 26500\n+```\n+\n+Uncommenting individual lines is a bit finicky, because YAML is sensitive to indentation. The best way to do it is to position the cursor before the `#` character and delete two characters (the dash and the space). Doing this consistently will give you a valid YAML file.\n+\n+When it comes to editing individual settings two data types are worth mentioning:\n+* Data Sizes (e.g. `logSegmentSize`)\n+  * Human friendly format: `500MB` (or `KB, GB`)\n+  * Machine friendly format: size in bytes as long\n+* Timeouts/Intervals (e.g. `requestTimeout`)\n+  * Human friendly format: `15s` (or `m, h`)\n+  * Machine friendly format: either duration in milliseconds as long, or [ISO-8601 Duration](ttps://en.wikipedia.org/wiki/ISO_8601#Durations) format (e.g. `PT15S`)\n+\n+## Passing configuration file to Zeebe\n+The configuration file can be specified through an environment variable, as a command line argument when launching Zeebe, or via conventions.\n+\n+*Environment Variable*\n+```shell script\n+export SPRING_CONFIG_LOCATION='file:./[path to config file]'\n+```\n+\n+*Command line argument*\n+```shell script\n+./bin/broker --spring.config.location=file:./[path to config file]\n+ \n+or \n+\n+./bin/gateway --spring.config.location=file:./[path to config file]\n+```\n+\n+*Conventions*\n+\n+Rename the configuration file to `application.yml` and place it in the following location:\n+```shell script\n+./config/application.yml\n+``` \n+\n+*Misc*\n+\n+Zeebe uses Spring Boot for its configuration parsing. So all other ways to configure a Spring Boot application should also work.\n+\n+## Verifying that configuration was applied\n+To verify that the configuration was applied, start the application and look at the applications log.\n+\n+If the configuration could be read, the application will log out the effective configuration during startup:", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2ODgzOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394468838", "bodyText": "same reason", "author": "pihme", "createdAt": "2020-03-18T16:12:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDQ4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQwNDI1NA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396404254", "bodyText": "In that case, maybe: \"Zeebe will log out...\"\nThe reasoning being, that \"application\" is not a term that we have used to refer to Zeebe previously. Usually we say \"the broker\".", "author": "jwulf", "createdAt": "2020-03-23T12:10:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDQ4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDY1MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392054651", "bodyText": "In some cases of invalid configuration the broker will fail to start", "author": "jwulf", "createdAt": "2020-03-13T06:45:44Z", "path": "docs/src/operations/configuration.md", "diffHunk": "@@ -0,0 +1,116 @@\n+# Configuration\n+\n+Zeebe can be configured through configuration files, environment variables or a mix of both. If both configuration files and environment variables are used, then the latter take precedence over configuration files.\n+\n+The configuration will be applied during startup of Zeebe. It is not possible to change the configuration at runtime.\n+\n+## Configuration file templates\n+We provide templates that contain all possible configuration settings, along with explanations for each setting:\n+* [Broker Configuration Template](/appendix/broker-config-template.md)\n+* [Gateway Configuration Template](/appendix/gateway-config-template.md)\n+\n+Note that these templates also include the corresponding environment variables to use for every setting.\n+\n+## Editing the configuration\n+You can either start from scratch or start from the configuration templates listed above.\n+\n+If you use the configuration template and want to uncomment certain lines, make sure to also uncomment their parent elements:\n+\n+```yaml\n+Valid Configuration\n+\n+    zeebe:\n+      gateway:\n+        network:\n+          # host: 0.0.0.0\n+          port: 26500\n+\n+Invalid configuration\n+\n+    # zeebe:\n+      # gateway:\n+        # network:\n+          # host: 0.0.0.0\n+          port: 26500\n+```\n+\n+Uncommenting individual lines is a bit finicky, because YAML is sensitive to indentation. The best way to do it is to position the cursor before the `#` character and delete two characters (the dash and the space). Doing this consistently will give you a valid YAML file.\n+\n+When it comes to editing individual settings two data types are worth mentioning:\n+* Data Sizes (e.g. `logSegmentSize`)\n+  * Human friendly format: `500MB` (or `KB, GB`)\n+  * Machine friendly format: size in bytes as long\n+* Timeouts/Intervals (e.g. `requestTimeout`)\n+  * Human friendly format: `15s` (or `m, h`)\n+  * Machine friendly format: either duration in milliseconds as long, or [ISO-8601 Duration](ttps://en.wikipedia.org/wiki/ISO_8601#Durations) format (e.g. `PT15S`)\n+\n+## Passing configuration file to Zeebe\n+The configuration file can be specified through an environment variable, as a command line argument when launching Zeebe, or via conventions.\n+\n+*Environment Variable*\n+```shell script\n+export SPRING_CONFIG_LOCATION='file:./[path to config file]'\n+```\n+\n+*Command line argument*\n+```shell script\n+./bin/broker --spring.config.location=file:./[path to config file]\n+ \n+or \n+\n+./bin/gateway --spring.config.location=file:./[path to config file]\n+```\n+\n+*Conventions*\n+\n+Rename the configuration file to `application.yml` and place it in the following location:\n+```shell script\n+./config/application.yml\n+``` \n+\n+*Misc*\n+\n+Zeebe uses Spring Boot for its configuration parsing. So all other ways to configure a Spring Boot application should also work.\n+\n+## Verifying that configuration was applied\n+To verify that the configuration was applied, start the application and look at the applications log.\n+\n+If the configuration could be read, the application will log out the effective configuration during startup:\n+\n+```\n+17:13:13.120 [] [main] INFO  io.zeebe.broker.system - Starting broker 0 with configuration {\n+  \"network\": {\n+    \"host\": \"0.0.0.0\",\n+    \"portOffset\": 0,\n+    \"maxMessageSize\": {\n+      \"bytes\": 4194304\n+    },\n+    \"commandApi\": {\n+      \"defaultPort\": 26501,\n+      \"host\": \"0.0.0.0\",\n+      \"port\": 26501,\n+...\n+```\n+\n+In some cases of invalid configuration the application will fail to start with a warning that explains which configuration setting could not be read.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ2OTAyMg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394469022", "bodyText": "same reason", "author": "pihme", "createdAt": "2020-03-18T16:13:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDY1MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjQwNDQyNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396404426", "bodyText": "In some cases of invalid configuration Zeebe will fail to start", "author": "jwulf", "createdAt": "2020-03-23T12:10:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NDY1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTEzNg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392055136", "bodyText": "The broker needs to receive", "author": "jwulf", "createdAt": "2020-03-13T06:47:38Z", "path": "docs/src/operations/network-ports.md", "diffHunk": "@@ -1,12 +1,51 @@\n # Network Ports\n \n-The broker cluster sits behind the gRPC Gateway, which handles all requests from workers. \n-The gateway needs to receive communication from clients on port 26500 and the brokers on port 26502. The brokers need to receive communication from the gateway on port 26501 and from other brokers on port 26502.\n \n-**9600** - Metrics and Readiness Probe. Prometheus metrics are exported on the route /metrics. There is a readiness probe on /ready.\n+The broker cluster sits behind the gRPC Gateway, which handles all requests from clients/workers and forwards events to brokers.\n \n-**26500** - gRPC Gateway. This is the Client API port. This should be exposed to clients external to the cluster.\n+## Gateway \n+The gateway needs to receive communication on \n+- `zeebe.gateway.network.port: 26500` from clients/workers, and \n+- `zeebe.gateway.cluster.contactPoint: 127.0.0.1:26502` from brokers \n \n-**26501** - Gateway-to-broker communication, using an internal SBE (Simple Binary Encoding) protocol. This is the Command API port. This should be exposed to the gateway.\n+The relevant [configuration](/appendix/gateway-config-template.md) settings are:\n+```\n+Config file\n+    zeebe:\n+      gateway:\n+        network:\n+          port: 26500\n+        cluster:\n+          contactPoint: 127.0.0.1:26502\n+        \n \n-**26502** - Inter-broker clustering using the Gossip and Raft protocols for partition replication, broker elections, topology sharing, and message subscriptions. This should be exposed to other brokers and the gateway.\n\\ No newline at end of file\n+Environment Variables\n+  ZEEBE_GATEWAY_CLUSTER_NETWORK_PORT = 26500\n+  ZEEBE_GATEWAY_CLUSTER_CONTACTPOINT = 127.0.0.1:26502  \n+```\n+\n+## Broker\n+The brokers need to receive communication from the gateway and from other brokers. It also exposes a port for monitoring.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NTQ0Nw==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392055447", "bodyText": "the following \"back of the envelope\" formula can be used", "author": "jwulf", "createdAt": "2020-03-13T06:48:52Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: ", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjE4OA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392056188", "bodyText": "can be observed in a production-like system", "author": "jwulf", "createdAt": "2020-03-13T06:51:40Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] \n+```\n+\n+Some observations on the scaling of the factors above:\n+- `eventLogSize`: This factor scales with the throughput of your system \n+- `singleSnapshotSize`: This factor scales with the number of in-flight workflows\n+- `reserveForPartialSystemFailure`: This factor is supposed to be a reserve to account for partial system failure (e.g. loss of quorum inside Zeebe cluster, or loss of connection to external system). See the remainder of this document for a further discussion on the effects of partial system failure on Zeebe cluster and disk space provisioning.\n+\n+Many of the factors influencing above formula can be fine-tuned in the [configuration](/appendix/broker-config-template.md). The relevant configuration settings are:\n+```\n+Config file\n+    zeebe:\n+      broker:\n+        data:\n+          logSegmentSize: 512MB\n+          snapshotPeriod: 15m\n+          maxSnapshots: 3\n+        cluster:\n+          partitionsCount: 1\n+          replicationFactor: 1\n+          clusterSize: 1\n+\n+Environment Variables\n+  ZEEBE_BROKER_DATA_LOGSEGMENTSIZE = 512MB\n+  ZEEBE_BROKER_DATA_SNAPSHOTPERIOD = 15m\n+  ZEEBE_BROKER_DATA_MAXSNAPSHOTS = 3\n+  ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT = 1\n+  ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR = 1\n+  ZEEBE_BROKER_CLUSTER_CLUSTERSIZE = 1\n+```\n+\n+Other factors can be observed in a productive system with representative throughput.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjU2Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392056562", "bodyText": "If we tell them this, users will immediately ask: \"why would I change this, and what to\"", "author": "jwulf", "createdAt": "2020-03-13T06:53:02Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] \n+```\n+\n+Some observations on the scaling of the factors above:\n+- `eventLogSize`: This factor scales with the throughput of your system \n+- `singleSnapshotSize`: This factor scales with the number of in-flight workflows\n+- `reserveForPartialSystemFailure`: This factor is supposed to be a reserve to account for partial system failure (e.g. loss of quorum inside Zeebe cluster, or loss of connection to external system). See the remainder of this document for a further discussion on the effects of partial system failure on Zeebe cluster and disk space provisioning.\n+\n+Many of the factors influencing above formula can be fine-tuned in the [configuration](/appendix/broker-config-template.md). The relevant configuration settings are:\n+```\n+Config file\n+    zeebe:\n+      broker:\n+        data:\n+          logSegmentSize: 512MB\n+          snapshotPeriod: 15m\n+          maxSnapshots: 3\n+        cluster:\n+          partitionsCount: 1\n+          replicationFactor: 1\n+          clusterSize: 1\n+\n+Environment Variables\n+  ZEEBE_BROKER_DATA_LOGSEGMENTSIZE = 512MB\n+  ZEEBE_BROKER_DATA_SNAPSHOTPERIOD = 15m\n+  ZEEBE_BROKER_DATA_MAXSNAPSHOTS = 3\n+  ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT = 1\n+  ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR = 1\n+  ZEEBE_BROKER_CLUSTER_CLUSTERSIZE = 1\n+```\n+\n+Other factors can be observed in a productive system with representative throughput.\n+\n+If you want to know where to look, by default this data is stored in \n \n - `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n - `snapshot` - the data of a state at a certain point in time\n \n-If you want a recipe to explode your disk space usage, here are a few ways to do it:\n+> **Pitfalls** \n+> \n+> If you want to avoid exploding your disk space usage, here are a few pitfalls to avoid:\n+> - Do not create a high number of snapshots with a long period between them.\n+> - Do not configure an exporter which does not not advance its record position (such as the Debug Exporter)\n \n-- Create a high number of snapshots with a long period between them.\n-- Load an exporter, such as the Debug Exporter, that does not advance its record position.\n+### Event Log\n \n-## Event Log\n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the [configuration](/appendix/broker-config-template.md) with the setting `zeebe.broker.data.logSegmentSize`. ", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjAzNQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394472035", "bodyText": "Removed the second sentence. Please look at it again whether it is better now", "author": "pihme", "createdAt": "2020-03-18T16:17:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA1NjU2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA3NTc4MQ==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392075781", "bodyText": "In our experience this grows", "author": "jwulf", "createdAt": "2020-03-13T07:55:47Z", "path": "docs/src/operations/resource-planning.md", "diffHunk": "@@ -8,64 +8,124 @@ While we cannot tell you exactly what you need - beyond _it depends_ - we can ex\n \n All Brokers in a partition use disk space to store:\n \n-- The event log for each partition they participate in. By default, this is a minimum of 512MB for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n-- One or more periodic snapshots of the running state (inflight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to three snapshots retained, and a snapshot [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/cca5aeda4bd9d7e7e83f68d221bbf1a3e4d0f000/dist/src/main/config/zeebe.cfg.toml#L145).\n+- The event log for each partition they participate in. By default, this is a minimum of _512MB_ for each partition, incrementing in 512MB segments. The event log is truncated on a given broker when data has been processed and successfully exported by all loaded exporters.\n+- One or more periodic snapshots of the running state (in-flight data) of each partition (unbounded, based on in-flight work). The number of snapshots to retain, and their frequency is configurable. By default it is set to _three_ snapshots retained, and a snapshot every _15 minutes_.\n \n Additionally, the leader of a partition also uses disk space to store:\n - A projection of the running state of the partition in RocksDB. (unbounded, based on in-flight work)\n \n-By default this data is stored in \n+To calculate the required amount of disk space, the following back of the envelope formula can be used as a starting point: \n+\n+```\n+neededDiskSpace = replicatedState + localState\n+\n+replicatedState = totalEventLogSize + totalSnapshotSize\n+totalEventLogSize = followerPartitionsPerNode * eventLogSize * reserveForPartialSystemFailure \n+totalSnapshotSize = followerPartitionsPerNode * maxSnapshots * singleSnapshotSize\n+\n+localState = leaderPartitionsPerNode * singleSnapshotSize\n+\n+leaderPartitionsPerNode = partitionsCount / numberOfNodes \n+followerPartitionsPerNode = partitionsCount * replicationFactor / numberOfNodes \n+\n+clusterSize = [number of broker nodes]\n+partitionsCount = [number of partitions]\n+replicationFactor = [number of replicas per partition]\n+reserveForPartialSystemFailure = [factor to account for partial system failure]  \n+singleSnapshotSize = [size of a single rocks DB snapshot]                  \n+eventLogSize = [event log size for duration of maxSnapshots * snapshotPeriod] \n+```\n+\n+Some observations on the scaling of the factors above:\n+- `eventLogSize`: This factor scales with the throughput of your system \n+- `singleSnapshotSize`: This factor scales with the number of in-flight workflows\n+- `reserveForPartialSystemFailure`: This factor is supposed to be a reserve to account for partial system failure (e.g. loss of quorum inside Zeebe cluster, or loss of connection to external system). See the remainder of this document for a further discussion on the effects of partial system failure on Zeebe cluster and disk space provisioning.\n+\n+Many of the factors influencing above formula can be fine-tuned in the [configuration](/appendix/broker-config-template.md). The relevant configuration settings are:\n+```\n+Config file\n+    zeebe:\n+      broker:\n+        data:\n+          logSegmentSize: 512MB\n+          snapshotPeriod: 15m\n+          maxSnapshots: 3\n+        cluster:\n+          partitionsCount: 1\n+          replicationFactor: 1\n+          clusterSize: 1\n+\n+Environment Variables\n+  ZEEBE_BROKER_DATA_LOGSEGMENTSIZE = 512MB\n+  ZEEBE_BROKER_DATA_SNAPSHOTPERIOD = 15m\n+  ZEEBE_BROKER_DATA_MAXSNAPSHOTS = 3\n+  ZEEBE_BROKER_CLUSTER_PARTITIONSCOUNT = 1\n+  ZEEBE_BROKER_CLUSTER_REPLICATIONFACTOR = 1\n+  ZEEBE_BROKER_CLUSTER_CLUSTERSIZE = 1\n+```\n+\n+Other factors can be observed in a productive system with representative throughput.\n+\n+If you want to know where to look, by default this data is stored in \n \n - `segments` - the data of the log split into segments. The log is only appended - until it is truncated by reaching the max snapshot count.\n - `state` - the active state. Deployed workflows, active workflow instances, etc. Completed workflow instances or jobs are removed.\n - `snapshot` - the data of a state at a certain point in time\n \n-If you want a recipe to explode your disk space usage, here are a few ways to do it:\n+> **Pitfalls** \n+> \n+> If you want to avoid exploding your disk space usage, here are a few pitfalls to avoid:\n+> - Do not create a high number of snapshots with a long period between them.\n+> - Do not configure an exporter which does not not advance its record position (such as the Debug Exporter)\n \n-- Create a high number of snapshots with a long period between them.\n-- Load an exporter, such as the Debug Exporter, that does not advance its record position.\n+### Event Log\n \n-## Event Log\n+The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the [configuration](/appendix/broker-config-template.md) with the setting `zeebe.broker.data.logSegmentSize`. \n \n-The event log for each partition is segmented. By default, the segment size is 512MB. This can be changed in the zeebe.cfg.toml file with the setting [logSegmentSize](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L148). \n+The event log will grow over time, unless and until individual event log segments are deleted.\n \n-An event log segment can be deleted once all the events it contains have been processed by exporters, replicated to other brokers, and processed. Three things can cause the event log to not be truncated:\n+An event log segment can be deleted once:\n+ - all the events it contains have been processed by exporters, \n+ - all the events it contains have been replicated to other brokers, \n+ - all the events it contains have been processed, and\n+ - the maximum number of snapshots has been reached. \n \n-- A cluster loses its quorum, in which case events are queued but not processed. \n-- An exporter does not advance its read position in the event log.\n-- The max number of snapshots has not been written.\n+The following conditions inhibit the automatic deletion of event log segments:\n+- A cluster loses its quorum. In this case events are queued but not processed. Once a quorum is reestablished, events will be replicated and eventually event log segments will be deleted.\n+- The max number of snapshots has not been written. Log segment deletion will begin as soon as the max number of snapshots has been reached \n+- An exporter does not advance its read position in the event log. In this case the event log will grow ad infinitum.\n \n An event log segment is not deleted until all the events in it have been exported by all configured exporters. This means that exporters that rely on side-effects, perform intensive computation, or experience back pressure from external storage will cause disk usage to grow, as they delay the deletion of event log segments. \n \n Exporting is only performed on the partition leader, but the followers of the partition do not delete segments in their replica of the partition until the leader marks all events in it as unneeded by exporters.\n \n No event log segments are deleted until the maximum number of snapshots has been reached. When the maximum number of snapshots has been reached, the event log is truncated up to the oldest snapshot.\n \n-## Snapshots\n+### Snapshots\n \n-The running state of the partition is captured periodically on the leader in a snapshot. By default, this period is [every 15 minutes](https://github.com/zeebe-io/zeebe/blob/0.20.0/dist/src/main/config/zeebe.cfg.toml#L151). This can be changed in the zeebe.cfg.toml file. The number of valid snapshots to retain is configured in zeebe.cfg.toml. By default, the leader and followers retain only the latest valid snapshot.\n+The running state of the partition is captured periodically on the leader in a snapshot. By default, this period is every 15 minutes. This can be changed in the [configuration](/appendix/broker-config-template.md). By default, the leader and followers retain only the latest three snapshots. This can also be changed in the configuration.\n \n A snapshot is a projection of all events that represent the current running state of the workflows running on the partition.  It contains all active data, for example, deployed workflows, active workflow instances, and not yet completed jobs.\n \n-When the broker has as many snapshots as configured by the parameter maxSnapshots, it deletes all data on the log which was written before the oldest snapshot.\n+When the broker has as many snapshots as configured by the parameter `zeebe.broker.data.maxSnapshots`, it deletes all data on the log which was written before the oldest snapshot.\n \n-## RocksDB\n+### RocksDB\n \n On the lead broker of a partition, the current running state is kept in memory, and on disk in RocksDB. In our experiments this grows to 2GB under a heavy load of long-running processes. The snapshots that are replicated to followers are snapshots of RocksDB.", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NDQ3MjM4Mg==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r394472382", "bodyText": "yeah, that was what I meant to type :-)", "author": "pihme", "createdAt": "2020-03-18T16:17:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA3NTc4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MjA3NjQyOA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r392076428", "bodyText": "By default, the official Zeebe clients (Java and Go) send keep alive pings", "author": "jwulf", "createdAt": "2020-03-13T07:57:32Z", "path": "docs/src/operations/setting-up-a-cluster.md", "diffHunk": "@@ -194,8 +194,8 @@ each node knows exactly, which partitions it has\n to bootstrap and for which it will become the leader at first (this\n could change later, if the node needs to step down for example).\n \n-## Keep Alive\n+## Keep Alive Intervals\n \n It's possible to specify how often Zeebe clients should send keep alive pings. By default, the official Zeebe clients send keep alive pings every 45 seconds. This interval can be configured through the clients' APIs and through the `ZEEBE_KEEP_ALIVE` environment variable. When configuring the clients with the environment variable, the time interval must be expressed a positive amount of milliseconds (e.g., 45000).", "originalCommit": "83f0c7247dd68344f5ba666c4c214784e2f21744", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "062b70c55d5df951a43d747aa2d4be64938f95da", "url": "https://github.com/camunda-cloud/zeebe/commit/062b70c55d5df951a43d747aa2d4be64938f95da", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-18T16:22:33Z", "type": "forcePushed"}, {"oid": "ede1ca85dd247e67e026a5ede5fce5b0d8a20d7d", "url": "https://github.com/camunda-cloud/zeebe/commit/ede1ca85dd247e67e026a5ede5fce5b0d8a20d7d", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-23T13:40:21Z", "type": "forcePushed"}, {"oid": "6e7f64cad50b38d1d6a6ada8419772a106d27cd4", "url": "https://github.com/camunda-cloud/zeebe/commit/6e7f64cad50b38d1d6a6ada8419772a106d27cd4", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-23T13:46:51Z", "type": "forcePushed"}, {"oid": "ea17a17918f1f90cceb20455b4587409ebe7cd47", "url": "https://github.com/camunda-cloud/zeebe/commit/ea17a17918f1f90cceb20455b4587409ebe7cd47", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-23T13:48:46Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjU4MzAyMA==", "url": "https://github.com/camunda-cloud/zeebe/pull/4025#discussion_r396583020", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - Legacy environment variables - deprecated in 0.23.0-alpha2, to be removed in 0.25.0\n          \n          \n            \n            - Legacy environment variables - deprecated in 0.23.0-alpha2, to be removed in 0.24.0\n          \n      \n    \n    \n  \n\nAs far as I know, we support two minor versions, so when we are on 0.24.x, we will be supporting 0.23.x and 0.24.x - so at that point we can already remove legacy configuration support. Correct me if I'm wrong @menski", "author": "npepinpe", "createdAt": "2020-03-23T16:26:45Z", "path": "docs/src/appendix/deprecated-features.md", "diffHunk": "@@ -0,0 +1,49 @@\n+# Deprecated Features\n+\n+This section lists deprecated features.\n+\n+## Deprecated in 0.23.0-alpha2\n+- TOML configuration - deprecated and removed in 0.23.0-alpha2\n+- Legacy environment variables - deprecated in 0.23.0-alpha2, to be removed in 0.25.0", "originalCommit": "ea17a17918f1f90cceb20455b4587409ebe7cd47", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "df2e4b0bfd99308521f78ea1823b84ffa6e6bf7b", "url": "https://github.com/camunda-cloud/zeebe/commit/df2e4b0bfd99308521f78ea1823b84ffa6e6bf7b", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-25T08:30:05Z", "type": "commit"}, {"oid": "df2e4b0bfd99308521f78ea1823b84ffa6e6bf7b", "url": "https://github.com/camunda-cloud/zeebe/commit/df2e4b0bfd99308521f78ea1823b84ffa6e6bf7b", "message": "docs(appendix): Update docs with new info on configuration", "committedDate": "2020-03-25T08:30:05Z", "type": "forcePushed"}]}