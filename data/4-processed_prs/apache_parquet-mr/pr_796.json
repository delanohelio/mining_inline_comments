{"pr_number": 796, "pr_title": "Parquet-1872: Add TransCompression command to parquet-tools", "pr_createdAt": "2020-06-12T03:54:59Z", "pr_url": "https://github.com/apache/parquet-mr/pull/796", "timeline": [{"oid": "228431e6b67c7553367b4a1e8f30befb319a4024", "url": "https://github.com/apache/parquet-mr/commit/228431e6b67c7553367b4a1e8f30befb319a4024", "message": "Parquet-1872: Add TransCompression command to parquet-tools", "committedDate": "2020-06-16T18:44:36Z", "type": "forcePushed"}, {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "url": "https://github.com/apache/parquet-mr/commit/d0cf386e7b51b1a2488df7c07edab509e288cd8e", "message": "Parquet-1872: Add TransCompression command to parquet-tools", "committedDate": "2020-06-16T22:37:41Z", "type": "commit"}, {"oid": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "url": "https://github.com/apache/parquet-mr/commit/d0cf386e7b51b1a2488df7c07edab509e288cd8e", "message": "Parquet-1872: Add TransCompression command to parquet-tools", "committedDate": "2020-06-16T22:37:41Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MTczMQ==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442041731", "bodyText": "You may add the writer to the resource opening code part just next to the reader (by separating them with ';').", "author": "gszadovszky", "createdAt": "2020-06-18T08:02:53Z", "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM3MjAxMQ==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442372011", "bodyText": "The writer doesn't extend the closable(). If we add, it will report build error.", "author": "shangxinli", "createdAt": "2020-06-18T16:59:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjczNzc1Ng==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442737756", "bodyText": "Hm, you're right. writer.end closes the file descriptor behind. So, I would suggest invoking end in the finally block so the file descriptor will be closed even if an exception occurs.", "author": "gszadovszky", "createdAt": "2020-06-19T09:38:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MTczMQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg4NjEzNA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442886134", "bodyText": "Sounds good", "author": "shangxinli", "createdAt": "2020-06-19T14:52:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjA0MTczMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442203303", "bodyText": "We stopped writing page statistics in purpose. It is never used in any of the implementations I aware of and using it would require to read every page headers which does not perform well. That's why we introduced column indexes. So, I would suggest writing page level statistics only if the original file has them.", "author": "gszadovszky", "createdAt": "2020-06-18T12:54:14Z", "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM3NDYyNg==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442374626", "bodyText": "True. The method convertStatistics() will read the statistic ether from page or ColumnIndex. The the returned value of convertStatistics() is passed to writeDataPage(). Then writeDataPage() will write to ColumnIndex only I think, since your change removed the page header's statistics.", "author": "shangxinli", "createdAt": "2020-06-18T17:03:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1ODQzNA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442758434", "bodyText": "You are completely right, I've forgot about my own implementation :)\nThen, just one comment for this code part. I think, column index is more reliable so I would use it for generating statistics at the first place.", "author": "gszadovszky", "createdAt": "2020-06-19T10:20:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg4OTA0Mg==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442889042", "bodyText": "Since your change removed page statistics, when ColumnIndex is not null, then page statistics must be null, and vice versa(for older version). So the order to check which one shouldn't matter to the result. Correct?", "author": "shangxinli", "createdAt": "2020-06-19T14:57:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQ0Njg2Mg==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443446862", "bodyText": "This is true for parquet-mr. I am not sure about other implementations, which one writes page statistics and/or column indexes. During the column index design we were talking about some edge cases of some data types (e.g. NaN for floating point numbers) and turned out that page statistics are not always handled the same way in the different implementations. At least for parquet-mr and Impala we synchronized these for column indexes.", "author": "gszadovszky", "createdAt": "2020-06-22T09:56:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3NDQwMQ==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443874401", "bodyText": "Sounds good!", "author": "shangxinli", "createdAt": "2020-06-22T23:13:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIwMzMwMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxMjgwNA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442212804", "bodyText": "I think, Dummy would be a better naming instead of Dump.", "author": "gszadovszky", "createdAt": "2020-06-18T13:09:39Z", "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private byte[] translatePageLoad(TransParquetFileReader reader, boolean isCompressed, BytesInputCompressor compressor,\n+                                   BytesInputDecompressor decompressor, int payloadLength, int rawDataLength) throws IOException {\n+    BytesInput data = readBlock(payloadLength, reader);\n+    if (isCompressed) {\n+      data = decompressor.decompress(data, rawDataLength);\n+    }\n+    BytesInput newCompressedData = compressor.compress(data);\n+    return newCompressedData.toByteArray();\n+  }\n+\n+  private BytesInput readBlock(int length, TransParquetFileReader reader) throws IOException {\n+    byte[] data = new byte[length];\n+    reader.blockRead(data);\n+    return BytesInput.from(data);\n+  }\n+\n+  private int toIntWithCheck(long size) {\n+    if ((int)size != size) {\n+      throw new ParquetEncodingException(\"size is bigger than \" + Integer.MAX_VALUE + \" bytes: \" + size);\n+    }\n+    return (int)size;\n+  }\n+\n+  private static final class DumpGroupConverter extends GroupConverter {\n+    @Override public void start() {}\n+    @Override public void end() {}\n+    @Override public Converter getConverter(int fieldIndex) { return new DumpConverter(); }\n+  }\n+\n+  private static final class DumpConverter extends PrimitiveConverter {\n+    @Override public GroupConverter asGroupConverter() { return new DumpGroupConverter(); }\n+  }", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjM3OTgyMw==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442379823", "bodyText": "Got it.", "author": "shangxinli", "createdAt": "2020-06-18T17:13:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxMjgwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxNTMzNg==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442215336", "bodyText": "Creating a byte array can be expensive if there are many pages. I would suggest maintaining a cache array that is reused and created only if a block is larger than the cache.", "author": "gszadovszky", "createdAt": "2020-06-18T13:13:35Z", "path": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/TransCompressionCommand.java", "diffHunk": "@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.parquet.cli.commands;\n+\n+import com.beust.jcommander.Parameter;\n+import com.beust.jcommander.Parameters;\n+import com.google.common.base.Preconditions;\n+import com.google.common.collect.Lists;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.cli.BaseCommand;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.impl.ColumnReadStoreImpl;\n+import org.apache.parquet.column.page.DictionaryPage;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.column.statistics.Statistics;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputCompressor;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.format.Util;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.api.Converter;\n+import org.apache.parquet.io.api.GroupConverter;\n+import org.apache.parquet.io.api.PrimitiveConverter;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.slf4j.Logger;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+@Parameters(commandDescription=\"Translate the compression from one to another\")\n+public class TransCompressionCommand extends BaseCommand {\n+\n+  public TransCompressionCommand(Logger console) {\n+    super(console);\n+  }\n+\n+  @Parameter(description = \"<input parquet file path>\")\n+  String input;\n+\n+  @Parameter(description = \"<output parquet file path>\")\n+  String output;\n+\n+  @Parameter(description = \"<new compression codec\")\n+  String codec;\n+\n+  @Override\n+  @SuppressWarnings(\"unchecked\")\n+  public int run() throws IOException {\n+    Preconditions.checkArgument(input != null && output != null,\n+      \"Both input and output parquet file paths are required.\");\n+\n+    Preconditions.checkArgument(codec != null,\n+      \"The codec cannot be null\");\n+\n+    Path inPath = new Path(input);\n+    Path outPath = new Path(output);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(getConf(), inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, getConf()), HadoopReadOptions.builder(getConf()).build())) {\n+      ParquetFileWriter writer = new ParquetFileWriter(getConf(), schema, outPath, ParquetFileWriter.Mode.CREATE);\n+      writer.start();\n+      processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+    return 0;\n+  }\n+\n+  @Override\n+  public List<String> getExamples() {\n+    return Lists.newArrayList(\n+        \"# Translate the compression from one to another\",\n+        \" input.parquet output.parquet ZSTD\"\n+    );\n+  }\n+  private void processBlocks(TransParquetFileReader reader, ParquetFileWriter writer, ParquetMetadata meta, MessageType schema,\n+                             String createdBy, CompressionCodecName codecName) throws IOException {\n+    int blockIndex = 0;\n+    PageReadStore store = reader.readNextRowGroup();\n+    while (store != null) {\n+      writer.startBlock(store.getRowCount());\n+      BlockMetaData blockMetaData = meta.getBlocks().get(blockIndex);\n+      List<ColumnChunkMetaData> columnsInOrder = blockMetaData.getColumns();\n+      Map<ColumnPath, ColumnDescriptor> descriptorsMap = schema.getColumns().stream().collect(\n+        Collectors.toMap(x -> ColumnPath.get(x.getPath()), x -> x));\n+      for (int i = 0; i < columnsInOrder.size(); i += 1) {\n+        ColumnChunkMetaData chunk = columnsInOrder.get(i);\n+        ColumnReadStoreImpl crstore = new ColumnReadStoreImpl(store, new DumpGroupConverter(), schema, createdBy);\n+        ColumnDescriptor columnDescriptor = descriptorsMap.get(chunk.getPath());\n+        writer.startColumn(columnDescriptor, crstore.getColumnReader(columnDescriptor).getTotalValueCount(), codecName);\n+        processChunk(reader, writer, chunk, createdBy, codecName);\n+        writer.endColumn();\n+      }\n+      writer.endBlock();\n+      store = reader.readNextRowGroup();\n+      blockIndex++;\n+    }\n+  }\n+\n+  private void processChunk(TransParquetFileReader reader, ParquetFileWriter writer, ColumnChunkMetaData chunk,\n+                            String createdBy, CompressionCodecName codecName) throws IOException {\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    BytesInputDecompressor decompressor = codecFactory.getDecompressor(chunk.getCodec());\n+    BytesInputCompressor compressor = codecFactory.getCompressor(codecName);\n+    ColumnIndex columnIndex = reader.readColumnIndex(chunk);\n+    OffsetIndex offsetIndex = reader.readOffsetIndex(chunk);\n+\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    DictionaryPage dictionaryPage = null;\n+    long readValues = 0;\n+    Statistics statistics = null;\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    int pageIndex = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      PageHeader pageHeader = reader.readPageHeader();\n+      byte[] pageLoad;\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          if (dictionaryPage != null) {\n+            throw new IOException(\"has more than one dictionary page in column chunk\");\n+          }\n+          DictionaryPageHeader dictPageHeader = pageHeader.dictionary_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          writer.writeDictionaryPage(new DictionaryPage(BytesInput.from(pageLoad),\n+            pageHeader.getUncompressed_page_size(),\n+            converter.getEncoding(dictPageHeader.getEncoding())));\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          pageLoad = translatePageLoad(reader, true, compressor, decompressor, pageHeader.getCompressed_page_size(), pageHeader.getUncompressed_page_size());\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV1.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV1.getNum_values();\n+          if (offsetIndex != null) {\n+            long rowCount = 1 + offsetIndex.getLastRowIndex(pageIndex, totalChunkValues) - offsetIndex.getFirstRowIndex(pageIndex);\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              toIntWithCheck(rowCount),\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          } else {\n+            writer.writeDataPage(toIntWithCheck(headerV1.getNum_values()),\n+              pageHeader.uncompressed_page_size,\n+              BytesInput.from(pageLoad),\n+              statistics,\n+              converter.getEncoding(headerV1.getRepetition_level_encoding()),\n+              converter.getEncoding(headerV1.getDefinition_level_encoding()),\n+              converter.getEncoding(headerV1.getEncoding()));\n+          }\n+          pageIndex++;\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          BytesInput rlLevels = readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          BytesInput dlLevels = readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          int rawDataLength = pageHeader.getUncompressed_page_size() - rlLength - dlLength;\n+          pageLoad = translatePageLoad(reader, headerV2.is_compressed, compressor, decompressor, payLoadLength, rawDataLength);\n+          statistics = convertStatistics(createdBy, chunk.getPrimitiveType(), headerV2.getStatistics(), columnIndex, pageIndex, converter);\n+          readValues += headerV2.getNum_values();\n+          writer.writeDataPageV2(headerV2.getNum_rows(),\n+            headerV2.getNum_nulls(),\n+            headerV2.getNum_values(),\n+            rlLevels,\n+            dlLevels,\n+            converter.getEncoding(headerV2.getEncoding()),\n+            BytesInput.from(pageLoad),\n+            pageHeader.uncompressed_page_size - rlLength - dlLength,\n+            statistics);\n+          pageIndex++;\n+          break;\n+        default:\n+          break;\n+      }\n+    }\n+  }\n+\n+  private Statistics convertStatistics(String createdBy, PrimitiveType type, org.apache.parquet.format.Statistics pageStatistics,\n+                                       ColumnIndex columnIndex, int pageIndex, ParquetMetadataConverter converter) throws IOException {\n+    if (pageStatistics != null) {\n+      return converter.fromParquetStatistics(createdBy, pageStatistics, type);\n+    } else if (columnIndex != null) {\n+      if (columnIndex.getNullPages() == null) {\n+        throw new IOException(\"columnIndex has null variable 'nullPages' which indicates corrupted data for type: \" +  type.getName());\n+      }\n+      if (pageIndex > columnIndex.getNullPages().size()) {\n+        throw new IOException(\"There are more pages \" + pageIndex + \" found in the column than in the columnIndex \" + columnIndex.getNullPages().size());\n+      }\n+      org.apache.parquet.column.statistics.Statistics.Builder statsBuilder = org.apache.parquet.column.statistics.Statistics.getBuilderForReading(type);\n+      statsBuilder.withNumNulls(columnIndex.getNullCounts().get(pageIndex));\n+\n+      if (!columnIndex.getNullPages().get(pageIndex)) {\n+        statsBuilder.withMin(columnIndex.getMinValues().get(pageIndex).array().clone());\n+        statsBuilder.withMax(columnIndex.getMaxValues().get(pageIndex).array().clone());\n+      }\n+      return statsBuilder.build();\n+    } else {\n+      return null;\n+    }\n+  }\n+\n+  private byte[] translatePageLoad(TransParquetFileReader reader, boolean isCompressed, BytesInputCompressor compressor,\n+                                   BytesInputDecompressor decompressor, int payloadLength, int rawDataLength) throws IOException {\n+    BytesInput data = readBlock(payloadLength, reader);\n+    if (isCompressed) {\n+      data = decompressor.decompress(data, rawDataLength);\n+    }\n+    BytesInput newCompressedData = compressor.compress(data);\n+    return newCompressedData.toByteArray();\n+  }\n+\n+  private BytesInput readBlock(int length, TransParquetFileReader reader) throws IOException {\n+    byte[] data = new byte[length];", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUzMzE2MA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442533160", "bodyText": "Sounds good! While creating the cache byte array, it uncovered an bug(I think it is a bug) in the H1SeeakableInputStream. I added that fix to this PR also.", "author": "shangxinli", "createdAt": "2020-06-18T22:20:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIxNTMzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyMTg2Mg==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442221862", "bodyText": "You are not testing OffsetIndex.getOffset(int) which is a big problem. These offsets are to reference the exact locations of the pages in the file. It is not trivial to test it but if you were it would fail because you are not rewriting it in the new file. Every other data in the column/offset indexes shall be the same in the source and the target file but these values shall be recalculated as the sizes of the pages will be different after compressed by another codec so the positions shift in the file.", "author": "gszadovszky", "createdAt": "2020-06-18T13:23:26Z", "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertRecordByRecord(CompressionCodecName codecName, Path inpath, Path outpath) throws Exception {\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    HadoopInputFile inputFile = HadoopInputFile.fromPath(inpath, conf);\n+    ParquetReadOptions readOptions = HadoopReadOptions.builder(conf).build();\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(outpath).withConf(conf).withCompressionCodec(codecName);\n+\n+    ParquetWriter parquetWriter = builder.build();\n+\n+    PageReadStore pages;\n+    ParquetFileReader reader = new ParquetFileReader(inputFile, readOptions);\n+\n+    while ((pages = reader.readNextRowGroup()) != null) {\n+      long rows = pages.getRowCount();\n+      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);\n+      RecordReader recordReader = columnIO.getRecordReader(pages, new GroupRecordConverter(schema));\n+\n+      for (int i = 0; i < rows; i++) {\n+        SimpleGroup simpleGroup = (SimpleGroup) recordReader.read();\n+        parquetWriter.write(simpleGroup);\n+      }\n+    }\n+\n+    parquetWriter.close();\n+  }\n+\n+  private void validateColumns(String inputFile, int numRecord) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) < 1000);\n+      assertEquals(group.getBinary(\"Name\", 0).length(), 100);\n+      assertEquals(group.getBinary(\"Gender\", 0).length(), 100);\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertEquals(subGroup.getBinary(\"Backward\", 0).length(), 100);\n+      assertEquals(subGroup.getBinary(\"Forward\", 0).length(), 100);\n+    }\n+    reader.close();\n+  }\n+\n+  private void validMeta(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getFileMetaData().getSchema(), outMetaData.getFileMetaData().getSchema());\n+    Assert.assertEquals(inMetaData.getFileMetaData().getKeyValueMetaData(), outMetaData.getFileMetaData().getKeyValueMetaData());\n+  }\n+\n+  private void validColumnIndex(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getBlocks().size(), outMetaData.getBlocks().size());\n+    try (ParquetFileReader inReader = new ParquetFileReader(HadoopInputFile.fromPath(new Path(inputFile), conf), HadoopReadOptions.builder(conf).build());\n+         ParquetFileReader outReader = new ParquetFileReader(HadoopInputFile.fromPath(new Path(outFile), conf), HadoopReadOptions.builder(conf).build())) {\n+      for (int i = 0; i < inMetaData.getBlocks().size(); i++) {\n+        BlockMetaData inBlockMetaData = inMetaData.getBlocks().get(i);\n+        BlockMetaData outBlockMetaData = outMetaData.getBlocks().get(i);\n+        Assert.assertEquals(inBlockMetaData.getColumns().size(), outBlockMetaData.getColumns().size());\n+        for (int j = 0; j < inBlockMetaData.getColumns().size(); j++) {\n+          ColumnChunkMetaData inChunk = inBlockMetaData.getColumns().get(j);\n+          ColumnIndex inColumnIndex = inReader.readColumnIndex(inChunk);\n+          OffsetIndex inOffsetIndex = inReader.readOffsetIndex(inChunk);\n+          ColumnChunkMetaData outChunk = outBlockMetaData.getColumns().get(j);\n+          ColumnIndex outColumnIndex = outReader.readColumnIndex(outChunk);\n+          OffsetIndex outOffsetIndex = outReader.readOffsetIndex(outChunk);\n+          if (inColumnIndex != null) {\n+            Assert.assertEquals(inColumnIndex.getBoundaryOrder(), outColumnIndex.getBoundaryOrder());\n+            Assert.assertEquals(inColumnIndex.getMaxValues(), outColumnIndex.getMaxValues());\n+            Assert.assertEquals(inColumnIndex.getMinValues(), outColumnIndex.getMinValues());\n+            Assert.assertEquals(inColumnIndex.getNullCounts(), outColumnIndex.getNullCounts());\n+          }\n+          if (inOffsetIndex != null) {\n+            Assert.assertEquals(inOffsetIndex.getPageCount(), outOffsetIndex.getPageCount());\n+            for (int k = 0; k < inOffsetIndex.getPageCount(); k++) {\n+              Assert.assertEquals(inOffsetIndex.getFirstRowIndex(k), outOffsetIndex.getFirstRowIndex(k));\n+              Assert.assertEquals(inOffsetIndex.getLastRowIndex(k, inChunk.getValueCount()),\n+                outOffsetIndex.getLastRowIndex(k, outChunk.getValueCount()));\n+            }", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjc1OTU1MA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442759550", "bodyText": "Update: I was wrong. The offset index is calculated at page write so the offsets should be correct. But testing them still makes sense.", "author": "gszadovszky", "createdAt": "2020-06-19T10:23:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyMTg2Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjg4MzExNA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442883114", "bodyText": "Sure. I will add it.", "author": "shangxinli", "createdAt": "2020-06-19T14:47:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyMTg2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNTg1OA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442225858", "bodyText": "I think, you should check the output file instead.", "author": "gszadovszky", "createdAt": "2020-06-18T13:29:09Z", "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEwMzUyNw==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443103527", "bodyText": "Good catch!", "author": "shangxinli", "createdAt": "2020-06-20T05:31:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNTg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNjk4Nw==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r442226987", "bodyText": "I don't like these tests. You should test for exact values. You may either keep the generated data in memory so you can match the file content with it or you may read both the source and the target file in the same time and match the values.", "author": "gszadovszky", "createdAt": "2020-06-18T13:30:41Z", "path": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/TransCompressionCommandTest.java", "diffHunk": "@@ -0,0 +1,271 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.cli.commands;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class TransCompressionCommandTest extends ParquetFileTest  {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSpeed() throws Exception {\n+    String inputFile = createParquetFile(\"input\", \"GZIP\", 100000,\n+    ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    long start = System.currentTimeMillis();\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = \"ZSTD\";\n+    command.run();\n+    long durationTrans = System.currentTimeMillis() - start;\n+\n+    outputFile = createTempFile(\"output_record\");\n+    start = System.currentTimeMillis();\n+    convertRecordByRecord(CompressionCodecName.valueOf(\"ZSTD\"), new Path(inputFile), new Path(outputFile));\n+    long durationRecord = System.currentTimeMillis() - start;\n+\n+    // The TransCompressionCommand is ~5 times faster than translating record by record\n+    Assert.assertTrue(durationTrans < durationRecord);\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    String inputFile = createParquetFile(\"input\", srcCodec, numRecord, writerVersion, pageSize);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    TransCompressionCommand command = new TransCompressionCommand(createLogger());\n+    command.setConf(new Configuration());\n+    command.input = inputFile;\n+    command.output = outputFile;\n+    command.codec = destCodec;\n+    command.run();\n+\n+    validateColumns(inputFile, numRecord);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertRecordByRecord(CompressionCodecName codecName, Path inpath, Path outpath) throws Exception {\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inpath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    HadoopInputFile inputFile = HadoopInputFile.fromPath(inpath, conf);\n+    ParquetReadOptions readOptions = HadoopReadOptions.builder(conf).build();\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(outpath).withConf(conf).withCompressionCodec(codecName);\n+\n+    ParquetWriter parquetWriter = builder.build();\n+\n+    PageReadStore pages;\n+    ParquetFileReader reader = new ParquetFileReader(inputFile, readOptions);\n+\n+    while ((pages = reader.readNextRowGroup()) != null) {\n+      long rows = pages.getRowCount();\n+      MessageColumnIO columnIO = new ColumnIOFactory().getColumnIO(schema);\n+      RecordReader recordReader = columnIO.getRecordReader(pages, new GroupRecordConverter(schema));\n+\n+      for (int i = 0; i < rows; i++) {\n+        SimpleGroup simpleGroup = (SimpleGroup) recordReader.read();\n+        parquetWriter.write(simpleGroup);\n+      }\n+    }\n+\n+    parquetWriter.close();\n+  }\n+\n+  private void validateColumns(String inputFile, int numRecord) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(inputFile)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) < 1000);\n+      assertEquals(group.getBinary(\"Name\", 0).length(), 100);\n+      assertEquals(group.getBinary(\"Gender\", 0).length(), 100);\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertEquals(subGroup.getBinary(\"Backward\", 0).length(), 100);\n+      assertEquals(subGroup.getBinary(\"Forward\", 0).length(), 100);", "originalCommit": "d0cf386e7b51b1a2488df7c07edab509e288cd8e", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzEwMzU1Mw==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443103553", "bodyText": "I will cache the input data and compare it.", "author": "shangxinli", "createdAt": "2020-06-20T05:32:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjIyNjk4Nw=="}], "type": "inlineReview"}, {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413", "url": "https://github.com/apache/parquet-mr/commit/64827a3ee339614b432c7424c13c03fbc8e91413", "message": "Address feedback", "committedDate": "2020-06-20T06:18:19Z", "type": "commit"}, {"oid": "64827a3ee339614b432c7424c13c03fbc8e91413", "url": "https://github.com/apache/parquet-mr/commit/64827a3ee339614b432c7424c13c03fbc8e91413", "message": "Address feedback", "committedDate": "2020-06-20T06:18:19Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzQ1Nzg4NA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443457884", "bodyText": "Good catch!", "author": "gszadovszky", "createdAt": "2020-06-22T10:16:26Z", "path": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java", "diffHunk": "@@ -53,7 +53,7 @@ public void readFully(byte[] bytes) throws IOException {\n \n   @Override\n   public void readFully(byte[] bytes, int start, int len) throws IOException {\n-    stream.readFully(bytes);\n+    stream.readFully(bytes, start, len);", "originalCommit": "64827a3ee339614b432c7424c13c03fbc8e91413", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyMjIzNA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443522234", "bodyText": "Instead of creating a Random instance or retrieving a ThreadLocalRandom instance for each generated values I would suggest creating one Random instance in TestDocs in the constructor and use it for all the values to be generated in that instance. Also suggest to use a hard-coded seed for Random to ensure the test is deterministic.", "author": "gszadovszky", "createdAt": "2020-06-22T12:28:20Z", "path": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/CompressionConveterTest.java", "diffHunk": "@@ -0,0 +1,318 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.hadoop.util;\n+\n+import com.google.common.collect.ImmutableMap;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.column.page.PageReadStore;\n+import org.apache.parquet.example.data.Group;\n+import org.apache.parquet.example.data.simple.SimpleGroup;\n+import org.apache.parquet.example.data.simple.convert.GroupRecordConverter;\n+import org.apache.parquet.format.DataPageHeader;\n+import org.apache.parquet.format.DataPageHeaderV2;\n+import org.apache.parquet.format.DictionaryPageHeader;\n+import org.apache.parquet.format.PageHeader;\n+import org.apache.parquet.hadoop.ParquetFileReader;\n+import org.apache.parquet.hadoop.ParquetFileWriter;\n+import org.apache.parquet.hadoop.ParquetReader;\n+import org.apache.parquet.hadoop.ParquetWriter;\n+import org.apache.parquet.hadoop.example.ExampleParquetWriter;\n+import org.apache.parquet.hadoop.example.GroupReadSupport;\n+import org.apache.parquet.hadoop.example.GroupWriteSupport;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.CompressionConverter.TransParquetFileReader;\n+import org.apache.parquet.internal.column.columnindex.ColumnIndex;\n+import org.apache.parquet.internal.column.columnindex.OffsetIndex;\n+import org.apache.parquet.io.ColumnIOFactory;\n+import org.apache.parquet.io.MessageColumnIO;\n+import org.apache.parquet.io.RecordReader;\n+import org.apache.parquet.schema.GroupType;\n+import org.apache.parquet.schema.MessageType;\n+import org.apache.parquet.schema.PrimitiveType;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.ThreadLocalRandom;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.BINARY;\n+import static org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName.INT64;\n+import static org.apache.parquet.schema.Type.Repetition.OPTIONAL;\n+import static org.apache.parquet.schema.Type.Repetition.REPEATED;\n+import static org.apache.parquet.schema.Type.Repetition.REQUIRED;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+public class CompressionConveterTest {\n+\n+  private Configuration conf = new Configuration();\n+  private Map<String, String> extraMeta\n+    = ImmutableMap.of(\"key1\", \"value1\", \"key2\", \"value2\");\n+  private CompressionConverter compressionConverter = new CompressionConverter();\n+\n+  @Test\n+  public void testTransCompression() throws Exception {\n+    String[] codecs = {\"UNCOMPRESSED\", \"SNAPPY\", \"GZIP\", \"ZSTD\"};\n+    for (int i = 0; i < codecs.length; i++) {\n+      for (int j = 0; j <codecs.length; j++) {\n+        // Same codec for both are considered as valid test case\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_2_0, ParquetProperties.DEFAULT_PAGE_SIZE);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, 64);\n+        testInternal(codecs[i], codecs[j], ParquetProperties.WriterVersion.PARQUET_1_0, ParquetProperties.DEFAULT_PAGE_SIZE * 100);\n+      }\n+    }\n+  }\n+\n+  private void testInternal(String srcCodec, String destCodec, ParquetProperties.WriterVersion writerVersion, int pageSize) throws Exception {\n+    int numRecord = 1000;\n+    TestDocs testDocs = new TestDocs(numRecord);\n+    String inputFile = createParquetFile(conf, extraMeta, numRecord, \"input\", srcCodec, writerVersion, pageSize, testDocs);\n+    String outputFile = createTempFile(\"output_trans\");\n+\n+    convertCompression(conf, inputFile, outputFile, destCodec);\n+\n+    validateColumns(outputFile, numRecord, testDocs);\n+    validMeta(inputFile, outputFile);\n+    validColumnIndex(inputFile, outputFile);\n+  }\n+\n+  private void convertCompression(Configuration conf, String inputFile, String outputFile, String codec) throws IOException {\n+    Path inPath = new Path(inputFile);\n+    Path outPath = new Path(outputFile);\n+    CompressionCodecName codecName = CompressionCodecName.valueOf(codec);\n+\n+    ParquetMetadata metaData = ParquetFileReader.readFooter(conf, inPath, NO_FILTER);\n+    MessageType schema = metaData.getFileMetaData().getSchema();\n+    ParquetFileWriter writer = new ParquetFileWriter(conf, schema, outPath, ParquetFileWriter.Mode.CREATE);\n+    writer.start();\n+\n+    try (TransParquetFileReader reader = new TransParquetFileReader(HadoopInputFile.fromPath(inPath, conf), HadoopReadOptions.builder(conf).build())) {\n+      compressionConverter.processBlocks(reader, writer, metaData, schema, metaData.getFileMetaData().getCreatedBy(), codecName);\n+    } finally {\n+      writer.end(metaData.getFileMetaData().getKeyValueMetaData());\n+    }\n+  }\n+\n+  private void validateColumns(String file, int numRecord, TestDocs testDocs) throws IOException {\n+    ParquetReader<Group> reader = ParquetReader.builder(new GroupReadSupport(), new Path(file)).withConf(conf).build();\n+    for (int i = 0; i < numRecord; i++) {\n+      Group group = reader.read();\n+      assertTrue(group.getLong(\"DocId\", 0) == testDocs.docId[i]);\n+      assertArrayEquals(group.getBinary(\"Name\", 0).getBytes(), testDocs.name[i].getBytes());\n+      assertArrayEquals(group.getBinary(\"Gender\", 0).getBytes(), testDocs.gender[i].getBytes());\n+      Group subGroup = group.getGroup(\"Links\", 0);\n+      assertArrayEquals(subGroup.getBinary(\"Backward\", 0).getBytes(), testDocs.linkBackward[i].getBytes());\n+      assertArrayEquals(subGroup.getBinary(\"Forward\", 0).getBytes(), testDocs.linkForward[i].getBytes());\n+    }\n+    reader.close();\n+  }\n+\n+  private void validMeta(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getFileMetaData().getSchema(), outMetaData.getFileMetaData().getSchema());\n+    Assert.assertEquals(inMetaData.getFileMetaData().getKeyValueMetaData(), outMetaData.getFileMetaData().getKeyValueMetaData());\n+  }\n+\n+  private void validColumnIndex(String inputFile, String outFile) throws Exception {\n+    ParquetMetadata inMetaData = ParquetFileReader.readFooter(conf, new Path(inputFile), NO_FILTER);\n+    ParquetMetadata outMetaData = ParquetFileReader.readFooter(conf, new Path(outFile), NO_FILTER);\n+    Assert.assertEquals(inMetaData.getBlocks().size(), outMetaData.getBlocks().size());\n+    try (TransParquetFileReader inReader = new TransParquetFileReader(HadoopInputFile.fromPath(new Path(inputFile), conf), HadoopReadOptions.builder(conf).build());\n+         TransParquetFileReader outReader = new TransParquetFileReader(HadoopInputFile.fromPath(new Path(outFile), conf), HadoopReadOptions.builder(conf).build())) {\n+      for (int i = 0; i < inMetaData.getBlocks().size(); i++) {\n+        BlockMetaData inBlockMetaData = inMetaData.getBlocks().get(i);\n+        BlockMetaData outBlockMetaData = outMetaData.getBlocks().get(i);\n+        Assert.assertEquals(inBlockMetaData.getColumns().size(), outBlockMetaData.getColumns().size());\n+        for (int j = 0; j < inBlockMetaData.getColumns().size(); j++) {\n+          ColumnChunkMetaData inChunk = inBlockMetaData.getColumns().get(j);\n+          ColumnIndex inColumnIndex = inReader.readColumnIndex(inChunk);\n+          OffsetIndex inOffsetIndex = inReader.readOffsetIndex(inChunk);\n+          ColumnChunkMetaData outChunk = outBlockMetaData.getColumns().get(j);\n+          ColumnIndex outColumnIndex = outReader.readColumnIndex(outChunk);\n+          OffsetIndex outOffsetIndex = outReader.readOffsetIndex(outChunk);\n+          if (inColumnIndex != null) {\n+            Assert.assertEquals(inColumnIndex.getBoundaryOrder(), outColumnIndex.getBoundaryOrder());\n+            Assert.assertEquals(inColumnIndex.getMaxValues(), outColumnIndex.getMaxValues());\n+            Assert.assertEquals(inColumnIndex.getMinValues(), outColumnIndex.getMinValues());\n+            Assert.assertEquals(inColumnIndex.getNullCounts(), outColumnIndex.getNullCounts());\n+          }\n+          if (inOffsetIndex != null) {\n+            List<Long> inOffsets = getOffsets(inReader, inChunk);\n+            List<Long> outOffsets = getOffsets(outReader, outChunk);\n+            Assert.assertEquals(inOffsets.size(), outOffsets.size());\n+            Assert.assertEquals(inOffsets.size(), inOffsetIndex.getPageCount());\n+            Assert.assertEquals(inOffsetIndex.getPageCount(), outOffsetIndex.getPageCount());\n+            for (int k = 0; k < inOffsetIndex.getPageCount(); k++) {\n+              Assert.assertEquals(inOffsetIndex.getFirstRowIndex(k), outOffsetIndex.getFirstRowIndex(k));\n+              Assert.assertEquals(inOffsetIndex.getLastRowIndex(k, inChunk.getValueCount()),\n+                outOffsetIndex.getLastRowIndex(k, outChunk.getValueCount()));\n+              Assert.assertEquals(inOffsetIndex.getOffset(k), (long)inOffsets.get(k));\n+              Assert.assertEquals(outOffsetIndex.getOffset(k), (long)outOffsets.get(k));\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private List<Long> getOffsets(TransParquetFileReader reader, ColumnChunkMetaData chunk) throws IOException {\n+    List<Long> offsets = new ArrayList<>();\n+    reader.setStreamPosition(chunk.getStartingPos());\n+    long readValues = 0;\n+    long totalChunkValues = chunk.getValueCount();\n+    while (readValues < totalChunkValues) {\n+      long curOffset = reader.getPos();\n+      PageHeader pageHeader = reader.readPageHeader();\n+      switch (pageHeader.type) {\n+        case DICTIONARY_PAGE:\n+          compressionConverter.readBlock(pageHeader.getCompressed_page_size(), reader);\n+          break;\n+        case DATA_PAGE:\n+          DataPageHeader headerV1 = pageHeader.data_page_header;\n+          offsets.add(curOffset);\n+          compressionConverter.readBlock(pageHeader.getCompressed_page_size(), reader);\n+          readValues += headerV1.getNum_values();\n+          break;\n+        case DATA_PAGE_V2:\n+          DataPageHeaderV2 headerV2 = pageHeader.data_page_header_v2;\n+          offsets.add(curOffset);\n+          int rlLength = headerV2.getRepetition_levels_byte_length();\n+          compressionConverter.readBlock(rlLength, reader);\n+          int dlLength = headerV2.getDefinition_levels_byte_length();\n+          compressionConverter.readBlock(dlLength, reader);\n+          int payLoadLength = pageHeader.getCompressed_page_size() - rlLength - dlLength;\n+          compressionConverter.readBlock(payLoadLength, reader);\n+          readValues += headerV2.getNum_values();\n+          break;\n+        default:\n+          throw new IOException(\"Not recognized page type\");\n+      }\n+    }\n+    return offsets;\n+  }\n+\n+  private String createParquetFile(Configuration conf, Map<String, String> extraMeta, int numRecord, String prefix, String codec,\n+                                         ParquetProperties.WriterVersion writerVersion, int pageSize, TestDocs testDocs) throws IOException {\n+    MessageType schema = new MessageType(\"schema\",\n+      new PrimitiveType(REQUIRED, INT64, \"DocId\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Name\"),\n+      new PrimitiveType(REQUIRED, BINARY, \"Gender\"),\n+      new GroupType(OPTIONAL, \"Links\",\n+        new PrimitiveType(REPEATED, BINARY, \"Backward\"),\n+        new PrimitiveType(REPEATED, BINARY, \"Forward\")));\n+\n+    conf.set(GroupWriteSupport.PARQUET_EXAMPLE_SCHEMA, schema.toString());\n+\n+    String file = createTempFile(prefix);\n+    ExampleParquetWriter.Builder builder = ExampleParquetWriter.builder(new Path(file))\n+      .withConf(conf)\n+      .withWriterVersion(writerVersion)\n+      .withExtraMetaData(extraMeta)\n+      .withDictionaryEncoding(\"DocId\", true)\n+      .withValidation(true)\n+      .enablePageWriteChecksum()\n+      .withPageSize(pageSize)\n+      .withCompressionCodec(CompressionCodecName.valueOf(codec));\n+    try (ParquetWriter writer = builder.build()) {\n+      for (int i = 0; i < numRecord; i++) {\n+        SimpleGroup g = new SimpleGroup(schema);\n+        g.add(\"DocId\", testDocs.docId[i]);\n+        g.add(\"Name\", testDocs.name[i]);\n+        g.add(\"Gender\", testDocs.gender[i]);\n+        Group links = g.addGroup(\"Links\");\n+        links.add(0, testDocs.linkBackward[i]);\n+        links.add(1, testDocs.linkForward[i]);\n+        writer.write(g);\n+      }\n+    }\n+\n+    return file;\n+  }\n+\n+  private static long getLong() {\n+    return ThreadLocalRandom.current().nextLong(1000);\n+  }\n+\n+  private String getString() {\n+    char[] chars = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'x', 'z', 'y'};\n+    StringBuilder sb = new StringBuilder();\n+    for (int i = 0; i < 100; i++) {\n+      sb.append(chars[new Random().nextInt(10)]);\n+    }\n+    return sb.toString();\n+  }", "originalCommit": "64827a3ee339614b432c7424c13c03fbc8e91413", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3NTg3MA==", "url": "https://github.com/apache/parquet-mr/pull/796#discussion_r443875870", "bodyText": "Sounds good!", "author": "shangxinli", "createdAt": "2020-06-22T23:18:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzUyMjIzNA=="}], "type": "inlineReview"}, {"oid": "99d1a08975b83d0df93c730edc2d7b007535fd70", "url": "https://github.com/apache/parquet-mr/commit/99d1a08975b83d0df93c730edc2d7b007535fd70", "message": "Address more feedbacks", "committedDate": "2020-06-22T23:19:28Z", "type": "commit"}]}