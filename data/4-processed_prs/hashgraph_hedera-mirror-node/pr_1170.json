{"pr_number": 1170, "pr_title": "Add custom alerts and dashboards", "pr_createdAt": "2020-10-22T14:51:15Z", "pr_url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170", "timeline": [{"oid": "37735be787c4bffcb2238967e492ce0d2949bdc6", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/37735be787c4bffcb2238967e492ce0d2949bdc6", "message": "Add custom alerts and dashboards\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T14:44:26Z", "type": "commit"}, {"oid": "041a277bfe4379c1f2de801da92696f31c3c5a5c", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/041a277bfe4379c1f2de801da92696f31c3c5a5c", "message": "Fix docker-compose by adding Redis\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T15:07:46Z", "type": "commit"}, {"oid": "105dcc55d1d6d4d1b80bd060a6f201839e1c4373", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/105dcc55d1d6d4d1b80bd060a6f201839e1c4373", "message": "Add namespace label to alerts\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T15:31:30Z", "type": "commit"}, {"oid": "ee8b1aeb8e6b48443047293fc80293345ab022e3", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/ee8b1aeb8e6b48443047293fc80293345ab022e3", "message": "Fix helm lint\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T15:34:45Z", "type": "commit"}, {"oid": "012926b3257b581aa4648fd04d93d08d1e6638d3", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/012926b3257b581aa4648fd04d93d08d1e6638d3", "message": "Disable prometheusRules by default to fix CI\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T15:54:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MDc3Ng==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170#discussion_r510450776", "bodyText": "Would something like this work\n  ImporterRecordAge:\n     annotations:\n       description: Averaging {{ $value }}s trying to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\n       summary: Record stream backlogged, stream file older than 30s\n     enabled: true\n     expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_latency_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) > 30\n     for: 1m\n     labels:\n       severity: critical\n\nWould need to disable or switch to warning when importer is running in historical catch up mode", "author": "Nana-EC", "createdAt": "2020-10-22T20:54:26Z", "path": "charts/hedera-mirror-importer/values.yaml", "diffHunk": "@@ -95,6 +95,148 @@ resources:\n \n revisionHistoryLimit: 3\n \n+prometheusRules:\n+  enabled: false\n+  ImporterNoTransactions:\n+    annotations:\n+      description: \"Record stream TPS has dropped to {{ $value }} for {{ $labels.namespace }}/{{ $labels.pod }}. This may be because importer is down, can't connect to cloud storage, main nodes are not uploading, error parsing the streams, no traffic, etc.\"\n+      summary: \"No transactions seen for 2m\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_transaction_latency_seconds_count{application=\"hedera-mirror-importer\"}[5m])) by (namespace, pod) <= 0\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterCloudStorageErrors:\n+    annotations:\n+      description: '{{ $value | humanizePercentage }}% Error rate trying to {{ if ne $labels.action \"list\" }} retrieve{{ end }} {{ $labels.action }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace }}/{{ $labels.pod }}'\n+      summary: \"Cloud storage error rate exceeds 5%\"\n+    enabled: true\n+    expr: (sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\", status!~\"^2.*\"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\"}[2m])) by (namespace, pod, type, action)) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterCloudStorageLatency:\n+    annotations:\n+      description: Cloud storage latency exceeded {{ $value }}s trying to {{ if ne $labels.action \"list\" }} retrieve{{ end }} {{ $labels.action }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace }}/{{ $labels.pod }}\n+      summary: Cloud storage latency exceeds 1s\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_request_seconds_sum{application=\"hedera-mirror-importer\", status=~\"^2.*\"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\", status=~\"^2.*\"}[2m])) by (namespace, pod, type, action) > 1\n+    for: 1m\n+    labels:\n+      severity: critical\n+\n+  ImporterNoConsensus:\n+    annotations:\n+      description: \"{{ $labels.namespace }}/{{ $labels.pod }} only able to achieve {{ $value | humanizePercentage }}% consensus during {{ $labels.type }} stream signature verification\"\n+      summary: Unable to verify {{ $labels.type }} stream signatures\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_signature_verification_total{application=\"hedera-mirror-importer\", status=\"CONSENSUS_REACHED\"}[2m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_signature_verification_total{application=\"hedera-mirror-importer\"}[2m])) by (namespace, pod, type) < 0.33\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterFileVerificationErrors:\n+    annotations:\n+      description: \"Error rate of {{ $value | humanizePercentage }}% trying to download and verify {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\"\n+      summary: \"{{ $labels.type }} file verification error rate exceeds 5%\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_stream_verification_seconds_count{application=\"hedera-mirror-importer\", success=\"false\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_stream_verification_seconds_count{application=\"hedera-mirror-importer\"[3m])) by (namespace, pod, type) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterStreamCloseInterval:\n+    annotations:\n+      description: \"{{ $labels.namespace }}/{{ $labels.pod }} file stream should close every 2s but is actually {{ $value }}s. This could just be due to the lack of traffic in the environment, but it could potentially be something more serious to look into.\"\n+      summary: Record stream close interval exceeds 10s\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_stream_close_latency_seconds_sum{application=\"hedera-mirror-importer\", type=\"RECORD\"}[5m])) by (namespace, pod) / sum(rate(hedera_mirror_stream_close_latency_seconds_count{application=\"hedera-mirror-importer\", type=\"RECORD\"}[5m])) by (namespace, pod) > 10\n+    for: 1m\n+    labels:\n+      severity: warning\n+\n+  ImporterParseErrors:\n+    annotations:\n+      description: \"Encountered {{ $value | humanizePercentage }}% errors trying to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\"\n+      summary: \"Error rate parsing {{ $labels.type }} exceeds 5%\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\", success=\"false\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterParseLatency:\n+    annotations:\n+      description: Averaging {{ $value }}s trying to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\n+      summary: Took longer than 2s to parse {{ $labels.type }} stream files\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) > 2\n+    for: 1m\n+    labels:\n+      severity: critical\n+", "originalCommit": "012926b3257b581aa4648fd04d93d08d1e6638d3", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ2MzIzOA==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170#discussion_r510463238", "bodyText": "True, that can work with some slight tweaking. Have to remove type from grouping and hardcode to filter by record since each stream has different intervals. Can use inhibiting to disable some alerts when others firing.\nAlso, apparently missing a graph for this metric so will add.", "author": "steven-sheehy", "createdAt": "2020-10-22T21:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MDc3Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUwNzA0Mg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170#discussion_r510507042", "bodyText": "Added hedera.mirror.parse.latency metric for balances\nAdded record/balance file age single stat and graph to importer dashboard\nAdded record/balance file age alert", "author": "steven-sheehy", "createdAt": "2020-10-22T23:09:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDQ1MDc3Ng=="}], "type": "inlineReview"}, {"oid": "e9f2e500ce21b8c33ff9b7e43e0bb0bfb4813265", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/e9f2e500ce21b8c33ff9b7e43e0bb0bfb4813265", "message": "Add record and balance file age graphs and alerts\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T23:05:49Z", "type": "commit"}, {"oid": "89d73ce181341423c40c008971c98b542664b5ee", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/89d73ce181341423c40c008971c98b542664b5ee", "message": "Fix linting\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-22T23:13:57Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMjYxNg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170#discussion_r510532616", "bodyText": "For this , wouldn't it be more appropriate to say\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  summary: Mirror Importer hasn't processed a record file in more than 15s\n          \n          \n            \n                  summary: Mirror Importer backlogged, record stream file older than 15s\n          \n      \n    \n    \n  \n\nGranted we do want an alert if the mirror node hasn't processed any files in an extended period of time.\nSeems like 2 related but different metrics I'm highlighting\n\nAn alert for when the age of the files being processed is over a threshold, to highlight the mirror node has fallen behind, likely due to load.\nAn alert for when the mirror node hasn't processed a file stream item within a certain threshold range of time.\n\n1 can only happen when steam files are received, 2 would happen when none are received.\nGranted in a rare situation of delayed and pulsated stream emits both alert could be triggered.\nWith the hedera_mirror_parse_latency latency I imagine 1 is what's possible here and 2 is a stretch that could be attempted in a future ticket.\nWhat do you think?", "author": "Nana-EC", "createdAt": "2020-10-23T00:31:21Z", "path": "charts/hedera-mirror-importer/values.yaml", "diffHunk": "@@ -95,6 +95,168 @@ resources:\n \n revisionHistoryLimit: 3\n \n+prometheusRules:\n+  enabled: false\n+  ImporterNoTransactions:\n+    annotations:\n+      description: \"Record stream TPS has dropped to {{ $value }} for {{ $labels.namespace }}/{{ $labels.pod }}. This may be because importer is down, can't connect to cloud storage, main nodes are not uploading, error parsing the streams, no traffic, etc.\"\n+      summary: \"No transactions seen for 2m\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_transaction_latency_seconds_count{application=\"hedera-mirror-importer\"}[5m])) by (namespace, pod) <= 0\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterCloudStorageErrors:\n+    annotations:\n+      description: '{{ $value | humanizePercentage }}% Error rate trying to {{ if ne $labels.action \"list\" }} retrieve{{ end }} {{ $labels.action }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace }}/{{ $labels.pod }}'\n+      summary: \"Cloud storage error rate exceeds 5%\"\n+    enabled: true\n+    expr: (sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\", status!~\"^2.*\"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\"}[2m])) by (namespace, pod, type, action)) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterCloudStorageLatency:\n+    annotations:\n+      description: Cloud storage latency exceeded {{ $value | humanizeDuration }} trying to {{ if ne $labels.action \"list\" }} retrieve{{ end }} {{ $labels.action }} {{ $labels.type }} files from cloud storage for {{ $labels.namespace }}/{{ $labels.pod }}\n+      summary: Cloud storage latency exceeds 1s\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_request_seconds_sum{application=\"hedera-mirror-importer\", status=~\"^2.*\"}[2m])) by (namespace, pod, type, action) / sum(rate(hedera_mirror_download_request_seconds_count{application=\"hedera-mirror-importer\", status=~\"^2.*\"}[2m])) by (namespace, pod, type, action) > 1\n+    for: 1m\n+    labels:\n+      severity: critical\n+\n+  ImporterNoConsensus:\n+    annotations:\n+      description: \"{{ $labels.namespace }}/{{ $labels.pod }} only able to achieve {{ $value | humanizePercentage }}% consensus during {{ $labels.type }} stream signature verification\"\n+      summary: Unable to verify {{ $labels.type }} stream signatures\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_signature_verification_total{application=\"hedera-mirror-importer\", status=\"CONSENSUS_REACHED\"}[2m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_signature_verification_total{application=\"hedera-mirror-importer\"}[2m])) by (namespace, pod, type) < 0.33\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterFileVerificationErrors:\n+    annotations:\n+      description: \"Error rate of {{ $value | humanizePercentage }}% trying to download and verify {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\"\n+      summary: \"{{ $labels.type }} file verification error rate exceeds 5%\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_download_stream_verification_seconds_count{application=\"hedera-mirror-importer\", success=\"false\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_download_stream_verification_seconds_count{application=\"hedera-mirror-importer\"[3m])) by (namespace, pod, type) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterStreamCloseInterval:\n+    annotations:\n+      description: \"{{ $labels.namespace }}/{{ $labels.pod }} file stream should close every 2s but is actually {{ $value | humanizeDuration }}. This could just be due to the lack of traffic in the environment, but it could potentially be something more serious to look into.\"\n+      summary: Record stream close interval exceeds 10s\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_stream_close_latency_seconds_sum{application=\"hedera-mirror-importer\", type=\"RECORD\"}[5m])) by (namespace, pod) / sum(rate(hedera_mirror_stream_close_latency_seconds_count{application=\"hedera-mirror-importer\", type=\"RECORD\"}[5m])) by (namespace, pod) > 10\n+    for: 1m\n+    labels:\n+      severity: warning\n+\n+  ImporterParseErrors:\n+    annotations:\n+      description: \"Encountered {{ $value | humanizePercentage }}% errors trying to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\"\n+      summary: \"Error rate parsing {{ $labels.type }} exceeds 5%\"\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\", success=\"false\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) > 0.05\n+    for: 2m\n+    labels:\n+      severity: critical\n+\n+  ImporterParseLatency:\n+    annotations:\n+      description: Averaging {{ $value | humanizeDuration }} trying to parse {{ $labels.type }} stream files for {{ $labels.namespace }}/{{ $labels.pod }}\n+      summary: Took longer than 2s to parse {{ $labels.type }} stream files\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_parse_duration_seconds_sum{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) / sum(rate(hedera_mirror_parse_duration_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type) > 2\n+    for: 1m\n+    labels:\n+      severity: critical\n+\n+  ImporterPublishLatency:\n+    annotations:\n+      description: Took {{ $value | humanizeDuration }} to publish {{ $labels.entity }}s to {{ $labels.type }} for {{ $labels.namespace }}/{{ $labels.pod }}\n+      summary: Slow {{ $labels.type }} publishing\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_importer_publish_duration_seconds_sum{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type, entity) / sum(rate(hedera_mirror_importer_publish_duration_seconds_count{application=\"hedera-mirror-importer\"}[3m])) by (namespace, pod, type, entity) > 1\n+    for: 1m\n+    labels:\n+      severity: critical\n+\n+  ImporterBalanceAge:\n+    annotations:\n+      description: Last processed balance file for {{ $labels.namespace }}/{{ $labels.pod }} was {{ $value | humanizeDuration }} ago\n+      summary: Mirror Importer hasn't processed a balance file in more than 16m\n+    enabled: true\n+    expr: sum(rate(hedera_mirror_parse_latency_seconds_sum{application=\"hedera-mirror-importer\",type=\"BALANCE\"}[3m])) by (namespace, pod) / sum(rate(hedera_mirror_parse_latency_seconds_count{application=\"hedera-mirror-importer\",type=\"BALANCE\"}[3m])) by (namespace, pod) > 960\n+    for: 1m\n+    labels:\n+      severity: critical\n+\n+  ImporterRecordAge:\n+    annotations:\n+      description: Last processed record file for {{ $labels.namespace }}/{{ $labels.pod }} was {{ $value | humanizeDuration }} ago\n+      summary: Mirror Importer hasn't processed a record file in more than 15s", "originalCommit": "89d73ce181341423c40c008971c98b542664b5ee", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMTE2MjUyMg==", "url": "https://github.com/hashgraph/hedera-mirror-node/pull/1170#discussion_r511162522", "bodyText": "True. I reworded it to the below, hopefully it addresses your concern.\ndescription: The difference between the file timestamp and when it was processed is {{ $value | humanizeDuration }} for {{ $labels.namespace }}/{{ $labels.pod }}\nsummary: Mirror Importer record stream processing has fallen behind\nAbout 2), I think we do effectively have that already, at least for the record stream. If the record stream TPS drops to zero we have an alert. Unfortunately that metric also includes unsuccessful record items that get repeatedly parsed from bad files, but we also have parser error percentage alert that will catch that. So while it's not exactly comparing to the expected close interval it should still catch any issues.\nWe are missing the TPS logic for balance stream though.", "author": "steven-sheehy", "createdAt": "2020-10-23T21:26:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMDUzMjYxNg=="}], "type": "inlineReview"}, {"oid": "95ba26ae02be68bf5a9a783ed2dc927b2668c921", "url": "https://github.com/hashgraph/hedera-mirror-node/commit/95ba26ae02be68bf5a9a783ed2dc927b2668c921", "message": "Refine fallen behind alert description\n\nSigned-off-by: Steven Sheehy <steven.sheehy@hedera.com>", "committedDate": "2020-10-23T21:21:26Z", "type": "commit"}]}