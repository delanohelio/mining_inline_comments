{"pr_number": 2520, "pr_title": "[MO] - [clients] -> recursive builder for all clients", "pr_createdAt": "2020-02-08T19:14:10Z", "pr_url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520", "timeline": [{"oid": "4add811011ba82e1bf140453195bad4d956fa0df", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4add811011ba82e1bf140453195bad4d956fa0df", "message": "[MO] - [rebase conflicts] -> done\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-03-15T14:40:45Z", "type": "forcePushed"}, {"oid": "0f856733348391dbcf9b51991f36a3b64668828a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/0f856733348391dbcf9b51991f36a3b64668828a", "message": "[MO] - [conflicts] -> rebase LAST TIME!\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-03-23T16:16:17Z", "type": "forcePushed"}, {"oid": "00618c3040907e3ef66f0d1d28160b77b5b339a0", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/00618c3040907e3ef66f0d1d28160b77b5b339a0", "message": "[MO] - [refactor] -> another fix;\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-03-24T13:26:02Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE4ODk4Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397188986", "bodyText": "Isn't this just for a consumers?", "author": "Frawless", "createdAt": "2020-03-24T14:20:44Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientProperties.java", "diffHunk": "@@ -0,0 +1,430 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.kafka.oauth.common.HttpUtil;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.test.executor.Exec;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.InvalidParameterException;\n+import java.security.KeyStore;\n+import java.security.cert.Certificate;\n+import java.security.cert.CertificateFactory;\n+import java.util.Base64;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static io.strimzi.kafka.oauth.common.OAuthAuthenticator.urlencode;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaClientProperties  {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(KafkaClientProperties.class);\n+\n+    private String namespaceName;\n+    private String clusterName;\n+    private String caSecretName;\n+    private String kafkaUsername;\n+    private String securityProtocol;\n+    private Properties properties;\n+\n+    public static class KafkaClientPropertiesBuilder {\n+\n+        private Properties properties = new Properties();\n+        private String namespaceName;\n+        private String clusterName;\n+        private String caSecretName;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+\n+        public KafkaClientPropertiesBuilder withBootstrapServerConfig(String bootstrapServer) {\n+\n+            this.properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeySerializerConfig(String keySerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeyDeSerializerConfig(String keyDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueSerializerConfig(String valueSerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueDeSerializerConfig(String valueDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withMaxBlockMsConfig(String maxBlockMsConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, maxBlockMsConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClientIdConfig(String clientId) {\n+\n+            this.properties.setProperty(ProducerConfig.CLIENT_ID_CONFIG, clientId);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAcksConfig(String acksConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.ACKS_CONFIG, acksConfig);\n+            return this;\n+        }\n+\n+        // TODO: do we need separate Producer and Consumer config ??\n+        public KafkaClientPropertiesBuilder withGroupIdConfig(String groupIdConfig) {", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA3NjU1Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r398076556", "bodyText": "That's why i write commend here // TODO: do we need separate Producer and Consumer config ?? and i have three alternatives for this....\n1) we can create two inner classes  inside this whole class so we will have following hierarchy\n - KafkaClientProperties\n      - KafkaClientConsumerProperties\n      - KafkaClientProducerProperties\n2) create two separate classes \n3) create abstract class and two which will extend from that....\n\nAnyway i would change it after this huge PR.", "author": "see-quick", "createdAt": "2020-03-25T18:25:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE4ODk4Ng=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTUyODkzMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401528933", "bodyText": "Make sense to leave it for another PR. We can offline discuss the best solution", "author": "Frawless", "createdAt": "2020-04-01T10:56:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE4ODk4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE4OTc1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397189758", "bodyText": "Maybe there is some constant with this string?", "author": "Frawless", "createdAt": "2020-03-24T14:21:46Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientProperties.java", "diffHunk": "@@ -0,0 +1,430 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.kafka.oauth.common.HttpUtil;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.test.executor.Exec;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.InvalidParameterException;\n+import java.security.KeyStore;\n+import java.security.cert.Certificate;\n+import java.security.cert.CertificateFactory;\n+import java.util.Base64;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static io.strimzi.kafka.oauth.common.OAuthAuthenticator.urlencode;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaClientProperties  {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(KafkaClientProperties.class);\n+\n+    private String namespaceName;\n+    private String clusterName;\n+    private String caSecretName;\n+    private String kafkaUsername;\n+    private String securityProtocol;\n+    private Properties properties;\n+\n+    public static class KafkaClientPropertiesBuilder {\n+\n+        private Properties properties = new Properties();\n+        private String namespaceName;\n+        private String clusterName;\n+        private String caSecretName;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+\n+        public KafkaClientPropertiesBuilder withBootstrapServerConfig(String bootstrapServer) {\n+\n+            this.properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeySerializerConfig(String keySerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeyDeSerializerConfig(String keyDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueSerializerConfig(String valueSerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueDeSerializerConfig(String valueDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withMaxBlockMsConfig(String maxBlockMsConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, maxBlockMsConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClientIdConfig(String clientId) {\n+\n+            this.properties.setProperty(ProducerConfig.CLIENT_ID_CONFIG, clientId);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAcksConfig(String acksConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.ACKS_CONFIG, acksConfig);\n+            return this;\n+        }\n+\n+        // TODO: do we need separate Producer and Consumer config ??\n+        public KafkaClientPropertiesBuilder withGroupIdConfig(String groupIdConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupIdConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAutoOffsetResetConfig(String autoOffsetResetConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withNamespaceName(String namespaceName) {\n+\n+            this.namespaceName = namespaceName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClusterName(String clusterName) {\n+\n+            this.clusterName = clusterName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withCaSecretName(String caSecretName) {\n+\n+            this.caSecretName = caSecretName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKafkaUsername(String kafkaUsername) {\n+\n+            this.kafkaUsername = kafkaUsername;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSecurityProtocol(String securityProtocol) {\n+\n+            this.securityProtocol = securityProtocol;\n+            return this;\n+        }\n+\n+        // oauth properties\n+\n+        public KafkaClientPropertiesBuilder withSaslMechanismOauthBearer() {\n+\n+            this.properties.setProperty(\"sasl.mechanism\", \"OAUTHBEARER\");\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslLoginCallbackHandlerClass() {\n+\n+            this.properties.setProperty(\"sasl.login.callback.handler.class\", \"io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler\");\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfig(String clientId, String clientSecretName, String oauthTokenEndpointUri) {\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            this.properties.setProperty(\"sasl.jaas.config\",\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\";\");\n+\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfigAndTls(String clientId, String clientSecretName, String oauthTokenEndpointUri) throws IOException {\n+            importKeycloakCertificateToTruststore(properties);\n+            fixBadlyImportedAuthzSettings();\n+\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            this.properties.setProperty(\"sasl.jaas.config\",\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\" \" +\n+                    \"oauth.ssl.endpoint.identification.algorithm=\\\"\\\"\" +\n+                    \"oauth.ssl.truststore.location=\\\"\" + properties.get(\"ssl.truststore.location\") + \"\\\" \" +\n+                    \"oauth.ssl.truststore.password=\\\"\" + properties.get(\"ssl.truststore.password\") + \"\\\" \" +\n+                    \"oauth.ssl.truststore.type=\\\"\" + properties.get(\"ssl.truststore.type\") + \"\\\" ;\");\n+\n+            return this;\n+        }\n+\n+\n+        public KafkaClientProperties build() {\n+            return new KafkaClientProperties(this);\n+        }\n+    }\n+\n+    private KafkaClientProperties(KafkaClientPropertiesBuilder builder) {\n+\n+        this.properties = builder.properties;\n+        this.caSecretName = builder.caSecretName;\n+        this.kafkaUsername = builder.kafkaUsername;\n+        this.namespaceName = builder.namespaceName;\n+        this.clusterName = builder.clusterName;\n+        this.securityProtocol = builder.securityProtocol;\n+\n+        if (securityProtocol == null) securityProtocol = \"PLAINTEXT\";\n+        if (kafkaUsername == null) kafkaUsername = \"\";\n+\n+        this.properties.putAll(sharedClientProperties());\n+    }\n+\n+    /**\n+     * Create properties which are same pro producer and consumer\n+     * @return shared client properties\n+     */\n+    private Properties sharedClientProperties() {\n+        Properties properties = new Properties();\n+        // For turn off hostname verification\n+        properties.setProperty(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"\");\n+        properties.setProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n+\n+        try {\n+            if (!securityProtocol.equals(\"PLAINTEXT\")) {", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE5MDkzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397190937", "bodyText": "this is general method or for a specific reason? If it's for specific usecase, it should have different name I think", "author": "Frawless", "createdAt": "2020-03-24T14:23:11Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientProperties.java", "diffHunk": "@@ -0,0 +1,430 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.kafka.oauth.common.HttpUtil;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.test.executor.Exec;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.InvalidParameterException;\n+import java.security.KeyStore;\n+import java.security.cert.Certificate;\n+import java.security.cert.CertificateFactory;\n+import java.util.Base64;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static io.strimzi.kafka.oauth.common.OAuthAuthenticator.urlencode;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+public class KafkaClientProperties  {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(KafkaClientProperties.class);\n+\n+    private String namespaceName;\n+    private String clusterName;\n+    private String caSecretName;\n+    private String kafkaUsername;\n+    private String securityProtocol;\n+    private Properties properties;\n+\n+    public static class KafkaClientPropertiesBuilder {\n+\n+        private Properties properties = new Properties();\n+        private String namespaceName;\n+        private String clusterName;\n+        private String caSecretName;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+\n+        public KafkaClientPropertiesBuilder withBootstrapServerConfig(String bootstrapServer) {\n+\n+            this.properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeySerializerConfig(String keySerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeyDeSerializerConfig(String keyDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueSerializerConfig(String valueSerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueDeSerializerConfig(String valueDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withMaxBlockMsConfig(String maxBlockMsConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, maxBlockMsConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClientIdConfig(String clientId) {\n+\n+            this.properties.setProperty(ProducerConfig.CLIENT_ID_CONFIG, clientId);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAcksConfig(String acksConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.ACKS_CONFIG, acksConfig);\n+            return this;\n+        }\n+\n+        // TODO: do we need separate Producer and Consumer config ??\n+        public KafkaClientPropertiesBuilder withGroupIdConfig(String groupIdConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupIdConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAutoOffsetResetConfig(String autoOffsetResetConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withNamespaceName(String namespaceName) {\n+\n+            this.namespaceName = namespaceName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClusterName(String clusterName) {\n+\n+            this.clusterName = clusterName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withCaSecretName(String caSecretName) {\n+\n+            this.caSecretName = caSecretName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKafkaUsername(String kafkaUsername) {\n+\n+            this.kafkaUsername = kafkaUsername;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSecurityProtocol(String securityProtocol) {\n+\n+            this.securityProtocol = securityProtocol;\n+            return this;\n+        }\n+\n+        // oauth properties\n+\n+        public KafkaClientPropertiesBuilder withSaslMechanismOauthBearer() {\n+\n+            this.properties.setProperty(\"sasl.mechanism\", \"OAUTHBEARER\");\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslLoginCallbackHandlerClass() {\n+\n+            this.properties.setProperty(\"sasl.login.callback.handler.class\", \"io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler\");\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfig(String clientId, String clientSecretName, String oauthTokenEndpointUri) {\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            this.properties.setProperty(\"sasl.jaas.config\",\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\";\");\n+\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfigAndTls(String clientId, String clientSecretName, String oauthTokenEndpointUri) throws IOException {\n+            importKeycloakCertificateToTruststore(properties);\n+            fixBadlyImportedAuthzSettings();\n+\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            this.properties.setProperty(\"sasl.jaas.config\",\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\" \" +\n+                    \"oauth.ssl.endpoint.identification.algorithm=\\\"\\\"\" +\n+                    \"oauth.ssl.truststore.location=\\\"\" + properties.get(\"ssl.truststore.location\") + \"\\\" \" +\n+                    \"oauth.ssl.truststore.password=\\\"\" + properties.get(\"ssl.truststore.password\") + \"\\\" \" +\n+                    \"oauth.ssl.truststore.type=\\\"\" + properties.get(\"ssl.truststore.type\") + \"\\\" ;\");\n+\n+            return this;\n+        }\n+\n+\n+        public KafkaClientProperties build() {\n+            return new KafkaClientProperties(this);\n+        }\n+    }\n+\n+    private KafkaClientProperties(KafkaClientPropertiesBuilder builder) {\n+\n+        this.properties = builder.properties;\n+        this.caSecretName = builder.caSecretName;\n+        this.kafkaUsername = builder.kafkaUsername;\n+        this.namespaceName = builder.namespaceName;\n+        this.clusterName = builder.clusterName;\n+        this.securityProtocol = builder.securityProtocol;\n+\n+        if (securityProtocol == null) securityProtocol = \"PLAINTEXT\";\n+        if (kafkaUsername == null) kafkaUsername = \"\";\n+\n+        this.properties.putAll(sharedClientProperties());\n+    }\n+\n+    /**\n+     * Create properties which are same pro producer and consumer\n+     * @return shared client properties\n+     */\n+    private Properties sharedClientProperties() {\n+        Properties properties = new Properties();\n+        // For turn off hostname verification\n+        properties.setProperty(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"\");\n+        properties.setProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n+\n+        try {\n+            if (!securityProtocol.equals(\"PLAINTEXT\")) {\n+                Secret clusterCaCertSecret = kubeClient(namespaceName).getSecret(caSecretName);\n+                File tsFile = File.createTempFile(KafkaClientProperties.class.getName(), \".truststore\");\n+                String tsPassword = \"foo\";\n+                if (caSecretName.contains(\"custom-certificate\")) {\n+                    tsFile.deleteOnExit();\n+                    KeyStore ts = KeyStore.getInstance(KeyStore.getDefaultType());\n+                    ts.load(null, tsPassword.toCharArray());\n+                    CertificateFactory cf = CertificateFactory.getInstance(\"X.509\");\n+                    String clusterCaCert = kubeClient(namespaceName).getSecret(caSecretName).getData().get(\"ca.crt\");\n+                    Certificate cert = cf.generateCertificate(new ByteArrayInputStream(Base64.getDecoder().decode(clusterCaCert)));\n+                    ts.setCertificateEntry(\"ca.crt\", cert);\n+                    try (FileOutputStream tsOs = new FileOutputStream(tsFile)) {\n+                        ts.store(tsOs, tsPassword.toCharArray());\n+                    }\n+                    properties.setProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, KeyStore.getDefaultType());\n+                } else {\n+                    tsPassword = new String(Base64.getDecoder().decode(clusterCaCertSecret.getData().get(\"ca.password\")), StandardCharsets.US_ASCII);\n+                    String truststore = clusterCaCertSecret.getData().get(\"ca.p12\");\n+                    Files.write(tsFile.toPath(), Base64.getDecoder().decode(truststore));\n+                    tsFile.deleteOnExit();\n+                }\n+                properties.setProperty(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, \"PKCS12\");\n+                properties.setProperty(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, tsPassword);\n+                properties.setProperty(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, tsFile.getAbsolutePath());\n+            }\n+\n+            if (!kafkaUsername.isEmpty() && securityProtocol.equals(SecurityProtocol.SASL_SSL.name)) {\n+                properties.setProperty(SaslConfigs.SASL_MECHANISM, \"SCRAM-SHA-512\");\n+                Secret userSecret = kubeClient(namespaceName).getSecret(kafkaUsername);\n+                String password = new String(Base64.getDecoder().decode(userSecret.getData().get(\"password\")), StandardCharsets.UTF_8);\n+\n+                String jaasTemplate = \"org.apache.kafka.common.security.scram.ScramLoginModule required username=\\\"%s\\\" password=\\\"%s\\\";\";\n+                String jaasCfg = String.format(jaasTemplate, kafkaUsername, password);\n+\n+                properties.setProperty(SaslConfigs.SASL_JAAS_CONFIG, jaasCfg);\n+            } else if (!kafkaUsername.isEmpty()) {\n+\n+                Secret userSecret = kubeClient(namespaceName).getSecret(kafkaUsername);\n+\n+                String clientsCaCert = userSecret.getData().get(\"ca.crt\");\n+                LOGGER.debug(\"Clients CA cert: {}\", clientsCaCert);\n+\n+                String userCaCert = userSecret.getData().get(\"user.crt\");\n+                String userCaKey = userSecret.getData().get(\"user.key\");\n+                String ksPassword = \"foo\";\n+                properties.setProperty(SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, ksPassword);\n+                LOGGER.debug(\"User CA cert: {}\", userCaCert);\n+                LOGGER.debug(\"User CA key: {}\", userCaKey);\n+                File ksFile = createKeystore(Base64.getDecoder().decode(clientsCaCert),\n+                    Base64.getDecoder().decode(userCaCert),\n+                    Base64.getDecoder().decode(userCaKey),\n+                    ksPassword);\n+                properties.setProperty(SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, ksFile.getAbsolutePath());\n+                properties.setProperty(SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, \"PKCS12\");\n+            }\n+        } catch (Exception e) {\n+            throw new RuntimeException(e);\n+        }\n+\n+        return properties;\n+    }\n+    /**\n+     * Create keystore\n+     * @param ca certificate authority\n+     * @param cert certificate\n+     * @param key key\n+     * @param password password\n+     * @return keystore location as File\n+     * @throws IOException\n+     * @throws InterruptedException\n+     */\n+    @SuppressFBWarnings(\"RV_RETURN_VALUE_IGNORED_BAD_PRACTICE\")\n+    private static File createKeystore(byte[] ca, byte[] cert, byte[] key, String password) throws IOException, InterruptedException {\n+        File caFile = File.createTempFile(KafkaClientProperties.class.getName(), \".crt\");\n+        caFile.deleteOnExit();\n+        Files.write(caFile.toPath(), ca);\n+        File certFile = File.createTempFile(KafkaClientProperties.class.getName(), \".crt\");\n+        certFile.deleteOnExit();\n+        Files.write(certFile.toPath(), cert);\n+        File keyFile = File.createTempFile(KafkaClientProperties.class.getName(), \".key\");\n+        keyFile.deleteOnExit();\n+        Files.write(keyFile.toPath(), key);\n+        File keystore = File.createTempFile(KafkaClientProperties.class.getName(), \".keystore\");\n+        keystore.delete(); // Note horrible race condition, but this is only for testing\n+        // RANDFILE=/tmp/.rnd openssl pkcs12 -export -in $3 -inkey $4 -name $HOSTNAME -password pass:$2 -out $1\n+        if (new ProcessBuilder(\"openssl\",\n+                \"pkcs12\",\n+                \"-export\",\n+                \"-in\", certFile.getAbsolutePath(),\n+                \"-inkey\", keyFile.getAbsolutePath(),\n+                \"-chain\",\n+                \"-CAfile\", caFile.getAbsolutePath(),\n+                \"-name\", \"dfbdbd\",\n+                \"-password\", \"pass:\" + password,\n+                \"-out\", keystore.getAbsolutePath()).inheritIO().start().waitFor() != 0) {\n+            fail();\n+        }\n+        keystore.deleteOnExit();\n+        return keystore;\n+    }\n+\n+    private static void importKeycloakCertificateToTruststore(Properties clientProperties) throws IOException {\n+        String responseKeycloak = Exec.exec(\"openssl\", \"s_client\", \"-showcerts\", \"-connect\",\n+            ResourceManager.kubeClient().getNodeAddress() + \":\" + Constants.HTTPS_KEYCLOAK_DEFAULT_NODE_PORT).out();\n+        Matcher matcher = Pattern.compile(\"-----(?s)(.*)-----\").matcher(responseKeycloak);\n+\n+        if (matcher.find()) {\n+            String keycloakCertificateData = matcher.group(0);\n+            LOGGER.info(\"Keycloak cert is:{}\\n\", keycloakCertificateData);\n+\n+            LOGGER.info(\"Creating keycloak.crt file\");\n+            File keycloakCertFile = File.createTempFile(\"keycloak\", \".crt\");\n+            Files.write(keycloakCertFile.toPath(), keycloakCertificateData.getBytes(StandardCharsets.UTF_8));\n+\n+            LOGGER.info(\"Importing keycloak certificate {} to truststore\", keycloakCertFile.getAbsolutePath());\n+            Exec.exec(\"keytool\", \"-v\", \"-import\", \"-trustcacerts\", \"-file\", keycloakCertFile.getAbsolutePath(),\n+                \"-alias\", \"keycloakCrt1\", \"-keystore\", clientProperties.get(\"ssl.truststore.location\").toString(),\n+                \"-noprompt\", \"-storepass\", clientProperties.get(\"ssl.truststore.password\").toString());\n+        }\n+    }\n+\n+    /**\n+     * Use Keycloak Admin API to update Authorization Services 'decisionStrategy' on 'kafka' client to AFFIRMATIVE\n+     * link to bug -> https://issues.redhat.com/browse/KEYCLOAK-12640\n+     *\n+     * @throws IOException\n+     */\n+    static void fixBadlyImportedAuthzSettings() throws IOException {\n+        URI masterTokenEndpoint = URI.create(\"http://\" + ResourceManager.kubeClient().getNodeAddress() + \":\" + Constants.HTTP_KEYCLOAK_DEFAULT_NODE_PORT + \"/auth/realms/master/protocol/openid-connect/token\");\n+\n+        String token = loginWithUsernamePassword(masterTokenEndpoint,\n+            \"admin\", \"admin\", \"admin-cli\");\n+\n+        String authorization = \"Bearer \" + token;\n+\n+        // This is quite a round-about way but here it goes\n+\n+        // We first need to identify the 'id' of the 'kafka' client by fetching the clients\n+        JsonNode clients = HttpUtil.get(URI.create(\"http://\" + ResourceManager.kubeClient().getNodeAddress() + \":\" + Constants.HTTP_KEYCLOAK_DEFAULT_NODE_PORT + \"/auth/admin/realms/kafka-authz/clients\"),\n+            authorization, JsonNode.class);\n+\n+        String id = null;\n+\n+        // iterate over clients\n+        Iterator<JsonNode> it = clients.iterator();\n+        while (it.hasNext()) {\n+            JsonNode client = it.next();\n+            String clientId = client.get(\"clientId\").asText();\n+            if (\"kafka\".equals(clientId)) {\n+                id = client.get(\"id\").asText();\n+                break;\n+            }\n+        }\n+\n+        if (id == null) {\n+            throw new IllegalStateException(\"It seems that 'kafka' client isn't configured\");\n+        }\n+\n+        URI authzUri = URI.create(\"http://\" + ResourceManager.kubeClient().getNodeAddress() + \":\" + Constants.HTTP_KEYCLOAK_DEFAULT_NODE_PORT + \"/auth/admin/realms/kafka-authz/clients/\" + id + \"/authz/resource-server\");\n+\n+        // Now we fetch from this client's resource-server the current configuration\n+        ObjectNode authzConf = (ObjectNode) HttpUtil.get(authzUri, authorization, JsonNode.class);\n+\n+        // And we update the configuration and send it back\n+        authzConf.put(\"decisionStrategy\", \"AFFIRMATIVE\");\n+        HttpUtil.put(authzUri, authorization, \"application/json\", authzConf.toString());\n+    }\n+\n+    static String loginWithUsernamePassword(URI tokenEndpointUri, String username, String password, String clientId) throws IOException {", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE5OTAxMw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397199013", "bodyText": "I prefer to use multiline if, it's more readable fro m my POV", "author": "Frawless", "createdAt": "2020-03-24T14:33:20Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/VerifiableClient.java", "diffHunk": "@@ -28,21 +31,122 @@\n     private List<String> messages = new ArrayList<>();\n     private List<String> arguments = new ArrayList<>();\n     private String executable;\n-    private ClientType clientType;\n     private Exec executor;\n+\n+    private ClientType clientType;\n     private String podName;\n     private String podNamespace;\n+    private String bootstrapServer;\n+    private String topicName;\n+    private int maxMessages;\n+    private String kafkaUsername;\n+    private String consumerGroupName;\n+    private String consumerInstanceId;\n+    // TODO: verbose ?? consumerArguments.put(ClientArgument.VERBOSE, \"\");\n+    private ClientArgumentMap clientArgumentMap;\n \n-    /**\n-     * Constructor of verifiable kafka client\n-     *\n-     * @param clientType type of kafka client\n-     */\n-    public VerifiableClient(ClientType clientType, String podName, String podNamespace) {\n-        this.setAllowedArguments(clientType);\n-        this.clientType = clientType;\n-        this.podName = podName;\n-        this.podNamespace = podNamespace;\n+    public static class VerifiableClientBuilder {\n+\n+        private ClientType clientType;\n+        private String podName;\n+        private String podNamespace;\n+        private String bootstrapServer;\n+        private String topicName;\n+        private int maxMessages;\n+        private String kafkaUsername;\n+        private String consumerGroupName;\n+        private String consumerInstanceId;\n+\n+        public VerifiableClientBuilder withClientType(ClientType clientType) {\n+\n+            this.clientType = clientType;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withUsingPodName(String podName) {\n+\n+            this.podName = podName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withPodNamespace(String podNamespace) {\n+\n+            this.podNamespace = podNamespace;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withBootstrapServer(String bootstrapServer) {\n+\n+            this.bootstrapServer = bootstrapServer;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withTopicName(String topicName) {\n+\n+            this.topicName = topicName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withMaxMessages(int maxMessages) {\n+\n+            this.maxMessages = maxMessages;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withKafkaUsername(String kafkaUsername) {\n+\n+            this.kafkaUsername = kafkaUsername;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withConsumerGroupName(String consumerGroupName) {\n+\n+            this.consumerGroupName = consumerGroupName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withConsumerInstanceId(String consumerInstanceId) {\n+\n+            this.consumerInstanceId = consumerInstanceId;\n+            return this;\n+        }\n+\n+        protected VerifiableClient build() {\n+            return new VerifiableClient(this);\n+\n+        }\n+    }\n+\n+    public VerifiableClient(VerifiableClientBuilder verifiableClientBuilder) {\n+\n+        this.clientType = verifiableClientBuilder.clientType;\n+        this.podName = verifiableClientBuilder.podName;\n+        this.podNamespace = verifiableClientBuilder.podNamespace;\n+        this.bootstrapServer = verifiableClientBuilder.bootstrapServer;\n+        this.topicName = verifiableClientBuilder.topicName;\n+        this.maxMessages = verifiableClientBuilder.maxMessages;\n+        this.kafkaUsername = verifiableClientBuilder.kafkaUsername;\n+\n+        this.setAllowedArguments(this.clientType);\n+        this.clientArgumentMap = new ClientArgumentMap();\n+        this.clientArgumentMap.put(ClientArgument.BROKER_LIST, bootstrapServer);\n+        this.clientArgumentMap.put(ClientArgument.TOPIC, topicName);\n+        this.clientArgumentMap.put(ClientArgument.MAX_MESSAGES, Integer.toString(maxMessages));\n+        if (kafkaUsername != null) this.clientArgumentMap.put(ClientArgument.USER,  kafkaUsername.replace(\"-\", \"_\"));", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzE5OTc2MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397199761", "bodyText": "We don't support Kafka lower than 2.3 so we can remove this I think", "author": "Frawless", "createdAt": "2020-03-24T14:34:15Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/VerifiableClient.java", "diffHunk": "@@ -28,21 +31,122 @@\n     private List<String> messages = new ArrayList<>();\n     private List<String> arguments = new ArrayList<>();\n     private String executable;\n-    private ClientType clientType;\n     private Exec executor;\n+\n+    private ClientType clientType;\n     private String podName;\n     private String podNamespace;\n+    private String bootstrapServer;\n+    private String topicName;\n+    private int maxMessages;\n+    private String kafkaUsername;\n+    private String consumerGroupName;\n+    private String consumerInstanceId;\n+    // TODO: verbose ?? consumerArguments.put(ClientArgument.VERBOSE, \"\");\n+    private ClientArgumentMap clientArgumentMap;\n \n-    /**\n-     * Constructor of verifiable kafka client\n-     *\n-     * @param clientType type of kafka client\n-     */\n-    public VerifiableClient(ClientType clientType, String podName, String podNamespace) {\n-        this.setAllowedArguments(clientType);\n-        this.clientType = clientType;\n-        this.podName = podName;\n-        this.podNamespace = podNamespace;\n+    public static class VerifiableClientBuilder {\n+\n+        private ClientType clientType;\n+        private String podName;\n+        private String podNamespace;\n+        private String bootstrapServer;\n+        private String topicName;\n+        private int maxMessages;\n+        private String kafkaUsername;\n+        private String consumerGroupName;\n+        private String consumerInstanceId;\n+\n+        public VerifiableClientBuilder withClientType(ClientType clientType) {\n+\n+            this.clientType = clientType;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withUsingPodName(String podName) {\n+\n+            this.podName = podName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withPodNamespace(String podNamespace) {\n+\n+            this.podNamespace = podNamespace;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withBootstrapServer(String bootstrapServer) {\n+\n+            this.bootstrapServer = bootstrapServer;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withTopicName(String topicName) {\n+\n+            this.topicName = topicName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withMaxMessages(int maxMessages) {\n+\n+            this.maxMessages = maxMessages;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withKafkaUsername(String kafkaUsername) {\n+\n+            this.kafkaUsername = kafkaUsername;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withConsumerGroupName(String consumerGroupName) {\n+\n+            this.consumerGroupName = consumerGroupName;\n+            return this;\n+        }\n+\n+        public VerifiableClientBuilder withConsumerInstanceId(String consumerInstanceId) {\n+\n+            this.consumerInstanceId = consumerInstanceId;\n+            return this;\n+        }\n+\n+        protected VerifiableClient build() {\n+            return new VerifiableClient(this);\n+\n+        }\n+    }\n+\n+    public VerifiableClient(VerifiableClientBuilder verifiableClientBuilder) {\n+\n+        this.clientType = verifiableClientBuilder.clientType;\n+        this.podName = verifiableClientBuilder.podName;\n+        this.podNamespace = verifiableClientBuilder.podNamespace;\n+        this.bootstrapServer = verifiableClientBuilder.bootstrapServer;\n+        this.topicName = verifiableClientBuilder.topicName;\n+        this.maxMessages = verifiableClientBuilder.maxMessages;\n+        this.kafkaUsername = verifiableClientBuilder.kafkaUsername;\n+\n+        this.setAllowedArguments(this.clientType);\n+        this.clientArgumentMap = new ClientArgumentMap();\n+        this.clientArgumentMap.put(ClientArgument.BROKER_LIST, bootstrapServer);\n+        this.clientArgumentMap.put(ClientArgument.TOPIC, topicName);\n+        this.clientArgumentMap.put(ClientArgument.MAX_MESSAGES, Integer.toString(maxMessages));\n+        if (kafkaUsername != null) this.clientArgumentMap.put(ClientArgument.USER,  kafkaUsername.replace(\"-\", \"_\"));\n+        if (clientType == ClientType.CLI_KAFKA_VERIFIABLE_CONSUMER) {\n+            this.consumerGroupName = verifiableClientBuilder.consumerGroupName;\n+            this.consumerInstanceId = verifiableClientBuilder.consumerInstanceId;\n+\n+            this.clientArgumentMap.put(ClientArgument.GROUP_ID, consumerGroupName);\n+\n+            String image = kubeClient().getPod(this.podName).getSpec().getContainers().get(0).getImage();\n+            String clientVersion = image.substring(image.length() - 5);\n+\n+            if (allowParameter(\"2.3.0\", clientVersion)) {", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwMTM5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397201394", "bodyText": "Maybe it's time to create some generator for random topic names, user names, group names, etc.", "author": "Frawless", "createdAt": "2020-03-24T14:36:27Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/ConnectS2IST.java", "diffHunk": "@@ -209,9 +211,17 @@ void testSecretsWithKafkaConnectS2IWithTlsAndScramShaAuthentication() {\n         KafkaClientsResource.deployKafkaClients(true, CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS, user).done();\n \n         final String defaultKafkaClientsPodName =\n-                ResourceManager.kubeClient().listPodsByPrefixInName(CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS).get(0).getMetadata().getName();\n-\n-        internalKafkaClient.setPodName(defaultKafkaClientsPodName);\n+            ResourceManager.kubeClient().listPodsByPrefixInName(CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS).get(0).getMetadata().getName();\n+\n+        InternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n+            .withUsingPodName(defaultKafkaClientsPodName)\n+            .withTopicName(CONNECT_S2I_TOPIC_NAME)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(\"TLS\")\n+            .withConsumerGroupName(\"my-group-\" + new Random().nextInt(Integer.MAX_VALUE))", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA4MTQ0Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r398081447", "bodyText": "sure, we can create some issue for that as enhancement ))", "author": "see-quick", "createdAt": "2020-03-25T18:33:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwMTM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA5MjA4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r398092084", "bodyText": "anyway, it would be better just to encapsulate this consumer-group-name and have two options:\n\nif specified create that with the concrete name\nif not specify create the random one", "author": "see-quick", "createdAt": "2020-03-25T18:50:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwMTM5NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTUyOTE3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401529170", "bodyText": "Agree", "author": "Frawless", "createdAt": "2020-04-01T10:57:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwMTM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwNDUwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397204504", "bodyText": "Use KafkaResources from api module to fill bootstrap", "author": "Frawless", "createdAt": "2020-03-24T14:40:30Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/InternalKafkaClient.java", "diffHunk": "@@ -16,267 +17,177 @@\n \n import static io.strimzi.systemtest.kafkaclients.internalClients.ClientType.CLI_KAFKA_VERIFIABLE_CONSUMER;\n import static io.strimzi.systemtest.kafkaclients.internalClients.ClientType.CLI_KAFKA_VERIFIABLE_PRODUCER;\n-import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n import static org.hamcrest.MatcherAssert.assertThat;\n \n /**\n  * The InternalKafkaClient for sending and receiving messages using basic properties.\n  * The client is using an internal listeners and communicate from the pod.\n  */\n-public class InternalKafkaClient implements IKafkaClient<Integer> {\n+public class InternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Integer> {\n \n     private static final Logger LOGGER = LogManager.getLogger(InternalKafkaClient.class);\n \n-    private int sent;\n-    private int received;\n-    private Random rng;\n     private String podName;\n \n-    public InternalKafkaClient() {\n-        this.sent = 0;\n-        this.received = 0;\n-        this.rng = new Random();\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        private String podName;\n+\n+        public Builder withUsingPodName(String podName) {\n+            this.podName = podName;\n+            return self();\n+        }\n+\n+        @Override\n+        public InternalKafkaClient build() {\n+            return new InternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+            return this;\n+        }\n+    }\n+\n+    private InternalKafkaClient(Builder builder) {\n+        super(builder);\n+        podName = builder.podName;\n+    }\n+\n+    public Integer sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n     }\n \n     /**\n      * Method for send messages to specific kafka cluster. It uses test-client API for communication with deployed clients inside kubernetes cluster\n-     * @param topicName topic name\n-     * @param namespace namespace\n-     * @param clusterName cluster name\n-     * @param messageCount messages count\n-     * @param securityProtocol option for tls listener inside kafka cluster\n      * @return count of send and acknowledged messages\n      */\n-    private Integer sendMessages(String topicName, String namespace, String clusterName, String kafkaUsername,\n-                                 int messageCount, String securityProtocol, String podName, long timeoutMs) {\n-        String bootstrapServer = securityProtocol.equals(\"TLS\") ?\n-                clusterName + \"-kafka-bootstrap:9093\" : clusterName + \"-kafka-bootstrap:9092\";\n-        ClientArgumentMap producerArguments = new ClientArgumentMap();\n-        producerArguments.put(ClientArgument.BROKER_LIST, bootstrapServer);\n-        producerArguments.put(ClientArgument.TOPIC, topicName);\n-        producerArguments.put(ClientArgument.MAX_MESSAGES, Integer.toString(messageCount));\n-\n-        VerifiableClient producer = new VerifiableClient(CLI_KAFKA_VERIFIABLE_PRODUCER,\n-            podName,\n-            namespace);\n-\n-        if (kafkaUsername != null) {\n-            producerArguments.put(ClientArgument.USER, kafkaUsername.replace(\"-\", \"_\"));\n-        }\n+    @Override\n+    public Integer sendMessagesPlain(long timeout) {\n+\n+        VerifiableClient producer = new VerifiableClient.VerifiableClientBuilder()\n+            .withClientType(CLI_KAFKA_VERIFIABLE_PRODUCER)\n+            .withUsingPodName(podName)\n+            .withPodNamespace(namespaceName)\n+            .withMaxMessages(messageCount)\n+            .withKafkaUsername(kafkaUsername)\n+            .withBootstrapServer(clusterName + \"-kafka-bootstrap:9092\")", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwNDY5NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397204694", "bodyText": "Use KafkaResources from api module to fill bootstrap", "author": "Frawless", "createdAt": "2020-03-24T14:40:44Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/InternalKafkaClient.java", "diffHunk": "@@ -16,267 +17,177 @@\n \n import static io.strimzi.systemtest.kafkaclients.internalClients.ClientType.CLI_KAFKA_VERIFIABLE_CONSUMER;\n import static io.strimzi.systemtest.kafkaclients.internalClients.ClientType.CLI_KAFKA_VERIFIABLE_PRODUCER;\n-import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n import static org.hamcrest.MatcherAssert.assertThat;\n \n /**\n  * The InternalKafkaClient for sending and receiving messages using basic properties.\n  * The client is using an internal listeners and communicate from the pod.\n  */\n-public class InternalKafkaClient implements IKafkaClient<Integer> {\n+public class InternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Integer> {\n \n     private static final Logger LOGGER = LogManager.getLogger(InternalKafkaClient.class);\n \n-    private int sent;\n-    private int received;\n-    private Random rng;\n     private String podName;\n \n-    public InternalKafkaClient() {\n-        this.sent = 0;\n-        this.received = 0;\n-        this.rng = new Random();\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        private String podName;\n+\n+        public Builder withUsingPodName(String podName) {\n+            this.podName = podName;\n+            return self();\n+        }\n+\n+        @Override\n+        public InternalKafkaClient build() {\n+            return new InternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+            return this;\n+        }\n+    }\n+\n+    private InternalKafkaClient(Builder builder) {\n+        super(builder);\n+        podName = builder.podName;\n+    }\n+\n+    public Integer sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n     }\n \n     /**\n      * Method for send messages to specific kafka cluster. It uses test-client API for communication with deployed clients inside kubernetes cluster\n-     * @param topicName topic name\n-     * @param namespace namespace\n-     * @param clusterName cluster name\n-     * @param messageCount messages count\n-     * @param securityProtocol option for tls listener inside kafka cluster\n      * @return count of send and acknowledged messages\n      */\n-    private Integer sendMessages(String topicName, String namespace, String clusterName, String kafkaUsername,\n-                                 int messageCount, String securityProtocol, String podName, long timeoutMs) {\n-        String bootstrapServer = securityProtocol.equals(\"TLS\") ?\n-                clusterName + \"-kafka-bootstrap:9093\" : clusterName + \"-kafka-bootstrap:9092\";\n-        ClientArgumentMap producerArguments = new ClientArgumentMap();\n-        producerArguments.put(ClientArgument.BROKER_LIST, bootstrapServer);\n-        producerArguments.put(ClientArgument.TOPIC, topicName);\n-        producerArguments.put(ClientArgument.MAX_MESSAGES, Integer.toString(messageCount));\n-\n-        VerifiableClient producer = new VerifiableClient(CLI_KAFKA_VERIFIABLE_PRODUCER,\n-            podName,\n-            namespace);\n-\n-        if (kafkaUsername != null) {\n-            producerArguments.put(ClientArgument.USER, kafkaUsername.replace(\"-\", \"_\"));\n-        }\n+    @Override\n+    public Integer sendMessagesPlain(long timeout) {\n+\n+        VerifiableClient producer = new VerifiableClient.VerifiableClientBuilder()\n+            .withClientType(CLI_KAFKA_VERIFIABLE_PRODUCER)\n+            .withUsingPodName(podName)\n+            .withPodNamespace(namespaceName)\n+            .withMaxMessages(messageCount)\n+            .withKafkaUsername(kafkaUsername)\n+            .withBootstrapServer(clusterName + \"-kafka-bootstrap:9092\")\n+            .withTopicName(topicName)\n+            .build();\n \n-        producer.setArguments(producerArguments);\n-        LOGGER.info(\"Sending {} messages to {}#{}\", messageCount, bootstrapServer, topicName);\n+        LOGGER.info(\"Sending {} messages to {}#{}\", messageCount, producer.getBootstrapServer(), topicName);\n \n         TestUtils.waitFor(\"Sending messages\", Constants.PRODUCER_POLL_INTERVAL, Constants.GLOBAL_CLIENTS_TIMEOUT, () -> {\n             LOGGER.info(\"Sending {} messages to {}\", messageCount, podName);\n             producer.run(Constants.PRODUCER_TIMEOUT);\n-            sent = getSentMessagesCount(producer.getMessages().toString(), messageCount);\n+            int sent = getSentMessagesCount(producer.getMessages().toString(), messageCount);\n             return sent == messageCount;\n         });\n \n-        sent = getSentMessagesCount(producer.getMessages().toString(), messageCount);\n+        int sent = getSentMessagesCount(producer.getMessages().toString(), messageCount);\n \n         LOGGER.info(\"Producer produced {} messages\", sent);\n \n         return sent;\n     }\n \n-    /**\n-     * Method for send messages to specific kafka cluster. It uses test-client API for communication with deployed clients inside kubernetes cluster\n-     * @param topicName topic name\n-     * @param namespace namespace\n-     * @param clusterName cluster name\n-     * @param messageCount messages count\n-     * @param securityProtocol option for tls listener inside kafka cluster\n-     * @return count of send and acknowledged messages\n-     */\n-    public Integer sendMessagesTls(String topicName, String namespace, String clusterName, String kafkaUsername,\n-                                   int messageCount, String securityProtocol) {\n-        LOGGER.info(\"Sending messages to from: {}\", this.podName);\n-        return sendMessages(topicName, namespace, clusterName, kafkaUsername, messageCount, securityProtocol,\n-                this.podName, Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    public Integer sendMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n     }\n \n-    /**\n-     * Method for send messages to specific kafka cluster. It uses test-client API for communication with deployed clients inside kubernetes cluster\n-     * @param topicName topic name\n-     * @param namespace namespace\n-     * @param clusterName cluster name\n-     * @param messageCount messages count\n-     * @param securityProtocol option for tls listener inside kafka cluster\n-     * @return count of send and acknowledged messages\n-     */\n     @Override\n-    public Integer sendMessagesTls(String topicName, String namespace, String clusterName, String kafkaUsername,\n-                                   int messageCount, String securityProtocol, long timeoutMs) throws RuntimeException {\n-        LOGGER.info(\"Sending messages to pod: {}\", this.podName);\n-        return sendMessages(topicName, namespace, clusterName, kafkaUsername, messageCount, securityProtocol, this.podName,\n-                timeoutMs);\n-    }\n+    public Integer sendMessagesTls(long timeout) {\n \n-    /**\n-     * Method for send messages to specific kafka cluster. It uses test-client API for communication with deployed clients inside kubernetes cluster\n-     * @param topicName topic name\n-     * @param namespace namespace\n-     * @param clusterName cluster name\n-     * @param messageCount messages count\n-     * @return count of send and acknowledged messages\n-     */\n-    public Integer sendMessages(String topicName, String namespace, String clusterName, int messageCount) throws RuntimeException {\n-        return sendMessagesTls(topicName, namespace, clusterName, null, messageCount, \"PLAIN\",\n-                Constants.GLOBAL_CLIENTS_TIMEOUT);\n-    }\n+        VerifiableClient producerTls = new VerifiableClient.VerifiableClientBuilder()\n+            .withClientType(CLI_KAFKA_VERIFIABLE_PRODUCER)\n+            .withUsingPodName(podName)\n+            .withPodNamespace(namespaceName)\n+            .withMaxMessages(messageCount)\n+            .withKafkaUsername(kafkaUsername)\n+            .withBootstrapServer(clusterName + \"-kafka-bootstrap:9093\")", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIwNTM0NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397205345", "bodyText": "This is redundant", "author": "Frawless", "createdAt": "2020-03-24T14:41:33Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/AllNamespaceST.java", "diffHunk": "@@ -163,13 +164,23 @@ void testUserInDifferentNamespace() {\n         final String defaultKafkaClientsPodName =\n                 ResourceManager.kubeClient().listPodsByPrefixInName(CLUSTER_NAME + \"-\" + Constants.KAFKA_CLIENTS).get(0).getMetadata().getName();\n \n-        internalKafkaClient.setPodName(defaultKafkaClientsPodName);\n+        InternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n+            .withUsingPodName(defaultKafkaClientsPodName)\n+            .withTopicName(TOPIC_NAME)\n+            .withNamespaceName(THIRD_NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withSecurityProtocol(\"TLS\")", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIyODYzNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r397228636", "bodyText": "WHy did you change the indent?", "author": "Frawless", "createdAt": "2020-03-24T15:10:33Z", "path": "systemtest/src/test/java/io/strimzi/systemtest/kafka/ListenersST.java", "diffHunk": "@@ -540,40 +741,48 @@ void testCustomCertLoadBalancerAndTlsRollingUpdate() throws Exception {\n         //External secret cert is same as internal in this case\n         assertThat(externalSecretCerts, is(internalCerts));\n \n-        Future producer = externalBasicKafkaClient.sendMessagesTls(topicName, NAMESPACE, CLUSTER_NAME, userName, 10, \"SSL\");\n-        Future consumer = externalBasicKafkaClient.receiveMessagesTls(topicName, NAMESPACE, CLUSTER_NAME, userName, 10, \"SSL\");\n+        BasicExternalKafkaClient basicExternalKafkaClient = new BasicExternalKafkaClient.Builder()\n+            .withTopicName(topicName)\n+            .withNamespaceName(NAMESPACE)\n+            .withClusterName(CLUSTER_NAME)\n+            .withKafkaUsername(userName)\n+            .withMessageCount(MESSAGE_COUNT)\n+            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n+            .withSecurityProtocol(\"SSL\")\n+            .build();\n \n-        assertThat(\"Producer didn't produce all messages\", producer.get(Constants.GLOBAL_CLIENTS_TIMEOUT, TimeUnit.MILLISECONDS), is(10));\n-        assertThat(\"Consumer didn't consume all messages\", consumer.get(Constants.GLOBAL_CLIENTS_TIMEOUT, TimeUnit.MILLISECONDS), is(10));\n+        Future<Integer> producer = basicExternalKafkaClient.sendMessagesPlain();\n+        Future<Integer> consumer = basicExternalKafkaClient.receiveMessagesPlain();\n+\n+        assertThat(\"Producer didn't produce all messages\", producer.get(Constants.GLOBAL_CLIENTS_TIMEOUT, TimeUnit.MILLISECONDS), is(MESSAGE_COUNT));\n+        assertThat(\"Consumer didn't consume all messages\", consumer.get(Constants.GLOBAL_CLIENTS_TIMEOUT, TimeUnit.MILLISECONDS), is(MESSAGE_COUNT));\n \n         Map<String, String> kafkaSnapshot = StatefulSetUtils.ssSnapshot(KafkaResources.kafkaStatefulSetName(CLUSTER_NAME));\n \n         KafkaResource.replaceKafkaResource(CLUSTER_NAME, kafka -> {\n-            kafka.getSpec().getKafka().getListeners().setExternal(new KafkaListenerExternalLoadBalancerBuilder()\n+            kafka.getSpec().getKafka().getListeners().setExternal(new KafkaListenerExternalNodePortBuilder()\n                 .withNewConfiguration()\n-                    .withNewBrokerCertChainAndKey()\n-                        .withSecretName(customCertServer1)\n-                        .withKey(\"ca.key\")\n-                        .withCertificate(\"ca.crt\")\n-                    .endBrokerCertChainAndKey()\n+                .withNewBrokerCertChainAndKey()\n+                .withSecretName(customCertServer1)\n+                .withKey(\"ca.key\")\n+                .withCertificate(\"ca.crt\")\n+                .endBrokerCertChainAndKey()", "originalCommit": "6af1d053032f26f1ec90d09420fa565d47826cdf", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA3OTcxOQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r398079719", "bodyText": "sorry...copy paste make sometimes problems )))", "author": "see-quick", "createdAt": "2020-03-25T18:30:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzIyODYzNg=="}], "type": "inlineReview"}, {"oid": "76c7e44ce587e2caf571ca64de44298e359357a2", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/76c7e44ce587e2caf571ca64de44298e359357a2", "message": "[MO] - [rebase] -> another 10000000 times rebases love it!\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-03-31T15:16:30Z", "type": "forcePushed"}, {"oid": "fe784dbb67cd2709e817197b59d87285d4db8c13", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fe784dbb67cd2709e817197b59d87285d4db8c13", "message": "[MO] - [system test] -> deencapsulation of basic client\n\nSigned-off-by: Seequick1 <morsak@redhat.com>", "committedDate": "2020-03-31T15:18:23Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU2NTE2MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401565160", "bodyText": "Do we need this now?", "author": "Frawless", "createdAt": "2020-04-01T12:08:44Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/VerifiableClient.java", "diffHunk": "@@ -84,6 +182,18 @@ public void setArguments(ClientArgumentMap args) {\n         }\n     }\n \n+    private boolean allowParameter(String minimalVersion, String clientVersion) {", "originalCommit": "fe784dbb67cd2709e817197b59d87285d4db8c13", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU4MzM1NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401583355", "bodyText": "spot-bugs already did this spot :D but nice catch \ud83d\udc4d", "author": "see-quick", "createdAt": "2020-04-01T12:40:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU2NTE2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTU2NTQwNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401565404", "bodyText": "Remove or?", "author": "Frawless", "createdAt": "2020-04-01T12:09:10Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/VerifiableClient.java", "diffHunk": "@@ -28,21 +30,117 @@\n     private List<String> messages = new ArrayList<>();\n     private List<String> arguments = new ArrayList<>();\n     private String executable;\n-    private ClientType clientType;\n     private Exec executor;\n+\n+    private ClientType clientType;\n     private String podName;\n     private String podNamespace;\n+    private String bootstrapServer;\n+    private String topicName;\n+    private int maxMessages;\n+    private String kafkaUsername;\n+    private String consumerGroupName;\n+    private String consumerInstanceId;\n+    // TODO: verbose ?? consumerArguments.put(ClientArgument.VERBOSE, \"\");", "originalCommit": "fe784dbb67cd2709e817197b59d87285d4db8c13", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "50fbd92b44d8acfc716931e9d12a5242367613fa", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/50fbd92b44d8acfc716931e9d12a5242367613fa", "message": "[MO] - [rebase] -> finally last!!! i hope\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-01T14:16:24Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NTQ0MQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401655441", "bodyText": "What if it's negative?", "author": "tombentley", "createdAt": "2020-04-01T14:23:56Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import io.fabric8.kubernetes.api.model.LoadBalancerIngress;\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.fabric8.openshift.api.model.Route;\n+import io.fabric8.openshift.client.OpenShiftClient;\n+\n+import java.security.InvalidParameterException;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.externalBootstrapServiceName;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public abstract class AbstractKafkaClient {\n+\n+    protected String topicName;\n+    protected String namespaceName;\n+    protected String clusterName;\n+    protected int messageCount;\n+    protected String consumerGroup;\n+    protected String kafkaUsername;\n+    protected String securityProtocol;\n+    protected String caCertName;\n+    protected KafkaClientProperties clientProperties;\n+\n+    public abstract static class Builder<T extends Builder<T>> {\n+\n+        private String topicName;\n+        private String namespaceName;\n+        private String clusterName;\n+        private int messageCount;\n+        private String consumerGroup;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+        private String caCertName;\n+        private KafkaClientProperties clientProperties;\n+\n+        public T withTopicName(String topicName) {\n+            this.topicName = topicName;\n+            return self();\n+        }\n+\n+        public T withNamespaceName(String namespaceName) {\n+            this.namespaceName = namespaceName;\n+            return self();\n+        }\n+\n+        public T withClusterName(String clusterName) {\n+            this.clusterName = clusterName;\n+            return self();\n+        }\n+\n+        public T withMessageCount(int messageCount) {\n+            this.messageCount = messageCount;\n+            return self();\n+        }\n+\n+        public T withConsumerGroupName(String consumerGroup) {\n+            this.consumerGroup = consumerGroup;\n+            return self();\n+        }\n+\n+        public T withKafkaUsername(String kafkaUsername) {\n+            this.kafkaUsername = kafkaUsername;\n+            return self();\n+        }\n+\n+        public T withSecurityProtocol(String securityProtocol) {\n+            this.securityProtocol = securityProtocol;\n+            return self();\n+        }\n+\n+        public T withCertificateAuthorityCertificateName(String caCertName) {\n+            this.caCertName = caCertName;\n+            return self();\n+        }\n+\n+        public T withKafkaClientProperties(KafkaClientProperties clientProperties) {\n+            this.clientProperties = clientProperties;\n+            return self();\n+        }\n+\n+        protected abstract AbstractKafkaClient build();\n+\n+        // Subclasses must override this method to return \"this\" protected abstract T self();\n+        // for not explicit casting..\n+        protected abstract T self();\n+    }\n+\n+    protected AbstractKafkaClient(Builder<?> builder) {\n+\n+        if (builder.topicName == null) throw new InvalidParameterException(\"Topic name is not set.\");\n+        if (builder.namespaceName == null) throw new InvalidParameterException(\"Namespace name is not set.\");\n+        if (builder.clusterName == null) throw  new InvalidParameterException(\"Cluster name is not set.\");\n+        if (builder.messageCount == 0) throw  new InvalidParameterException(\"Message count is set to 0\");", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgxODk0MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401818940", "bodyText": "fair point )", "author": "see-quick", "createdAt": "2020-04-01T18:23:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NTQ0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NTU0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401655548", "bodyText": "What if they're empty?", "author": "tombentley", "createdAt": "2020-04-01T14:24:04Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import io.fabric8.kubernetes.api.model.LoadBalancerIngress;\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.fabric8.openshift.api.model.Route;\n+import io.fabric8.openshift.client.OpenShiftClient;\n+\n+import java.security.InvalidParameterException;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.externalBootstrapServiceName;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public abstract class AbstractKafkaClient {\n+\n+    protected String topicName;\n+    protected String namespaceName;\n+    protected String clusterName;\n+    protected int messageCount;\n+    protected String consumerGroup;\n+    protected String kafkaUsername;\n+    protected String securityProtocol;\n+    protected String caCertName;\n+    protected KafkaClientProperties clientProperties;\n+\n+    public abstract static class Builder<T extends Builder<T>> {\n+\n+        private String topicName;\n+        private String namespaceName;\n+        private String clusterName;\n+        private int messageCount;\n+        private String consumerGroup;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+        private String caCertName;\n+        private KafkaClientProperties clientProperties;\n+\n+        public T withTopicName(String topicName) {\n+            this.topicName = topicName;\n+            return self();\n+        }\n+\n+        public T withNamespaceName(String namespaceName) {\n+            this.namespaceName = namespaceName;\n+            return self();\n+        }\n+\n+        public T withClusterName(String clusterName) {\n+            this.clusterName = clusterName;\n+            return self();\n+        }\n+\n+        public T withMessageCount(int messageCount) {\n+            this.messageCount = messageCount;\n+            return self();\n+        }\n+\n+        public T withConsumerGroupName(String consumerGroup) {\n+            this.consumerGroup = consumerGroup;\n+            return self();\n+        }\n+\n+        public T withKafkaUsername(String kafkaUsername) {\n+            this.kafkaUsername = kafkaUsername;\n+            return self();\n+        }\n+\n+        public T withSecurityProtocol(String securityProtocol) {\n+            this.securityProtocol = securityProtocol;\n+            return self();\n+        }\n+\n+        public T withCertificateAuthorityCertificateName(String caCertName) {\n+            this.caCertName = caCertName;\n+            return self();\n+        }\n+\n+        public T withKafkaClientProperties(KafkaClientProperties clientProperties) {\n+            this.clientProperties = clientProperties;\n+            return self();\n+        }\n+\n+        protected abstract AbstractKafkaClient build();\n+\n+        // Subclasses must override this method to return \"this\" protected abstract T self();\n+        // for not explicit casting..\n+        protected abstract T self();\n+    }\n+\n+    protected AbstractKafkaClient(Builder<?> builder) {\n+\n+        if (builder.topicName == null) throw new InvalidParameterException(\"Topic name is not set.\");\n+        if (builder.namespaceName == null) throw new InvalidParameterException(\"Namespace name is not set.\");\n+        if (builder.clusterName == null) throw  new InvalidParameterException(\"Cluster name is not set.\");", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NjE4MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401656180", "bodyText": "You've got a builder, so why can't these be final?", "author": "tombentley", "createdAt": "2020-04-01T14:24:53Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import io.fabric8.kubernetes.api.model.LoadBalancerIngress;\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.fabric8.openshift.api.model.Route;\n+import io.fabric8.openshift.client.OpenShiftClient;\n+\n+import java.security.InvalidParameterException;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.externalBootstrapServiceName;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public abstract class AbstractKafkaClient {\n+\n+    protected String topicName;\n+    protected String namespaceName;\n+    protected String clusterName;\n+    protected int messageCount;\n+    protected String consumerGroup;\n+    protected String kafkaUsername;\n+    protected String securityProtocol;\n+    protected String caCertName;\n+    protected KafkaClientProperties clientProperties;", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgyNTIyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401825220", "bodyText": "yeah, that's a problem that come in my mind. If you take a close look on how I am using the client for instance:\nInternalKafkaClient internalKafkaClient = new InternalKafkaClient.Builder()\n            .withUsingPodName(kafkaClientsPodName)\n            .withTopicName(\"my-topic-test-3\")\n            .withNamespaceName(NAMESPACE)\n            .withClusterName(kafkaClusterSourceName)\n            .withKafkaUsername(userSource.getMetadata().getName())\n            .withMessageCount(messagesCount)\n            .withConsumerGroupName(CONSUMER_GROUP_NAME + \"-\" + rng.nextInt(Integer.MAX_VALUE))\n            .build();\n\nand then after a few lines, I need to set some of the attributes to the new value\ninternalKafkaClient.setClusterName(kafkaClusterTargetName); internalKafkaClient.setKafkaUsername(userTarget.getMetadata().getName());\nand that's the problem why these properties some of them can't be final. Sure I can rewrite some of them to the final, which is not changed by the setter but that I consider as a not proper way :)", "author": "see-quick", "createdAt": "2020-04-01T18:33:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMjg3OQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402112879", "bodyText": "It seems strange to me that you want the client first to point to one cluster and then another. It would be more natural if a client, once created, was immutable and you use two clients instance. What you need is to be able to base a builder's state on a pre-existing client, and then tweak it with a few more with() calls, so you can build the 2nd client conveniently.", "author": "tombentley", "createdAt": "2020-04-02T07:43:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE1MjUxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402152511", "bodyText": "In the next PR, I want to create some default clients, which will satisfy it. Also, I agree, with immutalibity and will do it.", "author": "see-quick", "createdAt": "2020-04-02T08:51:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NjE4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjE2NTU5MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402165590", "bodyText": "I have already created a task for that.", "author": "see-quick", "createdAt": "2020-04-02T09:11:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NjE4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NzU4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401657584", "bodyText": "So how would you test non-Route external connections on OpenShift?", "author": "tombentley", "createdAt": "2020-04-01T14:26:45Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import io.fabric8.kubernetes.api.model.LoadBalancerIngress;\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.fabric8.openshift.api.model.Route;\n+import io.fabric8.openshift.client.OpenShiftClient;\n+\n+import java.security.InvalidParameterException;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.externalBootstrapServiceName;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public abstract class AbstractKafkaClient {\n+\n+    protected String topicName;\n+    protected String namespaceName;\n+    protected String clusterName;\n+    protected int messageCount;\n+    protected String consumerGroup;\n+    protected String kafkaUsername;\n+    protected String securityProtocol;\n+    protected String caCertName;\n+    protected KafkaClientProperties clientProperties;\n+\n+    public abstract static class Builder<T extends Builder<T>> {\n+\n+        private String topicName;\n+        private String namespaceName;\n+        private String clusterName;\n+        private int messageCount;\n+        private String consumerGroup;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+        private String caCertName;\n+        private KafkaClientProperties clientProperties;\n+\n+        public T withTopicName(String topicName) {\n+            this.topicName = topicName;\n+            return self();\n+        }\n+\n+        public T withNamespaceName(String namespaceName) {\n+            this.namespaceName = namespaceName;\n+            return self();\n+        }\n+\n+        public T withClusterName(String clusterName) {\n+            this.clusterName = clusterName;\n+            return self();\n+        }\n+\n+        public T withMessageCount(int messageCount) {\n+            this.messageCount = messageCount;\n+            return self();\n+        }\n+\n+        public T withConsumerGroupName(String consumerGroup) {\n+            this.consumerGroup = consumerGroup;\n+            return self();\n+        }\n+\n+        public T withKafkaUsername(String kafkaUsername) {\n+            this.kafkaUsername = kafkaUsername;\n+            return self();\n+        }\n+\n+        public T withSecurityProtocol(String securityProtocol) {\n+            this.securityProtocol = securityProtocol;\n+            return self();\n+        }\n+\n+        public T withCertificateAuthorityCertificateName(String caCertName) {\n+            this.caCertName = caCertName;\n+            return self();\n+        }\n+\n+        public T withKafkaClientProperties(KafkaClientProperties clientProperties) {\n+            this.clientProperties = clientProperties;\n+            return self();\n+        }\n+\n+        protected abstract AbstractKafkaClient build();\n+\n+        // Subclasses must override this method to return \"this\" protected abstract T self();\n+        // for not explicit casting..\n+        protected abstract T self();\n+    }\n+\n+    protected AbstractKafkaClient(Builder<?> builder) {\n+\n+        if (builder.topicName == null) throw new InvalidParameterException(\"Topic name is not set.\");\n+        if (builder.namespaceName == null) throw new InvalidParameterException(\"Namespace name is not set.\");\n+        if (builder.clusterName == null) throw  new InvalidParameterException(\"Cluster name is not set.\");\n+        if (builder.messageCount == 0) throw  new InvalidParameterException(\"Message count is set to 0\");\n+\n+        topicName = builder.topicName;\n+        namespaceName = builder.namespaceName;\n+        clusterName = builder.clusterName;\n+        messageCount = builder.messageCount;\n+        consumerGroup = builder.consumerGroup;\n+        kafkaUsername = builder.kafkaUsername;\n+        securityProtocol = builder.securityProtocol;\n+        caCertName = builder.caCertName;\n+        clientProperties = builder.clientProperties;\n+    }\n+\n+    public void setMessageCount(int messageCount) {\n+        this.messageCount = messageCount;\n+    }\n+\n+    protected boolean verifyProducedAndConsumedMessages(int producedMessages, int consumedMessages) {\n+        return producedMessages == consumedMessages;\n+    }\n+    /**\n+     * Get external bootstrap connection\n+     * @param namespace kafka namespace\n+     * @param clusterName kafka cluster name\n+     * @return bootstrap url as string\n+     */\n+    protected static String getExternalBootstrapConnect(String namespace, String clusterName) {\n+        if (kubeClient(namespace).getClient().isAdaptable(OpenShiftClient.class)) {\n+            Route route = kubeClient(namespace).getClient().adapt(OpenShiftClient.class).routes().inNamespace(namespace).withName(clusterName + \"-kafka-bootstrap\").get();", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1NjM1NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401856354", "bodyText": "non-Route i am not quite sure that I fully understand this statement.", "author": "see-quick", "createdAt": "2020-04-01T19:27:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExMzQxMQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402113411", "bodyText": "Well that code basically means you can only test Routes on OpenShift. You can't test NodePorts, for instance.", "author": "tombentley", "createdAt": "2020-04-02T07:44:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NzU4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEzMDA1OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402130058", "bodyText": "Hmmm, you can test Nodeports the same with the LoadBalancers. The tests will all failed if this method is not working.\nsee\nhttps://github.com/strimzi/strimzi-kafka-operator/pull/2520/files?file-filters%5B%5D=.java#diff-b75f4b3368b48a1b5ad2ccd692700010R145-R156", "author": "see-quick", "createdAt": "2020-04-02T08:13:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1NzU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1ODAyMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401658020", "bodyText": "Don't you want to mention what the unexpected extBootstrapServiceType was?", "author": "tombentley", "createdAt": "2020-04-01T14:27:18Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java", "diffHunk": "@@ -0,0 +1,197 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import io.fabric8.kubernetes.api.model.LoadBalancerIngress;\n+import io.fabric8.kubernetes.api.model.Service;\n+import io.fabric8.openshift.api.model.Route;\n+import io.fabric8.openshift.client.OpenShiftClient;\n+\n+import java.security.InvalidParameterException;\n+\n+import static io.strimzi.api.kafka.model.KafkaResources.externalBootstrapServiceName;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+\n+public abstract class AbstractKafkaClient {\n+\n+    protected String topicName;\n+    protected String namespaceName;\n+    protected String clusterName;\n+    protected int messageCount;\n+    protected String consumerGroup;\n+    protected String kafkaUsername;\n+    protected String securityProtocol;\n+    protected String caCertName;\n+    protected KafkaClientProperties clientProperties;\n+\n+    public abstract static class Builder<T extends Builder<T>> {\n+\n+        private String topicName;\n+        private String namespaceName;\n+        private String clusterName;\n+        private int messageCount;\n+        private String consumerGroup;\n+        private String kafkaUsername;\n+        private String securityProtocol;\n+        private String caCertName;\n+        private KafkaClientProperties clientProperties;\n+\n+        public T withTopicName(String topicName) {\n+            this.topicName = topicName;\n+            return self();\n+        }\n+\n+        public T withNamespaceName(String namespaceName) {\n+            this.namespaceName = namespaceName;\n+            return self();\n+        }\n+\n+        public T withClusterName(String clusterName) {\n+            this.clusterName = clusterName;\n+            return self();\n+        }\n+\n+        public T withMessageCount(int messageCount) {\n+            this.messageCount = messageCount;\n+            return self();\n+        }\n+\n+        public T withConsumerGroupName(String consumerGroup) {\n+            this.consumerGroup = consumerGroup;\n+            return self();\n+        }\n+\n+        public T withKafkaUsername(String kafkaUsername) {\n+            this.kafkaUsername = kafkaUsername;\n+            return self();\n+        }\n+\n+        public T withSecurityProtocol(String securityProtocol) {\n+            this.securityProtocol = securityProtocol;\n+            return self();\n+        }\n+\n+        public T withCertificateAuthorityCertificateName(String caCertName) {\n+            this.caCertName = caCertName;\n+            return self();\n+        }\n+\n+        public T withKafkaClientProperties(KafkaClientProperties clientProperties) {\n+            this.clientProperties = clientProperties;\n+            return self();\n+        }\n+\n+        protected abstract AbstractKafkaClient build();\n+\n+        // Subclasses must override this method to return \"this\" protected abstract T self();\n+        // for not explicit casting..\n+        protected abstract T self();\n+    }\n+\n+    protected AbstractKafkaClient(Builder<?> builder) {\n+\n+        if (builder.topicName == null) throw new InvalidParameterException(\"Topic name is not set.\");\n+        if (builder.namespaceName == null) throw new InvalidParameterException(\"Namespace name is not set.\");\n+        if (builder.clusterName == null) throw  new InvalidParameterException(\"Cluster name is not set.\");\n+        if (builder.messageCount == 0) throw  new InvalidParameterException(\"Message count is set to 0\");\n+\n+        topicName = builder.topicName;\n+        namespaceName = builder.namespaceName;\n+        clusterName = builder.clusterName;\n+        messageCount = builder.messageCount;\n+        consumerGroup = builder.consumerGroup;\n+        kafkaUsername = builder.kafkaUsername;\n+        securityProtocol = builder.securityProtocol;\n+        caCertName = builder.caCertName;\n+        clientProperties = builder.clientProperties;\n+    }\n+\n+    public void setMessageCount(int messageCount) {\n+        this.messageCount = messageCount;\n+    }\n+\n+    protected boolean verifyProducedAndConsumedMessages(int producedMessages, int consumedMessages) {\n+        return producedMessages == consumedMessages;\n+    }\n+    /**\n+     * Get external bootstrap connection\n+     * @param namespace kafka namespace\n+     * @param clusterName kafka cluster name\n+     * @return bootstrap url as string\n+     */\n+    protected static String getExternalBootstrapConnect(String namespace, String clusterName) {\n+        if (kubeClient(namespace).getClient().isAdaptable(OpenShiftClient.class)) {\n+            Route route = kubeClient(namespace).getClient().adapt(OpenShiftClient.class).routes().inNamespace(namespace).withName(clusterName + \"-kafka-bootstrap\").get();\n+            if (route != null && !route.getStatus().getIngress().isEmpty()) {\n+                return route.getStatus().getIngress().get(0).getHost() + \":443\";\n+            }\n+        }\n+\n+        Service extBootstrapService = kubeClient(namespace).getClient().services()\n+                .inNamespace(namespace)\n+                .withName(externalBootstrapServiceName(clusterName))\n+                .get();\n+\n+        if (extBootstrapService == null) {\n+            throw new RuntimeException(\"Kafka cluster \" + clusterName + \" doesn't have an external bootstrap service\");\n+        }\n+\n+        String extBootstrapServiceType = extBootstrapService.getSpec().getType();\n+\n+        if (extBootstrapServiceType.equals(\"NodePort\")) {\n+            int port = extBootstrapService.getSpec().getPorts().get(0).getNodePort();\n+            String externalAddress = kubeClient(namespace).listNodes().get(0).getStatus().getAddresses().get(0).getAddress();\n+            return externalAddress + \":\" + port;\n+        } else if (extBootstrapServiceType.equals(\"LoadBalancer\")) {\n+            LoadBalancerIngress loadBalancerIngress = extBootstrapService.getStatus().getLoadBalancer().getIngress().get(0);\n+            String result = loadBalancerIngress.getHostname();\n+\n+            if (result == null) {\n+                result = loadBalancerIngress.getIp();\n+            }\n+            return result + \":9094\";\n+        } else {\n+            throw new RuntimeException(\"Unexpected external bootstrap service for Kafka cluster \" + clusterName);", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY1ODg0OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401658848", "bodyText": "Why are we naming interfaces with I prefix here, when it's not something we do in the rest of the code base? Can't you think of a better name without the I?", "author": "tombentley", "createdAt": "2020-04-01T14:28:24Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/IKafkaClientOperations.java", "diffHunk": "@@ -0,0 +1,14 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+public interface IKafkaClientOperations<T> {", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2MTAxNA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401661014", "bodyText": "There's no need to document these it in a javadoc comment, a normal comment will suffice.\nThe reasons can get stale. The interested reader can presumable try removing the suppressions and see what warnings they get and why.", "author": "tombentley", "createdAt": "2020-04-01T14:30:59Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientProperties.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.kafka.oauth.common.HttpUtil;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.test.executor.Exec;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.InvalidParameterException;\n+import java.security.KeyStore;\n+import java.security.KeyStoreException;\n+import java.security.NoSuchAlgorithmException;\n+import java.security.cert.Certificate;\n+import java.security.cert.CertificateException;\n+import java.security.cert.CertificateFactory;\n+import java.util.Base64;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static io.strimzi.kafka.oauth.common.OAuthAuthenticator.urlencode;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Class KafkaClientProperties, which holds inner class builder for fluent way to invoke objects. It is used inside\n+ * all our external clients such as BasicExternalKafkaClient or OauthExternalKafkaClient.\n+ *\n+ * Description of SuppressWarnings:\n+ *\n+ * caSecretName field is not initialized first inside KafkaClientProperties constructor and he is de-referenced\n+ * in sharedClientProperties() method. This practically means, always make sure that before invoking this method\n+ * you need first execute withCaSecretName().", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2MjYyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401662627", "bodyText": "since we should always delete these it's better to remove from both branches and put it immediately after it's creation.", "author": "tombentley", "createdAt": "2020-04-01T14:33:04Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientProperties.java", "diffHunk": "@@ -0,0 +1,458 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+import com.fasterxml.jackson.databind.JsonNode;\n+import com.fasterxml.jackson.databind.node.ObjectNode;\n+import edu.umd.cs.findbugs.annotations.SuppressFBWarnings;\n+import io.fabric8.kubernetes.api.model.Secret;\n+import io.strimzi.kafka.oauth.common.HttpUtil;\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.resources.ResourceManager;\n+import io.strimzi.test.executor.Exec;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.producer.ProducerConfig;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.File;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n+import java.security.InvalidParameterException;\n+import java.security.KeyStore;\n+import java.security.KeyStoreException;\n+import java.security.NoSuchAlgorithmException;\n+import java.security.cert.Certificate;\n+import java.security.cert.CertificateException;\n+import java.security.cert.CertificateFactory;\n+import java.util.Base64;\n+import java.util.Iterator;\n+import java.util.Properties;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+\n+import static io.strimzi.kafka.oauth.common.OAuthAuthenticator.urlencode;\n+import static io.strimzi.test.k8s.KubeClusterResource.kubeClient;\n+import static org.junit.jupiter.api.Assertions.fail;\n+\n+/**\n+ * Class KafkaClientProperties, which holds inner class builder for fluent way to invoke objects. It is used inside\n+ * all our external clients such as BasicExternalKafkaClient or OauthExternalKafkaClient.\n+ *\n+ * Description of SuppressWarnings:\n+ *\n+ * caSecretName field is not initialized first inside KafkaClientProperties constructor and he is de-referenced\n+ * in sharedClientProperties() method. This practically means, always make sure that before invoking this method\n+ * you need first execute withCaSecretName().\n+ *\n+ * @see io.strimzi.systemtest.kafkaclients.externalClients.OauthExternalKafkaClient\n+ * @see io.strimzi.systemtest.kafkaclients.externalClients.BasicExternalKafkaClient\n+ * @see io.strimzi.systemtest.kafkaclients.externalClients.TracingExternalKafkaClient\n+ */\n+@SuppressFBWarnings({\"NP_NONNULL_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR\", \"UWF_FIELD_NOT_INITIALIZED_IN_CONSTRUCTOR\"})\n+public class KafkaClientProperties  {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(KafkaClientProperties.class);\n+\n+    private String namespaceName;\n+    private String clusterName;\n+    private String caSecretName;\n+    private String kafkaUsername;\n+    private Properties properties;\n+\n+    public static class KafkaClientPropertiesBuilder {\n+\n+        private static final String TRUSTSTORE_TYPE_CONFIG = \"PKCS12\";\n+\n+        private Properties properties = new Properties();\n+        private String namespaceName;\n+        private String clusterName;\n+        private String caSecretName;\n+        private String kafkaUsername = \"\";\n+\n+        public KafkaClientPropertiesBuilder withBootstrapServerConfig(String bootstrapServer) {\n+\n+            this.properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeySerializerConfig(String keySerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, keySerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKeyDeSerializerConfig(String keyDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueSerializerConfig(String valueSerializer) {\n+\n+            this.properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, valueSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withValueDeSerializerConfig(String valueDeSerializer) {\n+\n+            this.properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDeSerializer);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withMaxBlockMsConfig(String maxBlockMsConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG, maxBlockMsConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClientIdConfig(String clientId) {\n+\n+            this.properties.setProperty(ProducerConfig.CLIENT_ID_CONFIG, clientId);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAcksConfig(String acksConfig) {\n+\n+            this.properties.setProperty(ProducerConfig.ACKS_CONFIG, acksConfig);\n+            return this;\n+        }\n+\n+        // TODO: do we need separate Producer and Consumer config ??\n+        public KafkaClientPropertiesBuilder withGroupIdConfig(String groupIdConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupIdConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withAutoOffsetResetConfig(String autoOffsetResetConfig) {\n+\n+            this.properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, autoOffsetResetConfig);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withNamespaceName(String namespaceName) {\n+\n+            this.namespaceName = namespaceName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withClusterName(String clusterName) {\n+\n+            this.clusterName = clusterName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withCaSecretName(String caSecretName) {\n+\n+            this.caSecretName = caSecretName;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withKafkaUsername(String kafkaUsername) {\n+\n+            this.kafkaUsername = kafkaUsername;\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSecurityProtocol(String securityProtocol) {\n+\n+            this.properties.setProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, securityProtocol);\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslMechanism(String saslMechanismType) {\n+\n+            this.properties.setProperty(SaslConfigs.SASL_MECHANISM, saslMechanismType);\n+            return this;\n+        }\n+\n+        // oauth properties\n+\n+\n+        public KafkaClientPropertiesBuilder withSaslLoginCallbackHandlerClass() {\n+\n+            this.properties.setProperty(SaslConfigs.SASL_LOGIN_CALLBACK_HANDLER_CLASS, \"io.strimzi.kafka.oauth.client.JaasClientOauthLoginCallbackHandler\");\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfig(String clientId, String clientSecretName, String oauthTokenEndpointUri) {\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            this.properties.setProperty(SaslConfigs.SASL_JAAS_CONFIG,\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\";\");\n+\n+            return this;\n+        }\n+\n+        public KafkaClientPropertiesBuilder withSaslJassConfigAndTls(String clientId, String clientSecretName, String oauthTokenEndpointUri) {\n+\n+            try {\n+                importKeycloakCertificateToTruststore(properties);\n+                fixBadlyImportedAuthzSettings();\n+            } catch (Exception e) {\n+                e.printStackTrace();\n+            }\n+\n+            if (clientId.isEmpty() || clientSecretName.isEmpty() || oauthTokenEndpointUri.isEmpty()) {\n+                throw new InvalidParameterException(\"You do not specify client-id, client-secret name or oauth-token-endpoint-uri inside kafka client!\");\n+            }\n+\n+            properties.setProperty(SaslConfigs.SASL_JAAS_CONFIG,\n+                \"org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule \" +\n+                    \"required \" +\n+                    \"oauth.client.id=\\\"\" + clientId + \"\\\" \" +\n+                    \"oauth.client.secret=\\\"\" + clientSecretName + \"\\\" \" +\n+                    \"oauth.token.endpoint.uri=\\\"\" + oauthTokenEndpointUri + \"\\\" \" +\n+                    \"oauth.ssl.endpoint.identification.algorithm=\\\"\\\"\" +\n+                    \"oauth.ssl.truststore.location=\\\"\" + properties.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG) + \"\\\" \" +\n+                    \"oauth.ssl.truststore.password=\\\"\" + properties.get(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG) + \"\\\" \" +\n+                    \"oauth.ssl.truststore.type=\\\"\" + properties.get(SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG) + \"\\\" ;\");\n+\n+            return this;\n+        }\n+\n+        /**\n+         * Create properties which are same pro producer and consumer\n+         */\n+        public KafkaClientPropertiesBuilder withSharedProperties() {\n+            // For turn off hostname verification\n+            properties.setProperty(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, \"\");\n+\n+            try {\n+                if (!properties.getProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG).equals(CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL) &&\n+                    !properties.getProperty(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG).equals(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+                ) {\n+                    Secret clusterCaCertSecret = kubeClient(namespaceName).getSecret(caSecretName);\n+                    File tsFile = File.createTempFile(KafkaClientProperties.class.getName(), \".truststore\");\n+                    String tsPassword = \"foo\";\n+                    KeyStore ts = KeyStore.getInstance(TRUSTSTORE_TYPE_CONFIG);\n+                    if (caSecretName.contains(\"custom-certificate\")) {\n+                        tsFile.deleteOnExit();", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2NTE0NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401665144", "bodyText": "Why does the withKeySerializerConfig() class take a String if it could take a Class?", "author": "tombentley", "createdAt": "2020-04-01T14:36:25Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/BasicExternalKafkaClient.java", "diffHunk": "@@ -0,0 +1,236 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients.externalClients;\n+\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.AbstractKafkaClient;\n+import io.strimzi.systemtest.kafkaclients.IKafkaClientOperations;\n+import io.strimzi.systemtest.kafkaclients.KafkaClientProperties;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.IntPredicate;\n+\n+/**\n+ * The BasicExternalKafkaClient for sending and receiving messages with basic properties. The client is using an external listeners.\n+ */\n+public class BasicExternalKafkaClient extends AbstractKafkaClient implements AutoCloseable, IKafkaClientOperations<Future<Integer>> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(BasicExternalKafkaClient.class);\n+    private Vertx vertx = Vertx.vertx();\n+\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        @Override\n+        public BasicExternalKafkaClient build() {\n+            return new BasicExternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+            return this;\n+        }\n+    }\n+\n+    private BasicExternalKafkaClient(Builder builder) {\n+        super(builder);\n+    }\n+\n+    public Future<Integer> sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    /**\n+     * Send messages to external entrypoint of the cluster with PLAINTEXT security protocol setting\n+     * @return future with sent message count\n+     */\n+    @Override\n+    public Future<Integer> sendMessagesPlain(long timeoutMs) {\n+\n+        String clientName = \"sender-plain-\" + this.clusterName;\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        vertx.deployVerticle(new Producer(\n+            new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+                .withNamespaceName(namespaceName)\n+                .withClusterName(clusterName)\n+                .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+                .withKeySerializerConfig(StringSerializer.class.getName())", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTgzOTg3MA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401839870", "bodyText": "Yeah, I can do it to use Class instead of passing Strings thanks!", "author": "see-quick", "createdAt": "2020-04-01T18:58:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2NTE0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2NjU2Mw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401666563", "bodyText": "If you're going to the trouble of abstracting an interface it's probably worth documenting the semantics of these methods.", "author": "tombentley", "createdAt": "2020-04-01T14:38:21Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/IKafkaClientOperations.java", "diffHunk": "@@ -0,0 +1,14 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients;\n+\n+public interface IKafkaClientOperations<T> {\n+\n+    T sendMessagesPlain(long timeoutMs) throws Exception;\n+    T sendMessagesTls(long timeoutMs) throws Exception;\n+\n+    T receiveMessagesPlain(long timeoutMs) throws Exception;\n+    T receiveMessagesTls(long timeoutMs) throws Exception;", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2OTIyNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401669227", "bodyText": "withKeyDeSerializerConfig \u2192 withKeyDeserializerConfig\nAlso if everything is just going to use StringDeserializer why not make that the default so you only need to call the method if you're using a different Deserializer?", "author": "tombentley", "createdAt": "2020-04-01T14:41:56Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/OauthExternalKafkaClient.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients.externalClients;\n+\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.AbstractKafkaClient;\n+import io.strimzi.systemtest.kafkaclients.IKafkaClientOperations;\n+import io.strimzi.systemtest.kafkaclients.KafkaClientProperties;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.IntPredicate;\n+\n+/**\n+ * The OauthExternalKafkaClient for sending and receiving messages using access token provided by authorization server.\n+ * The client is using an external listeners.\n+ */\n+public class OauthExternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Future<Integer>> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(OauthExternalKafkaClient.class);\n+    private Vertx vertx = Vertx.vertx();\n+\n+    private String clientId;\n+    private String clientSecretName;\n+    private String oauthTokenEndpointUri;\n+    private String introspectionEndpointUri;\n+\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        private String clientId;\n+        private String clientSecretName;\n+        private String oauthTokenEndpointUri;\n+        private String introspectionEndpointUri;\n+\n+        public Builder withOauthClientId(String oauthClientId) {\n+\n+            this.clientId = oauthClientId;\n+            return self();\n+        }\n+\n+        public Builder withClientSecretName(String clientSecretName) {\n+\n+            this.clientSecretName = clientSecretName;\n+            return self();\n+        }\n+\n+        public Builder withOauthTokenEndpointUri(String oauthTokenEndpointUri) {\n+\n+            this.oauthTokenEndpointUri = oauthTokenEndpointUri;\n+            return self();\n+        }\n+\n+        public Builder withIntrospectionEndpointUri(String introspectionEndpointUri) {\n+\n+            this.introspectionEndpointUri = introspectionEndpointUri;\n+            return self();\n+        }\n+\n+        @Override\n+        public OauthExternalKafkaClient build() {\n+\n+            return new OauthExternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+\n+            return this;\n+        }\n+    }\n+\n+    private OauthExternalKafkaClient(Builder builder) {\n+\n+        super(builder);\n+        clientId = builder.clientId;\n+        clientSecretName = builder.clientSecretName;\n+        oauthTokenEndpointUri = builder.oauthTokenEndpointUri;\n+        introspectionEndpointUri = builder.introspectionEndpointUri;\n+    }\n+\n+    public Future<Integer> sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesPlain(long timeoutMs) {\n+        String clientName = \"sender-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> sendMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesTls(long timeoutMs) {\n+        String clientName = \"sender-ssl\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withCaSecretName(caCertName)\n+            .withKafkaUsername(kafkaUsername)\n+            .withSecurityProtocol(\"SASL_SSL\")\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfigAndTls(clientId, clientSecretName, oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesPlain() {\n+        return receiveMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesPlain(long timeoutMs) {\n+        String clientName = \"receiver-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withGroupIdConfig(consumerGroup)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())\n+            .withValueDeSerializerConfig(StringDeserializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-consumer\")\n+            .withAutoOffsetResetConfig(\"earliest\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Consumer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesTls(long timeoutMs) {\n+\n+        String clientName = \"receiver-ssl-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withCaSecretName(caCertName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg0NjM5Ng==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401846396", "bodyText": "Also if everything is just going to use StringDeserializer why not make that the default so you only need to call the method if you're using a different Deserializer?\nI can create the next PR on that", "author": "see-quick", "createdAt": "2020-04-01T19:09:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY2OTIyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTA5NQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401671095", "bodyText": "You could make the method take org.apache.kafka.common.security.auth.SecurityProtocol for better type safety, if you don't mind the dependency leaking to your clients.", "author": "tombentley", "createdAt": "2020-04-01T14:44:22Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/OauthExternalKafkaClient.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients.externalClients;\n+\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.AbstractKafkaClient;\n+import io.strimzi.systemtest.kafkaclients.IKafkaClientOperations;\n+import io.strimzi.systemtest.kafkaclients.KafkaClientProperties;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.IntPredicate;\n+\n+/**\n+ * The OauthExternalKafkaClient for sending and receiving messages using access token provided by authorization server.\n+ * The client is using an external listeners.\n+ */\n+public class OauthExternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Future<Integer>> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(OauthExternalKafkaClient.class);\n+    private Vertx vertx = Vertx.vertx();\n+\n+    private String clientId;\n+    private String clientSecretName;\n+    private String oauthTokenEndpointUri;\n+    private String introspectionEndpointUri;\n+\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        private String clientId;\n+        private String clientSecretName;\n+        private String oauthTokenEndpointUri;\n+        private String introspectionEndpointUri;\n+\n+        public Builder withOauthClientId(String oauthClientId) {\n+\n+            this.clientId = oauthClientId;\n+            return self();\n+        }\n+\n+        public Builder withClientSecretName(String clientSecretName) {\n+\n+            this.clientSecretName = clientSecretName;\n+            return self();\n+        }\n+\n+        public Builder withOauthTokenEndpointUri(String oauthTokenEndpointUri) {\n+\n+            this.oauthTokenEndpointUri = oauthTokenEndpointUri;\n+            return self();\n+        }\n+\n+        public Builder withIntrospectionEndpointUri(String introspectionEndpointUri) {\n+\n+            this.introspectionEndpointUri = introspectionEndpointUri;\n+            return self();\n+        }\n+\n+        @Override\n+        public OauthExternalKafkaClient build() {\n+\n+            return new OauthExternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+\n+            return this;\n+        }\n+    }\n+\n+    private OauthExternalKafkaClient(Builder builder) {\n+\n+        super(builder);\n+        clientId = builder.clientId;\n+        clientSecretName = builder.clientSecretName;\n+        oauthTokenEndpointUri = builder.oauthTokenEndpointUri;\n+        introspectionEndpointUri = builder.introspectionEndpointUri;\n+    }\n+\n+    public Future<Integer> sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesPlain(long timeoutMs) {\n+        String clientName = \"sender-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> sendMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesTls(long timeoutMs) {\n+        String clientName = \"sender-ssl\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withCaSecretName(caCertName)\n+            .withKafkaUsername(kafkaUsername)\n+            .withSecurityProtocol(\"SASL_SSL\")\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfigAndTls(clientId, clientSecretName, oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesPlain() {\n+        return receiveMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesPlain(long timeoutMs) {\n+        String clientName = \"receiver-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withGroupIdConfig(consumerGroup)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())\n+            .withValueDeSerializerConfig(StringDeserializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-consumer\")\n+            .withAutoOffsetResetConfig(\"earliest\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Consumer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesTls(long timeoutMs) {\n+\n+        String clientName = \"receiver-ssl-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withCaSecretName(caCertName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())\n+            .withValueDeSerializerConfig(StringDeserializer.class.getName())\n+            .withKafkaUsername(kafkaUsername)\n+            .withSecurityProtocol(\"SASL_SSL\")", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg0NzQzOA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401847438", "bodyText": "Great idea!! I was wondering about the Enum, but this one solves it immediately.", "author": "see-quick", "createdAt": "2020-04-01T19:11:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTA5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTg4NA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401671884", "bodyText": "Similarly you could use org.apache.kafka.clients.consumer.OffsetResetStrategy", "author": "tombentley", "createdAt": "2020-04-01T14:45:25Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/OauthExternalKafkaClient.java", "diffHunk": "@@ -0,0 +1,273 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients.externalClients;\n+\n+import io.strimzi.systemtest.Constants;\n+import io.strimzi.systemtest.kafkaclients.AbstractKafkaClient;\n+import io.strimzi.systemtest.kafkaclients.IKafkaClientOperations;\n+import io.strimzi.systemtest.kafkaclients.KafkaClientProperties;\n+import io.strimzi.systemtest.resources.crd.KafkaResource;\n+import io.vertx.core.Vertx;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+import org.apache.kafka.common.serialization.StringSerializer;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.function.IntPredicate;\n+\n+/**\n+ * The OauthExternalKafkaClient for sending and receiving messages using access token provided by authorization server.\n+ * The client is using an external listeners.\n+ */\n+public class OauthExternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Future<Integer>> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(OauthExternalKafkaClient.class);\n+    private Vertx vertx = Vertx.vertx();\n+\n+    private String clientId;\n+    private String clientSecretName;\n+    private String oauthTokenEndpointUri;\n+    private String introspectionEndpointUri;\n+\n+    public static class Builder extends AbstractKafkaClient.Builder<Builder> {\n+\n+        private String clientId;\n+        private String clientSecretName;\n+        private String oauthTokenEndpointUri;\n+        private String introspectionEndpointUri;\n+\n+        public Builder withOauthClientId(String oauthClientId) {\n+\n+            this.clientId = oauthClientId;\n+            return self();\n+        }\n+\n+        public Builder withClientSecretName(String clientSecretName) {\n+\n+            this.clientSecretName = clientSecretName;\n+            return self();\n+        }\n+\n+        public Builder withOauthTokenEndpointUri(String oauthTokenEndpointUri) {\n+\n+            this.oauthTokenEndpointUri = oauthTokenEndpointUri;\n+            return self();\n+        }\n+\n+        public Builder withIntrospectionEndpointUri(String introspectionEndpointUri) {\n+\n+            this.introspectionEndpointUri = introspectionEndpointUri;\n+            return self();\n+        }\n+\n+        @Override\n+        public OauthExternalKafkaClient build() {\n+\n+            return new OauthExternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected Builder self() {\n+\n+            return this;\n+        }\n+    }\n+\n+    private OauthExternalKafkaClient(Builder builder) {\n+\n+        super(builder);\n+        clientId = builder.clientId;\n+        clientSecretName = builder.clientSecretName;\n+        oauthTokenEndpointUri = builder.oauthTokenEndpointUri;\n+        introspectionEndpointUri = builder.introspectionEndpointUri;\n+    }\n+\n+    public Future<Integer> sendMessagesPlain() {\n+        return sendMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesPlain(long timeoutMs) {\n+        String clientName = \"sender-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> sendMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesTls(long timeoutMs) {\n+        String clientName = \"sender-ssl\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeySerializerConfig(StringSerializer.class.getName())\n+            .withValueSerializerConfig(StringSerializer.class.getName())\n+            .withCaSecretName(caCertName)\n+            .withKafkaUsername(kafkaUsername)\n+            .withSecurityProtocol(\"SASL_SSL\")\n+            .withClientIdConfig(kafkaUsername + \"-producer\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfigAndTls(clientId, clientSecretName, oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Producer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesPlain() {\n+        return receiveMessagesPlain(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesPlain(long timeoutMs) {\n+        String clientName = \"receiver-plain-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withGroupIdConfig(consumerGroup)\n+            .withSecurityProtocol(\"SASL_\" + CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())\n+            .withValueDeSerializerConfig(StringDeserializer.class.getName())\n+            .withClientIdConfig(kafkaUsername + \"-consumer\")\n+            .withAutoOffsetResetConfig(\"earliest\")\n+            .withSaslMechanism(\"OAUTHBEARER\")\n+            .withSaslLoginCallbackHandlerClass()\n+            .withSharedProperties()\n+            .withSaslJassConfig(this.clientId, this.clientSecretName, this.oauthTokenEndpointUri)\n+            .build();\n+\n+        vertx.deployVerticle(new Consumer(kafkaClientProperties, resultPromise, msgCntPredicate, topicName, clientName));\n+\n+        try {\n+            resultPromise.get(timeoutMs, TimeUnit.MILLISECONDS);\n+        } catch (Exception e) {\n+            resultPromise.completeExceptionally(e);\n+        }\n+        vertx.close();\n+        return resultPromise;\n+    }\n+\n+    public Future<Integer> receiveMessagesTls() {\n+        return sendMessagesTls(Constants.GLOBAL_CLIENTS_TIMEOUT);\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesTls(long timeoutMs) {\n+\n+        String clientName = \"receiver-ssl-\" + clusterName;\n+        vertx = Vertx.vertx();\n+        CompletableFuture<Integer> resultPromise = new CompletableFuture<>();\n+\n+        IntPredicate msgCntPredicate = x -> x == messageCount;\n+\n+        String caCertName = this.caCertName == null ?\n+                KafkaResource.getKafkaExternalListenerCaCertName(namespaceName, clusterName) : this.caCertName;\n+        LOGGER.info(\"Going to use the following CA certificate: {}\", caCertName);\n+\n+        KafkaClientProperties kafkaClientProperties = new KafkaClientProperties.KafkaClientPropertiesBuilder()\n+            .withNamespaceName(namespaceName)\n+            .withClusterName(clusterName)\n+            .withCaSecretName(caCertName)\n+            .withBootstrapServerConfig(getExternalBootstrapConnect(namespaceName, clusterName))\n+            .withKeyDeSerializerConfig(StringDeserializer.class.getName())\n+            .withValueDeSerializerConfig(StringDeserializer.class.getName())\n+            .withKafkaUsername(kafkaUsername)\n+            .withSecurityProtocol(\"SASL_SSL\")\n+            .withGroupIdConfig(consumerGroup)\n+            .withAutoOffsetResetConfig(\"earliest\")", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1NDY5OA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401854698", "bodyText": "This will be questioned for Serialization and also this one. What gives me that I can provide OffsetResetStrategy.class? It is just of type compatibility and the wreck of more exuberance of semanticism?  I totally agree it is better to use classes instead of literals like this, but from the performance POV it is easy to just provide string right not just whole class?", "author": "see-quick", "createdAt": "2020-04-01T19:24:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNTE1Nw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402115157", "bodyText": "I think you misunderstood my comment. The argument provided by the caller in this example would be OffsetResetStrategry.EARLIEST. For the serializer you'd need to provide a Class<? extends Serializer>. Different cases of the same general point, which is that it's not very type safe to use Strings for these things, and there are existing types you can easily use to provide more type safety to your API.", "author": "tombentley", "createdAt": "2020-04-02T07:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTg4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjEyODAwNQ==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402128005", "bodyText": "Yeah, got it. Maybe last question using ofClass<? extends Serializer> would be using this class in the raw. Would it be still ok instead of specifying concrete type Serializer. In that case T will be StringSerializer i assume", "author": "see-quick", "createdAt": "2020-04-02T08:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3MTg4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3NDMxNg==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401674316", "bodyText": "Something is wrong if you're implementing an interface with all methods throwing UnsupportedOperationException. I'm guessing you've had to do this because you wanted a different builder for the tracing case? I think you need to question why your builder has to be an inner class of some AbstractKafkaClient subclass.", "author": "tombentley", "createdAt": "2020-04-01T14:48:32Z", "path": "systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/TracingExternalKafkaClient.java", "diffHunk": "@@ -0,0 +1,85 @@\n+/*\n+ * Copyright Strimzi authors.\n+ * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n+ */\n+package io.strimzi.systemtest.kafkaclients.externalClients;\n+\n+import io.strimzi.systemtest.kafkaclients.AbstractKafkaClient;\n+import io.strimzi.systemtest.kafkaclients.IKafkaClientOperations;\n+import org.apache.logging.log4j.LogManager;\n+import org.apache.logging.log4j.Logger;\n+\n+import java.util.concurrent.Future;\n+\n+/**\n+ * The TracingKafkaClient for sending and receiving messages using tracing properties.\n+ * The client is using an external listeners.\n+ */\n+public class TracingExternalKafkaClient extends AbstractKafkaClient implements IKafkaClientOperations<Future<Integer>> {\n+\n+    private static final Logger LOGGER = LogManager.getLogger(TracingExternalKafkaClient.class);\n+    private String serviceName;\n+\n+    public static class Builder extends AbstractKafkaClient.Builder<TracingExternalKafkaClient.Builder> {\n+\n+        private String serviceName;\n+\n+        public Builder withServiceName(String serviceName) {\n+\n+            this.serviceName = serviceName;\n+            return self();\n+        }\n+\n+        @Override\n+        public TracingExternalKafkaClient build() {\n+\n+            return new TracingExternalKafkaClient(this);\n+        }\n+\n+        @Override\n+        protected TracingExternalKafkaClient.Builder self() {\n+            return this;\n+        }\n+    }\n+\n+    private TracingExternalKafkaClient(TracingExternalKafkaClient.Builder builder) {\n+\n+        super(builder);\n+        serviceName = builder.serviceName;\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesPlain(long timeoutMs) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Future<Integer> sendMessagesTls(long timeoutMs) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesPlain(long timeoutMs) {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    public Future<Integer> receiveMessagesTls(long timeoutMs) {\n+        throw new UnsupportedOperationException();\n+    }", "originalCommit": "50fbd92b44d8acfc716931e9d12a5242367613fa", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1OTQzNw==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r401859437", "bodyText": "Practically, this should be implemented in a few weeks. It will be the Tracing clients using these methods and yeah.", "author": "see-quick", "createdAt": "2020-04-01T19:33:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3NDMxNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjExNTgzMA==", "url": "https://github.com/strimzi/strimzi-kafka-operator/pull/2520#discussion_r402115830", "bodyText": "OK. A // TODO or exception message to that effect is always helpful to show the intent to come back and address it.", "author": "tombentley", "createdAt": "2020-04-02T07:48:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTY3NDMxNg=="}], "type": "inlineReview"}, {"oid": "391d2743ed5875aebd7e2b50b5b767bcd621c37f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/391d2743ed5875aebd7e2b50b5b767bcd621c37f", "message": "[MO] - [fixes] -> last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-02T14:23:07Z", "type": "forcePushed"}, {"oid": "ef937d21fcc2d430ee38a53350a9b4201f4e144d", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ef937d21fcc2d430ee38a53350a9b4201f4e144d", "message": "[MO] - [rebase] -> last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T09:05:59Z", "type": "forcePushed"}, {"oid": "5b7a9acd2c07161ca818f56d807145ad89938189", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/5b7a9acd2c07161ca818f56d807145ad89938189", "message": "[MO] - [system test] -> deencapsulation of basic client\n\nSigned-off-by: Seequick1 <morsak@redhat.com>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "ca86da5d06fcbaba2edccc3fbde3fcdfabc75796", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/ca86da5d06fcbaba2edccc3fbde3fcdfabc75796", "message": "[MO] - [refactor] -> finish spot bugs!\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "3efe85421971834423863b31019c321e7df92190", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/3efe85421971834423863b31019c321e7df92190", "message": "[MO] - [spotbugs] -> finally\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "fd2dde0a01fbb63eb17b406c32d026b80951042b", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/fd2dde0a01fbb63eb17b406c32d026b80951042b", "message": "[MO] - [rebase] -> finally last!!! i hope\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "461914db912f84a15839983d6258361010ef3781", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/461914db912f84a15839983d6258361010ef3781", "message": "[MO] - [commneds] -> Tom\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "d1bf5db861fc345b415a717d5675a29deb20d81a", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/d1bf5db861fc345b415a717d5675a29deb20d81a", "message": "[MO] - [commends] -> Tom\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "6695d841a78692bb2077d86d8f284ad2805fd23f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/6695d841a78692bb2077d86d8f284ad2805fd23f", "message": "[MO] - [fixes] -> last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "4fcdbb3656c7b62767704390d396be5a6958d1fe", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/4fcdbb3656c7b62767704390d396be5a6958d1fe", "message": "[MO] - [last]- [ast]/\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "99a66aad1201a2674a8c8daaa6a01bff807c2ae2", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/99a66aad1201a2674a8c8daaa6a01bff807c2ae2", "message": "[MO] - [fix] -> last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "87755099abf4533efd6001414bf3d3051c3902f4", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/87755099abf4533efd6001414bf3d3051c3902f4", "message": "[MO] - [rebase] -> last\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "f4add7438fd48ab19e1f68d37c4640999e31511f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f4add7438fd48ab19e1f68d37c4640999e31511f", "message": "[MO] - [spotbugs] -> gg\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "commit"}, {"oid": "f4add7438fd48ab19e1f68d37c4640999e31511f", "url": "https://github.com/strimzi/strimzi-kafka-operator/commit/f4add7438fd48ab19e1f68d37c4640999e31511f", "message": "[MO] - [spotbugs] -> gg\n\nSigned-off-by: morsak <xorsak02@stud.fit.vutbr.cz>", "committedDate": "2020-04-03T12:40:41Z", "type": "forcePushed"}]}