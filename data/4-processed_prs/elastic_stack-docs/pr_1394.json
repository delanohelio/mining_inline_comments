{"pr_number": 1394, "pr_title": "[DOCS] Adds data frame analytics at scale page to the ML DFA docs", "pr_createdAt": "2020-10-02T09:06:00Z", "pr_url": "https://github.com/elastic/stack-docs/pull/1394", "timeline": [{"oid": "67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "url": "https://github.com/elastic/stack-docs/commit/67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "message": "[DOCS] Adds data frame analytics at scale page to the ML DFA docs.", "committedDate": "2020-10-02T09:02:11Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA2OTE3NA==", "url": "https://github.com/elastic/stack-docs/pull/1394#discussion_r499069174", "bodyText": "Should this be a section title or bolded? It's merging with the subsequent phrase in the output.", "author": "lcawl", "createdAt": "2020-10-02T22:05:17Z", "path": "docs/en/stack/ml/df-analytics/ml-dfa-scale.asciidoc", "diffHunk": "@@ -0,0 +1,153 @@\n+[role=\"xpack\"]\n+[[ml-dfa-scale]]\n+= Working with {dfanalytics} at scale\n+\n+A {dfanalytics-job} has numerous configuration options. Some of them may have a \n+significant effect on the time taken to train a model. The training time depends \n+on various factors, like the statistical characteristics of your data, the \n+number of provided hyperparameters, the number of features included in the \n+analysis, the hardware you use, and so on. This guide contains a list of \n+considerations to help you plan for training {dfanalytics} models at scale and \n+optimizing training time.\n+\n+In this guide, you\u2019ll learn how to:\n+\n+* Understand the impact of configuration options on the time taken to train \n+  models for {dfanalytics-jobs}.\n+\n+\n+Prerequisites:", "originalCommit": "67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA3MTAxNw==", "url": "https://github.com/elastic/stack-docs/pull/1394#discussion_r499071017", "bodyText": "Though I like politeness IRL we tend to omit it from docs :)\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To learn more about the individual phases, please refer to <<ml-dfa-phases>>.\n          \n          \n            \n            To learn more about the individual phases, refer to <<ml-dfa-phases>>.", "author": "lcawl", "createdAt": "2020-10-02T22:12:05Z", "path": "docs/en/stack/ml/df-analytics/ml-dfa-scale.asciidoc", "diffHunk": "@@ -0,0 +1,153 @@\n+[role=\"xpack\"]\n+[[ml-dfa-scale]]\n+= Working with {dfanalytics} at scale\n+\n+A {dfanalytics-job} has numerous configuration options. Some of them may have a \n+significant effect on the time taken to train a model. The training time depends \n+on various factors, like the statistical characteristics of your data, the \n+number of provided hyperparameters, the number of features included in the \n+analysis, the hardware you use, and so on. This guide contains a list of \n+considerations to help you plan for training {dfanalytics} models at scale and \n+optimizing training time.\n+\n+In this guide, you\u2019ll learn how to:\n+\n+* Understand the impact of configuration options on the time taken to train \n+  models for {dfanalytics-jobs}.\n+\n+\n+Prerequisites:\n+This guide assumes you\u2019re already familiar with: \n+\n+* How to create data frame analytics jobs. If not, refer to <<ml-dfa-overview>>.\n+\n+* How data frame analytics jobs work. If not, refer to <<ml-dfa-phases>>.\n+\n+It is important to note that there is a correlation between the training time, \n+the complexity of the model, the size of the data, and the quality of the \n+analysis results. Improvements in quality, however, are not linear with the \n+amount of training data; for very large source data, it might take hours to \n+train a model for very small gains in quality. When you work at scale with \n+{dfanalytics}, you need to decide what quality of results is acceptable for your \n+use case. When you have determined your acceptance criteria, you have a better \n+picture of the factors you can trade off while still achieving your goal.\n+\n+\n+The following recommendations are not sequential \u2013 the numbers just help to \n+navigate between the list items; you can take action on one or more of them in \n+any order.\n+\n+\n+[discrete]\n+[[rapid-iteration]]\n+== 0. Start small and iterate rapidly\n+\n+Training is an iterative process. Experiment with different settings and \n+configuration options (including but not limited to hyperparameters and feature \n+importance), then evaluate the results and decide whether they are good enough \n+or need further experimentation.\n+\n+Every iteration takes time, so it is useful to start with a small set of data so \n+you can iterate rapidly and then build up from here.\n+\n+\n+[discrete]\n+[[small-training-percent]]\n+== 1. Set a small training percent\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+The number of documents used for training a model has an effect on the training \n+time. A higher training percent means a longer training time.\n+\n+Consider starting with a small percentage of training data so you can complete \n+iterations more quickly. Once you are happy with your configuration, increase \n+the training percent.  As a rule of thumb, if you have a data set with more than \n+100,000 data points, start with a training percent of 5 or 10.\n+\n+\n+[discrete]\n+[[disable-feature-importance]]\n+== 2. Disable {feat-imp} calculation\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+<<ml-feature-importance>> indicates which fields had the biggest impact on each \n+prediction that is generated by the analysis. Depending on the size of the data \n+set, {feat-imp} can take a long time to compute.\n+\n+For a shorter runtime, consider disabling {feat-imp} for some or all iterations \n+if you do not require it.\n+\n+\n+[discrete]\n+[[optimize-included-fields]]\n+== 3. Optimize the number of included fields \n+\n+You can speed up runtime by only analyzing relevant fields.\n+\n+By default, all the fields that are supported by the analysis type are included \n+in the analysis. In general, more fields analyzed requires more resources and \n+longer training times, including the time taken for automatic feature selection. \n+To reduce training time, consider limiting the scope of the analysis to the \n+relevant fields that contribute to the prediction. You may do this by either \n+excluding non-relevant fields or by including relevant ones.\n+\n+NOTE: {feat-imp-cap} can help you determine the fields that contribute most to \n+the prediction. However, as calculating {feat-imp} increases training time, this \n+is a trade-off that can be evaluated during an iterative training process. \n+\n+\n+[discrete]\n+[[increase-threads]]\n+== 4. Increase the maximum number of threads\n+\n+You can set the maximum number of threads that are used during the analysis. The \n+default value of `max_num_threads` is 1. Depending on the characteristics of the \n+data, using more threads may decrease the training time at the cost of increased \n+CPU usage. Note that trying to use more threads than the number of CPU cores has \n+no advantage.\n+\n+Hyperparameter optimization and calculating {feat-imp} gain the most benefit \n+from the increased number of threads. This can be seen in phases \n+`coarse_parameter_search`, `fine_tuning_parameters`, and `writing_results`. The \n+rest of the phases are not affected by the increased number of threads.\n+\n+To learn more about the individual phases, please refer to <<ml-dfa-phases>>.", "originalCommit": "67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA3MTQxMQ==", "url": "https://github.com/elastic/stack-docs/pull/1394#discussion_r499071411", "bodyText": "Not a big deal, but you can likely simplify as follows:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            NOTE: If your {ml} nodes are running concurrent jobs (either {anomaly-detect} or \n          \n          \n            \n            NOTE: If your {ml} nodes are running concurrent {anomaly-detect} or", "author": "lcawl", "createdAt": "2020-10-02T22:13:25Z", "path": "docs/en/stack/ml/df-analytics/ml-dfa-scale.asciidoc", "diffHunk": "@@ -0,0 +1,153 @@\n+[role=\"xpack\"]\n+[[ml-dfa-scale]]\n+= Working with {dfanalytics} at scale\n+\n+A {dfanalytics-job} has numerous configuration options. Some of them may have a \n+significant effect on the time taken to train a model. The training time depends \n+on various factors, like the statistical characteristics of your data, the \n+number of provided hyperparameters, the number of features included in the \n+analysis, the hardware you use, and so on. This guide contains a list of \n+considerations to help you plan for training {dfanalytics} models at scale and \n+optimizing training time.\n+\n+In this guide, you\u2019ll learn how to:\n+\n+* Understand the impact of configuration options on the time taken to train \n+  models for {dfanalytics-jobs}.\n+\n+\n+Prerequisites:\n+This guide assumes you\u2019re already familiar with: \n+\n+* How to create data frame analytics jobs. If not, refer to <<ml-dfa-overview>>.\n+\n+* How data frame analytics jobs work. If not, refer to <<ml-dfa-phases>>.\n+\n+It is important to note that there is a correlation between the training time, \n+the complexity of the model, the size of the data, and the quality of the \n+analysis results. Improvements in quality, however, are not linear with the \n+amount of training data; for very large source data, it might take hours to \n+train a model for very small gains in quality. When you work at scale with \n+{dfanalytics}, you need to decide what quality of results is acceptable for your \n+use case. When you have determined your acceptance criteria, you have a better \n+picture of the factors you can trade off while still achieving your goal.\n+\n+\n+The following recommendations are not sequential \u2013 the numbers just help to \n+navigate between the list items; you can take action on one or more of them in \n+any order.\n+\n+\n+[discrete]\n+[[rapid-iteration]]\n+== 0. Start small and iterate rapidly\n+\n+Training is an iterative process. Experiment with different settings and \n+configuration options (including but not limited to hyperparameters and feature \n+importance), then evaluate the results and decide whether they are good enough \n+or need further experimentation.\n+\n+Every iteration takes time, so it is useful to start with a small set of data so \n+you can iterate rapidly and then build up from here.\n+\n+\n+[discrete]\n+[[small-training-percent]]\n+== 1. Set a small training percent\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+The number of documents used for training a model has an effect on the training \n+time. A higher training percent means a longer training time.\n+\n+Consider starting with a small percentage of training data so you can complete \n+iterations more quickly. Once you are happy with your configuration, increase \n+the training percent.  As a rule of thumb, if you have a data set with more than \n+100,000 data points, start with a training percent of 5 or 10.\n+\n+\n+[discrete]\n+[[disable-feature-importance]]\n+== 2. Disable {feat-imp} calculation\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+<<ml-feature-importance>> indicates which fields had the biggest impact on each \n+prediction that is generated by the analysis. Depending on the size of the data \n+set, {feat-imp} can take a long time to compute.\n+\n+For a shorter runtime, consider disabling {feat-imp} for some or all iterations \n+if you do not require it.\n+\n+\n+[discrete]\n+[[optimize-included-fields]]\n+== 3. Optimize the number of included fields \n+\n+You can speed up runtime by only analyzing relevant fields.\n+\n+By default, all the fields that are supported by the analysis type are included \n+in the analysis. In general, more fields analyzed requires more resources and \n+longer training times, including the time taken for automatic feature selection. \n+To reduce training time, consider limiting the scope of the analysis to the \n+relevant fields that contribute to the prediction. You may do this by either \n+excluding non-relevant fields or by including relevant ones.\n+\n+NOTE: {feat-imp-cap} can help you determine the fields that contribute most to \n+the prediction. However, as calculating {feat-imp} increases training time, this \n+is a trade-off that can be evaluated during an iterative training process. \n+\n+\n+[discrete]\n+[[increase-threads]]\n+== 4. Increase the maximum number of threads\n+\n+You can set the maximum number of threads that are used during the analysis. The \n+default value of `max_num_threads` is 1. Depending on the characteristics of the \n+data, using more threads may decrease the training time at the cost of increased \n+CPU usage. Note that trying to use more threads than the number of CPU cores has \n+no advantage.\n+\n+Hyperparameter optimization and calculating {feat-imp} gain the most benefit \n+from the increased number of threads. This can be seen in phases \n+`coarse_parameter_search`, `fine_tuning_parameters`, and `writing_results`. The \n+rest of the phases are not affected by the increased number of threads.\n+\n+To learn more about the individual phases, please refer to <<ml-dfa-phases>>.\n+\n+NOTE: If your {ml} nodes are running concurrent jobs (either {anomaly-detect} or ", "originalCommit": "67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTA3MTUwMQ==", "url": "https://github.com/elastic/stack-docs/pull/1394#discussion_r499071501", "bodyText": "Continued:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {dfanalytics}), then you may want to keep the maximum number of threads set to a \n          \n          \n            \n            {dfanalytics-jobs}, you may want to keep the maximum number of threads set to a", "author": "lcawl", "createdAt": "2020-10-02T22:13:50Z", "path": "docs/en/stack/ml/df-analytics/ml-dfa-scale.asciidoc", "diffHunk": "@@ -0,0 +1,153 @@\n+[role=\"xpack\"]\n+[[ml-dfa-scale]]\n+= Working with {dfanalytics} at scale\n+\n+A {dfanalytics-job} has numerous configuration options. Some of them may have a \n+significant effect on the time taken to train a model. The training time depends \n+on various factors, like the statistical characteristics of your data, the \n+number of provided hyperparameters, the number of features included in the \n+analysis, the hardware you use, and so on. This guide contains a list of \n+considerations to help you plan for training {dfanalytics} models at scale and \n+optimizing training time.\n+\n+In this guide, you\u2019ll learn how to:\n+\n+* Understand the impact of configuration options on the time taken to train \n+  models for {dfanalytics-jobs}.\n+\n+\n+Prerequisites:\n+This guide assumes you\u2019re already familiar with: \n+\n+* How to create data frame analytics jobs. If not, refer to <<ml-dfa-overview>>.\n+\n+* How data frame analytics jobs work. If not, refer to <<ml-dfa-phases>>.\n+\n+It is important to note that there is a correlation between the training time, \n+the complexity of the model, the size of the data, and the quality of the \n+analysis results. Improvements in quality, however, are not linear with the \n+amount of training data; for very large source data, it might take hours to \n+train a model for very small gains in quality. When you work at scale with \n+{dfanalytics}, you need to decide what quality of results is acceptable for your \n+use case. When you have determined your acceptance criteria, you have a better \n+picture of the factors you can trade off while still achieving your goal.\n+\n+\n+The following recommendations are not sequential \u2013 the numbers just help to \n+navigate between the list items; you can take action on one or more of them in \n+any order.\n+\n+\n+[discrete]\n+[[rapid-iteration]]\n+== 0. Start small and iterate rapidly\n+\n+Training is an iterative process. Experiment with different settings and \n+configuration options (including but not limited to hyperparameters and feature \n+importance), then evaluate the results and decide whether they are good enough \n+or need further experimentation.\n+\n+Every iteration takes time, so it is useful to start with a small set of data so \n+you can iterate rapidly and then build up from here.\n+\n+\n+[discrete]\n+[[small-training-percent]]\n+== 1. Set a small training percent\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+The number of documents used for training a model has an effect on the training \n+time. A higher training percent means a longer training time.\n+\n+Consider starting with a small percentage of training data so you can complete \n+iterations more quickly. Once you are happy with your configuration, increase \n+the training percent.  As a rule of thumb, if you have a data set with more than \n+100,000 data points, start with a training percent of 5 or 10.\n+\n+\n+[discrete]\n+[[disable-feature-importance]]\n+== 2. Disable {feat-imp} calculation\n+\n+(This step only applies to {regression} and {classification} jobs.)\n+\n+<<ml-feature-importance>> indicates which fields had the biggest impact on each \n+prediction that is generated by the analysis. Depending on the size of the data \n+set, {feat-imp} can take a long time to compute.\n+\n+For a shorter runtime, consider disabling {feat-imp} for some or all iterations \n+if you do not require it.\n+\n+\n+[discrete]\n+[[optimize-included-fields]]\n+== 3. Optimize the number of included fields \n+\n+You can speed up runtime by only analyzing relevant fields.\n+\n+By default, all the fields that are supported by the analysis type are included \n+in the analysis. In general, more fields analyzed requires more resources and \n+longer training times, including the time taken for automatic feature selection. \n+To reduce training time, consider limiting the scope of the analysis to the \n+relevant fields that contribute to the prediction. You may do this by either \n+excluding non-relevant fields or by including relevant ones.\n+\n+NOTE: {feat-imp-cap} can help you determine the fields that contribute most to \n+the prediction. However, as calculating {feat-imp} increases training time, this \n+is a trade-off that can be evaluated during an iterative training process. \n+\n+\n+[discrete]\n+[[increase-threads]]\n+== 4. Increase the maximum number of threads\n+\n+You can set the maximum number of threads that are used during the analysis. The \n+default value of `max_num_threads` is 1. Depending on the characteristics of the \n+data, using more threads may decrease the training time at the cost of increased \n+CPU usage. Note that trying to use more threads than the number of CPU cores has \n+no advantage.\n+\n+Hyperparameter optimization and calculating {feat-imp} gain the most benefit \n+from the increased number of threads. This can be seen in phases \n+`coarse_parameter_search`, `fine_tuning_parameters`, and `writing_results`. The \n+rest of the phases are not affected by the increased number of threads.\n+\n+To learn more about the individual phases, please refer to <<ml-dfa-phases>>.\n+\n+NOTE: If your {ml} nodes are running concurrent jobs (either {anomaly-detect} or \n+{dfanalytics}), then you may want to keep the maximum number of threads set to a ", "originalCommit": "67f76ff7c334b2bb9f84b0fb1ce33ba15da420fc", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f1ebdf75bac9e5ed7603499cd3274142ef4d1ed3", "url": "https://github.com/elastic/stack-docs/commit/f1ebdf75bac9e5ed7603499cd3274142ef4d1ed3", "message": "[DOCS] Addresses feedback.", "committedDate": "2020-10-05T07:41:35Z", "type": "commit"}]}