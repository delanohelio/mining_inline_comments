{"pr_number": 1672, "pr_title": "Introduced groovy and added ExclusiveJoinTest case to verify the work\u2026", "pr_createdAt": "2020-05-07T00:27:54Z", "pr_url": "https://github.com/Netflix/conductor/pull/1672", "timeline": [{"oid": "d37f20630b9740e1619699e9ccf96a2d4dcc7ea5", "url": "https://github.com/Netflix/conductor/commit/d37f20630b9740e1619699e9ccf96a2d4dcc7ea5", "message": "Introduced groovy and added ExclusiveJoinTest case to verify the workings in the build", "committedDate": "2020-05-07T00:26:17Z", "type": "commit"}, {"oid": "e8b3894b3f70f59320d8d9ac69b5428d8c51ca3d", "url": "https://github.com/Netflix/conductor/commit/e8b3894b3f70f59320d8d9ac69b5428d8c51ca3d", "message": "Added KafkaTaskBasedWorkflowSpec and introduced WorkflowConfigurationSpec", "committedDate": "2020-05-08T16:52:44Z", "type": "commit"}, {"oid": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f", "url": "https://github.com/Netflix/conductor/commit/9e7223b2a4af6c138e4febe06f2cdb3d4b85370f", "message": "Simple workflow integration test", "committedDate": "2020-05-11T21:13:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4OTk5NA==", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r423389994", "bodyText": "leftovers?", "author": "kishorebanala", "createdAt": "2020-05-12T00:12:10Z", "path": "test-harness/src/test/groovy/com/netflix/counductor/integration/test/KafkaTaskBasedWorkflowSpec.groovy", "diffHunk": "@@ -0,0 +1,201 @@\n+package com.netflix.counductor.integration.test\n+\n+import com.fasterxml.jackson.databind.ObjectMapper\n+import com.netflix.archaius.guice.ArchaiusModule\n+import com.netflix.conductor.common.metadata.tasks.TaskDef\n+import com.netflix.conductor.common.metadata.tasks.TaskResult\n+import com.netflix.conductor.common.metadata.workflow.TaskType\n+import com.netflix.conductor.common.metadata.workflow.WorkflowDef\n+import com.netflix.conductor.common.metadata.workflow.WorkflowTask\n+import com.netflix.conductor.common.run.Workflow\n+import com.netflix.conductor.common.utils.JsonMapperProvider\n+import com.netflix.conductor.core.execution.WorkflowExecutor\n+import com.netflix.conductor.service.ExecutionService\n+import com.netflix.conductor.service.MetadataService\n+import com.netflix.conductor.tests.utils.TestModule\n+import com.netflix.conductor.tests.utils.WorkflowCleanUpUtil\n+import com.netflix.governator.guice.test.ModulesForTesting\n+import spock.lang.Shared\n+import spock.lang.Specification\n+\n+import javax.inject.Inject\n+\n+@ModulesForTesting([TestModule.class, ArchaiusModule.class])\n+class KafkaTaskBasedWorkflowSpec extends Specification {\n+\n+    @Inject\n+    ExecutionService workflowExecutionService\n+\n+    @Inject\n+    MetadataService metadataService\n+\n+    @Inject\n+    WorkflowExecutor workflowExecutor\n+\n+    @Inject\n+    WorkflowCleanUpUtil cleanUpUtil\n+\n+    @Shared\n+    ObjectMapper objectMapper = new JsonMapperProvider().get()\n+\n+    @Shared\n+    def isWorkflowRegistered = false\n+\n+    def kafkaInput = ['requestDetails': ['key1': 'value1', 'key2': 42],\n+                      'path1'         : 'file://path1',\n+                      'path2'         : 'file://path2',\n+                      'outputPath'    : 's3://bucket/outputPath'\n+    ]\n+\n+\n+    def expectedTaskInput = \"{\\\"kafka_request\\\":{\\\"topic\\\":\\\"test_kafka_topic\\\",\\\"bootStrapServers\\\":\\\"localhost:9092\\\",\\\"value\\\":{\\\"requestDetails\\\":{\\\"key1\\\":\\\"value1\\\",\\\"key2\\\":42},\\\"outputPath\\\":\\\"s3://bucket/outputPath\\\",\\\"inputPaths\\\":[\\\"file://path1\\\",\\\"file://path2\\\"]}}}\"\n+\n+    def cleanup() {\n+        cleanUpUtil.clearWorkflows()\n+    }\n+\n+    def setup() {\n+        if(!isWorkflowRegistered) {\n+            registerKafkaWorkflow()\n+            isWorkflowRegistered = true\n+        }\n+    }\n+\n+    def \"Test the kafka template usage failure case\"() {\n+\n+        given:\"Start a workflow based on the registered workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and fail the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.status = TaskResult.Status.FAILED\n+        taskResult.reasonForIncompletion = 'NON TRANSIENT ERROR OCCURRED: An integration point required to complete the task is down'\n+        taskResult.addOutputData(\"TERMINAL_ERROR\", \"Integration endpoint down: FOOBAR\")\n+        taskResult.addOutputData(\"ErrorMessage\", \"There was a terminal error\")\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is in a failed state\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.FAILED\n+\n+\n+    }\n+\n+\n+    def \"Test the kafka template usage success case\"() {\n+\n+        given:\"Start a workflow based on the registered kafka workflow\"\n+        def workflowInstanceId = workflowExecutor.startWorkflow(\"template_kafka_workflow\", 1,\n+                \"testTaskDefTemplate\", kafkaInput,\n+                null, null, null)\n+\n+        and:\"Get the workflow based on the Id that is being executed\"\n+        def workflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+        def task = workflow.tasks.get(0)\n+        def taskInput = task.inputData\n+\n+        when:\"Ensure that the task is pollable and complete the task\"\n+        def polledTask = workflowExecutionService.poll('KAFKA_PUBLISH', 'test')\n+        workflowExecutionService.ackTaskReceived(polledTask.taskId)\n+        def taskResult = new TaskResult(polledTask)\n+        taskResult.setStatus(TaskResult.Status.COMPLETED)\n+        workflowExecutionService.updateTask(taskResult)\n+\n+        and:\"Then run a decide to move the workflow forward\"\n+        workflowExecutor.decide(workflowInstanceId)\n+\n+        and:\"Get the updated workflow after the task result has been updated\"\n+        def updatedWorkflow = workflowExecutionService.getExecutionStatus(workflowInstanceId, true)\n+\n+        then:\"Check that the workflow is created and is not terminal\"\n+        workflowInstanceId\n+        workflow\n+        !workflow.getStatus().isTerminal()\n+        !workflow.getReasonForIncompletion()\n+\n+        and:\"Check if the input of the next task to be polled is as expected for a kafka task\"\n+        taskInput\n+        taskInput.containsKey('kafka_request')\n+        taskInput['kafka_request'] instanceof Map\n+        objectMapper.writeValueAsString(taskInput) == expectedTaskInput\n+\n+        and:\"Polled task is not null and the workflowInstanceId of the task is same as the workflow created initially\"\n+        polledTask\n+        polledTask.workflowInstanceId == workflowInstanceId\n+\n+        and:\"The updated workflow is complete\"\n+        updatedWorkflow\n+        updatedWorkflow.status == Workflow.WorkflowStatus.COMPLETED\n+\n+    }\n+\n+\n+    def registerKafkaWorkflow() {\n+        System.setProperty(\"STACK_KAFKA\", \"test_kafka_topic\")\n+        TaskDef templatedTask = new TaskDef()\n+        templatedTask.name = \"templated_kafka_task\"\n+        templatedTask.retryCount = 0\n+\n+        def kafkaRequest = new HashMap<>()\n+        kafkaRequest[\"topic\"] = '${STACK_KAFKA}'\n+        kafkaRequest[\"bootStrapServers\"] = \"localhost:9092\"\n+\n+        def value = new HashMap<>()\n+        value[\"inputPaths\"] = ['${workflow.input.path1}', '${workflow.input.path2}']\n+        value[\"requestDetails\"] = '${workflow.input.requestDetails}'\n+        value[\"outputPath\"] = '${workflow.input.outputPath}'\n+        kafkaRequest[\"value\"] = value\n+\n+        templatedTask.inputTemplate[\"kafka_request\"] = kafkaRequest\n+        metadataService.registerTaskDef([templatedTask])\n+\n+        WorkflowDef templateWf = new WorkflowDef()\n+        templateWf.name = \"template_kafka_workflow\"\n+        WorkflowTask wft = new WorkflowTask()\n+        wft.name = templatedTask.name\n+        wft.workflowTaskType = TaskType.KAFKA_PUBLISH\n+        wft.taskReferenceName = \"t0\"\n+        templateWf.tasks.add(wft)\n+        templateWf.schemaVersion = 2\n+        metadataService.registerWorkflowDef(templateWf)\n+    }\n+\n+\n+\n+    /*def value = ['inputPaths'    : ['${workflow.input.path1}', '${workflow.input.path2}'],", "originalCommit": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY3NjMzMA==", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r424676330", "bodyText": "Cleaned up", "author": "pctreddy", "createdAt": "2020-05-13T19:22:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM4OTk5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM5MTAwNw==", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r423391007", "bodyText": "Nit: Trying to understand what this class does. JavaDoc?", "author": "kishorebanala", "createdAt": "2020-05-12T00:15:42Z", "path": "test-harness/src/test/java/com/netflix/conductor/tests/utils/WorkflowCleanUpUtil.java", "diffHunk": "@@ -0,0 +1,54 @@\n+package com.netflix.conductor.tests.utils;\n+\n+import com.netflix.conductor.common.run.Workflow;\n+import com.netflix.conductor.core.execution.WorkflowExecutor;\n+import com.netflix.conductor.dao.QueueDAO;\n+import com.netflix.conductor.service.ExecutionService;\n+import com.netflix.conductor.service.MetadataService;\n+import org.apache.commons.lang.StringUtils;\n+\n+import javax.inject.Inject;\n+import javax.inject.Singleton;\n+import java.io.FileOutputStream;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+\n+@Singleton\n+public class WorkflowCleanUpUtil {", "originalCommit": "9e7223b2a4af6c138e4febe06f2cdb3d4b85370f", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDY5NTcxNg==", "url": "https://github.com/Netflix/conductor/pull/1672#discussion_r424695716", "bodyText": "Moved to another utility class : WorkflowTestUtil and added documentation", "author": "pctreddy", "createdAt": "2020-05-13T19:58:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzM5MTAwNw=="}], "type": "inlineReview"}, {"oid": "3cb19c807462c3718c0b2e8b6addb702ab696cc5", "url": "https://github.com/Netflix/conductor/commit/3cb19c807462c3718c0b2e8b6addb702ab696cc5", "message": "Simple workflow integration test additional tests", "committedDate": "2020-05-12T16:38:55Z", "type": "commit"}, {"oid": "204f2b13d98ed736ecd48e700fcbcb910d09eeb7", "url": "https://github.com/Netflix/conductor/commit/204f2b13d98ed736ecd48e700fcbcb910d09eeb7", "message": "Simple workflow integration test with retry at workflow level feature", "committedDate": "2020-05-12T18:19:20Z", "type": "commit"}, {"oid": "1176cf587a66bbf312810cc61bf60dd9a218ee3c", "url": "https://github.com/Netflix/conductor/commit/1176cf587a66bbf312810cc61bf60dd9a218ee3c", "message": "clean up", "committedDate": "2020-05-12T23:05:11Z", "type": "commit"}, {"oid": "b3e58729e875870eb72e1b08891eaccf9217dfd7", "url": "https://github.com/Netflix/conductor/commit/b3e58729e875870eb72e1b08891eaccf9217dfd7", "message": "Introduced WorkflowTestUtil to enable reuse of registration and clean up in all specifications", "committedDate": "2020-05-13T19:20:07Z", "type": "commit"}, {"oid": "54f5b544c388565759d1f87323963be9a7676554", "url": "https://github.com/Netflix/conductor/commit/54f5b544c388565759d1f87323963be9a7676554", "message": "Added documentation to the WorkflowTestUtil", "committedDate": "2020-05-13T19:56:58Z", "type": "commit"}]}