{"pr_number": 8672, "pr_title": "KAFKA-10002; Improve performances of StopReplicaRequest with large number of partitions to be deleted", "pr_createdAt": "2020-05-15T09:27:45Z", "pr_url": "https://github.com/apache/kafka/pull/8672", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjA4MzU4MQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r426083581", "bodyText": "Nit: typo ofr.", "author": "ijuma", "createdAt": "2020-05-15T23:10:37Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -324,8 +324,17 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n-  def stopReplica(topicPartition: TopicPartition, deletePartition: Boolean): Unit  = {\n+  def stopReplica(topicPartition: TopicPartition,\n+                  deletePartition: Boolean,\n+                  logDirs: mutable.Set[File]): Unit  = {\n     if (deletePartition) {\n+      val log = logManager.getLog(topicPartition)\n+      val futureLog = logManager.getLog(topicPartition, isFuture = true)\n+\n+      // Collect log dirs ofr the partition for future checkpointing", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjExNTgzMQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436115831", "bodyText": "nit: remove Logs from name?", "author": "hachikuji", "createdAt": "2020-06-05T19:15:02Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -465,12 +465,15 @@ class LogManager(logDirs: Seq[File],\n       for ((dir, dirJobs) <- jobs) {\n         dirJobs.foreach(_.get)\n \n+        val logsInDir = localLogsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+\n         // update the last flush point\n         debug(s\"Updating recovery points at $dir\")\n-        checkpointRecoveryOffsetsAndCleanSnapshot(dir, localLogsByDir.getOrElse(dir.toString, Map()).values.toSeq)\n+        checkpointLogsRecoveryOffsetsInDir(logsInDir, dir)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyMDg0Mg==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436120842", "bodyText": "There are a couple place where we have this code. Maybe it make sense to have an overload which accepts the directory.\nBy the way, logsByDir can be private.", "author": "hachikuji", "createdAt": "2020-06-05T19:26:44Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -580,53 +586,105 @@ class LogManager(logDirs: Seq[File],\n    * to avoid exposing data that have been deleted by DeleteRecordsRequest\n    */\n   def checkpointLogStartOffsets(): Unit = {\n-    liveLogDirs.foreach(checkpointLogStartOffsetsInDir)\n+    val logsByDirCached = logsByDir\n+    liveLogDirs.foreach { logDir =>\n+      checkpointLogsStartOffsetsInDir(\n+        logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty), logDir)\n+    }\n   }\n \n   /**\n-    * Write the recovery checkpoint file for all logs in provided directory and clean older snapshots for provided logs.\n-    *\n-    * @param dir the directory in which logs are checkpointed\n-    * @param logsToCleanSnapshot logs whose snapshots need to be cleaned\n-    */\n+   * Write the checkpoint files for all the provided directories. This is used to cleanup\n+   * checkpoints after having deleted partitions.\n+   */\n+  def checkpoint(logDirs: Set[File]): Unit = {\n+    val logsByDirCached = logsByDir\n+    logDirs.foreach { logDir =>\n+      val partitionToLog = logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty)\n+      if (cleaner != null) {\n+        cleaner.updateCheckpoints(logDir)\n+      }\n+      checkpointLogsRecoveryOffsetsInDir(partitionToLog, logDir)\n+      checkpointLogsStartOffsetsInDir(partitionToLog, logDir)\n+    }\n+  }\n+\n+  /**\n+   * Clean snapshots of the provided logs in the provided directory.\n+   *\n+   * @param logsToCleanSnapshot the logs whose snapshots will be cleaned\n+   * @param dir the directory in which the logs are\n+   */\n   // Only for testing\n-  private[log] def checkpointRecoveryOffsetsAndCleanSnapshot(dir: File, logsToCleanSnapshot: Seq[Log]): Unit = {\n+  private[log] def cleanSnapshotsInDir(logsToCleanSnapshot: Seq[Log], dir: File): Unit = {\n     try {\n-      checkpointLogRecoveryOffsetsInDir(dir)\n       logsToCleanSnapshot.foreach(_.deleteSnapshotsAfterRecoveryPointCheckpoint())\n     } catch {\n       case e: IOException =>\n-        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath, s\"Disk error while writing to recovery point \" +\n-          s\"file in directory $dir\", e)\n+        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath,\n+          s\"Disk error while writing to recovery point file in directory $dir\", e)\n     }\n   }\n \n-  private def checkpointLogRecoveryOffsetsInDir(dir: File): Unit = {\n-    for {\n-      partitionToLog <- logsByDir.get(dir.getAbsolutePath)\n-      checkpoint <- recoveryPointCheckpoints.get(dir)\n-    } {\n-      checkpoint.write(partitionToLog.map { case (tp, log) => tp -> log.recoveryPoint })\n+  /**\n+   * Checkpoint log recovery offsets for all the logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  // Only for testing\n+  private[log] def checkpointRecoveryOffsetsInDir(dir: File): Unit = {\n+    val partitionToLog = logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+    checkpointLogsRecoveryOffsetsInDir(partitionToLog, dir)\n+  }\n+\n+  /**\n+   * Checkpoint log recovery and start offsets for all logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  private def checkpointRecoveryAndLogStartOffsetsInDir(dir: File): Unit = {\n+    val partitionToLog = logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNDU5NQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436124595", "bodyText": "Given that we split checkpointRecoveryOffsetsAndCleanSnapshot, I'm curious why we added this.", "author": "hachikuji", "createdAt": "2020-06-05T19:36:00Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -580,53 +586,105 @@ class LogManager(logDirs: Seq[File],\n    * to avoid exposing data that have been deleted by DeleteRecordsRequest\n    */\n   def checkpointLogStartOffsets(): Unit = {\n-    liveLogDirs.foreach(checkpointLogStartOffsetsInDir)\n+    val logsByDirCached = logsByDir\n+    liveLogDirs.foreach { logDir =>\n+      checkpointLogsStartOffsetsInDir(\n+        logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty), logDir)\n+    }\n   }\n \n   /**\n-    * Write the recovery checkpoint file for all logs in provided directory and clean older snapshots for provided logs.\n-    *\n-    * @param dir the directory in which logs are checkpointed\n-    * @param logsToCleanSnapshot logs whose snapshots need to be cleaned\n-    */\n+   * Write the checkpoint files for all the provided directories. This is used to cleanup\n+   * checkpoints after having deleted partitions.\n+   */\n+  def checkpoint(logDirs: Set[File]): Unit = {\n+    val logsByDirCached = logsByDir\n+    logDirs.foreach { logDir =>\n+      val partitionToLog = logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty)\n+      if (cleaner != null) {\n+        cleaner.updateCheckpoints(logDir)\n+      }\n+      checkpointLogsRecoveryOffsetsInDir(partitionToLog, logDir)\n+      checkpointLogsStartOffsetsInDir(partitionToLog, logDir)\n+    }\n+  }\n+\n+  /**\n+   * Clean snapshots of the provided logs in the provided directory.\n+   *\n+   * @param logsToCleanSnapshot the logs whose snapshots will be cleaned\n+   * @param dir the directory in which the logs are\n+   */\n   // Only for testing\n-  private[log] def checkpointRecoveryOffsetsAndCleanSnapshot(dir: File, logsToCleanSnapshot: Seq[Log]): Unit = {\n+  private[log] def cleanSnapshotsInDir(logsToCleanSnapshot: Seq[Log], dir: File): Unit = {\n     try {\n-      checkpointLogRecoveryOffsetsInDir(dir)\n       logsToCleanSnapshot.foreach(_.deleteSnapshotsAfterRecoveryPointCheckpoint())\n     } catch {\n       case e: IOException =>\n-        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath, s\"Disk error while writing to recovery point \" +\n-          s\"file in directory $dir\", e)\n+        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath,\n+          s\"Disk error while writing to recovery point file in directory $dir\", e)\n     }\n   }\n \n-  private def checkpointLogRecoveryOffsetsInDir(dir: File): Unit = {\n-    for {\n-      partitionToLog <- logsByDir.get(dir.getAbsolutePath)\n-      checkpoint <- recoveryPointCheckpoints.get(dir)\n-    } {\n-      checkpoint.write(partitionToLog.map { case (tp, log) => tp -> log.recoveryPoint })\n+  /**\n+   * Checkpoint log recovery offsets for all the logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  // Only for testing\n+  private[log] def checkpointRecoveryOffsetsInDir(dir: File): Unit = {\n+    val partitionToLog = logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+    checkpointLogsRecoveryOffsetsInDir(partitionToLog, dir)\n+  }\n+\n+  /**\n+   * Checkpoint log recovery and start offsets for all logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  private def checkpointRecoveryAndLogStartOffsetsInDir(dir: File): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUxOTc5Ng==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436519796", "bodyText": "It is a small optimization to avoid computing the logsByDir mapping twice when both we checkpoint both at the same time. Computing the mapping is quite expensive when there are many logs.", "author": "dajac", "createdAt": "2020-06-08T08:02:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNDU5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyNzg0OA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436127848", "bodyText": "nit: k -> tp?", "author": "hachikuji", "createdAt": "2020-06-05T19:43:57Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -580,53 +586,105 @@ class LogManager(logDirs: Seq[File],\n    * to avoid exposing data that have been deleted by DeleteRecordsRequest\n    */\n   def checkpointLogStartOffsets(): Unit = {\n-    liveLogDirs.foreach(checkpointLogStartOffsetsInDir)\n+    val logsByDirCached = logsByDir\n+    liveLogDirs.foreach { logDir =>\n+      checkpointLogsStartOffsetsInDir(\n+        logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty), logDir)\n+    }\n   }\n \n   /**\n-    * Write the recovery checkpoint file for all logs in provided directory and clean older snapshots for provided logs.\n-    *\n-    * @param dir the directory in which logs are checkpointed\n-    * @param logsToCleanSnapshot logs whose snapshots need to be cleaned\n-    */\n+   * Write the checkpoint files for all the provided directories. This is used to cleanup\n+   * checkpoints after having deleted partitions.\n+   */\n+  def checkpoint(logDirs: Set[File]): Unit = {\n+    val logsByDirCached = logsByDir\n+    logDirs.foreach { logDir =>\n+      val partitionToLog = logsByDirCached.getOrElse(logDir.getAbsolutePath, Map.empty)\n+      if (cleaner != null) {\n+        cleaner.updateCheckpoints(logDir)\n+      }\n+      checkpointLogsRecoveryOffsetsInDir(partitionToLog, logDir)\n+      checkpointLogsStartOffsetsInDir(partitionToLog, logDir)\n+    }\n+  }\n+\n+  /**\n+   * Clean snapshots of the provided logs in the provided directory.\n+   *\n+   * @param logsToCleanSnapshot the logs whose snapshots will be cleaned\n+   * @param dir the directory in which the logs are\n+   */\n   // Only for testing\n-  private[log] def checkpointRecoveryOffsetsAndCleanSnapshot(dir: File, logsToCleanSnapshot: Seq[Log]): Unit = {\n+  private[log] def cleanSnapshotsInDir(logsToCleanSnapshot: Seq[Log], dir: File): Unit = {\n     try {\n-      checkpointLogRecoveryOffsetsInDir(dir)\n       logsToCleanSnapshot.foreach(_.deleteSnapshotsAfterRecoveryPointCheckpoint())\n     } catch {\n       case e: IOException =>\n-        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath, s\"Disk error while writing to recovery point \" +\n-          s\"file in directory $dir\", e)\n+        logDirFailureChannel.maybeAddOfflineLogDir(dir.getAbsolutePath,\n+          s\"Disk error while writing to recovery point file in directory $dir\", e)\n     }\n   }\n \n-  private def checkpointLogRecoveryOffsetsInDir(dir: File): Unit = {\n-    for {\n-      partitionToLog <- logsByDir.get(dir.getAbsolutePath)\n-      checkpoint <- recoveryPointCheckpoints.get(dir)\n-    } {\n-      checkpoint.write(partitionToLog.map { case (tp, log) => tp -> log.recoveryPoint })\n+  /**\n+   * Checkpoint log recovery offsets for all the logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  // Only for testing\n+  private[log] def checkpointRecoveryOffsetsInDir(dir: File): Unit = {\n+    val partitionToLog = logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+    checkpointLogsRecoveryOffsetsInDir(partitionToLog, dir)\n+  }\n+\n+  /**\n+   * Checkpoint log recovery and start offsets for all logs in the provided directory.\n+   *\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  private def checkpointRecoveryAndLogStartOffsetsInDir(dir: File): Unit = {\n+    val partitionToLog = logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+    checkpointLogsRecoveryOffsetsInDir(partitionToLog, dir)\n+    checkpointLogsStartOffsetsInDir(partitionToLog, dir)\n+  }\n+\n+  /**\n+   * Checkpoint log recovery offsets for all the provided logs in the provided directory.\n+   *\n+   * @param logs the logs and logs to be checkpointed\n+   * @param dir the directory in which logs are checkpointed\n+   */\n+  private def checkpointLogsRecoveryOffsetsInDir(logs: Map[TopicPartition, Log],\n+                                                 dir: File): Unit = {\n+    try {\n+      recoveryPointCheckpoints.get(dir).foreach { checkpoint =>\n+        val recoveryOffsets = logs.map { case (tp, log) => tp -> log.recoveryPoint }\n+        checkpoint.write(recoveryOffsets)\n+      }\n+    } catch {\n+      case e: KafkaStorageException =>\n+        error(s\"Disk error while writing recovery offsets checkpoint in directory $dir: ${e.getMessage}\")\n     }\n   }\n \n   /**\n-   * Checkpoint log start offset for all logs in provided directory.\n+   * Checkpoint log start offsets for all the provided logs in the provided directory.\n+   *\n+   * @param logs the partitions and logs to be checkpointed\n+   * @param dir the directory in which logs are checkpointed\n    */\n-  private def checkpointLogStartOffsetsInDir(dir: File): Unit = {\n-    for {\n-      partitionToLog <- logsByDir.get(dir.getAbsolutePath)\n-      checkpoint <- logStartOffsetCheckpoints.get(dir)\n-    } {\n-      try {\n-        val logStartOffsets = partitionToLog.collect {\n+  private def checkpointLogsStartOffsetsInDir(logs: Map[TopicPartition, Log],\n+                                              dir: File): Unit = {\n+    try {\n+      logStartOffsetCheckpoints.get(dir).foreach { checkpoint =>\n+        val logStartOffsets = logs.collect {\n           case (k, log) if log.logStartOffset > log.logSegments.head.baseOffset => k -> log.logStartOffset", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzMTA3Mw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436131073", "bodyText": "Previously we were cleaning snapshots here. I agree it was probably unnecessary, but there seems little harm in it. I guess the main thing is that logically it makes little sense to couple these operations together even if the events which trigger them are often the same. Is that what you were thinking?", "author": "hachikuji", "createdAt": "2020-06-05T19:50:27Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -878,8 +936,7 @@ class LogManager(logDirs: Seq[File],\n         // Now that replica in source log directory has been successfully renamed for deletion.\n         // Close the log, update checkpoint files, and enqueue this log to be deleted.\n         sourceLog.close()\n-        checkpointRecoveryOffsetsAndCleanSnapshot(sourceLog.parentDirFile, ArrayBuffer.empty)\n-        checkpointLogStartOffsetsInDir(sourceLog.parentDirFile)\n+        checkpointRecoveryAndLogStartOffsetsInDir(sourceLog.parentDirFile)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUyMzkxNg==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436523916", "bodyText": "Actually, we were not cleaning snapshots here. checkpointRecoveryOffsetsAndCleanSnapshot was cleaning the snapshots of the logs passed in the second argument. ArrayBuffer.empty in our particular case.\nIndeed, I thought that there is little sense to couple checkpointing and cleaning the snapshots given that they operate on separate arguments. We were checkpointing based on the first argument and cleaning based on the second one. It is more misleading than anything else.", "author": "dajac", "createdAt": "2020-06-08T08:10:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzMTA3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjc5MTYwMg==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436791602", "bodyText": "Well, they are tightly coupled right? The method name of deleteSnapshotsAfterRecoveryPointCheckpoint makes it clear that this should be called after the recovery point is checkpointed. Generally, we've had bugs whenever we left it to the callers to make the same multiple calls in sequence in multiple places.\nI haven't looked at this PR in detail, so there are probably good reasons to change it. Also keep in mind #7929 which tries to improve the approach on how we handle this more generally.", "author": "ijuma", "createdAt": "2020-06-08T15:22:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzMTA3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgxMTk0NA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436811944", "bodyText": "Indeed, they are coupled from that perspective. We can keep them together in one method though.", "author": "dajac", "createdAt": "2020-06-08T15:51:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzMTA3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzNTA4Nw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436135087", "bodyText": "I believe it would be straightforward to update the uses of this method in the test case to use stopReplicas instead. Then we could make this private, which would make the side effect less annoying.", "author": "hachikuji", "createdAt": "2020-06-05T19:55:46Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -324,8 +324,17 @@ class ReplicaManager(val config: KafkaConfig,\n       brokerTopicStats.removeMetrics(topic)\n   }\n \n-  def stopReplica(topicPartition: TopicPartition, deletePartition: Boolean): Unit  = {\n+  def stopReplica(topicPartition: TopicPartition,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUzMjEzNQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436532135", "bodyText": "I agree.", "author": "dajac", "createdAt": "2020-06-08T08:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzNTA4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzNjg1OA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436136858", "bodyText": "It is a little weird to abort and pause cleaning if the partition is getting deleted. Maybe we should have a separate method like removePartition or something like that.", "author": "hachikuji", "createdAt": "2020-06-05T19:58:20Z", "path": "core/src/main/scala/kafka/log/LogCleanerManager.scala", "diffHunk": "@@ -273,7 +272,7 @@ private[log] class LogCleanerManager(val logDirs: Seq[File],\n    *  6. If the partition is already paused, a new call to this function\n    *     will increase the paused count by one.\n    */\n-  def abortAndPauseCleaning(topicPartition: TopicPartition): Unit = {\n+  def abortAndPauseCleaning(topicPartition: TopicPartition, partitionDeleted: Boolean = false): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjcwNDgwMA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436704800", "bodyText": "Actually, we already have abortCleaning for this purpose and abortCleaning calls abortAndPauseCleaning. I was trying to avoid logging a message when the partition is deleted because it does not bring much and literally flood the log when many partitions are deleted.\nWhile re-looking at this, I have done it differently now. I have found that logs were spread between the LogCleanerManager and the LogManager and that we were logging when resuming cleaning but not all the time. I have consolidated all the cases where we want to log explicitly in helper methods. It also helps with not always having to check if cleaner != null. Let me know what you think.", "author": "dajac", "createdAt": "2020-06-08T13:34:22Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEzNjg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MDAzMw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436140033", "bodyText": "With this change, it is up to the caller to ensure offsets are checkpointed correctly after a deletion. It would be better from an encapsulation perspective to keep LogManager in charge of that. One idea would be to offer a batched asyncDelete. The involvement of Partition may make this a bit complex though.", "author": "hachikuji", "createdAt": "2020-06-05T20:06:05Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -912,12 +969,9 @@ class LogManager(logDirs: Seq[File],\n     if (removedLog != null) {\n       //We need to wait until there is no more cleaning task on the log to be deleted before actually deleting it.\n       if (cleaner != null && !isFuture) {\n-        cleaner.abortCleaning(topicPartition)\n-        cleaner.updateCheckpoints(removedLog.parentDirFile)\n+        cleaner.abortCleaning(topicPartition, partitionDeleted = true)\n       }\n       removedLog.renameDir(Log.logDeleteDirName(topicPartition))\n-      checkpointRecoveryOffsetsAndCleanSnapshot(removedLog.parentDirFile, ArrayBuffer.empty)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjcxODMyMA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436718320", "bodyText": "I totally agree with you. I have tried different ways trying to keep a better encapsulation but I have found a really satisfying way to get there. As you pointed out, the involvement of Partition makes this complex.\nHere is my best idea so far:\n\nWe remove the calls to asyncDelete in the Partition.delete method. It seems safe to \"delete the partition\" while keeping the files on disk while holding the replicaStateChangeLock in the ReplicaManager.\nWe use a asyncDelete that takes a batch of logs, deletes them and checkpoint.\nWe rename Partition.delete to something like Partition.close as the log is not really deleted any more.\n\nWhat do you think?", "author": "dajac", "createdAt": "2020-06-08T13:50:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MDAzMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjgwNDcyNQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r436804725", "bodyText": "I have implemented it to see. You can check out the last commit.", "author": "dajac", "createdAt": "2020-06-08T15:41:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE0MDAzMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzM1ODc5Nw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r437358797", "bodyText": "FYI: This block of code appears twice so it would merit its own method. I haven't done it because that I felt that it would mix too many things in one method and the method name would be too long. Therefore, I left the optimisation of computing and reusing the logsByDir on the caller side. I don't feel too strongly about this though.", "author": "dajac", "createdAt": "2020-06-09T12:11:11Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -869,17 +903,18 @@ class LogManager(logDirs: Seq[File],\n       currentLogs.put(topicPartition, destLog)\n       if (cleaner != null) {\n         cleaner.alterCheckpointDir(topicPartition, sourceLog.parentDirFile, destLog.parentDirFile)\n-        cleaner.resumeCleaning(Seq(topicPartition))\n-        info(s\"Compaction for partition $topicPartition is resumed\")\n+        resumeCleaning(topicPartition)\n       }\n \n       try {\n         sourceLog.renameDir(Log.logDeleteDirName(topicPartition))\n         // Now that replica in source log directory has been successfully renamed for deletion.\n         // Close the log, update checkpoint files, and enqueue this log to be deleted.\n         sourceLog.close()\n-        checkpointRecoveryOffsetsAndCleanSnapshot(sourceLog.parentDirFile, ArrayBuffer.empty)\n-        checkpointLogStartOffsetsInDir(sourceLog.parentDirFile)\n+        val logDir = sourceLog.parentDirFile\n+        val logsToCheckpoint = logsByDir(logDir)\n+        checkpointRecoveryOffsetsAndCleanSnapshotsInDir(logDir, logsToCheckpoint, ArrayBuffer.empty)\n+        checkpointLogStartOffsetsInDir(logDir, logsToCheckpoint)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "4ea6df3eb76cf4b7b5eb9f27cd099171ff428f57", "url": "https://github.com/apache/kafka/commit/4ea6df3eb76cf4b7b5eb9f27cd099171ff428f57", "message": "KAFKA-10002; Improve performances of StopReplicaRequest with large number of partitions to be deleted", "committedDate": "2020-06-18T19:43:32Z", "type": "commit"}, {"oid": "affaff5becff5bd8d48279eb6f2cd917f6317d6b", "url": "https://github.com/apache/kafka/commit/affaff5becff5bd8d48279eb6f2cd917f6317d6b", "message": "Refactor based on feedback", "committedDate": "2020-06-18T19:43:32Z", "type": "commit"}, {"oid": "afb990a8dbc8075001f7b558ee5647ab0e0a8efe", "url": "https://github.com/apache/kafka/commit/afb990a8dbc8075001f7b558ee5647ab0e0a8efe", "message": "Few more refactor", "committedDate": "2020-06-18T19:43:32Z", "type": "commit"}, {"oid": "543536bee3bcb1910d957a7a80c412695e279f24", "url": "https://github.com/apache/kafka/commit/543536bee3bcb1910d957a7a80c412695e279f24", "message": "Add a new LogManager.asyncDelete that works on a batch of topic-partitions", "committedDate": "2020-06-18T19:43:32Z", "type": "commit"}, {"oid": "115ff6bb27ac6b3200a9316c4e8fed92f248d32f", "url": "https://github.com/apache/kafka/commit/115ff6bb27ac6b3200a9316c4e8fed92f248d32f", "message": "Rework checkpointing methods", "committedDate": "2020-06-18T19:43:32Z", "type": "commit"}, {"oid": "da7216dec5d8d18ab728004c77b87ebdbfa94ca3", "url": "https://github.com/apache/kafka/commit/da7216dec5d8d18ab728004c77b87ebdbfa94ca3", "message": "nits", "committedDate": "2020-06-18T19:43:33Z", "type": "commit"}, {"oid": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "url": "https://github.com/apache/kafka/commit/d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "message": "fixup", "committedDate": "2020-06-18T19:48:55Z", "type": "commit"}, {"oid": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "url": "https://github.com/apache/kafka/commit/d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "message": "fixup", "committedDate": "2020-06-18T19:48:55Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3NTQ3Mg==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r445675472", "bodyText": "The caller seems to always set deleteLogs to false. Could we just remove this param and the associated code?", "author": "junrao", "createdAt": "2020-06-25T16:12:53Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -459,7 +459,12 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  def delete(): Unit = {\n+  /**\n+   * Delete the partition. The underlying logs are deleted by default but one can choose to not\n+   * delete them automatically and to delete them manually later one. For instance, we do this\n+   * in the handling of the StopReplicaRequest to batch the deletions and checkpoint only once.\n+   */\n+  def delete(deleteLogs: Boolean = true): Unit = {", "originalCommit": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5MDkzNw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r446190937", "bodyText": "Yes, we can.", "author": "dajac", "createdAt": "2020-06-26T13:41:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY3NTQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4MDkyNA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r445680924", "bodyText": "It's probably useful to log the class of the exception too.", "author": "junrao", "createdAt": "2020-06-25T16:21:03Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -433,29 +425,34 @@ class ReplicaManager(val config: KafkaConfig,\n             case HostedPartition.None =>\n               // Delete log and corresponding folders in case replica manager doesn't hold them anymore.\n               // This could happen when topic is being deleted while broker is down and recovers.\n-              stoppedPartitions += topicPartition -> partitionState\n+              stoppedPartitions += topicPartition\n+              if (deletePartition)\n+                deletedPartitions += topicPartition\n+              responseMap.put(topicPartition, Errors.NONE)\n           }\n         }\n \n         // First stop fetchers for all partitions, then stop the corresponding replicas\n-        val partitions = stoppedPartitions.keySet\n-        replicaFetcherManager.removeFetcherForPartitions(partitions)\n-        replicaAlterLogDirsManager.removeFetcherForPartitions(partitions)\n+        replicaFetcherManager.removeFetcherForPartitions(stoppedPartitions)\n+        replicaAlterLogDirsManager.removeFetcherForPartitions(stoppedPartitions)\n \n-        stoppedPartitions.foreach { case (topicPartition, partitionState) =>\n-          val deletePartition = partitionState.deletePartition\n-          try {\n-            stopReplica(topicPartition, deletePartition)\n-            responseMap.put(topicPartition, Errors.NONE)\n-          } catch {\n+        // Delete the logs and checkpoint\n+        logManager.asyncDelete(deletedPartitions, (topicPartition, exception) => {\n+          exception match {\n             case e: KafkaStorageException =>\n-              stateChangeLogger.error(s\"Ignoring StopReplica request (delete=$deletePartition) from \" +\n+              stateChangeLogger.error(s\"Ignoring StopReplica request (delete=true) from \" +\n                 s\"controller $controllerId with correlation id $correlationId \" +\n                 s\"epoch $controllerEpoch for partition $topicPartition as the local replica for the \" +\n-                \"partition is in an offline log directory\", e)\n+                \"partition is in an offline log directory\")\n               responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)\n+\n+            case e =>\n+              stateChangeLogger.error(s\"Ignoring StopReplica request (delete=true) from \" +\n+                s\"controller $controllerId with correlation id $correlationId \" +\n+                s\"epoch $controllerEpoch for partition $topicPartition due to ${e.getMessage}\")", "originalCommit": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5MjIxNw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r446192217", "bodyText": "That makes sense.", "author": "dajac", "createdAt": "2020-06-26T13:43:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4MDkyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4MzY1Ng==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r445683656", "bodyText": "This probably should be called logsInDir() ?", "author": "junrao", "createdAt": "2020-06-25T16:25:16Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -1018,6 +1097,15 @@ class LogManager(logDirs: Seq[File],\n     byDir\n   }\n \n+  private def logsByDir(dir: File): Map[TopicPartition, Log] = {", "originalCommit": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5MjkwOQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r446192909", "bodyText": "Good point.", "author": "dajac", "createdAt": "2020-06-26T13:44:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4MzY1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4NjEyNA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r445686124", "bodyText": "This probably should be called logsInDir() ?", "author": "junrao", "createdAt": "2020-06-25T16:29:01Z", "path": "core/src/main/scala/kafka/log/LogManager.scala", "diffHunk": "@@ -1018,6 +1097,15 @@ class LogManager(logDirs: Seq[File],\n     byDir\n   }\n \n+  private def logsByDir(dir: File): Map[TopicPartition, Log] = {\n+    logsByDir.getOrElse(dir.getAbsolutePath, Map.empty)\n+  }\n+\n+  private def logsByDir(cachedLogsByDir: Map[String, Map[TopicPartition, Log]],", "originalCommit": "d7cd1f6df32b5db4f78a470edb4a5f967476e16a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjE5Mjk1Mw==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r446192953", "bodyText": "Good point.", "author": "dajac", "createdAt": "2020-06-26T13:44:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTY4NjEyNA=="}], "type": "inlineReview"}, {"oid": "7557c7e147d958ee2cfd0cc74ad5644b02052551", "url": "https://github.com/apache/kafka/commit/7557c7e147d958ee2cfd0cc74ad5644b02052551", "message": "Address Jun's comments", "committedDate": "2020-06-26T13:50:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjc2OQ==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r453132769", "bodyText": "Could we verify the expected logStartOffset?", "author": "junrao", "createdAt": "2020-07-11T00:41:36Z", "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -2058,13 +2070,37 @@ class ReplicaManagerTest {\n     val partition = replicaManager.createPartition(tp0)\n     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints)\n \n+    val logDirFailureChannel = new LogDirFailureChannel(replicaManager.config.logDirs.size)\n+    val logDir = partition.log.get.parentDirFile\n+\n+    def readRecoveryPointCheckpoint(): Map[TopicPartition, Long] = {\n+      new OffsetCheckpointFile(new File(logDir, LogManager.RecoveryPointCheckpointFile),\n+        logDirFailureChannel).read()\n+    }\n+\n+    def readLogStartOffsetCheckpoint(): Map[TopicPartition, Long] = {\n+      new OffsetCheckpointFile(new File(logDir, LogManager.LogStartOffsetCheckpointFile),\n+        logDirFailureChannel).read()\n+    }\n+\n     val becomeLeaderRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n       Seq(leaderAndIsrPartitionState(tp0, 1, 0, Seq(0, 1), true)).asJava,\n       Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava\n     ).build()\n \n     replicaManager.becomeLeaderOrFollower(1, becomeLeaderRequest, (_, _) => ())\n \n+    val batch = TestUtils.records(records = List(\n+      new SimpleRecord(10, \"k1\".getBytes, \"v1\".getBytes),\n+      new SimpleRecord(11, \"k2\".getBytes, \"v2\".getBytes)))\n+    partition.appendRecordsToLeader(batch, AppendOrigin.Client, requiredAcks = 0)\n+    partition.log.get.updateHighWatermark(2L)\n+    partition.log.get.maybeIncrementLogStartOffset(1L, LeaderOffsetIncremented)\n+    replicaManager.logManager.checkpointLogRecoveryOffsets()\n+    replicaManager.logManager.checkpointLogStartOffsets()\n+    assertTrue(readRecoveryPointCheckpoint().contains(tp0))\n+    assertTrue(readLogStartOffsetCheckpoint().contains(tp0))", "originalCommit": "7557c7e147d958ee2cfd0cc74ad5644b02052551", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY0MjUxMA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r453642510", "bodyText": "Sure.", "author": "dajac", "createdAt": "2020-07-13T13:19:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjc2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjgxNA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r453132814", "bodyText": "Could we verify the expected recovery offset?", "author": "junrao", "createdAt": "2020-07-11T00:41:57Z", "path": "core/src/test/scala/unit/kafka/server/ReplicaManagerTest.scala", "diffHunk": "@@ -2058,13 +2070,37 @@ class ReplicaManagerTest {\n     val partition = replicaManager.createPartition(tp0)\n     partition.createLogIfNotExists(isNew = false, isFutureReplica = false, offsetCheckpoints)\n \n+    val logDirFailureChannel = new LogDirFailureChannel(replicaManager.config.logDirs.size)\n+    val logDir = partition.log.get.parentDirFile\n+\n+    def readRecoveryPointCheckpoint(): Map[TopicPartition, Long] = {\n+      new OffsetCheckpointFile(new File(logDir, LogManager.RecoveryPointCheckpointFile),\n+        logDirFailureChannel).read()\n+    }\n+\n+    def readLogStartOffsetCheckpoint(): Map[TopicPartition, Long] = {\n+      new OffsetCheckpointFile(new File(logDir, LogManager.LogStartOffsetCheckpointFile),\n+        logDirFailureChannel).read()\n+    }\n+\n     val becomeLeaderRequest = new LeaderAndIsrRequest.Builder(ApiKeys.LEADER_AND_ISR.latestVersion, 0, 0, brokerEpoch,\n       Seq(leaderAndIsrPartitionState(tp0, 1, 0, Seq(0, 1), true)).asJava,\n       Set(new Node(0, \"host1\", 0), new Node(1, \"host2\", 1)).asJava\n     ).build()\n \n     replicaManager.becomeLeaderOrFollower(1, becomeLeaderRequest, (_, _) => ())\n \n+    val batch = TestUtils.records(records = List(\n+      new SimpleRecord(10, \"k1\".getBytes, \"v1\".getBytes),\n+      new SimpleRecord(11, \"k2\".getBytes, \"v2\".getBytes)))\n+    partition.appendRecordsToLeader(batch, AppendOrigin.Client, requiredAcks = 0)\n+    partition.log.get.updateHighWatermark(2L)\n+    partition.log.get.maybeIncrementLogStartOffset(1L, LeaderOffsetIncremented)\n+    replicaManager.logManager.checkpointLogRecoveryOffsets()\n+    replicaManager.logManager.checkpointLogStartOffsets()\n+    assertTrue(readRecoveryPointCheckpoint().contains(tp0))", "originalCommit": "7557c7e147d958ee2cfd0cc74ad5644b02052551", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzY0MjU2OA==", "url": "https://github.com/apache/kafka/pull/8672#discussion_r453642568", "bodyText": "Sure.", "author": "dajac", "createdAt": "2020-07-13T13:19:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzEzMjgxNA=="}], "type": "inlineReview"}, {"oid": "3e23b90f9ac98309c0ec88de9315ef90537aba7d", "url": "https://github.com/apache/kafka/commit/3e23b90f9ac98309c0ec88de9315ef90537aba7d", "message": "Address Jun's comments", "committedDate": "2020-07-13T13:18:34Z", "type": "commit"}]}