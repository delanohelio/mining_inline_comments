{"pr_number": 8680, "pr_title": "KAFKA-10027: Implement read path for feature versioning system (KIP-584)", "pr_createdAt": "2020-05-17T09:00:58Z", "pr_url": "https://github.com/apache/kafka/pull/8680", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzM5Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426783396", "bodyText": "nit: we could use {@link VersionRangeType} to reference to the classes.", "author": "abbccdda", "createdAt": "2020-05-18T17:25:17Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMjMxNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427022316", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T04:25:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzM5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzY1Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426783652", "bodyText": "Could be simplified as new Features<>", "author": "abbccdda", "createdAt": "2020-05-18T17:25:44Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMzM3Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427023377", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T04:29:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzY1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzcyMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426783720", "bodyText": "Same here", "author": "abbccdda", "createdAt": "2020-05-18T17:25:51Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMzM5NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427023394", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T04:30:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjc4MzcyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNTQyNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426805425", "bodyText": "nit: extra line", "author": "abbccdda", "createdAt": "2020-05-18T18:06:33Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMzc3MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427023770", "bodyText": "Done. Removed it.", "author": "kowshik", "createdAt": "2020-05-19T04:31:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNTQyNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjMyMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426806321", "bodyText": "Is this function only used in unit test?", "author": "abbccdda", "createdAt": "2020-05-18T18:08:17Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMzcwNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427023705", "bodyText": "Done. Yes, I've changed it to default visibility now.", "author": "kowshik", "createdAt": "2020-05-19T04:31:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjMyMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjY3MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426806671", "bodyText": "We should ensure features is not null", "author": "abbccdda", "createdAt": "2020-05-18T18:09:02Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyMzI4MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427023280", "bodyText": "Do you feel strongly about this?\nThe reasons why I ask the question is:\n\nCaller is unlikely to pass null.\nI looked over a number of other existing classes in Kafka, and there aren't any null checks for most constructor parameters.\n\nIt will help me if you could share couple examples from existing code where the null check convention is followed in Kafka.", "author": "kowshik", "createdAt": "2020-05-19T04:29:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjY3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODA4MDA3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428080079", "bodyText": "Yea, the reasoning is that we have get call blindly look up inside features which in this case null is not valid. And I don't feel passing null makes sense for the caller, correct?", "author": "abbccdda", "createdAt": "2020-05-20T14:55:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjY3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3NjQxMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428576410", "bodyText": "Done. I'm raising an exception now if it is null.\nI see your point.\nWill be good to learn what is the convention in Kafka for constructor param null checks.", "author": "kowshik", "createdAt": "2020-05-21T10:39:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNjY3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNzY0Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426807646", "bodyText": "nit: just a personal preference, but getting one less internal reference to a public function all makes the code usage check easier, like features.get(feature).", "author": "abbccdda", "createdAt": "2020-05-18T18:10:58Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    public VersionRangeType get(String feature) {\n+        return all().get(feature);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNDM3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427024379", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T04:34:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwNzY0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwODQ1Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426808456", "bodyText": "Also if we could potentially have a not-found feature, we should either fail with illegal state, or make the return type Optional<VersionRangeType>", "author": "abbccdda", "createdAt": "2020-05-18T18:12:36Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    public VersionRangeType get(String feature) {\n+        return all().get(feature);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNTM3OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427025378", "bodyText": "The underlying data structure is a Map. It would be simpler if this method just returns null if the feature doesn't exist. For example, that is how java's Map.get works, here is the javadoc: https://docs.oracle.com/javase/8/docs/api/java/util/Map.html#get-java.lang.Object-.\nAlso, I've documented this method now (doc was previously absent).", "author": "kowshik", "createdAt": "2020-05-19T04:38:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwODQ1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwODg3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426808879", "bodyText": "Maybe rephrase as a map with the underlying features serialized", "author": "abbccdda", "createdAt": "2020-05-18T18:13:21Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    public VersionRangeType get(String feature) {\n+        return all().get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   Serializes the underlying features to a map, and returns the same.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNTUzOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427025539", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T04:38:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwODg3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwOTUwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426809504", "bodyText": "s/Deserializes/Deserialize", "author": "abbccdda", "createdAt": "2020-05-18T18:14:35Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    public VersionRangeType get(String feature) {\n+        return all().get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   Serializes the underlying features to a map, and returns the same.\n+     *           The returned value can be deserialized using one of the deserialize* APIs.\n+     */\n+    public Map<String, Map<String, Long>> serialize() {\n+        return features.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().serialize()));\n+    }\n+\n+    /**\n+     * Deserializes a map to Features<VersionLevelRange>.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNTU5Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427025597", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T04:39:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgwOTUwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMDkwOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426810909", "bodyText": "We should check null for other.", "author": "abbccdda", "createdAt": "2020-05-18T18:17:09Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    public VersionRangeType get(String feature) {\n+        return all().get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   Serializes the underlying features to a map, and returns the same.\n+     *           The returned value can be deserialized using one of the deserialize* APIs.\n+     */\n+    public Map<String, Map<String, Long>> serialize() {\n+        return features.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().serialize()));\n+    }\n+\n+    /**\n+     * Deserializes a map to Features<VersionLevelRange>.\n+     *\n+     * @param serialized   the serialized representation of a Features<VersionLevelRange> object,\n+     *                     generated using the serialize() API.\n+     *\n+     * @return             the deserialized Features<VersionLevelRange> object\n+     */\n+    public static Features<VersionLevelRange> deserializeFinalizedFeatures(\n+        Map<String, Map<String, Long>> serialized) {\n+        return finalizedFeatures(serialized.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> VersionLevelRange.deserialize(entry.getValue()))));\n+    }\n+\n+    /**\n+     * Deserializes a map to Features<VersionRange>.\n+     *\n+     * @param serialized   the serialized representation of a Features<VersionRange> object,\n+     *                     generated using the serialize() API.\n+     *\n+     * @return             the deserialized Features<VersionRange> object\n+     */\n+    public static Features<VersionRange> deserializeSupportedFeatures(\n+        Map<String, Map<String, Long>> serialized) {\n+        return supportedFeatures(serialized.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> VersionRange.deserialize(entry.getValue()))));\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNTg2Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427025866", "bodyText": "Done. Good point! Added test as well.", "author": "kowshik", "createdAt": "2020-05-19T04:40:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgxMDkwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgyOTQ1Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426829453", "bodyText": "nit: might make sense to build meta comment for parameters:\n/**\n  * \n  * @param id\n  * @param endPoints\n  * @param rack\n  * @param features\n  */", "author": "abbccdda", "createdAt": "2020-05-18T18:53:37Z", "path": "core/src/main/scala/kafka/cluster/Broker.scala", "diffHunk": "@@ -34,14 +36,19 @@ object Broker {\n                                          brokerId: Int,\n                                          endpoints: util.List[Endpoint],\n                                          interBrokerEndpoint: Endpoint) extends AuthorizerServerInfo\n+\n+  def apply(id: Int, endPoints: Seq[EndPoint], rack: Option[String]): Broker = {\n+    new Broker(id, endPoints, rack, emptySupportedFeatures)\n+  }\n }\n \n /**\n  * A Kafka broker.\n- * A broker has an id, a collection of end-points, an optional rack and a listener to security protocol map.\n+ * A broker has an id, a collection of end-points, an optional rack and a listener to security protocol map,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTEzNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427045136", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T05:51:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgyOTQ1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzNjk2Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426836963", "bodyText": "s/ !featuresAndEpoch.isEmpty / featuresAndEpoch.isDefined", "author": "abbccdda", "createdAt": "2020-05-18T19:07:57Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {\n+    newEpoch >= epoch\n+  }\n+\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * FinalizedFeatureChangeListener.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest\n+ * returning the features information in the response. In the future, as the feature versioning\n+ * system in KIP-584 is used more widely, this cache could be read by other read paths trying to\n+ * learn the finalized feature information.\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {\n+    featuresAndEpoch.isEmpty\n+  }\n+\n+  /**\n+   * Clears all existing finalized features and epoch from the cache.\n+   */\n+  def clear(): Unit = {\n+    featuresAndEpoch = Option.empty\n+    info(\"Cleared cache\")\n+  }\n+\n+  /**\n+   * Updates the cache to the latestFeatures, and updates the existing epoch to latestEpoch.\n+   * Raises an exception when the operation is not successful.\n+   *\n+   * @param latestFeatures   the latest finalized features to be set in the cache\n+   * @param latestEpoch      the latest epoch value to be set in the cache\n+   *\n+   * @throws                 FeatureCacheUpdateException if the cache update operation fails\n+   *                         due to invalid parameters or incompatibilities with the broker's\n+   *                         supported features. In such a case, the existing cache contents are\n+   *                         not modified.\n+   */\n+  def updateOrThrow(latestFeatures: Features[VersionLevelRange], latestEpoch: Int): Unit = {\n+    updateOrThrow(FinalizedFeaturesAndEpoch(latestFeatures, latestEpoch))\n+  }\n+\n+  private def updateOrThrow(latest: FinalizedFeaturesAndEpoch): Unit = {\n+    val existingStr = featuresAndEpoch.map(existing => existing.toString).getOrElse(\"<empty>\")\n+    if (!featuresAndEpoch.isEmpty && featuresAndEpoch.get.epoch > latest.epoch) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NjAwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427046004", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:54:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzNjk2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzODA2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426838064", "bodyText": "This is because the write path has not been implemented?", "author": "abbccdda", "createdAt": "2020-05-18T19:10:06Z", "path": "core/src/main/scala/kafka/cluster/Broker.scala", "diffHunk": "@@ -34,14 +36,19 @@ object Broker {\n                                          brokerId: Int,\n                                          endpoints: util.List[Endpoint],\n                                          interBrokerEndpoint: Endpoint) extends AuthorizerServerInfo\n+\n+  def apply(id: Int, endPoints: Seq[EndPoint], rack: Option[String]): Broker = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NDM0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427044348", "bodyText": "No, this constructor overload was simply created to avoid a churn of test code at number of places adding the additional SupportedFeatures parameter. How do you feel about keeping it?", "author": "kowshik", "createdAt": "2020-05-19T05:49:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjgzODA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg2NTc3Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426865773", "bodyText": "nit: add a line", "author": "abbccdda", "createdAt": "2020-05-18T20:07:15Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,70 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionRange, VersionLevelRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NzQwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427047400", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:58:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg2NTc3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3MzM1OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426873359", "bodyText": "I think we need to bump the schema version to 4? Same with ApiVersionsRequest.json", "author": "abbccdda", "createdAt": "2020-05-18T20:23:39Z", "path": "clients/src/main/resources/common/message/ApiVersionsResponse.json", "diffHunk": "@@ -42,6 +42,33 @@\n         \"about\": \"The maximum supported version, inclusive.\" }\n     ]},\n     { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"1+\", \"ignorable\": true,\n-      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" }\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzNDc1OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427034758", "bodyText": "Are you sure? All newly added fields are tagged (i.e. optional).\nGoing by this documentation in KIP-482, it is not required to change the schema version whenever tagged fields are introduced.", "author": "kowshik", "createdAt": "2020-05-19T05:14:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3MzM1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NTc2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429495765", "bodyText": "I see, makes sense.", "author": "abbccdda", "createdAt": "2020-05-23T00:22:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3MzM1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NTQzNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426875434", "bodyText": "Looks like we have some gaps for unit testing ApiVersionsResponse. Could we add unit tests for this class, since the logic createApiVersionsResponse becomes non-trivial now?", "author": "abbccdda", "createdAt": "2020-05-18T20:28:01Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +141,26 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<VersionRange> latestSupportedFeatures,\n+        Optional<Features<VersionLevelRange>> finalizedFeatures,\n+        Optional<Long> finalizedFeaturesEpoch) {\n         if (maxMagic == RecordBatch.CURRENT_MAGIC_VALUE && throttleTimeMs == DEFAULT_THROTTLE_TIME) {\n             return DEFAULT_API_VERSIONS_RESPONSE;\n         }\n-        return createApiVersionsResponse(throttleTimeMs, maxMagic);\n+        return createApiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, finalizedFeatures, finalizedFeaturesEpoch);\n     }\n \n-    public static ApiVersionsResponse createApiVersionsResponse(int throttleTimeMs, final byte minMagic) {\n+    public static ApiVersionsResponse createApiVersionsResponse(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA2MDc2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427060765", "bodyText": "The tests have been already added. Pls check out the tests added in ApiVersionsResponseTest.java, particularly: shouldReturnFeatureKeysWhenMagicIsCurrentValueAndThrottleMsIsDefaultThrottle.\nLet me know if this test does not look sufficient.", "author": "kowshik", "createdAt": "2020-05-19T06:34:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NTQzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NTg3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426875879", "bodyText": "s/AllAPI/GetAllFeatures", "author": "abbccdda", "createdAt": "2020-05-18T20:28:59Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,216 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Long>> emptyMap = new HashMap<>();\n+\n+        Features<VersionLevelRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertEquals(new HashMap<>(), emptyFinalizedFeatures.all());\n+        assertEquals(emptyMap, emptyFinalizedFeatures.serialize());\n+        assertEquals(emptyFinalizedFeatures, Features.deserializeFinalizedFeatures(emptyMap));\n+\n+        Features<VersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertEquals(new HashMap<>(), emptySupportedFeatures.all());\n+        assertEquals(\n+            new HashMap<String, HashMap<String, Long>>(),\n+            emptySupportedFeatures.serialize());\n+        assertEquals(emptySupportedFeatures, Features.deserializeSupportedFeatures(emptyMap));\n+    }\n+\n+    @Test\n+    public void testAllAPI() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzNTM4MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427035380", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:17:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NTg3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NjEzNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426876134", "bodyText": "We need the apache license title", "author": "abbccdda", "createdAt": "2020-05-18T20:29:35Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,216 @@\n+package org.apache.kafka.common.feature;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzNTI5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427035290", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:16:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3NjEzNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Njc5Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426876793", "bodyText": "We could use org.apache.kafka.common.utils.Utils#mkMap here", "author": "abbccdda", "createdAt": "2020-05-18T20:30:54Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,216 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Long>> emptyMap = new HashMap<>();\n+\n+        Features<VersionLevelRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertEquals(new HashMap<>(), emptyFinalizedFeatures.all());\n+        assertEquals(emptyMap, emptyFinalizedFeatures.serialize());\n+        assertEquals(emptyFinalizedFeatures, Features.deserializeFinalizedFeatures(emptyMap));\n+\n+        Features<VersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertEquals(new HashMap<>(), emptySupportedFeatures.all());\n+        assertEquals(\n+            new HashMap<String, HashMap<String, Long>>(),\n+            emptySupportedFeatures.serialize());\n+        assertEquals(emptySupportedFeatures, Features.deserializeSupportedFeatures(emptyMap));\n+    }\n+\n+    @Test\n+    public void testAllAPI() {\n+        VersionRange v1 = new VersionRange(1, 2);\n+        VersionRange v2 = new VersionRange(3, 4);\n+        Map<String, VersionRange> allFeatures = new HashMap<String, VersionRange>() {\n+            {\n+                put(\"feature_1\", v1);\n+                put(\"feature_2\", v2);\n+            }\n+        };\n+        Features<VersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(allFeatures, features.all());\n+    }\n+\n+    @Test\n+    public void testGetAPI() {\n+        VersionRange v1 = new VersionRange(1, 2);\n+        VersionRange v2 = new VersionRange(3, 4);\n+        Map<String, VersionRange> allFeatures = new HashMap<String, VersionRange>() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0MjMxMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427042310", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:42:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Njc5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Njg1Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426876852", "bodyText": "Same here", "author": "abbccdda", "createdAt": "2020-05-18T20:31:01Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,216 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Long>> emptyMap = new HashMap<>();\n+\n+        Features<VersionLevelRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertEquals(new HashMap<>(), emptyFinalizedFeatures.all());\n+        assertEquals(emptyMap, emptyFinalizedFeatures.serialize());\n+        assertEquals(emptyFinalizedFeatures, Features.deserializeFinalizedFeatures(emptyMap));\n+\n+        Features<VersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertEquals(new HashMap<>(), emptySupportedFeatures.all());\n+        assertEquals(\n+            new HashMap<String, HashMap<String, Long>>(),\n+            emptySupportedFeatures.serialize());\n+        assertEquals(emptySupportedFeatures, Features.deserializeSupportedFeatures(emptyMap));\n+    }\n+\n+    @Test\n+    public void testAllAPI() {\n+        VersionRange v1 = new VersionRange(1, 2);\n+        VersionRange v2 = new VersionRange(3, 4);\n+        Map<String, VersionRange> allFeatures = new HashMap<String, VersionRange>() {\n+            {\n+                put(\"feature_1\", v1);\n+                put(\"feature_2\", v2);\n+            }\n+        };\n+        Features<VersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(allFeatures, features.all());\n+    }\n+\n+    @Test\n+    public void testGetAPI() {\n+        VersionRange v1 = new VersionRange(1, 2);\n+        VersionRange v2 = new VersionRange(3, 4);\n+        Map<String, VersionRange> allFeatures = new HashMap<String, VersionRange>() {\n+            {\n+                put(\"feature_1\", v1);\n+                put(\"feature_2\", v2);\n+            }\n+        };\n+        Features<VersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(v1, features.get(\"feature_1\"));\n+        assertEquals(v2, features.get(\"feature_2\"));\n+        assertNull(features.get(\"nonexistent_feature\"));\n+    }\n+\n+    @Test\n+    public void testSerializeDeserializeSupportedFeatures() {\n+        VersionRange v1 = new VersionRange(1, 2);\n+        VersionRange v2 = new VersionRange(3, 4);\n+        Map<String, VersionRange> allFeatures = new HashMap<String, VersionRange>() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0MjQzNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427042434", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:42:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Njg1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Nzg5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426877891", "bodyText": "nit: new line", "author": "abbccdda", "createdAt": "2020-05-18T20:33:12Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/VersionLevelRangeTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class VersionLevelRangeTest {\n+\n+    @Test\n+    public void testCreateFailDueToInvalidParams() {\n+        // min and max can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(0, 0));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(-1, -1));\n+        // min can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(0, 1));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(-1, 1));\n+        // max can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(1, 0));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(1, -1));\n+        // min can't be > max.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionLevelRange(2, 1));\n+    }\n+\n+    @Test\n+    public void testSerializeDeserialize() {\n+        VersionLevelRange versionLevelRange = new VersionLevelRange(1, 2);\n+        assertEquals(1, versionLevelRange.min());\n+        assertEquals(2, versionLevelRange.max());\n+\n+        Map<String, Long> serialized = versionLevelRange.serialize();\n+        assertEquals(\n+            new HashMap<String, Long>() {\n+                {\n+                    put(\"min_version_level\", versionLevelRange.min());\n+                    put(\"max_version_level\", versionLevelRange.max());\n+                }\n+            },\n+            serialized\n+        );\n+\n+        VersionLevelRange deserialized = VersionLevelRange.deserialize(serialized);\n+        assertEquals(1, deserialized.min());\n+        assertEquals(2, deserialized.max());\n+        assertEquals(versionLevelRange, deserialized);\n+    }\n+\n+    @Test\n+    public void testDeserializationFailureTest() {\n+        // min_version_level can't be < 1.\n+        Map<String, Long> invalidWithBadMinVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version_level\", 0L);\n+                put(\"max_version_level\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithBadMinVersion));\n+\n+        // max_version_level can't be < 1.\n+        Map<String, Long> invalidWithBadMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version_level\", 1L);\n+                put(\"max_version_level\", 0L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithBadMaxVersion));\n+\n+        // min_version_level and max_version_level can't be < 1.\n+        Map<String, Long> invalidWithBadMinMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version_level\", 0L);\n+                put(\"max_version_level\", 0L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithBadMinMaxVersion));\n+\n+        // min_version_level can't be > max_version_level.\n+        Map<String, Long> invalidWithLowerMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version_level\", 2L);\n+                put(\"max_version_level\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithLowerMaxVersion));\n+\n+        // min_version_level key missing.\n+        Map<String, Long> invalidWithMinKeyMissing = new HashMap<String, Long>() {\n+            {\n+                put(\"max_version_level\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithMinKeyMissing));\n+\n+        // max_version_level key missing.\n+        Map<String, Long> invalidWithMaxKeyMissing = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version_level\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionLevelRange.deserialize(invalidWithMaxKeyMissing));\n+    }\n+\n+    @Test\n+    public void testToString() {\n+        assertEquals(\"VersionLevelRange[1, 1]\", new VersionLevelRange(1, 1).toString());\n+        assertEquals(\"VersionLevelRange[1, 2]\", new VersionLevelRange(1, 2).toString());\n+    }\n+\n+    @Test\n+    public void testEquals() {\n+        assertTrue(new VersionLevelRange(1, 1).equals(new VersionLevelRange(1, 1)));\n+        assertFalse(new VersionLevelRange(1, 1).equals(new VersionLevelRange(1, 2)));\n+    }\n+\n+    @Test\n+    public void testIsCompatibleWith() {\n+        assertTrue(new VersionLevelRange(1, 1).isCompatibleWith(new VersionRange(1, 1)));\n+        assertTrue(new VersionLevelRange(2, 3).isCompatibleWith(new VersionRange(1, 4)));\n+        assertTrue(new VersionLevelRange(1, 4).isCompatibleWith(new VersionRange(1, 4)));\n+\n+        assertFalse(new VersionLevelRange(1, 4).isCompatibleWith(new VersionRange(2, 3)));\n+        assertFalse(new VersionLevelRange(1, 4).isCompatibleWith(new VersionRange(2, 4)));\n+        assertFalse(new VersionLevelRange(2, 4).isCompatibleWith(new VersionRange(2, 3)));\n+    }\n+\n+    @Test\n+    public void testGetters() {\n+        assertEquals(1, new VersionLevelRange(1, 2).min());\n+        assertEquals(2, new VersionLevelRange(1, 2).max());\n+    }\n+}", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA1NzkxNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427057916", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:27:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg3Nzg5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4NDg5Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426884892", "bodyText": "In terms of naming, do you think FinalizedVersionRange is more explicit? Also when I look closer at the class hierarchy, I feel the sharing point between finalized version range and supported version range should be extracted to avoid weird inheritance. What I'm proposing is to have VersionRange as a super class with two subclasses: SupportedVersionRange and FinalizedVersionRange, and make minKeyLabel and maxKeyLabel abstract functions, WDYT?", "author": "abbccdda", "createdAt": "2020-05-18T20:47:45Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/VersionLevelRange.java", "diffHunk": "@@ -0,0 +1,39 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+/**\n+ * A specialization of VersionRange representing a range of version levels. The main specialization\n+ * is that the class uses different serialization keys for min/max attributes.\n+ *\n+ * NOTE: This is the backing class used to define the min/max version levels for finalized features.\n+ */\n+public class VersionLevelRange extends VersionRange {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg4NTAyNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427885025", "bodyText": "Done. Good point!\n\nI have now created 3 classes as you proposed. BaseVersionRange is the base class, and, SupportedVersionRange & FinalizedVersionRange are it's child classes.\nThe key labels couldn't be made into abstract functions since these constants are needed within deserialize() which is a static method defined in the child classes.", "author": "kowshik", "createdAt": "2020-05-20T09:53:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg4NDg5Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg5MDM5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426890390", "bodyText": "Note this function is public, which suggests there could be external dependency that we need to take care of. The safer approach is to keep this static function and create a separate one with augmented parameters. cc @ijuma for validation.", "author": "abbccdda", "createdAt": "2020-05-18T21:00:09Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +141,26 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzNDQ5OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427034499", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T05:13:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjg5MDM5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMTg3NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426931875", "bodyText": "I think we could delay the addition for these helpers until we actually need them.", "author": "abbccdda", "createdAt": "2020-05-18T22:44:55Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -77,6 +93,18 @@ public boolean shouldClientThrottle(short version) {\n         return version >= 2;\n     }\n \n+    public SupportedFeatureKey supportedFeature(String featureName) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNzkwOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427027909", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T04:48:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMTg3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMzQ3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426933479", "bodyText": "I gave it more thought, and wonder whether we could just call this function features to be more consistent with our convention for getters.", "author": "abbccdda", "createdAt": "2020-05-18T22:49:35Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,143 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ */\n+public class Features<VersionRangeType extends VersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<VersionRange> supportedFeatures(Map<String, VersionRange> features) {\n+        return new Features<VersionRange>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionLevelRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<VersionLevelRange> finalizedFeatures(Map<String, VersionLevelRange> features) {\n+        return new Features<VersionLevelRange>(features);\n+    }\n+\n+    public static Features<VersionLevelRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<VersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+\n+    public Map<String, VersionRangeType> all() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNDM0MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427024341", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T04:34:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMzQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMzc3Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426933776", "bodyText": "Need to check null", "author": "abbccdda", "createdAt": "2020-05-18T22:50:20Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/VersionRange.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max of type long.\n+ * The min and max attributes are expected to be >= 1, and with max >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ *\n+ * NOTE: This is the backing class used to define the min/max versions for supported features.\n+ */\n+public class VersionRange {\n+    // Label for the min version key, that's used only for serialization/deserialization purposes.\n+    private static final String MIN_VERSION_KEY_LABEL = \"min_version\";\n+\n+    // Label for the max version key, that's used only for serialization/deserialization purposes.\n+    private static final String MAX_VERSION_KEY_LABEL = \"max_version\";\n+\n+    private final String minKeyLabel;\n+\n+    private final long minValue;\n+\n+    private final String maxKeyLabel;\n+\n+    private final long maxValue;\n+\n+    protected VersionRange(String minKey, long minValue, String maxKeyLabel, long maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue > 1, maxValue > 1 and maxValue >= minValue, but received\" +\n+                    \" minValue: %d, maxValue: %d\", minValue, maxValue));\n+        }\n+        this.minKeyLabel = minKey;\n+        this.minValue = minValue;\n+        this.maxKeyLabel = maxKeyLabel;\n+        this.maxValue = maxValue;\n+    }\n+\n+    public VersionRange(long minVersion, long maxVersion) {\n+        this(MIN_VERSION_KEY_LABEL, minVersion, MAX_VERSION_KEY_LABEL, maxVersion);\n+    }\n+\n+    public long min() {\n+        return minValue;\n+    }\n+\n+    public long max() {\n+        return maxValue;\n+    }\n+\n+    public String toString() {\n+        return String.format(\"%s[%d, %d]\", this.getClass().getSimpleName(), min(), max());\n+    }\n+\n+    public Map<String, Long> serialize() {\n+        return new HashMap<String, Long>() {\n+            {\n+                put(minKeyLabel, min());\n+                put(maxKeyLabel, max());\n+            }\n+        };\n+    }\n+\n+    public static VersionRange deserialize(Map<String, Long> serialized) {\n+        return new VersionRange(\n+            valueOrThrow(MIN_VERSION_KEY_LABEL, serialized),\n+            valueOrThrow(MAX_VERSION_KEY_LABEL, serialized));\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {\n+            return true;\n+        }\n+        if (!(other instanceof VersionRange)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAzMzYxNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427033616", "bodyText": "Done. Also added a test. Good catch!", "author": "kowshik", "createdAt": "2020-05-19T05:10:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzMzc3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNDU1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426934551", "bodyText": "Is there a difference between Objects.equals and this.minKeyLabel.equals(that.minKeyLabel)?", "author": "abbccdda", "createdAt": "2020-05-18T22:52:20Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/VersionRange.java", "diffHunk": "@@ -0,0 +1,104 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max of type long.\n+ * The min and max attributes are expected to be >= 1, and with max >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ *\n+ * NOTE: This is the backing class used to define the min/max versions for supported features.\n+ */\n+public class VersionRange {\n+    // Label for the min version key, that's used only for serialization/deserialization purposes.\n+    private static final String MIN_VERSION_KEY_LABEL = \"min_version\";\n+\n+    // Label for the max version key, that's used only for serialization/deserialization purposes.\n+    private static final String MAX_VERSION_KEY_LABEL = \"max_version\";\n+\n+    private final String minKeyLabel;\n+\n+    private final long minValue;\n+\n+    private final String maxKeyLabel;\n+\n+    private final long maxValue;\n+\n+    protected VersionRange(String minKey, long minValue, String maxKeyLabel, long maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue > 1, maxValue > 1 and maxValue >= minValue, but received\" +\n+                    \" minValue: %d, maxValue: %d\", minValue, maxValue));\n+        }\n+        this.minKeyLabel = minKey;\n+        this.minValue = minValue;\n+        this.maxKeyLabel = maxKeyLabel;\n+        this.maxValue = maxValue;\n+    }\n+\n+    public VersionRange(long minVersion, long maxVersion) {\n+        this(MIN_VERSION_KEY_LABEL, minVersion, MAX_VERSION_KEY_LABEL, maxVersion);\n+    }\n+\n+    public long min() {\n+        return minValue;\n+    }\n+\n+    public long max() {\n+        return maxValue;\n+    }\n+\n+    public String toString() {\n+        return String.format(\"%s[%d, %d]\", this.getClass().getSimpleName(), min(), max());\n+    }\n+\n+    public Map<String, Long> serialize() {\n+        return new HashMap<String, Long>() {\n+            {\n+                put(minKeyLabel, min());\n+                put(maxKeyLabel, max());\n+            }\n+        };\n+    }\n+\n+    public static VersionRange deserialize(Map<String, Long> serialized) {\n+        return new VersionRange(\n+            valueOrThrow(MIN_VERSION_KEY_LABEL, serialized),\n+            valueOrThrow(MAX_VERSION_KEY_LABEL, serialized));\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {\n+            return true;\n+        }\n+        if (!(other instanceof VersionRange)) {\n+            return false;\n+        }\n+\n+        final VersionRange that = (VersionRange) other;\n+        return Objects.equals(this.minKeyLabel, that.minKeyLabel) &&", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzAyNzY3MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427027670", "bodyText": "It provides slightly better convenience: Object.equals will also take care of the null checks for you.\nAlso it turned out it was overkill to use Objects.equals for primitive type checks for minValue and maxValue. So I've simplified the code to use == those attributes.\nGood point!", "author": "kowshik", "createdAt": "2020-05-19T04:47:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNDU1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNTA4Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426935082", "bodyText": "nit: testMinMax, and we could reuse the same new VersionRange(1, 2) by only creating it once.", "author": "abbccdda", "createdAt": "2020-05-18T22:53:54Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/VersionRangeTest.java", "diffHunk": "@@ -0,0 +1,150 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertTrue;\n+\n+public class VersionRangeTest {\n+    @Test\n+    public void testFailDueToInvalidParams() {\n+        // min and max can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(0, 0));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(-1, -1));\n+        // min can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(0, 1));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(-1, 1));\n+        // max can't be < 1.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(1, 0));\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(1, -1));\n+        // min can't be > max.\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> new VersionRange(2, 1));\n+    }\n+\n+    @Test\n+    public void testSerializeDeserializeTest() {\n+        VersionRange versionRange = new VersionRange(1, 2);\n+        assertEquals(1, versionRange.min());\n+        assertEquals(2, versionRange.max());\n+\n+        Map<String, Long> serialized = versionRange.serialize();\n+        assertEquals(\n+            new HashMap<String, Long>() {\n+                {\n+                    put(\"min_version\", versionRange.min());\n+                    put(\"max_version\", versionRange.max());\n+                }\n+            },\n+            serialized\n+        );\n+\n+        VersionRange deserialized = VersionRange.deserialize(serialized);\n+        assertEquals(1, deserialized.min());\n+        assertEquals(2, deserialized.max());\n+        assertEquals(versionRange, deserialized);\n+    }\n+\n+    @Test\n+    public void testDeserializationFailureTest() {\n+        // min_version can't be < 1.\n+        Map<String, Long> invalidWithBadMinVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version\", 0L);\n+                put(\"max_version\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithBadMinVersion));\n+\n+        // max_version can't be < 1.\n+        Map<String, Long> invalidWithBadMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version\", 1L);\n+                put(\"max_version\", 0L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithBadMaxVersion));\n+\n+        // min_version and max_version can't be < 1.\n+        Map<String, Long> invalidWithBadMinMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version\", 0L);\n+                put(\"max_version\", 0L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithBadMinMaxVersion));\n+\n+        // min_version can't be > max_version.\n+        Map<String, Long> invalidWithLowerMaxVersion = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version\", 2L);\n+                put(\"max_version\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithLowerMaxVersion));\n+\n+        // min_version key missing.\n+        Map<String, Long> invalidWithMinKeyMissing = new HashMap<String, Long>() {\n+            {\n+                put(\"max_version\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithMinKeyMissing));\n+\n+        // max_version key missing.\n+        Map<String, Long> invalidWithMaxKeyMissing = new HashMap<String, Long>() {\n+            {\n+                put(\"min_version\", 1L);\n+            }\n+        };\n+        assertThrows(\n+            IllegalArgumentException.class,\n+            () -> VersionRange.deserialize(invalidWithMaxKeyMissing));\n+    }\n+\n+    @Test\n+    public void testToString() {\n+        assertEquals(\"VersionRange[1, 1]\", new VersionRange(1, 1).toString());\n+        assertEquals(\"VersionRange[1, 2]\", new VersionRange(1, 2).toString());\n+    }\n+\n+    @Test\n+    public void testEquals() {\n+        assertTrue(new VersionRange(1, 1).equals(new VersionRange(1, 1)));\n+        assertFalse(new VersionRange(1, 1).equals(new VersionRange(1, 2)));\n+    }\n+\n+    @Test\n+    public void testGetters() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0MzcxMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427043710", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNTA4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNjQ2OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426936469", "bodyText": "Does L17-23 really necessary for testing?", "author": "abbccdda", "createdAt": "2020-05-18T22:57:45Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/VersionLevelRangeTest.java", "diffHunk": "@@ -0,0 +1,162 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class VersionLevelRangeTest {\n+\n+    @Test\n+    public void testCreateFailDueToInvalidParams() {\n+        // min and max can't be < 1.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0MzI0NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427043245", "bodyText": "Done. Some of it is not required. Good point, I have removed the unnecessary testing now.\nWe still need to check if exception is thrown in these 4 basic tests: min < 1, max < 1, min & max < 1 and max > min.", "author": "kowshik", "createdAt": "2020-05-19T05:45:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNjQ2OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNjc1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426936751", "bodyText": "Could we add a reference to the class?", "author": "abbccdda", "createdAt": "2020-05-18T22:58:34Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {\n+    newEpoch >= epoch\n+  }\n+\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * FinalizedFeatureChangeListener.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTc1NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427045754", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNjc1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNzUzMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426937532", "bodyText": "It seems that we don't have the handling logic for this FeatureCacheUpdateException. Do we think this is fatal?", "author": "abbccdda", "createdAt": "2020-05-18T23:00:50Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MDcxNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426990716", "bodyText": "Might worth getting a ticket to define the handling strategy for such exception, and in general how updateOrThrow will be used.", "author": "abbccdda", "createdAt": "2020-05-19T02:15:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNzUzMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzgyNzAxOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427827018", "bodyText": "As we discussed offline today, this exception is already handled in ChangeNotificationProcessorThread.doWork() method defined in FinalizedFeatureChangeListener.scala. Basically, the ZK change notification processor thread exits the Broker with a fatal error (non-zero exit code) when this exception (or any exception) is caught while trying to update FinalizedFeatureCache.", "author": "kowshik", "createdAt": "2020-05-20T08:21:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNzUzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNzk0NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426937944", "bodyText": "Is this function being used?", "author": "abbccdda", "createdAt": "2020-05-18T23:02:01Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTMzOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427045338", "bodyText": "Done. It was unused and I have eliminated it now.", "author": "kowshik", "createdAt": "2020-05-19T05:52:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjkzNzk0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDU5Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426940597", "bodyText": "Do you expect these helper functions actually to be used in production logic with subsequent PRs?", "author": "abbccdda", "createdAt": "2020-05-18T23:10:12Z", "path": "core/src/main/scala/kafka/zk/KafkaZkClient.scala", "diffHunk": "@@ -1567,6 +1567,36 @@ class KafkaZkClient private[zk] (zooKeeperClient: ZooKeeperClient, isSecure: Boo\n     createRecursive(path, data = null, throwIfPathExists = false)\n   }\n \n+  // Visible for testing.\n+  def createFeatureZNode(nodeContents: FeatureZNode): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA1MDAzMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427050030", "bodyText": "Yes, this will get used in the future. For example the write path will use it.", "author": "kowshik", "createdAt": "2020-05-19T06:06:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDU5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMjkwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428432904", "bodyText": "If that's the case, I feel we could remove the testing only comment.", "author": "abbccdda", "createdAt": "2020-05-21T04:02:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDU5Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4OTgxNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429089817", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:36:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDU5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDgzMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426940830", "bodyText": "I don't think we need a nested if-else:\n val version = {\n      if (apiVersion >= KAFKA_2_6_IV1) \n        5\n      else if (apiVersion >= KAFKA_0_10_0_IV1)\n        4\n      else\n        2\n    }", "author": "abbccdda", "createdAt": "2020-05-18T23:10:56Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -81,17 +83,27 @@ object BrokerIdsZNode {\n object BrokerInfo {\n \n   /**\n-   * Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n-   * the apiVersion is 0.10.0.X or above. Register the broker with v2 json format otherwise.\n+   * - Create a broker info with v5 json format if the apiVersion is 2.6.x or above.\n+   * - Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n+   *   the apiVersion is 0.10.0.X or above but lesser than 2.6.x.\n+   * - Register the broker with v2 json format otherwise.\n    *\n    * Due to KAFKA-3100, 0.9.0.0 broker and old clients will break if JSON version is above 2.\n    *\n-   * We include v2 to make it possible for the broker to migrate from 0.9.0.0 to 0.10.0.X or above without having to\n-   * upgrade to 0.9.0.1 first (clients have to be upgraded to 0.9.0.1 in any case).\n+   * We include v2 to make it possible for the broker to migrate from 0.9.0.0 to 0.10.0.X or above\n+   * without having to upgrade to 0.9.0.1 first (clients have to be upgraded to 0.9.0.1 in\n+   * any case).\n    */\n   def apply(broker: Broker, apiVersion: ApiVersion, jmxPort: Int): BrokerInfo = {\n-    // see method documentation for the reason why we do this\n-    val version = if (apiVersion >= KAFKA_0_10_0_IV1) 4 else 2\n+    val version = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA1MDU5Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427050597", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-19T06:07:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MDgzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MjM3Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426942377", "bodyText": "Could we make feature extraction as a helper function?", "author": "abbccdda", "createdAt": "2020-05-18T23:16:03Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -225,7 +255,12 @@ object BrokerIdZNode {\n           }\n \n         val rack = brokerInfo.get(RackKey).flatMap(_.to[Option[String]])\n-        BrokerInfo(Broker(id, endpoints, rack), version, jmxPort)\n+        val features = FeatureZNode.asJavaMap(brokerInfo", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA2MzA1Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427063053", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:39:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0MjM3Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0Mjg0Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426942842", "bodyText": "Could we make this parameter configurable?", "author": "abbccdda", "createdAt": "2020-05-18T23:17:26Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -210,6 +215,14 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         /* setup zookeeper */\n         initZkClient(time)\n \n+        /* initialize features */\n+        _featureChangeListener = new FinalizedFeatureChangeListener(_zkClient)\n+        if (config.interBrokerProtocolVersion >= KAFKA_2_6_IV1) {\n+          // The feature versioning system (KIP-584) is active only when:\n+          // config.interBrokerProtocolVersion is >= KAFKA_2_6_IV1.\n+          _featureChangeListener.initOrThrow(60000)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzg4NDE2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427884165", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-20T09:51:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk0Mjg0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjI2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426976264", "bodyText": "What would happen if we are dealing with a V4 json map containing features?", "author": "abbccdda", "createdAt": "2020-05-19T01:19:38Z", "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "diffHunk": "@@ -149,6 +153,53 @@ class BrokerEndPointTest {\n     assertEquals(None, broker.rack)\n   }\n \n+  @Test\n+  def testFromJsonV5(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA2NDI3MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427064271", "bodyText": "In my understanding, this is an impossible case. Because, we always write features into the JSON only in v5 or above. That is why, there is no test for it. Let me know how you feel about it.", "author": "kowshik", "createdAt": "2020-05-19T06:42:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjI2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjM5Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426976396", "bodyText": "nit: This test could move closer to testFromJsonV4WithNoRack", "author": "abbccdda", "createdAt": "2020-05-19T01:20:02Z", "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "diffHunk": "@@ -149,6 +153,53 @@ class BrokerEndPointTest {\n     assertEquals(None, broker.rack)\n   }\n \n+  @Test\n+  def testFromJsonV5(): Unit = {\n+    val json = \"\"\"{\n+      \"version\":5,\n+      \"host\":\"localhost\",\n+      \"port\":9092,\n+      \"jmx_port\":9999,\n+      \"timestamp\":\"2233345666\",\n+      \"endpoints\":[\"CLIENT://host1:9092\", \"REPLICATION://host1:9093\"],\n+      \"listener_security_protocol_map\":{\"CLIENT\":\"SSL\", \"REPLICATION\":\"PLAINTEXT\"},\n+      \"rack\":\"dc1\",\n+      \"features\": {\"feature1\": {\"min_version\": 1, \"max_version\": 2}, \"feature2\": {\"min_version\": 2, \"max_version\": 4}}\n+    }\"\"\"\n+    val broker = parseBrokerJson(1, json)\n+    assertEquals(1, broker.id)\n+    val brokerEndPoint = broker.brokerEndPoint(new ListenerName(\"CLIENT\"))\n+    assertEquals(\"host1\", brokerEndPoint.host)\n+    assertEquals(9092, brokerEndPoint.port)\n+    assertEquals(Some(\"dc1\"), broker.rack)\n+    assertEquals(Features.supportedFeatures(\n+      Map[String, VersionRange](\n+        \"feature1\" -> new VersionRange(1, 2),\n+        \"feature2\" -> new VersionRange(2, 4)).asJava),\n+      broker.features)\n+  }\n+\n+  @Test\n+  def testFromJsonV4WithNoFeatures(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA2MzcyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427063724", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:40:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjM5Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjkzMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426976932", "bodyText": "Should we test isDefined before calling get?", "author": "abbccdda", "createdAt": "2020-05-19T01:22:05Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureCacheTest.scala", "diffHunk": "@@ -0,0 +1,95 @@\n+package kafka.server\n+\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange, VersionRange}\n+import org.junit.Assert.{assertEquals, assertThrows, assertTrue}\n+import org.junit.{Before, Test}\n+\n+import scala.jdk.CollectionConverters._\n+\n+class FinalizedFeatureCacheTest {\n+\n+  @Before\n+  def setUp(): Unit = {\n+    FinalizedFeatureCache.clear()\n+    SupportedFeatures.clear()\n+  }\n+\n+  @Test\n+  def testEmpty(): Unit = {\n+    assertTrue(FinalizedFeatureCache.get.isEmpty)\n+  }\n+\n+  @Test\n+  def testUpdateOrThrowFailedDueToInvalidEpoch(): Unit = {\n+    val supportedFeatures = Map[String, VersionRange](\n+      \"feature_1\" -> new VersionRange(1, 4))\n+    SupportedFeatures.update(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val features = Map[String, VersionLevelRange](\n+      \"feature_1\" -> new VersionLevelRange(1, 4))\n+    val finalizedFeatures = Features.finalizedFeatures(features.asJava)\n+\n+    FinalizedFeatureCache.updateOrThrow(finalizedFeatures, 10)\n+    assertEquals(finalizedFeatures, FinalizedFeatureCache.get.get.features)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA2NDkyMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427064923", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:43:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3NjkzMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3Nzk3OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426977978", "bodyText": "s/existingStr/oldFeatureAndEpoch", "author": "abbccdda", "createdAt": "2020-05-19T01:26:17Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {\n+    newEpoch >= epoch\n+  }\n+\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * FinalizedFeatureChangeListener.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest\n+ * returning the features information in the response. In the future, as the feature versioning\n+ * system in KIP-584 is used more widely, this cache could be read by other read paths trying to\n+ * learn the finalized feature information.\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {\n+    featuresAndEpoch.isEmpty\n+  }\n+\n+  /**\n+   * Clears all existing finalized features and epoch from the cache.\n+   */\n+  def clear(): Unit = {\n+    featuresAndEpoch = Option.empty\n+    info(\"Cleared cache\")\n+  }\n+\n+  /**\n+   * Updates the cache to the latestFeatures, and updates the existing epoch to latestEpoch.\n+   * Raises an exception when the operation is not successful.\n+   *\n+   * @param latestFeatures   the latest finalized features to be set in the cache\n+   * @param latestEpoch      the latest epoch value to be set in the cache\n+   *\n+   * @throws                 FeatureCacheUpdateException if the cache update operation fails\n+   *                         due to invalid parameters or incompatibilities with the broker's\n+   *                         supported features. In such a case, the existing cache contents are\n+   *                         not modified.\n+   */\n+  def updateOrThrow(latestFeatures: Features[VersionLevelRange], latestEpoch: Int): Unit = {\n+    updateOrThrow(FinalizedFeaturesAndEpoch(latestFeatures, latestEpoch))\n+  }\n+\n+  private def updateOrThrow(latest: FinalizedFeaturesAndEpoch): Unit = {\n+    val existingStr = featuresAndEpoch.map(existing => existing.toString).getOrElse(\"<empty>\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NTkwNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427045906", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T05:54:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3Nzk3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODA5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426978090", "bodyText": "This val seems redundant.", "author": "abbccdda", "createdAt": "2020-05-19T01:26:43Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {\n+    newEpoch >= epoch\n+  }\n+\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * FinalizedFeatureChangeListener.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest\n+ * returning the features information in the response. In the future, as the feature versioning\n+ * system in KIP-584 is used more widely, this cache could be read by other read paths trying to\n+ * learn the finalized feature information.\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {\n+    featuresAndEpoch.isEmpty\n+  }\n+\n+  /**\n+   * Clears all existing finalized features and epoch from the cache.\n+   */\n+  def clear(): Unit = {\n+    featuresAndEpoch = Option.empty\n+    info(\"Cleared cache\")\n+  }\n+\n+  /**\n+   * Updates the cache to the latestFeatures, and updates the existing epoch to latestEpoch.\n+   * Raises an exception when the operation is not successful.\n+   *\n+   * @param latestFeatures   the latest finalized features to be set in the cache\n+   * @param latestEpoch      the latest epoch value to be set in the cache\n+   *\n+   * @throws                 FeatureCacheUpdateException if the cache update operation fails\n+   *                         due to invalid parameters or incompatibilities with the broker's\n+   *                         supported features. In such a case, the existing cache contents are\n+   *                         not modified.\n+   */\n+  def updateOrThrow(latestFeatures: Features[VersionLevelRange], latestEpoch: Int): Unit = {\n+    updateOrThrow(FinalizedFeaturesAndEpoch(latestFeatures, latestEpoch))\n+  }\n+\n+  private def updateOrThrow(latest: FinalizedFeaturesAndEpoch): Unit = {\n+    val existingStr = featuresAndEpoch.map(existing => existing.toString).getOrElse(\"<empty>\")\n+    if (!featuresAndEpoch.isEmpty && featuresAndEpoch.get.epoch > latest.epoch) {\n+      val errorMsg = (\"FinalizedFeatureCache update failed due to invalid epoch in new finalized %s.\" +\n+        \" The existing finalized is %s\").format(latest, existingStr)\n+      throw new FeatureCacheUpdateException(errorMsg)\n+    } else {\n+      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      if (incompatibleFeatures.nonEmpty) {\n+        val errorMsg = (\"FinalizedFeatureCache updated failed since feature compatibility\" +\n+          \" checks failed! Supported %s has incompatibilities with the latest finalized %s.\" +\n+          \" The incompatible features are: %s.\").format(\n+          SupportedFeatures.get, latest, incompatibleFeatures)\n+        throw new FeatureCacheUpdateException(errorMsg)\n+      }\n+    }\n+    val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NjM1Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427046353", "bodyText": "It is used intentionally to split the log message into 2 lines (for ~100-char readability limit per line). Otherwise the string will be huge and all in the same line.", "author": "kowshik", "createdAt": "2020-05-19T05:55:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODA5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQxODUxNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428418517", "bodyText": "Could we move this logic as part of inner else? Like:\nelse {\n      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n      if (incompatibleFeatures.nonEmpty) {\n        val errorMsg = (\"FinalizedFeatureCache updated failed since feature compatibility\" +\n          \" checks failed! Supported %s has incompatibilities with the latest finalized %s.\" +\n          \" The incompatible features are: %s.\").format(\n          SupportedFeatures.get, latest, incompatibleFeatures)\n        throw new FeatureCacheUpdateException(errorMsg)\n      } else {\n        val logMsg = \"Updated cache from existing finalized %s to latest finalized %s\".format(\n          oldFeatureAndEpoch, latest)\n        featuresAndEpoch = Some(latest)\n        info(logMsg)\n      }\n    }\n\nIt makes the if-else logic more tight.", "author": "abbccdda", "createdAt": "2020-05-21T02:58:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODA5MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU4MTAxMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428581013", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-21T10:51:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODA5MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODI2Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426978267", "bodyText": "nit: this errorMsg val seems redundant.", "author": "abbccdda", "createdAt": "2020-05-19T01:27:25Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,93 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionLevelRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[VersionLevelRange], epoch: Int) {\n+\n+  def isValid(newEpoch: Int): Boolean = {\n+    newEpoch >= epoch\n+  }\n+\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * FinalizedFeatureChangeListener.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest\n+ * returning the features information in the response. In the future, as the feature versioning\n+ * system in KIP-584 is used more widely, this cache could be read by other read paths trying to\n+ * learn the finalized feature information.\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {\n+    featuresAndEpoch.isEmpty\n+  }\n+\n+  /**\n+   * Clears all existing finalized features and epoch from the cache.\n+   */\n+  def clear(): Unit = {\n+    featuresAndEpoch = Option.empty\n+    info(\"Cleared cache\")\n+  }\n+\n+  /**\n+   * Updates the cache to the latestFeatures, and updates the existing epoch to latestEpoch.\n+   * Raises an exception when the operation is not successful.\n+   *\n+   * @param latestFeatures   the latest finalized features to be set in the cache\n+   * @param latestEpoch      the latest epoch value to be set in the cache\n+   *\n+   * @throws                 FeatureCacheUpdateException if the cache update operation fails\n+   *                         due to invalid parameters or incompatibilities with the broker's\n+   *                         supported features. In such a case, the existing cache contents are\n+   *                         not modified.\n+   */\n+  def updateOrThrow(latestFeatures: Features[VersionLevelRange], latestEpoch: Int): Unit = {\n+    updateOrThrow(FinalizedFeaturesAndEpoch(latestFeatures, latestEpoch))\n+  }\n+\n+  private def updateOrThrow(latest: FinalizedFeaturesAndEpoch): Unit = {\n+    val existingStr = featuresAndEpoch.map(existing => existing.toString).getOrElse(\"<empty>\")\n+    if (!featuresAndEpoch.isEmpty && featuresAndEpoch.get.epoch > latest.epoch) {\n+      val errorMsg = (\"FinalizedFeatureCache update failed due to invalid epoch in new finalized %s.\" +\n+        \" The existing finalized is %s\").format(latest, existingStr)\n+      throw new FeatureCacheUpdateException(errorMsg)\n+    } else {\n+      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)\n+      if (incompatibleFeatures.nonEmpty) {\n+        val errorMsg = (\"FinalizedFeatureCache updated failed since feature compatibility\" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NjMzMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427046331", "bodyText": "It is used intentionally to split the log message into 2 lines (for ~100-char readability limit per line). Otherwise the string will be huge and all in the same line.", "author": "kowshik", "createdAt": "2020-05-19T05:55:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3ODI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3OTk4NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426979984", "bodyText": "This is only used on L53, maybe we could just use supportedFeatures instead", "author": "abbccdda", "createdAt": "2020-05-19T01:34:31Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,70 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionRange, VersionLevelRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.\n+   */\n+  def get: Features[VersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[VersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be incompatible between the latest features supported\n+   * by the Broker, and the provided cluster-wide finalized features.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The set of incompatible feature names. If the returned set is empty, it\n+   *                    means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[VersionLevelRange]): Set[String] = {\n+    val supported = get", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0OTA5Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427049093", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:03:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk3OTk4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk4MDg1NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426980855", "bodyText": "This comment is a bit vague to me, what are you referring by incompatibilities?", "author": "abbccdda", "createdAt": "2020-05-19T01:37:40Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,70 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionRange, VersionLevelRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.\n+   */\n+  def get: Features[VersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[VersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be incompatible between the latest features supported\n+   * by the Broker, and the provided cluster-wide finalized features.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0ODY5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427048695", "bodyText": "Good point. I have improved the doc now. Let me know how you feel about it.", "author": "kowshik", "createdAt": "2020-05-19T06:02:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk4MDg1NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MDAwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426990007", "bodyText": "nit: maybe rename to incompatibleWith and flip the boolean", "author": "abbccdda", "createdAt": "2020-05-19T02:12:55Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,70 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, VersionRange, VersionLevelRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.\n+   */\n+  def get: Features[VersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[VersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be incompatible between the latest features supported\n+   * by the Broker, and the provided cluster-wide finalized features.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The set of incompatible feature names. If the returned set is empty, it\n+   *                    means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[VersionLevelRange]): Set[String] = {\n+    val supported = get\n+\n+    val incompatibilities = finalized.all.asScala.collect {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supported.get(feature);\n+        if (supportedVersions == null) {\n+          (feature, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (!versionLevels.isCompatibleWith(supportedVersions)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0OTgzMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427049831", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-19T06:05:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5MDAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5NzEwOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r426997108", "bodyText": "Does this event actually happen? Will we hit illegal state exception in updateLatestOrThrow?", "author": "abbccdda", "createdAt": "2020-05-19T02:39:40Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,200 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      processNotification()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzA0NzI1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r427047251", "bodyText": "I have added comments now to the code.\nThe idea I had was that this event may happen, rarely (ex: operational error).\nIn such a case, we do not want to kill the brokers, so we just log a warning and treat the case as if the node is absent, and populate the cache with empty features.\nSo, this case is actually handled inside FeatureCacheUpdater.updateLatestOrThrow().\nThe call to read ZK node will return ZkVersion.UnknownVersion whenever the node does not exist in ZK, and I've explicitly handled this returned version.", "author": "kowshik", "createdAt": "2020-05-19T05:58:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5NzEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzUxNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428433517", "bodyText": "I think even if this is an operational error, the cluster is at risk of violating the feature semantics previously enabled, which is different from an unknown feature version from the beginning. I feel we should just exit in fatal error for this case, but would open for discussion.", "author": "abbccdda", "createdAt": "2020-05-21T04:05:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5NzEwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMzQ5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429113491", "bodyText": "You bring up a good point.\nMy main concern is availability. If we exit the Broker here, then, whenever the feature ZK node gets deleted (accidentally), it could crash all brokers in the fleet all at once leading to an availability problem.\nWith regards to violating feature semantics, good point.\nI'm in 2 minds here, and perhaps we can also hear @hachikuji 's thoughts on this topic.", "author": "kowshik", "createdAt": "2020-05-22T08:28:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjk5NzEwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM2NzQwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428367400", "bodyText": "deserialize()? I think the second sentence is redundant.", "author": "abbccdda", "createdAt": "2020-05-20T23:39:32Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ * @see SupportedVersionRange\n+ * @see FinalizedVersionRange\n+ */\n+public class Features<VersionRangeType extends BaseVersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<SupportedVersionRange> supportedFeatures(Map<String, SupportedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to FinalizedVersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<FinalizedVersionRange> finalizedFeatures(Map<String, FinalizedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    // Visible for testing.\n+    public static Features<FinalizedVersionRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<SupportedVersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public Map<String, VersionRangeType> features() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    /**\n+     * @param  feature   name of the feature\n+     *\n+     * @return           the VersionRangeType corresponding to the feature name, or null if absent\n+     */\n+    public VersionRangeType get(String feature) {\n+        return features.get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   A map with underlying features serialized. The returned value can be deserialized\n+     *           using one of the deserialize* APIs.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3NzE1Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428577156", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:41:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM2NzQwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MDI1NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428370254", "bodyText": "Do we want to get a unit test class for BaseVersionRange?", "author": "abbccdda", "createdAt": "2020-05-20T23:48:46Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max of type long.\n+ * The min and max attributes are expected to be >= 1, and with max >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3MzQwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428573407", "bodyText": "It is thoroughly tested in it's child class test suite: SupportedVersionRangeTest.\nPersonally I feel it is good enough this way, because, anyway to test this class we need to inherit into a sub-class (since constructor is protected). And by testing via SupportedVersionRangeTest, we achieve exactly the same.\nI have now added top-level documentation in the test suite of SupportedVersionRangeTest, explaining the above.", "author": "kowshik", "createdAt": "2020-05-21T10:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MDI1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MDkwNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428370905", "bodyText": "Should be SupportedVersionRange", "author": "abbccdda", "createdAt": "2020-05-20T23:51:04Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+/**\n+ * A specialization of {@link BaseVersionRange} representing a range of version levels.\n+ * NOTE: This is the backing class used to define the min/max version levels for finalized features.\n+ */\n+public class FinalizedVersionRange extends BaseVersionRange {\n+    // Label for the min version key, that's used only for serialization/deserialization purposes.\n+    private static final String MIN_VERSION_LEVEL_KEY_LABEL = \"min_version_level\";\n+\n+    // Label for the max version key, that's used only for serialization/deserialization purposes.\n+    private static final String MAX_VERSION_LEVEL_KEY_LABEL = \"max_version_level\";\n+\n+    public FinalizedVersionRange(long minVersionLevel, long maxVersionLevel) {\n+        super(MIN_VERSION_LEVEL_KEY_LABEL, minVersionLevel, MAX_VERSION_LEVEL_KEY_LABEL, maxVersionLevel);\n+    }\n+\n+    public static FinalizedVersionRange deserialize(Map<String, Long> serialized) {\n+        return new FinalizedVersionRange(\n+            BaseVersionRange.valueOrThrow(MIN_VERSION_LEVEL_KEY_LABEL, serialized),\n+            BaseVersionRange.valueOrThrow(MAX_VERSION_LEVEL_KEY_LABEL, serialized));\n+    }\n+\n+    private boolean isCompatibleWith(BaseVersionRange versionRange) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3ODQyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428578424", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:44:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MDkwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MTI2MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428371260", "bodyText": "Just for the sake of argument, I feel we could remove this method and just test:\nmin() < supportedVersionRange.min() || max() > supportedVersionRange.max()\n\nfor incompatibility.", "author": "abbccdda", "createdAt": "2020-05-20T23:52:20Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+/**\n+ * A specialization of {@link BaseVersionRange} representing a range of version levels.\n+ * NOTE: This is the backing class used to define the min/max version levels for finalized features.\n+ */\n+public class FinalizedVersionRange extends BaseVersionRange {\n+    // Label for the min version key, that's used only for serialization/deserialization purposes.\n+    private static final String MIN_VERSION_LEVEL_KEY_LABEL = \"min_version_level\";\n+\n+    // Label for the max version key, that's used only for serialization/deserialization purposes.\n+    private static final String MAX_VERSION_LEVEL_KEY_LABEL = \"max_version_level\";\n+\n+    public FinalizedVersionRange(long minVersionLevel, long maxVersionLevel) {\n+        super(MIN_VERSION_LEVEL_KEY_LABEL, minVersionLevel, MAX_VERSION_LEVEL_KEY_LABEL, maxVersionLevel);\n+    }\n+\n+    public static FinalizedVersionRange deserialize(Map<String, Long> serialized) {\n+        return new FinalizedVersionRange(\n+            BaseVersionRange.valueOrThrow(MIN_VERSION_LEVEL_KEY_LABEL, serialized),\n+            BaseVersionRange.valueOrThrow(MAX_VERSION_LEVEL_KEY_LABEL, serialized));\n+    }\n+\n+    private boolean isCompatibleWith(BaseVersionRange versionRange) {\n+        return min() >= versionRange.min() && max() <= versionRange.max();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3ODQ3Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428578472", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-21T10:45:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM3MTI2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM5Mjc1NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428392754", "bodyText": "nit: supportedVersionRange", "author": "abbccdda", "createdAt": "2020-05-21T01:13:18Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/FinalizedVersionRange.java", "diffHunk": "@@ -0,0 +1,57 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+/**\n+ * A specialization of {@link BaseVersionRange} representing a range of version levels.\n+ * NOTE: This is the backing class used to define the min/max version levels for finalized features.\n+ */\n+public class FinalizedVersionRange extends BaseVersionRange {\n+    // Label for the min version key, that's used only for serialization/deserialization purposes.\n+    private static final String MIN_VERSION_LEVEL_KEY_LABEL = \"min_version_level\";\n+\n+    // Label for the max version key, that's used only for serialization/deserialization purposes.\n+    private static final String MAX_VERSION_LEVEL_KEY_LABEL = \"max_version_level\";\n+\n+    public FinalizedVersionRange(long minVersionLevel, long maxVersionLevel) {\n+        super(MIN_VERSION_LEVEL_KEY_LABEL, minVersionLevel, MAX_VERSION_LEVEL_KEY_LABEL, maxVersionLevel);\n+    }\n+\n+    public static FinalizedVersionRange deserialize(Map<String, Long> serialized) {\n+        return new FinalizedVersionRange(\n+            BaseVersionRange.valueOrThrow(MIN_VERSION_LEVEL_KEY_LABEL, serialized),\n+            BaseVersionRange.valueOrThrow(MAX_VERSION_LEVEL_KEY_LABEL, serialized));\n+    }\n+\n+    private boolean isCompatibleWith(BaseVersionRange versionRange) {\n+        return min() >= versionRange.min() && max() <= versionRange.max();\n+    }\n+\n+    /**\n+     * Checks if the [min, max] version level range of this object does *NOT* fall within the\n+     * [min, max] version range of the provided SupportedVersionRange parameter.\n+     *\n+     * @param versionRange   the SupportedVersionRange to be checked", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3ODgyOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428578829", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:45:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM5Mjc1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM5MjkzNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428392936", "bodyText": "Why this is a NOTE? Could we just comment like:\nAn extended BaseVersionRange representing the min/max versions for supported features.", "author": "abbccdda", "createdAt": "2020-05-21T01:14:10Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/SupportedVersionRange.java", "diffHunk": "@@ -0,0 +1,41 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+/**\n+ * A specialization of VersionRange representing a range of versions.\n+ * NOTE: This is the backing class used to define the min/max versions for supported features.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3OTA3MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428579070", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-21T10:46:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODM5MjkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMjg2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428402864", "bodyText": "Maybe I'm a bit too obsessive about code duplication, but after I made an attempt I thought we could actually have the internal deserialization logic shared between deserializeFinalizedFeatures and deserializeSupportedFeatures by making a template\n public static Features<FinalizedVersionRange> deserializeFinalizedFeatures(Map<String, Map<String, Long>> serialized) {\n        return deserializeFeatures(serialized, FinalizedVersionRange::deserialize);\n    }\n\n    public static Features<SupportedVersionRange> deserializeSupportedFeatures(\n        Map<String, Map<String, Long>> serialized) {\n        return deserializeFeatures(serialized, SupportedVersionRange::deserialize);\n    }\n        \n    \n    private interface Deserializer<V> {\n        V deserialize(Map<String, Long> serialized);\n    }\n\n\n    private static <V extends BaseVersionRange> Features<V> deserializeFeatures(Map<String, Map<String, Long>> serialized, Deserializer<V> deserializer) {\n        return new Features<>(serialized.entrySet().stream().collect(\n            Collectors.toMap(\n                Map.Entry::getKey,\n                entry -> deserializer.deserialize(entry.getValue()))));\n    }", "author": "abbccdda", "createdAt": "2020-05-21T01:54:32Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being VersionRangeType.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ * @see SupportedVersionRange\n+ * @see FinalizedVersionRange\n+ */\n+public class Features<VersionRangeType extends BaseVersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"supported\" features.\n+     */\n+    public static Features<SupportedVersionRange> supportedFeatures(Map<String, SupportedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to FinalizedVersionRange, as the backing data structure\n+     *                   for the Features object.\n+     * @return           Returns a new Features object representing \"finalized\" features.\n+     */\n+    public static Features<FinalizedVersionRange> finalizedFeatures(Map<String, FinalizedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    // Visible for testing.\n+    public static Features<FinalizedVersionRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<SupportedVersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public Map<String, VersionRangeType> features() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    /**\n+     * @param  feature   name of the feature\n+     *\n+     * @return           the VersionRangeType corresponding to the feature name, or null if absent\n+     */\n+    public VersionRangeType get(String feature) {\n+        return features.get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   A map with underlying features serialized. The returned value can be deserialized\n+     *           using one of the deserialize* APIs.\n+     */\n+    public Map<String, Map<String, Long>> serialize() {\n+        return features.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().serialize()));\n+    }\n+\n+    /**\n+     * Deserialize a map to Features<FinalizedVersionRange>.\n+     *\n+     * @param serialized   the serialized representation of a Features<FinalizedVersionRange> object,\n+     *                     generated using the serialize() API.\n+     *\n+     * @return             the deserialized Features<FinalizedVersionRange> object\n+     */\n+    public static Features<FinalizedVersionRange> deserializeFinalizedFeatures(\n+        Map<String, Map<String, Long>> serialized) {\n+        return finalizedFeatures(serialized.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> FinalizedVersionRange.deserialize(entry.getValue()))));\n+    }\n+\n+    /**\n+     * Deserializes a map to Features<SupportedVersionRange>.\n+     *\n+     * @param serialized   the serialized representation of a Features<SupportedVersionRange> object,\n+     *                     generated using the serialize() API.\n+     *\n+     * @return             the deserialized Features<SupportedVersionRange> object\n+     */\n+    public static Features<SupportedVersionRange> deserializeSupportedFeatures(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA2OTY3Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429069672", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T06:41:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMjg2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMzkwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428403904", "bodyText": "Missing header", "author": "abbccdda", "createdAt": "2020-05-21T01:58:35Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FinalizedVersionRangeTest.java", "diffHunk": "@@ -0,0 +1,135 @@\n+package org.apache.kafka.common.feature;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU3OTg4NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428579884", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:48:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwMzkwNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwNDA2OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428404068", "bodyText": "Seems we didn't trigger style check on this new class.", "author": "abbccdda", "createdAt": "2020-05-21T01:59:14Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/ApiVersionsResponseTest.java", "diffHunk": "@@ -17,13 +17,19 @@\n \n package org.apache.kafka.common.requests;\n \n+import org.apache.kafka.common.feature.Features;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyODgxMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429128812", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T09:01:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwNDA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwNDQ5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428404491", "bodyText": "What's the difference between this test class and its super class test case? Same question for SupportedVersionRangeTest", "author": "abbccdda", "createdAt": "2020-05-21T02:00:52Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FinalizedVersionRangeTest.java", "diffHunk": "@@ -0,0 +1,135 @@\n+package org.apache.kafka.common.feature;\n+\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThrows;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+public class FinalizedVersionRangeTest {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3MTUwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429071500", "bodyText": "Done. I have simplified this test suite eliminating the redundant tests, and only keeping the ones specific to FinalizedVersionRange. Also I have added documentation to both test suites explaining their purpose.", "author": "kowshik", "createdAt": "2020-05-22T06:47:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQwNDQ5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQxOTUzOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428419538", "bodyText": "I think we don't need to talk about future work inside the comment, just making it clear that the read path for serving ApiVersionsRequest is the only reader as of now.", "author": "abbccdda", "createdAt": "2020-05-21T03:02:18Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,88 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * {@link FinalizedFeatureChangeListener}.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU4MDMzNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428580337", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:49:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQxOTUzOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMDE0MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428420141", "bodyText": "nit: provide", "author": "abbccdda", "createdAt": "2020-05-21T03:04:59Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODU4MTI1Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428581253", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-21T10:51:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMDE0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMTcxMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428421713", "bodyText": "Do we need the comment to be on info level?", "author": "abbccdda", "createdAt": "2020-05-21T03:11:58Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMTEwMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429111103", "bodyText": "I didn't understand the question. Are you saying the logging severity should be lower or higher?\nThis is a rare case anyway as the feature node doesn't get created often, so, info logging seems fine to me.", "author": "kowshik", "createdAt": "2020-05-22T08:23:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMTcxMw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ4MDYwMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429480603", "bodyText": "My feeling is that this could be on debug level, but no strong perference.", "author": "abbccdda", "createdAt": "2020-05-22T22:38:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMTcxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjI4MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428422280", "bodyText": "nit: don't feel strong about having this parameter", "author": "abbccdda", "createdAt": "2020-05-21T03:14:25Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      processNotification()\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * would exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    thread.start()\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)\n+\n+    if (waitOnceForCacheUpdateMs > 0) {\n+      val barrier = new CountDownLatch(1)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExNzkxOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429117918", "bodyText": "Done. Removed.", "author": "kowshik", "createdAt": "2020-05-22T08:37:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjI4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjQ4Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428422483", "bodyText": "feel neutral about this helper function", "author": "abbccdda", "createdAt": "2020-05-21T03:15:18Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEwNzAyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429107026", "bodyText": "Done. Removed.", "author": "kowshik", "createdAt": "2020-05-22T08:14:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjQ4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjk2Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428422966", "bodyText": "I don't think this is scala accepted comment style to add -, do you see a warning?", "author": "abbccdda", "createdAt": "2020-05-21T03:17:24Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExMDA4Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429110087", "bodyText": "Done. Removed.", "author": "kowshik", "createdAt": "2020-05-22T08:21:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyMjk2Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNDM5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428424395", "bodyText": "Feature cache update gets interrupted", "author": "abbccdda", "createdAt": "2020-05-21T03:23:53Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEwNzM0Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429107347", "bodyText": "Done. But it's actually \"Change notification queue interrupted\".", "author": "kowshik", "createdAt": "2020-05-22T08:15:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNDM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNjE4OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428426189", "bodyText": "Does the version field existence guarantee there is a valid feature data node or not? In fact, getDataAndVersion returns an optional data. I checked the getDataAndVersion caller ProducerIdManager, there is a handling for empty data which I feel we should have as well.\nAdditionally, I think since we haven't implemented the write path yet, could we get a ticket to write down a short description on how the write path shall look like, by defining the different cases like:\nempty dataBytes, valid version \nvalid dataBytes, valid version \nempty dataBytes, unknown version \nvalid dataBytes, unknown version \n\nif that makes sense, so that we could keep track of the design decisions we made in the read path PR when implementing the write path.", "author": "abbccdda", "createdAt": "2020-05-21T03:31:57Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyMzk4OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429123988", "bodyText": "I have added documentation here in this method describing all the cases.\nThe empty data case should never happen and can indicate a corruption. The reason is that we always return non-empty data in FeatureZNode.encode, so the ZK node content should never empty.\nYes, I can add some more info to KAFKA-10028 or in the write path PR summary.", "author": "kowshik", "createdAt": "2020-05-22T08:50:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNjE4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNjQ0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428426448", "bodyText": "Could we summary the possible thrown error code in the comment as well? For example, does a JSON deserialization error should be treated as fatal?", "author": "abbccdda", "createdAt": "2020-05-21T03:33:03Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEwMTAxMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429101010", "bodyText": "Done. Yes, I feel JSON deserialization should be treated as fatal. It should never happen, and, can indicate corruption.", "author": "kowshik", "createdAt": "2020-05-22T08:01:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyNjQ0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyOTAxMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428429013", "bodyText": "Is it possible to have no enqueued updater, and cause this function block the thread indefinitely?", "author": "abbccdda", "createdAt": "2020-05-21T03:44:51Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEwOTE3MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429109170", "bodyText": "The function blocks indefinitely - yes. But this shouldn't cause a problem or lead to deadlock/limbo situation.\nEven if this thread is waiting for an item to become available in the queue, the waiting thread can always get interrupted by the FinalizedFeatureChangeListener.close() call which calls ShutdownableThread.shutdown().\nNote that the ShutdownableThread.shutdown() method interrupts the thread, which should unblock any waiting queue.take() operation and makes it raise an InterruptedException:\nhttps://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/ShutdownableThread.scala#L32-L59\nhttps://docs.oracle.com/javase/7/docs/api/java/util/concurrent/LinkedBlockingQueue.html#take()", "author": "kowshik", "createdAt": "2020-05-22T08:19:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyOTAxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyOTYxNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428429615", "bodyText": "For an educational question, does the zkClient have a separate thread to do the node change monitoring?", "author": "abbccdda", "createdAt": "2020-05-21T03:47:38Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      processNotification()\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * would exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    thread.start()\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTExNzI3OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429117278", "bodyText": "Yes. Here is the documentation explaining the same: https://zookeeper.apache.org/doc/r3.5.7/zookeeperProgrammers.html#Java+Binding.\n\nWhen a ZooKeeper object is created, two threads are created as well: an IO thread and an event thread. All IO happens on the IO thread (using Java NIO). All event callbacks happen on the event thread. Session maintenance such as reconnecting to ZooKeeper servers and maintaining heartbeat is done on the IO thread. Responses for synchronous methods are also processed in the IO thread. All responses to asynchronous methods and watch events are processed on the event thread.", "author": "kowshik", "createdAt": "2020-05-22T08:36:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQyOTYxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDA5Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428430093", "bodyText": "Does the order matter here? I was wondering if there is any concurrent issue if we unregister before the queue and thread get cleaned up.", "author": "abbccdda", "createdAt": "2020-05-21T03:49:45Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      processNotification()\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * would exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    thread.start()\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)\n+\n+    if (waitOnceForCacheUpdateMs > 0) {\n+      val barrier = new CountDownLatch(1)\n+      val ensureCacheUpdateOnce = new FeatureCacheUpdater(FeatureZNodeChangeHandler.path, Some(barrier))\n+      queue.add(ensureCacheUpdateOnce)\n+      try {\n+        ensureCacheUpdateOnce.awaitUpdateOrThrow(waitOnceForCacheUpdateMs)\n+      } catch {\n+        case e: Exception => {\n+          close()\n+          throw e\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Closes the feature ZK node change listener by unregistering the listener from ZK client,\n+   * clearing the queue and shutting down the ChangeNotificationProcessorThread.\n+   */\n+  def close(): Unit = {\n+    zkClient.unregisterZNodeChangeHandler(FeatureZNodeChangeHandler.path)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyMDU4NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429120585", "bodyText": "The order probably doesn't matter in this case. But logically I decided to follow the below order since I could reason about it better:\n\nStop the inflow of new events\nClear pending events\nStop the processing of all events", "author": "kowshik", "createdAt": "2020-05-22T08:43:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDA5Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDM1Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428430356", "bodyText": "We could just comment For testing only", "author": "abbccdda", "createdAt": "2020-05-21T03:50:57Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,203 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to propvide notification\n+   *                            when an update operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             - RuntimeException if the thread was interrupted during wait\n+     *                     - TimeoutException if the wait can not be completed in waitTimeMs\n+     *                       milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    private def processNotification(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      processNotification()\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      processNotification()\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * would exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    thread.start()\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)\n+\n+    if (waitOnceForCacheUpdateMs > 0) {\n+      val barrier = new CountDownLatch(1)\n+      val ensureCacheUpdateOnce = new FeatureCacheUpdater(FeatureZNodeChangeHandler.path, Some(barrier))\n+      queue.add(ensureCacheUpdateOnce)\n+      try {\n+        ensureCacheUpdateOnce.awaitUpdateOrThrow(waitOnceForCacheUpdateMs)\n+      } catch {\n+        case e: Exception => {\n+          close()\n+          throw e\n+        }\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Closes the feature ZK node change listener by unregistering the listener from ZK client,\n+   * clearing the queue and shutting down the ChangeNotificationProcessorThread.\n+   */\n+  def close(): Unit = {\n+    zkClient.unregisterZNodeChangeHandler(FeatureZNodeChangeHandler.path)\n+    queue.clear()\n+    thread.shutdown()\n+    thread.join()\n+  }\n+\n+  // Useful for testing.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyMDczOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429120739", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T08:44:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDM1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDU1Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428430553", "bodyText": "wait time for the first feature cache update upon initialization", "author": "abbccdda", "createdAt": "2020-05-21T03:51:55Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -812,6 +817,8 @@ object KafkaConfig {\n   val ControlledShutdownMaxRetriesDoc = \"Controlled shutdown can fail for multiple reasons. This determines the number of retries when such failure happens\"\n   val ControlledShutdownRetryBackoffMsDoc = \"Before each retry, the system needs time to recover from the state that caused the previous failure (Controller fail over, replica lag etc). This config determines the amount of time to wait before retrying.\"\n   val ControlledShutdownEnableDoc = \"Enable controlled shutdown of the server\"\n+  /** ********* Feature configuration ***********/\n+  val FeatureChangeListenerCacheUpdateWaitTimeMsDoc = \"# of milli seconds to wait for feature cache to be updated once.\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyNjc1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429126751", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T08:56:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDU1Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDc3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428430779", "bodyText": "I think the comment is not necessary, since we have already commented on KAFKA_2_6_IV1", "author": "abbccdda", "createdAt": "2020-05-21T03:52:57Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -210,6 +215,14 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         /* setup zookeeper */\n         initZkClient(time)\n \n+        /* initialize features */\n+        _featureChangeListener = new FinalizedFeatureChangeListener(_zkClient)\n+        if (config.interBrokerProtocolVersion >= KAFKA_2_6_IV1) {\n+          // The feature versioning system (KIP-584) is active only when:", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTEyNjg2Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429126867", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T08:57:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMDc3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMTU4NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428431584", "bodyText": "nit: Returns a reference to the latest features supported by the broker.", "author": "abbccdda", "createdAt": "2020-05-21T03:56:16Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA5MDAyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429090026", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:36:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMTU4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMjQyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428432424", "bodyText": "This logging is duplicate", "author": "abbccdda", "createdAt": "2020-05-21T04:00:05Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.\n+   */\n+  def get: Features[SupportedVersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided cluster-wide finalized feature. This can happen because a provided\n+   * cluster-wide finalized feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The set of incompatible feature names. If the returned set is empty, it\n+   *                    means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Set[String] = {\n+    val incompatibilities = finalized.features.asScala.collect {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature);\n+        if (supportedVersions == null) {\n+          (feature, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, \"{feature=%s, reason='Finalized %s is incompatible with supported %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, null)\n+        }\n+      }\n+    }.filter(entry => entry._2 != null)\n+\n+    if (incompatibilities.nonEmpty) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA5NjYxMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429096612", "bodyText": "Done. Removed extra logging in the caller of this method (see FinalizedFeatureCache).", "author": "kowshik", "createdAt": "2020-05-22T07:51:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMjQyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMjcyNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428432727", "bodyText": "I'm slightly inclined to return a set of features instead of just strings, and make the string conversion as a helper. But I leave this up to you to decide, and we could always adapt the function to make it more useful in other scenarios as needed.", "author": "abbccdda", "createdAt": "2020-05-21T04:01:36Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,74 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common object used in the Broker to define the latest features supported by the Broker.\n+ * Also provides API to check for incompatibilities between the latest features supported by the\n+ * Broker and cluster-wide finalized features.\n+ */\n+object SupportedFeatures extends Logging {\n+\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns the latest features supported by the Broker.\n+   */\n+  def get: Features[SupportedVersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided cluster-wide finalized feature. This can happen because a provided\n+   * cluster-wide finalized feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The set of incompatible feature names. If the returned set is empty, it\n+   *                    means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Set[String] = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA5MzcyNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429093727", "bodyText": "Done. Good point!", "author": "kowshik", "createdAt": "2020-05-22T07:45:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMjcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzI3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428433279", "bodyText": "aha, the order is wrong for KAFKA_0_10_0_IV1 and KAFKA_2_6_IV1", "author": "abbccdda", "createdAt": "2020-05-21T04:04:10Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -81,17 +83,26 @@ object BrokerIdsZNode {\n object BrokerInfo {\n \n   /**\n-   * Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n-   * the apiVersion is 0.10.0.X or above. Register the broker with v2 json format otherwise.\n+   * - Create a broker info with v5 json format if the apiVersion is 2.6.x or above.\n+   * - Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n+   *   the apiVersion is 0.10.0.X or above but lesser than 2.6.x.\n+   * - Register the broker with v2 json format otherwise.\n    *\n    * Due to KAFKA-3100, 0.9.0.0 broker and old clients will break if JSON version is above 2.\n    *\n-   * We include v2 to make it possible for the broker to migrate from 0.9.0.0 to 0.10.0.X or above without having to\n-   * upgrade to 0.9.0.1 first (clients have to be upgraded to 0.9.0.1 in any case).\n+   * We include v2 to make it possible for the broker to migrate from 0.9.0.0 to 0.10.0.X or above\n+   * without having to upgrade to 0.9.0.1 first (clients have to be upgraded to 0.9.0.1 in\n+   * any case).\n    */\n   def apply(broker: Broker, apiVersion: ApiVersion, jmxPort: Int): BrokerInfo = {\n-    // see method documentation for the reason why we do this\n-    val version = if (apiVersion >= KAFKA_0_10_0_IV1) 4 else 2\n+    val version = {\n+      if (apiVersion >= KAFKA_0_10_0_IV1)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4OTU3NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429089574", "bodyText": "Done. Good catch!", "author": "kowshik", "createdAt": "2020-05-22T07:35:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzMzI3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNDE1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428434151", "bodyText": "s/asJavaMap/featuresAsJavaMap", "author": "abbccdda", "createdAt": "2020-05-21T04:08:21Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -146,7 +162,14 @@ object BrokerIdZNode {\n     val plaintextEndpoint = broker.endPoints.find(_.securityProtocol == SecurityProtocol.PLAINTEXT).getOrElse(\n       new EndPoint(null, -1, null, null))\n     encode(brokerInfo.version, plaintextEndpoint.host, plaintextEndpoint.port, broker.endPoints, brokerInfo.jmxPort,\n-      broker.rack)\n+      broker.rack, broker.features)\n+  }\n+\n+  def asJavaMap(brokerInfo: JsonObject): util.Map[String, util.Map[String, java.lang.Long]] = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4OTI0NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429089245", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:34:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNDE1MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNDk0NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428434945", "bodyText": "Could we log statusInt here as well? Also I feel the exception should be thrown from  FeatureZNodeStatus.withNameOpt", "author": "abbccdda", "createdAt": "2020-05-21T04:11:27Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,90 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // Version0 contains 'version', 'status' and 'features' keys.\n+  val Version0 = 0\n+  val CurrentVersion = Version0\n+\n+  def path = \"/feature\"\n+\n+  def asJavaMap(scalaMap: Map[String, Map[String, Long]]): util.Map[String, util.Map[String, java.lang.Long]] = {\n+    scalaMap\n+      .view.mapValues(_.view.mapValues(scalaLong => java.lang.Long.valueOf(scalaLong)).toMap.asJava)\n+      .toMap\n+      .asJava\n+  }\n+\n+  def encode(featureZNode: FeatureZNode): Array[Byte] = {\n+    val jsonMap = collection.mutable.Map(\n+      VersionKey -> CurrentVersion,\n+      StatusKey -> featureZNode.status.id,\n+      FeaturesKey -> featureZNode.features.serialize)\n+    Json.encodeAsBytes(jsonMap.asJava)\n+  }\n+\n+  def decode(jsonBytes: Array[Byte]): FeatureZNode = {\n+    Json.tryParseBytes(jsonBytes) match {\n+      case Right(js) =>\n+        val featureInfo = js.asJsonObject\n+        val version = featureInfo(VersionKey).to[Int]\n+        if (version < Version0 || version > CurrentVersion) {\n+          throw new KafkaException(s\"Unsupported version: $version of feature information: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+\n+        val featuresMap = featureInfo\n+          .get(FeaturesKey)\n+          .flatMap(_.to[Option[Map[String, Map[String, Long]]]])\n+        if (featuresMap.isEmpty) {\n+          throw new KafkaException(\"Features map can not be absent in: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+        val features = asJavaMap(featuresMap.get)\n+\n+        val statusInt = featureInfo\n+          .get(StatusKey)\n+          .flatMap(_.to[Option[Int]])\n+        if (statusInt.isEmpty) {\n+          throw new KafkaException(\"Status can not be absent in feature information: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+        val status = FeatureZNodeStatus.withNameOpt(statusInt.get)\n+        if (status.isEmpty) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NjMwMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429076303", "bodyText": "Done.\nFor the other point, I don't feel strongly for it. I feel it is OK to have an API that doesn't throw and just lets the caller decide (based on the context) if an empty returned value is incorrect.", "author": "kowshik", "createdAt": "2020-05-22T07:01:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNDk0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNTY5NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428435694", "bodyText": "Is there a more dedicated exception code for deserialization error? I feel the KafkaException is a bit too general compared with IllegalArgument", "author": "abbccdda", "createdAt": "2020-05-21T04:15:15Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,90 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // Version0 contains 'version', 'status' and 'features' keys.\n+  val Version0 = 0\n+  val CurrentVersion = Version0\n+\n+  def path = \"/feature\"\n+\n+  def asJavaMap(scalaMap: Map[String, Map[String, Long]]): util.Map[String, util.Map[String, java.lang.Long]] = {\n+    scalaMap\n+      .view.mapValues(_.view.mapValues(scalaLong => java.lang.Long.valueOf(scalaLong)).toMap.asJava)\n+      .toMap\n+      .asJava\n+  }\n+\n+  def encode(featureZNode: FeatureZNode): Array[Byte] = {\n+    val jsonMap = collection.mutable.Map(\n+      VersionKey -> CurrentVersion,\n+      StatusKey -> featureZNode.status.id,\n+      FeaturesKey -> featureZNode.features.serialize)\n+    Json.encodeAsBytes(jsonMap.asJava)\n+  }\n+\n+  def decode(jsonBytes: Array[Byte]): FeatureZNode = {\n+    Json.tryParseBytes(jsonBytes) match {\n+      case Right(js) =>\n+        val featureInfo = js.asJsonObject\n+        val version = featureInfo(VersionKey).to[Int]\n+        if (version < Version0 || version > CurrentVersion) {\n+          throw new KafkaException(s\"Unsupported version: $version of feature information: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+\n+        val featuresMap = featureInfo\n+          .get(FeaturesKey)\n+          .flatMap(_.to[Option[Map[String, Map[String, Long]]]])\n+        if (featuresMap.isEmpty) {\n+          throw new KafkaException(\"Features map can not be absent in: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+        val features = asJavaMap(featuresMap.get)\n+\n+        val statusInt = featureInfo\n+          .get(StatusKey)\n+          .flatMap(_.to[Option[Int]])\n+        if (statusInt.isEmpty) {\n+          throw new KafkaException(\"Status can not be absent in feature information: \" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NDIxNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429074215", "bodyText": "Done. Changed to IllegalArgumentException. Good point!", "author": "kowshik", "createdAt": "2020-05-22T06:55:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNTY5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjA0Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428436046", "bodyText": "Could we name it V0 for simplicity?", "author": "abbccdda", "createdAt": "2020-05-21T04:16:59Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,90 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // Version0 contains 'version', 'status' and 'features' keys.\n+  val Version0 = 0", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NzQ2OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429077469", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:04:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjA0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjU3Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428436573", "bodyText": "I feel we might worth creating a separate thread discussing whether we could get some benefit of the automated protocol generation framework here, as I think this could be easily represented as JSON if we define it in the common package like other RPC data. The difficulty right now is mostly on the serialization and deserialization for feature itself, but these could have workarounds if we want to do so.", "author": "abbccdda", "createdAt": "2020-05-21T04:19:58Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,90 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3ODY2MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429078661", "bodyText": "As far as I can see, no ZK node class defined in this file is defined in such a way. Every class in this file encodes/decodes JSON by itself, and manages its own attributes.\nShould we break the norm?", "author": "kowshik", "createdAt": "2020-05-22T07:07:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMTIzNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429511236", "bodyText": "I think the norm exists because we don't have automated framework by then, and doing hand-written json serialization and deserialization is a bit wasting. cc @hachikuji @cmccabe as this is a major direction discussion.", "author": "abbccdda", "createdAt": "2020-05-23T03:52:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU5ODcyMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437598721", "bodyText": "I think I'm no longer insisting on this point, as we could make this as a follow-up work. Filed JIRA here: https://issues.apache.org/jira/browse/KAFKA-10130", "author": "abbccdda", "createdAt": "2020-06-09T17:27:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjU3Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NTcwOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437875708", "bodyText": "Thanks. Good idea to leave a jira. I have linked it to KAFKA-9755.", "author": "kowshik", "createdAt": "2020-06-10T05:54:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNjU3Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzMxOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428437318", "bodyText": "I'm a bit surprised, do we want to support feature znode deletion in long term?", "author": "abbccdda", "createdAt": "2020-05-21T04:23:22Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,228 @@\n+package kafka.server\n+\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion, ZooKeeperTestHarness}\n+import kafka.utils.{Exit, TestUtils}\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.internals.FatalExitError\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertThrows, assertTrue}\n+import org.junit.{Before, Test}\n+\n+import scala.concurrent.TimeoutException\n+import scala.jdk.CollectionConverters._\n+\n+class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n+  @Before\n+  override def setUp(): Unit = {\n+    super.setUp()\n+    FinalizedFeatureCache.clear()\n+    SupportedFeatures.clear()\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can listen to ZK notifications\n+   * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   */\n+  @Test\n+  def testInitSuccessAndNotificationSuccess(): Unit = {\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"feature_2\" -> new SupportedVersionRange(1, 3))\n+    SupportedFeatures.update(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val initialFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(initialFinalizedFeaturesMap.asJava)\n+    zkClient.createFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, initialFinalizedFeatures))\n+    val (mayBeFeatureZNodeBytes, initialVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(initialVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeBytes.isEmpty)\n+\n+    val listener = new FinalizedFeatureChangeListener(zkClient)\n+    assertFalse(listener.isListenerInitiated)\n+    assertTrue(FinalizedFeatureCache.empty)\n+    listener.initOrThrow(15000)\n+    assertTrue(listener.isListenerInitiated)\n+    val mayBeNewCacheContent = FinalizedFeatureCache.get\n+    assertFalse(mayBeNewCacheContent.isEmpty)\n+    val newCacheContent = mayBeNewCacheContent.get\n+    assertEquals(initialFinalizedFeatures, newCacheContent.features)\n+    assertEquals(initialVersion, newCacheContent.epoch)\n+\n+    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n+    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n+    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n+    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+    assertTrue(updatedVersion > initialVersion)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n+    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can process FeatureZNode deletion\n+   * successfully.\n+   */\n+  @Test\n+  def testFeatureZNodeDeleteNotificationProcessing(): Unit = {\n+    val supportedFeatures = Map[String, SupportedVersionRange](\n+      \"feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"feature_2\" -> new SupportedVersionRange(1, 3))\n+    SupportedFeatures.update(Features.supportedFeatures(supportedFeatures.asJava))\n+\n+    val initialFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val initialFinalizedFeatures = Features.finalizedFeatures(initialFinalizedFeaturesMap.asJava)\n+    zkClient.createFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, initialFinalizedFeatures))\n+    val (mayBeFeatureZNodeBytes, initialVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(initialVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeBytes.isEmpty)\n+\n+    val listener = new FinalizedFeatureChangeListener(zkClient)\n+    assertFalse(listener.isListenerInitiated)\n+    assertTrue(FinalizedFeatureCache.empty)\n+    listener.initOrThrow(15000)\n+    assertTrue(listener.isListenerInitiated)\n+    val mayBeNewCacheContent = FinalizedFeatureCache.get\n+    assertFalse(mayBeNewCacheContent.isEmpty)\n+    val newCacheContent = mayBeNewCacheContent.get\n+    assertEquals(initialFinalizedFeatures, newCacheContent.features)\n+    assertEquals(initialVersion, newCacheContent.epoch)\n+\n+    zkClient.deleteFeatureZNode()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3Njc0Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429076742", "bodyText": "No. But we want to test the behavior about what happens during a deletion (ex: operational error).", "author": "kowshik", "createdAt": "2020-05-22T07:02:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzMxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzU3MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428437571", "bodyText": "Could we extract some common initialization logic for the tests to reduce duplication?", "author": "abbccdda", "createdAt": "2020-05-21T04:24:39Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,228 @@\n+package kafka.server\n+\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion, ZooKeeperTestHarness}\n+import kafka.utils.{Exit, TestUtils}\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.internals.FatalExitError\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertThrows, assertTrue}\n+import org.junit.{Before, Test}\n+\n+import scala.concurrent.TimeoutException\n+import scala.jdk.CollectionConverters._\n+\n+class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4NjE4NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429086184", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:27:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzY1OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428437659", "bodyText": "nit: space", "author": "abbccdda", "createdAt": "2020-05-21T04:24:58Z", "path": "core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala", "diffHunk": "@@ -776,6 +776,9 @@ class KafkaConfigTest {\n         case KafkaConfig.KafkaMetricsReporterClassesProp => // ignore\n         case KafkaConfig.KafkaMetricsPollingIntervalSecondsProp => //ignore\n \n+        //Feature configuration", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA3NzA2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429077064", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-22T07:03:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzNzY1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzODA2Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r428438063", "bodyText": "If we are not validating the features by extracting them, I think we do not need to pass in a non-empty feature list?", "author": "abbccdda", "createdAt": "2020-05-21T04:27:00Z", "path": "core/src/test/scala/unit/kafka/zk/KafkaZkClientTest.scala", "diffHunk": "@@ -811,8 +828,16 @@ class KafkaZkClientTest extends ZooKeeperTestHarness {\n     assertEquals(Seq.empty, zkClient.getSortedBrokerList)\n     assertEquals(None, zkClient.getBroker(0))\n \n-    val brokerInfo0 = createBrokerInfo(0, \"test.host0\", 9998, SecurityProtocol.PLAINTEXT)\n-    val brokerInfo1 = createBrokerInfo(1, \"test.host1\", 9999, SecurityProtocol.SSL)\n+    val brokerInfo0 = createBrokerInfo(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTA4Nzk3Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429087972", "bodyText": "See L848 below where it is validated. The call to zkClient. getAllBrokersInCluster decodes each BrokerIdZNode content from JSON to BrokerInfo object. Then, we check whether the call returns exactly the same BrokerInfo objects defined here, and, along the way features are checked too.", "author": "kowshik", "createdAt": "2020-05-22T07:31:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyODQzODA2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ3NDc2MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429474760", "bodyText": "We could have multiple @throws here", "author": "abbccdda", "createdAt": "2020-05-22T22:08:39Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -37,6 +37,15 @@ class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n      *\n      * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n      * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDIxOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590218", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:17:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ3NDc2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ3NjkyOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429476928", "bodyText": "I didn't look thoroughly enough, but the only IllegalArgumentException I found is\n case invalidVersion =>\n        throw new IllegalArgumentException(s\"Expected controller epoch zkVersion $invalidVersion should be non-negative or equal to ${ZkVersion.MatchAnyVersion}\")\n\nwhich should never happen as we always use MatchAnyVersion in retryRequestsUntilConnected. Are we trying to catch some other exceptions here?", "author": "abbccdda", "createdAt": "2020-05-22T22:19:31Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -47,7 +56,30 @@ class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n       })\n \n       info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n-      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+      var mayBeFeatureZNodeBytes: Option[Array[Byte]] = null\n+      var version: Int = ZkVersion.UnknownVersion\n+      try {\n+        val result = zkClient.getDataAndVersion(featureZkNodePath)\n+        mayBeFeatureZNodeBytes = result._1\n+        version = result._2\n+      } catch {\n+        // Convert to RuntimeException, to avoid a confusion that there is no argument passed\n+        // to the updateOrThrow() method.\n+        case e: IllegalArgumentException => throw new RuntimeException(e)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDQyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590426", "bodyText": "Fixed now.\nGood point. Actually the code was incorrect. I meant to wrap FeatureZNode.decode call with the try-catch, since, it throws IllegalArgumentException. I have fixed the code now to do the same.", "author": "kowshik", "createdAt": "2020-05-24T01:23:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ3NjkyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDYzNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429494635", "bodyText": "nit: minKeyLabel", "author": "abbccdda", "createdAt": "2020-05-23T00:13:07Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max of type long.\n+ * The min and max attributes are expected to be >= 1, and with max >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    private final String minKeyLabel;\n+\n+    private final long minValue;\n+\n+    private final String maxKeyLabel;\n+\n+    private final long maxValue;\n+\n+    protected BaseVersionRange(String minKey, long minValue, String maxKeyLabel, long maxValue) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU4OTYwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429589607", "bodyText": "Done. Also added doc.", "author": "kowshik", "createdAt": "2020-05-24T01:00:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NDYzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjAzNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429496037", "bodyText": "nit: we could test emptySupportedFeatures.features().isEmpty()", "author": "abbccdda", "createdAt": "2020-05-23T00:24:54Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,167 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Long>> emptyMap = new HashMap<>();\n+\n+        Features<FinalizedVersionRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertEquals(new HashMap<>(), emptyFinalizedFeatures.features());\n+        assertEquals(emptyMap, emptyFinalizedFeatures.serialize());\n+        assertEquals(emptyFinalizedFeatures, Features.deserializeFinalizedFeatures(emptyMap));\n+\n+        Features<SupportedVersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertEquals(new HashMap<>(), emptySupportedFeatures.features());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDAxMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590012", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:12:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5NjAzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Nzc0OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429497749", "bodyText": "nit: could you elaborate why this helper function and FinalizedFeaturesAndEpoch struct is useful in this context? Just for easier message printing?", "author": "abbccdda", "createdAt": "2020-05-23T00:40:54Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,86 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A mutable cache containing the latest finalized features and epoch. This cache is populated by a\n+ * {@link FinalizedFeatureChangeListener}.\n+ *\n+ * Currently the main reader of this cache is the read path that serves an ApiVersionsRequest,\n+ * returning the features information in the response.\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {\n+    featuresAndEpoch.isEmpty\n+  }\n+\n+  /**\n+   * Clears all existing finalized features and epoch from the cache.\n+   */\n+  def clear(): Unit = {\n+    featuresAndEpoch = Option.empty\n+    info(\"Cleared cache\")\n+  }\n+\n+  /**\n+   * Updates the cache to the latestFeatures, and updates the existing epoch to latestEpoch.\n+   * Raises an exception when the operation is not successful.\n+   *\n+   * @param latestFeatures   the latest finalized features to be set in the cache\n+   * @param latestEpoch      the latest epoch value to be set in the cache\n+   *\n+   * @throws                 FeatureCacheUpdateException if the cache update operation fails\n+   *                         due to invalid parameters or incompatibilities with the broker's\n+   *                         supported features. In such a case, the existing cache contents are\n+   *                         not modified.\n+   */\n+  def updateOrThrow(latestFeatures: Features[FinalizedVersionRange], latestEpoch: Int): Unit = {\n+    updateOrThrow(FinalizedFeaturesAndEpoch(latestFeatures, latestEpoch))\n+  }\n+\n+  private def updateOrThrow(latest: FinalizedFeaturesAndEpoch): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDE3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590179", "bodyText": "Actually I've eliminated the helper method now, and, there is only 1 method: updateOrThrow(...).", "author": "kowshik", "createdAt": "2020-05-24T01:16:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5Nzc0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODM5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429498395", "bodyText": "So here we will directly throw NoSuchElementException if mayBeFeatureZNodeBytes is empty? Do we want to check this case and throw a customized exception instead?", "author": "abbccdda", "createdAt": "2020-05-23T00:48:10Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     *\n+     *           RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      var mayBeFeatureZNodeBytes: Option[Array[Byte]] = null\n+      var version: Int = ZkVersion.UnknownVersion\n+      try {\n+        val result = zkClient.getDataAndVersion(featureZkNodePath)\n+        mayBeFeatureZNodeBytes = result._1\n+        version = result._2\n+      } catch {\n+        // Convert to RuntimeException, to avoid a confusion that there is no argument passed\n+        // to the updateOrThrow() method.\n+        case e: IllegalArgumentException => throw new RuntimeException(e)\n+      }\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDUwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590504", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:25:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODM5NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODQwOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429498408", "bodyText": "nit: space", "author": "abbccdda", "createdAt": "2020-05-23T00:48:17Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     *\n+     *           RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      var mayBeFeatureZNodeBytes: Option[Array[Byte]] = null\n+      var version: Int = ZkVersion.UnknownVersion\n+      try {\n+        val result = zkClient.getDataAndVersion(featureZkNodePath)\n+        mayBeFeatureZNodeBytes = result._1\n+        version = result._2\n+      } catch {\n+        // Convert to RuntimeException, to avoid a confusion that there is no argument passed\n+        // to the updateOrThrow() method.\n+        case e: IllegalArgumentException => throw new RuntimeException(e)\n+      }\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MTkxNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429591914", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:59:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5ODQwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTAwOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429499008", "bodyText": "This leads to a more general question: is there a way to cleanup all the ZK feature path? Reading from the KIP, I don't see we have any admin API to do so, which makes me wonder how could this case happen in reality. In terms of severity, I think crushing the entire cluster seems to be an overkill as well, maybe we should have some blocking mechanism in place for any feature extraction call here, until we see handleCreation gets triggered again?", "author": "abbccdda", "createdAt": "2020-05-23T00:54:28Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     *\n+     *           RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      var mayBeFeatureZNodeBytes: Option[Array[Byte]] = null\n+      var version: Int = ZkVersion.UnknownVersion\n+      try {\n+        val result = zkClient.getDataAndVersion(featureZkNodePath)\n+        mayBeFeatureZNodeBytes = result._1\n+        version = result._2\n+      } catch {\n+        // Convert to RuntimeException, to avoid a confusion that there is no argument passed\n+        // to the updateOrThrow() method.\n+        case e: IllegalArgumentException => throw new RuntimeException(e)\n+      }\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed\n+     * successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             RuntimeException if the thread was interrupted during wait\n+     *\n+     *                     TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Change notification queue interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: operational error).", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MzE1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429593151", "bodyText": "The deletion of ZNode is a rare case, it should never happen in reality unless it is ZK corruption, or rarely an operational error that deletes some ZK nodes. It's not easy to prevent damage in such a case. From a correctness standpoint, imagine what would happen if the feature ZNode gets deleted, and, afterwards a broker restarts. It will start with empty cache, so the damage is done. Therefore, it seems that even if we add a special logic here, we can not prevent damage if the source of truth is lost.\nTwo things to note here:\n\n\nThe client should anyway ignore older stale epoch responses, if it had seen newer epochs that are greater. In that spirit, the client can be also made to treat the absence of finalized features in an ApiVersionsResponse just like a stale epoch case, if, it had seen at least one valid ApiVersionsResponse earlier (i.e. at least one response with some valid epoch).\n\n\nDeletion of individual finalized feature is actually supported in KIP-584, but not deletion of the entire ZNode. Search for the word 'deletion' in the KIP write-up. If needed, this deletion functionality could be extended to provide the ability to delete all features too.", "author": "kowshik", "createdAt": "2020-05-24T02:26:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTAwOA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMDczNzYwNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r430737606", "bodyText": "Thanks, I don't think we need to be super paranoid with this rare scenario, but we should also be indicating this error state to the client suggesting that some manual fix is necessary. My proposed idea above is to add such an error state to the feature cache to refuse any further updates until we have: 1. a node creation event 2. restart of the broker (once the issue gets fixed), so this blocking behavior shall be ephemeral and recoverable from broker perspective. We don't have to implement this logic in the current PR, as we don't have a write path yet, just get a JIRA to track it sounds fine.\nMake sense to cc @cmccabe and @hachikuji as well.", "author": "abbccdda", "createdAt": "2020-05-26T22:15:42Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTQ5OTAwOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwOTE3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429509179", "bodyText": "nit: one liner: this.features = Objects.requireNonNull(features, \"Provided features can not be null.\");", "author": "abbccdda", "createdAt": "2020-05-23T03:18:11Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,187 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being <VersionRangeType>.\n+ * Also provides API to serialize/deserialize the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ * @see SupportedVersionRange\n+ * @see FinalizedVersionRange\n+ */\n+public class Features<VersionRangeType extends BaseVersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to type of VersionRange, as the backing data structure\n+     *                   for the Features object.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        if (features == null) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU4OTI5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429589291", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T00:51:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwOTE3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwOTk0MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429509941", "bodyText": "nit: remove only", "author": "abbccdda", "createdAt": "2020-05-23T03:30:12Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDIwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590200", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:17:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUwOTk0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDA5OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429510099", "bodyText": "This comment should be frequent and the featureZkNodePath is staying constant, could we just make it for debugging level?", "author": "abbccdda", "createdAt": "2020-05-23T03:32:02Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     *\n+     *           RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDI0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590248", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDA5OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDIyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429510224", "bodyText": "I don't think this note is necessary, maybe just merge with the first line as:\nWaits until exactly one updateLatestOrThrow completes successfully.", "author": "abbccdda", "createdAt": "2020-05-23T03:34:25Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,232 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked\n+     * only exactly once successfully.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     *\n+     *           FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     *\n+     *           RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      info(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      var mayBeFeatureZNodeBytes: Option[Array[Byte]] = null\n+      var version: Int = ZkVersion.UnknownVersion\n+      try {\n+        val result = zkClient.getDataAndVersion(featureZkNodePath)\n+        mayBeFeatureZNodeBytes = result._1\n+        version = result._2\n+      } catch {\n+        // Convert to RuntimeException, to avoid a confusion that there is no argument passed\n+        // to the updateOrThrow() method.\n+        case e: IllegalArgumentException => throw new RuntimeException(e)\n+      }\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status\")\n+          FinalizedFeatureCache.clear()\n+        } else if(featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully.\n+     * NOTE: The method returns immediately if an updateLatestOrThrow call has already completed", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDUzMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590532", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:25:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDIyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDUyMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429510520", "bodyText": "Do you think we should add this config as part of the KIP since it is public? I think it would just be a minor update, but let's wait and see others thoughts on this.", "author": "abbccdda", "createdAt": "2020-05-23T03:39:24Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -442,6 +445,8 @@ object KafkaConfig {\n   val ControlledShutdownMaxRetriesProp = \"controlled.shutdown.max.retries\"\n   val ControlledShutdownRetryBackoffMsProp = \"controlled.shutdown.retry.backoff.ms\"\n   val ControlledShutdownEnableProp = \"controlled.shutdown.enable\"\n+  /** ********* Features configuration ***********/\n+  val FeatureChangeListenerCacheUpdateWaitTimeMsProp = \"feature.listener.cache.update.wait.ms\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDU5Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590593", "bodyText": "Sounds good. Yeah, it is minor and feels like an implementation detail to me. But we can wait to see what others say.", "author": "kowshik", "createdAt": "2020-05-24T01:27:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDUyMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjYwNzQwMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436607402", "bodyText": "This config has been eliminated now.", "author": "kowshik", "createdAt": "2020-06-08T10:44:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDUyMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDc0Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429510746", "bodyText": "remove semi-colon", "author": "abbccdda", "createdAt": "2020-05-23T03:43:32Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,76 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common immutable object used in the Broker to define the latest features supported by the\n+ * Broker. Also provides API to check for incompatibilities between the latest features supported\n+ * by the Broker and cluster-wide finalized features.\n+ *\n+ * NOTE: the update() and clear() APIs of this class should be used only for testing purposes.\n+ */\n+object SupportedFeatures extends Logging {\n+\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns a reference to the latest features supported by the Broker.\n+   */\n+  def get: Features[SupportedVersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def update(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // Should be used only for testing.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided cluster-wide finalized feature. This can happen because a provided\n+   * cluster-wide finalized feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The sub-set of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalized.features.asScala.collect {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDYxNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590614", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:27:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMDc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMTA5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429511091", "bodyText": "s/it's/its", "author": "abbccdda", "createdAt": "2020-05-23T03:50:12Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,161 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+/**\n+ * Represents the status of the FeatureZNode.\n+ *\n+ * Enabled  -> This status means the feature versioning system (KIP-584) is enabled, and, the\n+ *             finalized features stored in the FeatureZNode are active. This status is written by\n+ *             the controller to the FeatureZNode only when the broker IBP config is greater than\n+ *             or equal to KAFKA_2_6_IV1.\n+ *\n+ * Disabled -> This status means the feature versioning system (KIP-584) is disabled, and, the\n+ *             the finalized features stored in the FeatureZNode is not relevant. This status is\n+ *             written by the controller to the FeatureZNode only when the broker IBP config\n+ *             is less than KAFKA_2_6_IV1.\n+ *\n+ * The purpose behind the FeatureZNodeStatus is that it helps differentiates between the following\n+ * cases:\n+ *\n+ * 1. New cluster bootstrap:\n+ *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+ *    with all the possible supported features finalized immediately. The new cluster will almost\n+ *    never be started with an old IBP config that\u2019s less than KAFKA_2_6_IV1. In such a case, the\n+ *    controller will start up and notice that the FeatureZNode is absent in the new cluster.\n+ *    To handle the requirement, the controller will create a FeatureZNode (with enabled status)\n+ *    containing the entire list of supported features as it\u2019s finalized features.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDYyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590626", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:28:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMTA5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMTE0Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429511146", "bodyText": "a Kafka cluster exists already and the IBP config is less than KAFKA_2_6_IV1 to an existing Kafka cluster with  IBP config less than KAFKA_2_6_IV1", "author": "abbccdda", "createdAt": "2020-05-23T03:51:19Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,161 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+/**\n+ * Represents the status of the FeatureZNode.\n+ *\n+ * Enabled  -> This status means the feature versioning system (KIP-584) is enabled, and, the\n+ *             finalized features stored in the FeatureZNode are active. This status is written by\n+ *             the controller to the FeatureZNode only when the broker IBP config is greater than\n+ *             or equal to KAFKA_2_6_IV1.\n+ *\n+ * Disabled -> This status means the feature versioning system (KIP-584) is disabled, and, the\n+ *             the finalized features stored in the FeatureZNode is not relevant. This status is\n+ *             written by the controller to the FeatureZNode only when the broker IBP config\n+ *             is less than KAFKA_2_6_IV1.\n+ *\n+ * The purpose behind the FeatureZNodeStatus is that it helps differentiates between the following\n+ * cases:\n+ *\n+ * 1. New cluster bootstrap:\n+ *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+ *    with all the possible supported features finalized immediately. The new cluster will almost\n+ *    never be started with an old IBP config that\u2019s less than KAFKA_2_6_IV1. In such a case, the\n+ *    controller will start up and notice that the FeatureZNode is absent in the new cluster.\n+ *    To handle the requirement, the controller will create a FeatureZNode (with enabled status)\n+ *    containing the entire list of supported features as it\u2019s finalized features.\n+ *\n+ * 2. Cluster upgrade:\n+ *    Imagine that a Kafka cluster exists already and the IBP config is less than KAFKA_2_6_IV1, but", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTU5MDY1Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r429590656", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-05-24T01:29:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyOTUxMTE0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MTIxNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r432781217", "bodyText": "This won't make 2.6.0 release. So, perhaps we should use KAFKA_2_7 or whatever the next release is?", "author": "junrao", "createdAt": "2020-05-29T23:29:07Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -22,7 +22,7 @@ import java.util.Properties\n \n import com.fasterxml.jackson.annotation.JsonProperty\n import com.fasterxml.jackson.core.JsonProcessingException\n-import kafka.api.{ApiVersion, KAFKA_0_10_0_IV1, LeaderAndIsr}\n+import kafka.api.{ApiVersion, KAFKA_0_10_0_IV1, KAFKA_2_6_IV1, LeaderAndIsr}", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDM0MzA2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434343065", "bodyText": "Done. Made it KAFKA_2_7_IV0.", "author": "kowshik", "createdAt": "2020-06-03T06:47:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MTIxNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MjA2OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r432782068", "bodyText": "I missed this in the KIP, but it seems that long is overkilling for version. The version in request is short and the version in ZK data is int. So, perhaps this should just be short?", "author": "junrao", "createdAt": "2020-05-29T23:33:18Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max, each of type long.\n+ * The min and max attributes need to satisfy 2 rules:\n+ *  - they are each expected to be >= 1, as we only consider positive version values to be valid.\n+ *  - max should be >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    // Non-empty label for the min version key, that's used only for serialization/deserialization purposes.\n+    private final String minKeyLabel;\n+\n+    // The value of the minimum version.\n+    private final long minValue;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDM0NDI1Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434344257", "bodyText": "Done. I have made it int16 now. Great point.", "author": "kowshik", "createdAt": "2020-06-03T06:50:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MjA2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MjkyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r432782926", "bodyText": "minValue > 1, maxValue > 1 => minValue >= 1, maxValue >= 1", "author": "junrao", "createdAt": "2020-05-29T23:37:35Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max, each of type long.\n+ * The min and max attributes need to satisfy 2 rules:\n+ *  - they are each expected to be >= 1, as we only consider positive version values to be valid.\n+ *  - max should be >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    // Non-empty label for the min version key, that's used only for serialization/deserialization purposes.\n+    private final String minKeyLabel;\n+\n+    // The value of the minimum version.\n+    private final long minValue;\n+\n+    // Non-empty label for the max version key, that's used only for serialization/deserialization purposes.\n+    private final String maxKeyLabel;\n+\n+    // The value of the maximum version.\n+    private final long maxValue;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minValue >= 1 and maxValue >= 1 and maxValue >= minValue.\n+     *\n+     * @param minKeyLabel   Label for the min version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param minValue      The minimum version value.\n+     * @param maxKeyLabel   Label for the max version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param maxValue      The maximum version value.\n+     *\n+     * @throws IllegalArgumentException   If any of the following conditions are true:\n+     *                                     - (minValue < 1) OR (maxValue < 1) OR (maxValue < minValue).\n+     *                                     - minKeyLabel is empty, OR, minKeyLabel is empty.\n+     */\n+    protected BaseVersionRange(String minKeyLabel, long minValue, String maxKeyLabel, long maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue > 1, maxValue > 1 and maxValue >= minValue, but received\" +", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDM0NTQwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434345400", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T06:53:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MjkyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MzE0NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r432783145", "bodyText": "Should we include the label too?", "author": "junrao", "createdAt": "2020-05-29T23:38:43Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max, each of type long.\n+ * The min and max attributes need to satisfy 2 rules:\n+ *  - they are each expected to be >= 1, as we only consider positive version values to be valid.\n+ *  - max should be >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    // Non-empty label for the min version key, that's used only for serialization/deserialization purposes.\n+    private final String minKeyLabel;\n+\n+    // The value of the minimum version.\n+    private final long minValue;\n+\n+    // Non-empty label for the max version key, that's used only for serialization/deserialization purposes.\n+    private final String maxKeyLabel;\n+\n+    // The value of the maximum version.\n+    private final long maxValue;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minValue >= 1 and maxValue >= 1 and maxValue >= minValue.\n+     *\n+     * @param minKeyLabel   Label for the min version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param minValue      The minimum version value.\n+     * @param maxKeyLabel   Label for the max version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param maxValue      The maximum version value.\n+     *\n+     * @throws IllegalArgumentException   If any of the following conditions are true:\n+     *                                     - (minValue < 1) OR (maxValue < 1) OR (maxValue < minValue).\n+     *                                     - minKeyLabel is empty, OR, minKeyLabel is empty.\n+     */\n+    protected BaseVersionRange(String minKeyLabel, long minValue, String maxKeyLabel, long maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue > 1, maxValue > 1 and maxValue >= minValue, but received\" +\n+                    \" minValue: %d, maxValue: %d\", minValue, maxValue));\n+        }\n+        if (minKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected minKeyLabel to be non-empty.\");\n+        }\n+        if (maxKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected maxKeyLabel to be non-empty.\");\n+        }\n+        this.minKeyLabel = minKeyLabel;\n+        this.minValue = minValue;\n+        this.maxKeyLabel = maxKeyLabel;\n+        this.maxValue = maxValue;\n+    }\n+\n+    public long min() {\n+        return minValue;\n+    }\n+\n+    public long max() {\n+        return maxValue;\n+    }\n+\n+    public String toString() {\n+        return String.format(\"%s[%d, %d]\", this.getClass().getSimpleName(), min(), max());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDM0NjI4MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434346281", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T06:55:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MzE0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MzQ3Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r432783472", "bodyText": "serialize typically means generating binary data. Perhaps this is better called toMap()?", "author": "junrao", "createdAt": "2020-05-29T23:40:25Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max, each of type long.\n+ * The min and max attributes need to satisfy 2 rules:\n+ *  - they are each expected to be >= 1, as we only consider positive version values to be valid.\n+ *  - max should be >= min.\n+ *\n+ * The class also provides API to serialize/deserialize the version range to/from a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    // Non-empty label for the min version key, that's used only for serialization/deserialization purposes.\n+    private final String minKeyLabel;\n+\n+    // The value of the minimum version.\n+    private final long minValue;\n+\n+    // Non-empty label for the max version key, that's used only for serialization/deserialization purposes.\n+    private final String maxKeyLabel;\n+\n+    // The value of the maximum version.\n+    private final long maxValue;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minValue >= 1 and maxValue >= 1 and maxValue >= minValue.\n+     *\n+     * @param minKeyLabel   Label for the min version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param minValue      The minimum version value.\n+     * @param maxKeyLabel   Label for the max version key, that's used only for\n+     *                      serialization/deserialization purposes.\n+     * @param maxValue      The maximum version value.\n+     *\n+     * @throws IllegalArgumentException   If any of the following conditions are true:\n+     *                                     - (minValue < 1) OR (maxValue < 1) OR (maxValue < minValue).\n+     *                                     - minKeyLabel is empty, OR, minKeyLabel is empty.\n+     */\n+    protected BaseVersionRange(String minKeyLabel, long minValue, String maxKeyLabel, long maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue > 1, maxValue > 1 and maxValue >= minValue, but received\" +\n+                    \" minValue: %d, maxValue: %d\", minValue, maxValue));\n+        }\n+        if (minKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected minKeyLabel to be non-empty.\");\n+        }\n+        if (maxKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected maxKeyLabel to be non-empty.\");\n+        }\n+        this.minKeyLabel = minKeyLabel;\n+        this.minValue = minValue;\n+        this.maxKeyLabel = maxKeyLabel;\n+        this.maxValue = maxValue;\n+    }\n+\n+    public long min() {\n+        return minValue;\n+    }\n+\n+    public long max() {\n+        return maxValue;\n+    }\n+\n+    public String toString() {\n+        return String.format(\"%s[%d, %d]\", this.getClass().getSimpleName(), min(), max());\n+    }\n+\n+    public Map<String, Long> serialize() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQxNzAyMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434417020", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:01:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjc4MzQ3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMyNzMyOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433327328", "bodyText": "missing license header", "author": "junrao", "createdAt": "2020-06-01T15:53:29Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,85 @@\n+package kafka.server", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNTE5Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434435193", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:31:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMyNzMyOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMzNzgyOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433337829", "bodyText": "empty => isEmpty ?", "author": "junrao", "createdAt": "2020-06-01T16:10:17Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,85 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A common mutable cache containing the latest finalized features and epoch. By default the contents of\n+ * the cache are empty. This cache needs to be populated at least once for it's contents to become\n+ * non-empty. Currently the main reader of this cache is the read path that serves an ApiVersionsRequest,\n+ * returning the features information in the response.\n+ *\n+ * @see FinalizedFeatureChangeListener\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means\n+   *           no FinalizedFeaturesAndEpoch exists in the cache at the time when this\n+   *           method is invoked. This result could change in the future whenever the\n+   *           updateOrThrow method is invoked.\n+   */\n+  def get: Option[FinalizedFeaturesAndEpoch] = {\n+    featuresAndEpoch\n+  }\n+\n+  def empty: Boolean = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNTgyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434435824", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:32:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzMzNzgyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM0NzQwMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433347401", "bodyText": "Could we use map {case(feature, versionLevel, _) => ...} to avoid unnamed references like _1?", "author": "junrao", "createdAt": "2020-06-01T16:27:29Z", "path": "core/src/main/scala/kafka/server/SupportedFeatures.scala", "diffHunk": "@@ -0,0 +1,76 @@\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * A common immutable object used in the Broker to define the latest features supported by the\n+ * Broker. Also provides API to check for incompatibilities between the latest features supported\n+ * by the Broker and cluster-wide finalized features.\n+ *\n+ * NOTE: the update() and clear() APIs of this class should be used only for testing purposes.\n+ */\n+object SupportedFeatures extends Logging {\n+\n+  /**\n+   * This is the latest features supported by the Broker.\n+   * This is currently empty, but in the future as we define supported features, this map should be\n+   * populated.\n+   */\n+  @volatile private var supportedFeatures = emptySupportedFeatures\n+\n+  /**\n+   * Returns a reference to the latest features supported by the Broker.\n+   */\n+  def get: Features[SupportedVersionRange] = {\n+    supportedFeatures\n+  }\n+\n+  // For testing only.\n+  def update(newFeatures: Features[SupportedVersionRange]): Unit = {\n+    supportedFeatures = newFeatures\n+  }\n+\n+  // For testing only.\n+  def clear(): Unit = {\n+    supportedFeatures = emptySupportedFeatures\n+  }\n+\n+  /**\n+   * Returns the set of feature names found to be 'incompatible'.\n+   * A feature incompatibility is a version mismatch between the latest feature supported by the\n+   * Broker, and the provided finalized feature. This can happen because a provided finalized\n+   * feature:\n+   *  1) Does not exist in the Broker (i.e. it is unknown to the Broker).\n+   *           [OR]\n+   *  2) Exists but the FinalizedVersionRange does not match with the supported feature's SupportedVersionRange.\n+   *\n+   * @param finalized   The finalized features against which incompatibilities need to be checked for.\n+   *\n+   * @return            The subset of input features which are incompatible. If the returned object\n+   *                    is empty, it means there were no feature incompatibilities found.\n+   */\n+  def incompatibleFeatures(finalized: Features[FinalizedVersionRange]): Features[FinalizedVersionRange] = {\n+    val incompatibilities = finalized.features.asScala.collect {\n+      case (feature, versionLevels) => {\n+        val supportedVersions = supportedFeatures.get(feature)\n+        if (supportedVersions == null) {\n+          (feature, versionLevels, \"{feature=%s, reason='Unsupported feature'}\".format(feature))\n+        } else if (versionLevels.isIncompatibleWith(supportedVersions)) {\n+          (feature, versionLevels, \"{feature=%s, reason='%s is incompatible with %s'}\".format(\n+            feature, versionLevels, supportedVersions))\n+        } else {\n+          (feature, versionLevels, null)\n+        }\n+      }\n+    }.filter{ case(_, _, errorReason) => errorReason != null}.toList\n+\n+    if (incompatibilities.nonEmpty) {\n+      warn(\"Feature incompatibilities seen: \" + incompatibilities.map{ case(_, _, errorReason) => errorReason })\n+    }\n+    Features.finalizedFeatures(incompatibilities.map(item => (item._1, item._2)).toMap.asJava)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0NTQyOA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434445428", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:48:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM0NzQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1OTgwNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433359805", "bodyText": "The KIP doesn't seems to include this field. Could we add it to the KIP wiki?", "author": "junrao", "createdAt": "2020-06-01T16:50:36Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +782,161 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+/**\n+ * Represents the status of the FeatureZNode.\n+ *\n+ * Enabled  -> This status means the feature versioning system (KIP-584) is enabled, and, the\n+ *             finalized features stored in the FeatureZNode are active. This status is written by\n+ *             the controller to the FeatureZNode only when the broker IBP config is greater than\n+ *             or equal to KAFKA_2_6_IV1.\n+ *\n+ * Disabled -> This status means the feature versioning system (KIP-584) is disabled, and, the\n+ *             the finalized features stored in the FeatureZNode is not relevant. This status is\n+ *             written by the controller to the FeatureZNode only when the broker IBP config\n+ *             is less than KAFKA_2_6_IV1.\n+ *\n+ * The purpose behind the FeatureZNodeStatus is that it helps differentiates between the following\n+ * cases:\n+ *\n+ * 1. New cluster bootstrap:\n+ *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+ *    with all the possible supported features finalized immediately. The new cluster will almost\n+ *    never be started with an old IBP config that\u2019s less than KAFKA_2_6_IV1. In such a case, the\n+ *    controller will start up and notice that the FeatureZNode is absent in the new cluster.\n+ *    To handle the requirement, the controller will create a FeatureZNode (with enabled status)\n+ *    containing the entire list of supported features as its finalized features.\n+ *\n+ * 2. Cluster upgrade:\n+ *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_6_IV1, but\n+ *    the Broker binary has been upgraded to a state where it supports the feature versioning\n+ *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+ *    binary. In this case, we want to start with no finalized features and allow the user to enable\n+ *    them whenever they are ready i.e. in the future whenever the user sets IBP config\n+ *    to be greater than or equal to KAFKA_2_6_IV1. The reason is that enabling all the possible\n+ *    features immediately after an upgrade could be harmful to the cluster.\n+ *    In such a case:\n+ *      - Before the Broker upgrade (i.e. IBP config set to less than KAFKA_2_6_IV1), the controller\n+ *        will start up and check if the FeatureZNode is absent. If true, then it will react by\n+ *        creating a FeatureZNode with disabled status and empty features.\n+ *      - After the Broker upgrade (i.e. IBP config set to greater than or equal to KAFKA_2_6_IV1),\n+ *        when the controller starts up it will check if the FeatureZNode exists and whether it is\n+ *        disabled. In such a case, it won\u2019t upgrade all features immediately. Instead it will just\n+ *        switch the FeatureZNode status to enabled status. This lets the user finalize the features\n+ *        later.\n+ *\n+ * 2. Cluster downgrade:\n+ *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+ *    KAFKA_2_6_IV1. Then, the user decided to downgrade the cluster by setting IBP config to a\n+ *    value less than KAFKA_2_6_IV1. This means the user is also disabling the feature versioning\n+ *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+ *    will switch the FeatureZNode status to disabled with empty features.\n+ */\n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+/**\n+ * Represents the contents of the ZK node containing finalized feature information.\n+ *\n+ * @param status     the status of the ZK node\n+ * @param features   the cluster-wide finalized features\n+ */\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // V0 contains 'version', 'status' and 'features' keys.\n+  val V0 = 0\n+  val CurrentVersion = V0\n+\n+  def path = \"/feature\"\n+\n+  def asJavaMap(scalaMap: Map[String, Map[String, Long]]): util.Map[String, util.Map[String, java.lang.Long]] = {\n+    scalaMap\n+      .view.mapValues(_.view.mapValues(scalaLong => java.lang.Long.valueOf(scalaLong)).toMap.asJava)\n+      .toMap\n+      .asJava\n+  }\n+\n+  /**\n+   * Encodes a FeatureZNode to JSON.\n+   *\n+   * @param featureZNode   FeatureZNode to be encoded\n+   *\n+   * @return               JSON representation of the FeatureZNode, as an Array[Byte]\n+   */\n+  def encode(featureZNode: FeatureZNode): Array[Byte] = {\n+    val jsonMap = collection.mutable.Map(\n+      VersionKey -> CurrentVersion,\n+      StatusKey -> featureZNode.status.id,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ1MDE5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434450195", "bodyText": "Sure. I will be happy to follow up on this. Trying to understand the process -- should I update the KIP and send an email as FYI to dev@kafka.apache.org ?", "author": "kowshik", "createdAt": "2020-06-03T09:56:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM1OTgwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM2Njg3MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433366871", "bodyText": "InterruptedException can be thrown if the thread is shut down explicitly. In this case, we probably don't want to throw RuntimeException to the caller.", "author": "junrao", "createdAt": "2020-06-01T17:03:47Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,219 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status, clearing feature cache.\")\n+          FinalizedFeatureCache.clear()\n+        } else if (featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             RuntimeException if the thread was interrupted during wait\n+     *\n+     *                     TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzODI4OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434438288", "bodyText": "Done. Removed the catch clause and exception wrapping.", "author": "kowshik", "createdAt": "2020-06-03T09:36:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM2Njg3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3MzMwOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433373309", "bodyText": "Hmm, could we just use config.zkConnectionTimeoutMs for this, instead of introducing a new config?", "author": "junrao", "createdAt": "2020-06-01T17:16:17Z", "path": "core/src/main/scala/kafka/server/KafkaConfig.scala", "diffHunk": "@@ -442,6 +445,8 @@ object KafkaConfig {\n   val ControlledShutdownMaxRetriesProp = \"controlled.shutdown.max.retries\"\n   val ControlledShutdownRetryBackoffMsProp = \"controlled.shutdown.retry.backoff.ms\"\n   val ControlledShutdownEnableProp = \"controlled.shutdown.enable\"\n+  /** ********* Features configuration ***********/\n+  val FeatureChangeListenerCacheUpdateWaitTimeMsProp = \"feature.listener.cache.update.wait.ms\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0NDg0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434444848", "bodyText": "Done. Great point!", "author": "kowshik", "createdAt": "2020-06-03T09:47:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3MzMwOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3NTQ1OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433375459", "bodyText": "Hmm, is waitOnceForCacheUpdateMs <=0 supported? In that case, it seems that we still need to read the /features path in ZK?", "author": "junrao", "createdAt": "2020-06-01T17:20:23Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,219 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status, clearing feature cache.\")\n+          FinalizedFeatureCache.clear()\n+        } else if (featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             RuntimeException if the thread was interrupted during wait\n+     *\n+     *                     TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Change notification queue interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: ZK corruption or operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * will exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    thread.start()\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)\n+\n+    if (waitOnceForCacheUpdateMs > 0) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0NDcxNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434444716", "bodyText": "Done. I have changed the code disallowing values <= 0.", "author": "kowshik", "createdAt": "2020-06-03T09:46:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM3NTQ1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM4MjI1Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433382257", "bodyText": "If this thread is being closed, the InterruptedException is expected and we don't need to log this.", "author": "junrao", "createdAt": "2020-06-01T17:32:58Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,219 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status, clearing feature cache.\")\n+          FinalizedFeatureCache.clear()\n+        } else if (featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             RuntimeException if the thread was interrupted during wait\n+     *\n+     *                     TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Change notification queue interrupted\", e)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0MTIwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434441200", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:41:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM4MjI1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM4NDQ3NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433384475", "bodyText": "Hmm, this just kills the thread, but not the broker as the comment says. Also, not sure about killing the broker. We probably should just log an error and continue since this is not necessarily fatal.", "author": "junrao", "createdAt": "2020-06-01T17:37:12Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,219 @@\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        if (featureZNode.status == FeatureZNodeStatus.Disabled) {\n+          info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status, clearing feature cache.\")\n+          FinalizedFeatureCache.clear()\n+        } else if (featureZNode.status == FeatureZNodeStatus.Enabled) {\n+          FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+        } else {\n+          throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             RuntimeException if the thread was interrupted during wait\n+     *\n+     *                     TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        var success = false\n+        try {\n+          success = notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)\n+        } catch {\n+          case e: InterruptedException =>\n+            throw new RuntimeException(\n+              \"Unable to wait for FinalizedFeatureCache update to finish.\", e)\n+        }\n+\n+        if (!success) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        queue.take.updateLatestOrThrow()\n+      } catch {\n+        case e: InterruptedException => info(s\"Change notification queue interrupted\", e)\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will exit.\", e)\n+          throw new FatalExitError(1)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0MzAwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434443007", "bodyText": "It kills the broker because ShutdownableThread catches FatalExitError and triggers exit sequence: \n  \n    \n      kafka/core/src/main/scala/kafka/utils/ShutdownableThread.scala\n    \n    \n        Lines 98 to 102\n      in\n      b8d609c\n    \n    \n    \n    \n\n        \n          \n           case e: FatalExitError => \n        \n\n        \n          \n             shutdownInitiated.countDown() \n        \n\n        \n          \n             shutdownComplete.countDown() \n        \n\n        \n          \n             info(\"Stopped\") \n        \n\n        \n          \n             Exit.exit(e.statusCode()) \n        \n    \n  \n\n\nI have updated the comment to use the word \"eventually\".\nRegarding logging fatal and continuing -- the exception caught here almost always indicates a feature incompatibility, and, that means the broker can cause damage if it sticks around. That is why I felt it is better to kill the broker in such a rare incompatibility case.\nPlease, let me know your thoughts.", "author": "kowshik", "createdAt": "2020-06-03T09:44:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM4NDQ3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM5NTcyNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433395727", "bodyText": "Hmm, the epoch returned from ZK is int32. Does FinalizedFeaturesEpoch need to be int64?", "author": "junrao", "createdAt": "2020-06-01T17:58:03Z", "path": "clients/src/main/resources/common/message/ApiVersionsResponse.json", "diffHunk": "@@ -42,6 +42,33 @@\n         \"about\": \"The maximum supported version, inclusive.\" }\n     ]},\n     { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"1+\", \"ignorable\": true,\n-      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" }\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\":  \"SupportedFeatures\", \"type\": \"[]SupportedFeatureKey\",\n+      \"versions\":  \"3+\", \"tag\": 0, \"taggedVersions\": \"3+\",\n+      \"about\": \"Features supported by the broker.\",\n+      \"fields\":  [\n+        { \"name\": \"Name\", \"type\": \"string\", \"versions\": \"3+\", \"mapKey\": true,\n+          \"about\": \"The name of the feature.\" },\n+        { \"name\": \"MinVersion\", \"type\": \"int64\", \"versions\": \"3+\",\n+          \"about\": \"The minimum supported version for the feature.\" },\n+        { \"name\": \"MaxVersion\", \"type\": \"int64\", \"versions\": \"3+\",\n+          \"about\": \"The maximum supported version for the feature.\" }\n+      ]\n+    },\n+    {\"name\": \"FinalizedFeaturesEpoch\", \"type\": \"int64\", \"versions\": \"3+\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzMTg4MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434431880", "bodyText": "Done. Changed to int32 now. Great point!", "author": "kowshik", "createdAt": "2020-06-03T09:26:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM5NTcyNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM5OTU3NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433399575", "bodyText": "Hmm, why is FinalizedFeaturesEpoch an optional but latestSupportedFeatures is not?", "author": "junrao", "createdAt": "2020-06-01T18:05:16Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +127,44 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.empty(), Optional.empty());\n+    }\n+\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Features<FinalizedVersionRange> finalizedFeatures,\n+        long finalizedFeaturesEpoch) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.of(finalizedFeatures), Optional.of(finalizedFeaturesEpoch));\n+    }\n+\n+    private static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Optional<Features<FinalizedVersionRange>> finalizedFeatures,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQyODE3Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434428173", "bodyText": "It's because non-existing supported features can be represented by an empty map (i.e. broker does not advertise any features). But on the other hand, non-existing finalized features can not be represented by empty map alone, as we need a suitable epoch value that indicates the absence of finalized features. To address this case, I saw 2 ways:\n\nProvide a negative epoch value indicating absence of finalized features, OR\nRepresent using an empty Optional for both finalized features and epoch.\n\nI chose the latter approach. Please, let me know if you have concerns.", "author": "kowshik", "createdAt": "2020-06-03T09:20:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzM5OTU3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMDAxMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433400012", "bodyText": "Should we add public methods for accessing those fields?", "author": "junrao", "createdAt": "2020-06-01T18:06:06Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +127,44 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.empty(), Optional.empty());\n+    }\n+\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Features<FinalizedVersionRange> finalizedFeatures,\n+        long finalizedFeaturesEpoch) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.of(finalizedFeatures), Optional.of(finalizedFeaturesEpoch));\n+    }\n+\n+    private static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Optional<Features<FinalizedVersionRange>> finalizedFeatures,\n+        Optional<Long> finalizedFeaturesEpoch) {\n         if (maxMagic == RecordBatch.CURRENT_MAGIC_VALUE && throttleTimeMs == DEFAULT_THROTTLE_TIME) {\n             return DEFAULT_API_VERSIONS_RESPONSE;\n         }\n-        return createApiVersionsResponse(throttleTimeMs, maxMagic);\n+        return createApiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, finalizedFeatures, finalizedFeaturesEpoch);\n     }\n \n-    public static ApiVersionsResponse createApiVersionsResponse(int throttleTimeMs, final byte minMagic) {\n+    public static ApiVersionsResponse createApiVersionsResponse(\n+        int throttleTimeMs,\n+        final byte minMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Optional<Features<FinalizedVersionRange>> finalizedFeatures,\n+        Optional<Long> finalizedFeaturesEpoch", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzMTMzMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434431330", "bodyText": "I had added such APIs previously. But @abbccdda wanted these removed, as they are not currently unused. Please refer to this comment: #8680 (comment).\nPlease, let me know, and I can add them back if you prefer.", "author": "kowshik", "createdAt": "2020-06-03T09:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMDAxMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMjc4NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433402784", "bodyText": "It doesn't seem we store security protocol map in the broker registration.", "author": "junrao", "createdAt": "2020-06-01T18:11:26Z", "path": "core/src/main/scala/kafka/cluster/Broker.scala", "diffHunk": "@@ -34,14 +36,22 @@ object Broker {\n                                          brokerId: Int,\n                                          endpoints: util.List[Endpoint],\n                                          interBrokerEndpoint: Endpoint) extends AuthorizerServerInfo\n+\n+  def apply(id: Int, endPoints: Seq[EndPoint], rack: Option[String]): Broker = {\n+    new Broker(id, endPoints, rack, emptySupportedFeatures)\n+  }\n }\n \n /**\n  * A Kafka broker.\n- * A broker has an id, a collection of end-points, an optional rack and a listener to security protocol map.\n- * Each end-point is (host, port, listenerName).\n+ *\n+ * @param id          a broker id\n+ * @param endPoints   a collection of: end-point and a listener to security protocol map.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNDM4MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434434381", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-03T09:30:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMjc4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMzI2MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433403260", "bodyText": "Should we include the new field in toString()?", "author": "junrao", "createdAt": "2020-06-01T18:12:25Z", "path": "core/src/main/scala/kafka/cluster/Broker.scala", "diffHunk": "@@ -54,7 +64,7 @@ case class Broker(id: Int, endPoints: Seq[EndPoint], rack: Option[String]) {\n     s\"$id : ${endPointsMap.values.mkString(\"(\",\",\",\")\")} : ${rack.orNull}\"\n \n   def this(id: Int, host: String, port: Int, listenerName: ListenerName, protocol: SecurityProtocol) = {\n-    this(id, Seq(EndPoint(host, port, listenerName, protocol)), None)\n+    this(id, Seq(EndPoint(host, port, listenerName, protocol)), None, emptySupportedFeatures)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQzNDgwNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434434804", "bodyText": "Done. Nice catch!", "author": "kowshik", "createdAt": "2020-06-03T09:31:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwMzI2MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwOTk0NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r433409944", "bodyText": "The existing comments seem incorrect since we don't store listener_security_protocol_map in ZK.", "author": "junrao", "createdAt": "2020-06-01T18:25:22Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -196,6 +219,19 @@ object BrokerIdZNode {\n     *   \"listener_security_protocol_map\":{\"CLIENT\":\"SSL\", \"REPLICATION\":\"PLAINTEXT\"},\n     *   \"rack\":\"dc1\"\n     * }\n+    *\n+    * Version 5 (current) JSON schema for a broker is:\n+    * {\n+    *   \"version\":5,\n+    *   \"host\":\"localhost\",\n+    *   \"port\":9092,\n+    *   \"jmx_port\":9999,\n+    *   \"timestamp\":\"2233345666\",\n+    *   \"endpoints\":[\"CLIENT://host1:9092\", \"REPLICATION://host1:9093\"],\n+    *   \"listener_security_protocol_map\":{\"CLIENT\":\"SSL\", \"REPLICATION\":\"PLAINTEXT\"},", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDQ0ODQ1NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r434448455", "bodyText": "Done. Removed. Great catch!", "author": "kowshik", "createdAt": "2020-06-03T09:53:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzQwOTk0NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU0Mzg2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435543865", "bodyText": "To handle ZK session expiration, we need to register a StateChangeHandler. That way, we can read the /features path from ZK when the new session is established since the feature could have changed btw the old and the new ZK sessions. See object ZkStateChangeHandler as an example.", "author": "junrao", "createdAt": "2020-06-04T20:56:15Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,231 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode,FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.ZNodeChangeHandler\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        featureZNode.status match {\n+          case FeatureZNodeStatus.Disabled => {\n+            info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n+            FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+          }\n+          case FeatureZNodeStatus.Enabled => {\n+            FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+          }\n+          case _ => throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (!notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        ignoring(classOf[InterruptedException]) {\n+          queue.take.updateLatestOrThrow()\n+        }\n+      } catch {\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will eventually exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: ZK corruption or operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * will exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    if (waitOnceForCacheUpdateMs <= 0) {\n+      throw new IllegalArgumentException(\n+        s\"Expected waitOnceForCacheUpdateMs > 0, but provided: $waitOnceForCacheUpdateMs\")\n+    }\n+\n+    thread.start()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5MzYzMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436493633", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:05:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU0Mzg2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU1ODY4Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435558683", "bodyText": "Could we pass in Optional<FinalizedFeaturesAndEpoch> instead of two separate Optional?", "author": "junrao", "createdAt": "2020-06-04T21:20:35Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +127,44 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.empty(), Optional.empty());\n+    }\n+\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Features<FinalizedVersionRange> finalizedFeatures,\n+        int finalizedFeaturesEpoch) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Optional.of(finalizedFeatures), Optional.of(finalizedFeaturesEpoch));\n+    }\n+\n+    private static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Optional<Features<FinalizedVersionRange>> finalizedFeatures,\n+        Optional<Integer> finalizedFeaturesEpoch) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU0NDQwOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436544409", "bodyText": "Done. I'm no longer passing 2 optionals, since, we decided (below) that epoch can be set as -1 whenever it is absent.", "author": "kowshik", "createdAt": "2020-06-08T08:49:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU1ODY4Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2MDk0MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435560941", "bodyText": "If finalizedFeaturesEpoch is not present, we probably want to set the field to sth like -1 instead of leaving it as the default value of 0.", "author": "junrao", "createdAt": "2020-06-04T21:23:29Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -135,7 +179,42 @@ public static ApiVersionsResponse createApiVersionsResponse(int throttleTimeMs,\n         data.setThrottleTimeMs(throttleTimeMs);\n         data.setErrorCode(Errors.NONE.code());\n         data.setApiKeys(apiKeys);\n+        data.setSupportedFeatures(createSupportedFeatureKeys(latestSupportedFeatures));\n+        if (finalizedFeatures.isPresent()) {\n+            data.setFinalizedFeatures(createFinalizedFeatureKeys(finalizedFeatures.get()));\n+        }\n+        if (finalizedFeaturesEpoch.isPresent()) {\n+            data.setFinalizedFeaturesEpoch(finalizedFeaturesEpoch.get());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjU0MzI1MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436543251", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T08:47:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2MDk0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2MjM0MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435562341", "bodyText": "The comment can be a bit misleading since features is not Optional.", "author": "junrao", "createdAt": "2020-06-04T21:26:15Z", "path": "core/src/main/scala/kafka/cluster/Broker.scala", "diffHunk": "@@ -34,14 +36,21 @@ object Broker {\n                                          brokerId: Int,\n                                          endpoints: util.List[Endpoint],\n                                          interBrokerEndpoint: Endpoint) extends AuthorizerServerInfo\n+\n+  def apply(id: Int, endPoints: Seq[EndPoint], rack: Option[String]): Broker = {\n+    new Broker(id, endPoints, rack, emptySupportedFeatures)\n+  }\n }\n \n /**\n  * A Kafka broker.\n- * A broker has an id, a collection of end-points, an optional rack and a listener to security protocol map.\n- * Each end-point is (host, port, listenerName).\n+ *\n+ * @param id          a broker id\n+ * @param endPoints   a collection of EndPoint. Each end-point is (host, port, listener name, security protocol).\n+ * @param rack        an optional rack\n+ * @param features    optional supported features", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5MzQyNg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436493426", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:04:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2MjM0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2Nzk0Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435567942", "bodyText": "It's probably better to close this before zkClient since the close call unregister from zkClient.", "author": "junrao", "createdAt": "2020-06-04T21:39:10Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -660,6 +674,10 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         if (zkClient != null)\n           CoreUtils.swallow(zkClient.close(), this)\n \n+        if (featureChangeListener != null) {\n+          CoreUtils.swallow(featureChangeListener.close(), this)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NDA0OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436494049", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:06:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU2Nzk0Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NTU3MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435595571", "bodyText": "The name of the method probably should include failure?", "author": "junrao", "createdAt": "2020-06-04T22:53:02Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Short>> emptyMap = new HashMap<>();\n+\n+        Features<FinalizedVersionRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertTrue(emptyFinalizedFeatures.features().isEmpty());\n+        assertTrue(emptyFinalizedFeatures.toMap().isEmpty());\n+        assertEquals(emptyFinalizedFeatures, Features.fromFinalizedFeaturesMap(emptyMap));\n+\n+        Features<SupportedVersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertTrue(emptySupportedFeatures.features().isEmpty());\n+        assertTrue(emptySupportedFeatures.toMap().isEmpty());\n+        assertEquals(emptySupportedFeatures, Features.fromSupportedFeaturesMap(emptyMap));\n+    }\n+\n+    @Test\n+    public void testNullFeatures() {\n+        assertThrows(\n+            NullPointerException.class,\n+            () -> Features.finalizedFeatures(null));\n+        assertThrows(\n+            NullPointerException.class,\n+            () -> Features.supportedFeatures(null));\n+    }\n+\n+    @Test\n+    public void testGetAllFeaturesAPI() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures =\n+            mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(allFeatures, features.features());\n+    }\n+\n+    @Test\n+    public void testGetAPI() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(v1, features.get(\"feature_1\"));\n+        assertEquals(v2, features.get(\"feature_2\"));\n+        assertNull(features.get(\"nonexistent_feature\"));\n+    }\n+\n+    @Test\n+    public void testFromFeaturesMapToFeaturesMap() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+\n+        Map<String, Map<String, Short>> expected = mkMap(\n+            mkEntry(\"feature_1\", mkMap(mkEntry(\"min_version\", (short) 1), mkEntry(\"max_version\", (short) 2))),\n+            mkEntry(\"feature_2\", mkMap(mkEntry(\"min_version\", (short) 3), mkEntry(\"max_version\", (short) 4))));\n+        assertEquals(expected, features.toMap());\n+        assertEquals(features, Features.fromSupportedFeaturesMap(expected));\n+    }\n+\n+    @Test\n+    public void testFromToFinalizedFeaturesMap() {\n+        FinalizedVersionRange v1 = new FinalizedVersionRange((short) 1, (short) 2);\n+        FinalizedVersionRange v2 = new FinalizedVersionRange((short) 3, (short) 4);\n+        Map<String, FinalizedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<FinalizedVersionRange> features = Features.finalizedFeatures(allFeatures);\n+\n+        Map<String, Map<String, Short>> expected = mkMap(\n+            mkEntry(\"feature_1\", mkMap(mkEntry(\"min_version_level\", (short) 1), mkEntry(\"max_version_level\", (short) 2))),\n+            mkEntry(\"feature_2\", mkMap(mkEntry(\"min_version_level\", (short) 3), mkEntry(\"max_version_level\", (short) 4))));\n+        assertEquals(expected, features.toMap());\n+        assertEquals(features, Features.fromFinalizedFeaturesMap(expected));\n+    }\n+\n+    @Test\n+    public void testToStringFinalizedFeatures() {\n+        FinalizedVersionRange v1 = new FinalizedVersionRange((short) 1, (short) 2);\n+        FinalizedVersionRange v2 = new FinalizedVersionRange((short) 3, (short) 4);\n+        Map<String, FinalizedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<FinalizedVersionRange> features = Features.finalizedFeatures(allFeatures);\n+\n+        assertEquals(\n+            \"Features{(feature_1 -> FinalizedVersionRange[min_version_level:1, max_version_level:2]), (feature_2 -> FinalizedVersionRange[min_version_level:3, max_version_level:4])}\",\n+            features.toString());\n+    }\n+\n+    @Test\n+    public void testToStringSupportedFeatures() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures\n+            = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+\n+        assertEquals(\n+            \"Features{(feature_1 -> SupportedVersionRange[min_version:1, max_version:2]), (feature_2 -> SupportedVersionRange[min_version:3, max_version:4])}\",\n+            features.toString());\n+    }\n+\n+    @Test\n+    public void testFromToSupportedFeaturesMap() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUzMjE0Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436532143", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T08:26:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NTU3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NTkxNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435595915", "bodyText": "missing license header", "author": "junrao", "createdAt": "2020-06-04T22:54:00Z", "path": "core/src/test/scala/kafka/zk/FeatureZNodeTest.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+package kafka.zk", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NTQwNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436495405", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:09:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NTkxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NjQwMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435596401", "bodyText": "Could we just assertEquals(featureZNode, decoded)?", "author": "junrao", "createdAt": "2020-06-04T22:55:36Z", "path": "core/src/test/scala/kafka/zk/FeatureZNodeTest.scala", "diffHunk": "@@ -0,0 +1,111 @@\n+package kafka.zk\n+\n+import java.nio.charset.StandardCharsets\n+\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+import org.apache.kafka.common.feature.Features._\n+import org.junit.Assert.{assertEquals, assertThrows}\n+import org.junit.Test\n+\n+import scala.jdk.CollectionConverters._\n+\n+class FeatureZNodeTest {\n+\n+  @Test\n+  def testEncodeDecode(): Unit = {\n+    val featureZNode = FeatureZNode(\n+      FeatureZNodeStatus.Enabled,\n+      Features.finalizedFeatures(\n+        Map[String, FinalizedVersionRange](\n+          \"feature1\" -> new FinalizedVersionRange(1, 2),\n+          \"feature2\" -> new FinalizedVersionRange(2, 4)).asJava))\n+    val decoded = FeatureZNode.decode(FeatureZNode.encode(featureZNode))\n+    assertEquals(featureZNode.status, decoded.status)\n+    assertEquals(featureZNode.features, decoded.features)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NTY2MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436495661", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:10:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5NjQwMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5Nzc0NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435597745", "bodyText": "version > CurrentVersion means that we can't downgrade the broker. We will need to relax this check.", "author": "junrao", "createdAt": "2020-06-04T22:59:39Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +781,165 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+/**\n+ * Represents the status of the FeatureZNode.\n+ *\n+ * Enabled  -> This status means the feature versioning system (KIP-584) is enabled, and, the\n+ *             finalized features stored in the FeatureZNode are active. This status is written by\n+ *             the controller to the FeatureZNode only when the broker IBP config is greater than\n+ *             or equal to KAFKA_2_6_IV1.\n+ *\n+ * Disabled -> This status means the feature versioning system (KIP-584) is disabled, and, the\n+ *             the finalized features stored in the FeatureZNode is not relevant. This status is\n+ *             written by the controller to the FeatureZNode only when the broker IBP config\n+ *             is less than KAFKA_2_6_IV1.\n+ *\n+ * The purpose behind the FeatureZNodeStatus is that it helps differentiates between the following\n+ * cases:\n+ *\n+ * 1. New cluster bootstrap:\n+ *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+ *    with all the possible supported features finalized immediately. The new cluster will almost\n+ *    never be started with an old IBP config that\u2019s less than KAFKA_2_6_IV1. In such a case, the\n+ *    controller will start up and notice that the FeatureZNode is absent in the new cluster.\n+ *    To handle the requirement, the controller will create a FeatureZNode (with enabled status)\n+ *    containing the entire list of supported features as its finalized features.\n+ *\n+ * 2. Cluster upgrade:\n+ *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_6_IV1, but\n+ *    the Broker binary has been upgraded to a state where it supports the feature versioning\n+ *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+ *    binary. In this case, we want to start with no finalized features and allow the user to enable\n+ *    them whenever they are ready i.e. in the future whenever the user sets IBP config\n+ *    to be greater than or equal to KAFKA_2_6_IV1. The reason is that enabling all the possible\n+ *    features immediately after an upgrade could be harmful to the cluster.\n+ *    In such a case:\n+ *      - Before the Broker upgrade (i.e. IBP config set to less than KAFKA_2_6_IV1), the controller\n+ *        will start up and check if the FeatureZNode is absent. If true, then it will react by\n+ *        creating a FeatureZNode with disabled status and empty features.\n+ *      - After the Broker upgrade (i.e. IBP config set to greater than or equal to KAFKA_2_6_IV1),\n+ *        when the controller starts up it will check if the FeatureZNode exists and whether it is\n+ *        disabled. In such a case, it won\u2019t upgrade all features immediately. Instead it will just\n+ *        switch the FeatureZNode status to enabled status. This lets the user finalize the features\n+ *        later.\n+ *\n+ * 2. Cluster downgrade:\n+ *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+ *    KAFKA_2_6_IV1. Then, the user decided to downgrade the cluster by setting IBP config to a\n+ *    value less than KAFKA_2_6_IV1. This means the user is also disabling the feature versioning\n+ *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+ *    will switch the FeatureZNode status to disabled with empty features.\n+ */\n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+/**\n+ * Represents the contents of the ZK node containing finalized feature information.\n+ *\n+ * @param status     the status of the ZK node\n+ * @param features   the cluster-wide finalized features\n+ */\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // V0 contains 'version', 'status' and 'features' keys.\n+  val V0 = 0\n+  val CurrentVersion = V0\n+\n+  def path = \"/feature\"\n+\n+  def asJavaMap(scalaMap: Map[String, Map[String, Short]]): util.Map[String, util.Map[String, java.lang.Short]] = {\n+    scalaMap\n+      .view.mapValues(_.view.mapValues(scalaShort => java.lang.Short.valueOf(scalaShort)).toMap.asJava)\n+      .toMap\n+      .asJava\n+  }\n+\n+  /**\n+   * Encodes a FeatureZNode to JSON.\n+   *\n+   * @param featureZNode   FeatureZNode to be encoded\n+   *\n+   * @return               JSON representation of the FeatureZNode, as an Array[Byte]\n+   */\n+  def encode(featureZNode: FeatureZNode): Array[Byte] = {\n+    val jsonMap = collection.mutable.Map(\n+      VersionKey -> CurrentVersion,\n+      StatusKey -> featureZNode.status.id,\n+      FeaturesKey -> featureZNode.features.toMap)\n+    Json.encodeAsBytes(jsonMap.asJava)\n+  }\n+\n+  /**\n+   * Decodes the contents of the feature ZK node from Array[Byte] to a FeatureZNode.\n+   *\n+   * @param jsonBytes   the contents of the feature ZK node\n+   *\n+   * @return            the FeatureZNode created from jsonBytes\n+   *\n+   * @throws IllegalArgumentException   if the Array[Byte] can not be decoded.\n+   */\n+  def decode(jsonBytes: Array[Byte]): FeatureZNode = {\n+    Json.tryParseBytes(jsonBytes) match {\n+      case Right(js) =>\n+        val featureInfo = js.asJsonObject\n+        val version = featureInfo(VersionKey).to[Int]\n+        if (version < V0 || version > CurrentVersion) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NTAwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436495007", "bodyText": "Done. Great point!", "author": "kowshik", "createdAt": "2020-06-08T07:08:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5Nzc0NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5ODk1Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435598956", "bodyText": "missing license header", "author": "junrao", "createdAt": "2020-06-04T23:03:22Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureCacheTest.scala", "diffHunk": "@@ -0,0 +1,99 @@\n+package kafka.server", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NjAzNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436496035", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:11:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTU5ODk1Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwMTEyNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435601124", "bodyText": "missing license header", "author": "junrao", "createdAt": "2020-06-04T23:10:31Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+package kafka.server", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjQ5NjI1NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436496254", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T07:11:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwMTEyNA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNDk2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435604964", "bodyText": "Hmm, if the feature is disabled, it seems that updatedFinalizedFeatures shouldn't be reflected in the cache, right?", "author": "junrao", "createdAt": "2020-06-04T23:23:22Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+package kafka.server\n+\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion, ZooKeeperTestHarness}\n+import kafka.utils.{Exit, TestUtils}\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.internals.FatalExitError\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertThrows, assertTrue}\n+import org.junit.{Before, Test}\n+\n+import scala.concurrent.TimeoutException\n+import scala.jdk.CollectionConverters._\n+\n+class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n+  @Before\n+  override def setUp(): Unit = {\n+    super.setUp()\n+    FinalizedFeatureCache.clear()\n+    SupportedFeatures.clear()\n+  }\n+\n+  private def createSupportedFeatures(): Features[SupportedVersionRange] = {\n+    val supportedFeaturesMap = Map[String, SupportedVersionRange](\n+      \"feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"feature_2\" -> new SupportedVersionRange(1, 3))\n+    SupportedFeatures.update(Features.supportedFeatures(supportedFeaturesMap.asJava))\n+    SupportedFeatures.get\n+  }\n+\n+  private def createFinalizedFeatures(): FinalizedFeaturesAndEpoch = {\n+    val finalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val finalizedFeatures = Features.finalizedFeatures(finalizedFeaturesMap.asJava)\n+    zkClient.createFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, finalizedFeatures))\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeBytes.isEmpty)\n+    FinalizedFeaturesAndEpoch(finalizedFeatures, version)\n+  }\n+\n+  private def createListener(expectedCacheContent: Option[FinalizedFeaturesAndEpoch]): FinalizedFeatureChangeListener = {\n+    val listener = new FinalizedFeatureChangeListener(zkClient)\n+    assertFalse(listener.isListenerInitiated)\n+    assertTrue(FinalizedFeatureCache.isEmpty)\n+    listener.initOrThrow(15000)\n+    assertTrue(listener.isListenerInitiated)\n+    if (expectedCacheContent.isDefined) {\n+      val mayBeNewCacheContent = FinalizedFeatureCache.get\n+      assertFalse(mayBeNewCacheContent.isEmpty)\n+      val newCacheContent = mayBeNewCacheContent.get\n+      assertEquals(expectedCacheContent.get.features, newCacheContent.features)\n+      assertEquals(expectedCacheContent.get.epoch, newCacheContent.epoch)\n+    } else {\n+      val mayBeNewCacheContent = FinalizedFeatureCache.get\n+      assertTrue(mayBeNewCacheContent.isEmpty)\n+    }\n+    listener\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can listen to ZK notifications\n+   * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   */\n+  @Test\n+  def testInitSuccessAndNotificationSuccess(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n+    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n+    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n+    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n+    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can process FeatureZNode deletion\n+   * successfully.\n+   */\n+  @Test\n+  def testFeatureZNodeDeleteNotificationProcessing(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    zkClient.deleteFeatureZNode()\n+    val (mayBeFeatureZNodeDeletedBytes, deletedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertEquals(deletedVersion, ZkVersion.UnknownVersion)\n+    assertTrue(mayBeFeatureZNodeDeletedBytes.isEmpty)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.isEmpty\n+    }, \"Timed out waiting for FinalizedFeatureCache to become empty\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can process disabling of a FeatureZNode\n+   * successfully.\n+   */\n+  @Test\n+  def testFeatureZNodeDisablingNotificationProcessing(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange]()\n+    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n+    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Disabled, updatedFinalizedFeatures))\n+    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.get.isDefined &&\n+        FinalizedFeatureCache.get.get.features.equals(updatedFinalizedFeatures) &&", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUxNTE5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436515190", "bodyText": "Done. I have modified the code such that FeatureCacheUpdater.updateLatestOrThrow will now clear the cache whenever it sees that the feature ZK node is disabled.\nGreat point!", "author": "kowshik", "createdAt": "2020-06-08T07:53:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNDk2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNjIyMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435606223", "bodyText": "Do we want to throw an Exception here?", "author": "junrao", "createdAt": "2020-06-04T23:27:47Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,185 @@\n+package kafka.server\n+\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, ZkVersion, ZooKeeperTestHarness}\n+import kafka.utils.{Exit, TestUtils}\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange, SupportedVersionRange}\n+import org.apache.kafka.common.internals.FatalExitError\n+import org.junit.Assert.{assertEquals, assertFalse, assertNotEquals, assertThrows, assertTrue}\n+import org.junit.{Before, Test}\n+\n+import scala.concurrent.TimeoutException\n+import scala.jdk.CollectionConverters._\n+\n+class FinalizedFeatureChangeListenerTest extends ZooKeeperTestHarness {\n+  @Before\n+  override def setUp(): Unit = {\n+    super.setUp()\n+    FinalizedFeatureCache.clear()\n+    SupportedFeatures.clear()\n+  }\n+\n+  private def createSupportedFeatures(): Features[SupportedVersionRange] = {\n+    val supportedFeaturesMap = Map[String, SupportedVersionRange](\n+      \"feature_1\" -> new SupportedVersionRange(1, 4),\n+      \"feature_2\" -> new SupportedVersionRange(1, 3))\n+    SupportedFeatures.update(Features.supportedFeatures(supportedFeaturesMap.asJava))\n+    SupportedFeatures.get\n+  }\n+\n+  private def createFinalizedFeatures(): FinalizedFeaturesAndEpoch = {\n+    val finalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 3))\n+    val finalizedFeatures = Features.finalizedFeatures(finalizedFeaturesMap.asJava)\n+    zkClient.createFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, finalizedFeatures))\n+    val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(version, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeBytes.isEmpty)\n+    FinalizedFeaturesAndEpoch(finalizedFeatures, version)\n+  }\n+\n+  private def createListener(expectedCacheContent: Option[FinalizedFeaturesAndEpoch]): FinalizedFeatureChangeListener = {\n+    val listener = new FinalizedFeatureChangeListener(zkClient)\n+    assertFalse(listener.isListenerInitiated)\n+    assertTrue(FinalizedFeatureCache.isEmpty)\n+    listener.initOrThrow(15000)\n+    assertTrue(listener.isListenerInitiated)\n+    if (expectedCacheContent.isDefined) {\n+      val mayBeNewCacheContent = FinalizedFeatureCache.get\n+      assertFalse(mayBeNewCacheContent.isEmpty)\n+      val newCacheContent = mayBeNewCacheContent.get\n+      assertEquals(expectedCacheContent.get.features, newCacheContent.features)\n+      assertEquals(expectedCacheContent.get.epoch, newCacheContent.epoch)\n+    } else {\n+      val mayBeNewCacheContent = FinalizedFeatureCache.get\n+      assertTrue(mayBeNewCacheContent.isEmpty)\n+    }\n+    listener\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can listen to ZK notifications\n+   * successfully from an \"Enabled\" FeatureZNode (the ZK data has no feature incompatibilities).\n+   */\n+  @Test\n+  def testInitSuccessAndNotificationSuccess(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 4))\n+    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n+    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, updatedFinalizedFeatures))\n+    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.get.get.equals(FinalizedFeaturesAndEpoch(updatedFinalizedFeatures, updatedVersion))\n+    }, \"Timed out waiting for FinalizedFeatureCache to be updated with new features\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can process FeatureZNode deletion\n+   * successfully.\n+   */\n+  @Test\n+  def testFeatureZNodeDeleteNotificationProcessing(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    zkClient.deleteFeatureZNode()\n+    val (mayBeFeatureZNodeDeletedBytes, deletedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertEquals(deletedVersion, ZkVersion.UnknownVersion)\n+    assertTrue(mayBeFeatureZNodeDeletedBytes.isEmpty)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.isEmpty\n+    }, \"Timed out waiting for FinalizedFeatureCache to become empty\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener can be initialized, and that it can process disabling of a FeatureZNode\n+   * successfully.\n+   */\n+  @Test\n+  def testFeatureZNodeDisablingNotificationProcessing(): Unit = {\n+    createSupportedFeatures()\n+    val initialFinalizedFeatures = createFinalizedFeatures()\n+    val listener = createListener(Some(initialFinalizedFeatures))\n+\n+    val updatedFinalizedFeaturesMap = Map[String, FinalizedVersionRange]()\n+    val updatedFinalizedFeatures = Features.finalizedFeatures(updatedFinalizedFeaturesMap.asJava)\n+    zkClient.updateFeatureZNode(FeatureZNode(FeatureZNodeStatus.Disabled, updatedFinalizedFeatures))\n+    val (mayBeFeatureZNodeNewBytes, updatedVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(updatedVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeNewBytes.isEmpty)\n+    assertTrue(updatedVersion > initialFinalizedFeatures.epoch)\n+    TestUtils.waitUntilTrue(() => {\n+      FinalizedFeatureCache.get.isDefined &&\n+        FinalizedFeatureCache.get.get.features.equals(updatedFinalizedFeatures) &&\n+        FinalizedFeatureCache.get.get.epoch == updatedVersion\n+    }, \"Timed out waiting for FinalizedFeatureCache to become empty\")\n+    assertTrue(listener.isListenerInitiated)\n+  }\n+\n+  /**\n+   * Tests that the listener initialization fails when it picks up a feature incompatibility from\n+   * ZK from an \"Enabled\" FeatureZNode.\n+   */\n+  @Test\n+  def testInitFailureDueToFeatureIncompatibility(): Unit = {\n+    createSupportedFeatures()\n+\n+    val incompatibleFinalizedFeaturesMap = Map[String, FinalizedVersionRange](\n+      \"feature_1\" -> new FinalizedVersionRange(2, 5))\n+    val incompatibleFinalizedFeatures = Features.finalizedFeatures(incompatibleFinalizedFeaturesMap.asJava)\n+    zkClient.createFeatureZNode(FeatureZNode(FeatureZNodeStatus.Enabled, incompatibleFinalizedFeatures))\n+    val (mayBeFeatureZNodeBytes, initialVersion) = zkClient.getDataAndVersion(FeatureZNode.path)\n+    assertNotEquals(initialVersion, ZkVersion.UnknownVersion)\n+    assertFalse(mayBeFeatureZNodeBytes.isEmpty)\n+\n+    Exit.setExitProcedure((status, _) => throw new FatalExitError(status))", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUyNDQxNA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436524414", "bodyText": "Done. Changed it to use a latch that gets notified when the exit procedure is called.\nGreat point!", "author": "kowshik", "createdAt": "2020-06-08T08:11:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNjIyMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNzM5MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r435607391", "bodyText": "missing license header", "author": "junrao", "createdAt": "2020-06-04T23:31:54Z", "path": "core/src/test/scala/unit/kafka/server/SupportedFeaturesTest.scala", "diffHunk": "@@ -0,0 +1,39 @@\n+package kafka.server", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjUyNzAwMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r436527003", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-08T08:16:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTYwNzM5MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0NTA4MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437045081", "bodyText": "This is not really \"change-notification\". So, the name can just be FeatureZNode.path.", "author": "junrao", "createdAt": "2020-06-08T22:54:43Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler}\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        featureZNode.status match {\n+          case FeatureZNodeStatus.Disabled => {\n+            info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n+            FinalizedFeatureCache.clear()\n+          }\n+          case FeatureZNodeStatus.Enabled => {\n+            FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+          }\n+          case _ => throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (!notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        ignoring(classOf[InterruptedException]) {\n+          queue.take.updateLatestOrThrow()\n+        }\n+      } catch {\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will eventually exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: ZK corruption or operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+  }\n+\n+  object ZkStateChangeHandler extends StateChangeHandler {\n+    val path: String = FeatureZNode.path\n+\n+    override val name: String = s\"change-notification-$path\"", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzEyMzgyMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437123822", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-09T03:56:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0NTA4MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0OTA3Mg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437049072", "bodyText": "2.6.x => 2.7.x", "author": "junrao", "createdAt": "2020-06-08T23:06:49Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -81,17 +83,26 @@ object BrokerIdsZNode {\n object BrokerInfo {\n \n   /**\n-   * Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n-   * the apiVersion is 0.10.0.X or above. Register the broker with v2 json format otherwise.\n+   * - Create a broker info with v5 json format if the apiVersion is 2.6.x or above.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzEyMzk2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437123965", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-09T03:57:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0OTA3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0OTIyNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437049227", "bodyText": "2.6.x => 2.7.x", "author": "junrao", "createdAt": "2020-06-08T23:07:19Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -81,17 +83,26 @@ object BrokerIdsZNode {\n object BrokerInfo {\n \n   /**\n-   * Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n-   * the apiVersion is 0.10.0.X or above. Register the broker with v2 json format otherwise.\n+   * - Create a broker info with v5 json format if the apiVersion is 2.6.x or above.\n+   * - Create a broker info with v4 json format (which includes multiple endpoints and rack) if\n+   *   the apiVersion is 0.10.0.X or above but lesser than 2.6.x.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzEyNDA2MQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437124061", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-09T03:57:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzA0OTIyNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MDkwMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437140900", "bodyText": "nit: we could use Utils.mkMap here", "author": "abbccdda", "createdAt": "2020-06-09T05:10:27Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/BaseVersionRange.java", "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Objects;\n+\n+/**\n+ * Represents an immutable basic version range using 2 attributes: min and max, each of type short.\n+ * The min and max attributes need to satisfy 2 rules:\n+ *  - they are each expected to be >= 1, as we only consider positive version values to be valid.\n+ *  - max should be >= min.\n+ *\n+ * The class also provides API to convert the version range to a map.\n+ * The class allows for configurable labels for the min/max attributes, which can be specialized by\n+ * sub-classes (if needed).\n+ */\n+class BaseVersionRange {\n+    // Non-empty label for the min version key, that's used only to convert to/from a map.\n+    private final String minKeyLabel;\n+\n+    // The value of the minimum version.\n+    private final short minValue;\n+\n+    // Non-empty label for the max version key, that's used only to convert to/from a map.\n+    private final String maxKeyLabel;\n+\n+    // The value of the maximum version.\n+    private final short maxValue;\n+\n+    /**\n+     * Raises an exception unless the following condition is met:\n+     * minValue >= 1 and maxValue >= 1 and maxValue >= minValue.\n+     *\n+     * @param minKeyLabel   Label for the min version key, that's used only to convert to/from a map.\n+     * @param minValue      The minimum version value.\n+     * @param maxKeyLabel   Label for the max version key, that's used only to convert to/from a map.\n+     * @param maxValue      The maximum version value.\n+     *\n+     * @throws IllegalArgumentException   If any of the following conditions are true:\n+     *                                     - (minValue < 1) OR (maxValue < 1) OR (maxValue < minValue).\n+     *                                     - minKeyLabel is empty, OR, minKeyLabel is empty.\n+     */\n+    protected BaseVersionRange(String minKeyLabel, short minValue, String maxKeyLabel, short maxValue) {\n+        if (minValue < 1 || maxValue < 1 || maxValue < minValue) {\n+            throw new IllegalArgumentException(\n+                String.format(\n+                    \"Expected minValue >= 1, maxValue >= 1 and maxValue >= minValue, but received\" +\n+                    \" minValue: %d, maxValue: %d\", minValue, maxValue));\n+        }\n+        if (minKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected minKeyLabel to be non-empty.\");\n+        }\n+        if (maxKeyLabel.isEmpty()) {\n+            throw new IllegalArgumentException(\"Expected maxKeyLabel to be non-empty.\");\n+        }\n+        this.minKeyLabel = minKeyLabel;\n+        this.minValue = minValue;\n+        this.maxKeyLabel = maxKeyLabel;\n+        this.maxValue = maxValue;\n+    }\n+\n+    public short min() {\n+        return minValue;\n+    }\n+\n+    public short max() {\n+        return maxValue;\n+    }\n+\n+    public String toString() {\n+        return String.format(\"%s[%s:%d, %s:%d]\", this.getClass().getSimpleName(), this.minKeyLabel, min(), this.maxKeyLabel, max());\n+    }\n+\n+    public Map<String, Short> toMap() {\n+        return new HashMap<String, Short>() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MTI4OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437851289", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:20:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MDkwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTY3OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437141678", "bodyText": "we don't need to check other == null here, the next condition check covers it.", "author": "abbccdda", "createdAt": "2020-06-09T05:13:44Z", "path": "clients/src/main/java/org/apache/kafka/common/feature/Features.java", "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.Objects;\n+\n+import static java.util.stream.Collectors.joining;\n+\n+/**\n+ * Represents an immutable dictionary with key being feature name, and value being <VersionRangeType>.\n+ * Also provides API to convert the features and their version ranges to/from a map.\n+ *\n+ * This class can be instantiated only using its factory functions, with the important ones being:\n+ * Features.supportedFeatures(...) and Features.finalizedFeatures(...).\n+ *\n+ * @param <VersionRangeType> is the type of version range.\n+ * @see SupportedVersionRange\n+ * @see FinalizedVersionRange\n+ */\n+public class Features<VersionRangeType extends BaseVersionRange> {\n+    private final Map<String, VersionRangeType> features;\n+\n+    /**\n+     * Constructor is made private, as for readability it is preferred the caller uses one of the\n+     * static factory functions for instantiation (see below).\n+     *\n+     * @param features   Map of feature name to a type of VersionRange.\n+     */\n+    private Features(Map<String, VersionRangeType> features) {\n+        Objects.requireNonNull(features,\"Provided features can not be null.\");\n+        this.features = features;\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to SupportedVersionRange.\n+     *\n+     * @return           Returns a new Features object representing supported features.\n+     */\n+    public static Features<SupportedVersionRange> supportedFeatures(Map<String, SupportedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    /**\n+     * @param features   Map of feature name to FinalizedVersionRange.\n+     *\n+     * @return           Returns a new Features object representing finalized features.\n+     */\n+    public static Features<FinalizedVersionRange> finalizedFeatures(Map<String, FinalizedVersionRange> features) {\n+        return new Features<>(features);\n+    }\n+\n+    // Visible for testing.\n+    public static Features<FinalizedVersionRange> emptyFinalizedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public static Features<SupportedVersionRange> emptySupportedFeatures() {\n+        return new Features<>(new HashMap<>());\n+    }\n+\n+    public Map<String, VersionRangeType> features() {\n+        return features;\n+    }\n+\n+    public boolean empty() {\n+        return features.isEmpty();\n+    }\n+\n+    /**\n+     * @param  feature   name of the feature\n+     *\n+     * @return           the VersionRangeType corresponding to the feature name, or null if the\n+     *                   feature is absent\n+     */\n+    public VersionRangeType get(String feature) {\n+        return features.get(feature);\n+    }\n+\n+    public String toString() {\n+        return String.format(\n+            \"Features{%s}\",\n+            features\n+                .entrySet()\n+                .stream()\n+                .map(entry -> String.format(\"(%s -> %s)\", entry.getKey(), entry.getValue()))\n+                .collect(joining(\", \"))\n+        );\n+    }\n+\n+    /**\n+     * @return   A map representation of the underlying features. The returned value can be converted\n+     *           back to Features using one of the from*FeaturesMap() APIs of this class.\n+     */\n+    public Map<String, Map<String, Short>> toMap() {\n+        return features.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> entry.getValue().toMap()));\n+    }\n+\n+    /**\n+     * An interface that defines behavior to convert from a Map to an object of type BaseVersionRange.\n+     */\n+    private interface MapToBaseVersionRangeConverter<V extends BaseVersionRange> {\n+\n+        /**\n+         * Convert the map representation of an object of type <V>, to an object of type <V>.\n+         *\n+         * @param  baseVersionRangeMap   the map representation of a BaseVersionRange object.\n+         *\n+         * @return                       the object of type <V>\n+         */\n+        V fromMap(Map<String, Short> baseVersionRangeMap);\n+    }\n+\n+    private static <V extends BaseVersionRange> Features<V> fromFeaturesMap(\n+        Map<String, Map<String, Short>> featuresMap, MapToBaseVersionRangeConverter<V> converter) {\n+        return new Features<>(featuresMap.entrySet().stream().collect(\n+            Collectors.toMap(\n+                Map.Entry::getKey,\n+                entry -> converter.fromMap(entry.getValue()))));\n+    }\n+\n+    /**\n+     * Converts from a map to Features<FinalizedVersionRange>.\n+     *\n+     * @param featuresMap  the map representation of a Features<FinalizedVersionRange> object,\n+     *                     generated using the toMap() API.\n+     *\n+     * @return             the Features<FinalizedVersionRange> object\n+     */\n+    public static Features<FinalizedVersionRange> fromFinalizedFeaturesMap(\n+        Map<String, Map<String, Short>> featuresMap) {\n+        return fromFeaturesMap(featuresMap, FinalizedVersionRange::fromMap);\n+    }\n+\n+    /**\n+     * Converts from a map to Features<SupportedVersionRange>.\n+     *\n+     * @param featuresMap  the map representation of a Features<SupportedVersionRange> object,\n+     *                     generated using the toMap() API.\n+     *\n+     * @return             the Features<SupportedVersionRange> object\n+     */\n+    public static Features<SupportedVersionRange> fromSupportedFeaturesMap(\n+        Map<String, Map<String, Short>> featuresMap) {\n+        return fromFeaturesMap(featuresMap, SupportedVersionRange::fromMap);\n+    }\n+\n+    @Override\n+    public boolean equals(Object other) {\n+        if (this == other) {\n+            return true;\n+        }\n+        if (other == null || !(other instanceof Features)) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1MTYyMg==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437851622", "bodyText": "Done. Good point.", "author": "kowshik", "createdAt": "2020-06-10T04:21:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MTY3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzMzMQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437143331", "bodyText": "nit: should all the parameters be final here, not just minMagic?", "author": "abbccdda", "createdAt": "2020-06-09T05:19:56Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +128,45 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Features.emptyFinalizedFeatures(), UNKNOWN_FINALIZED_FEATURES_EPOCH);\n+    }\n+\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Features<FinalizedVersionRange> finalizedFeatures,\n+        int finalizedFeaturesEpoch) {\n         if (maxMagic == RecordBatch.CURRENT_MAGIC_VALUE && throttleTimeMs == DEFAULT_THROTTLE_TIME) {\n             return DEFAULT_API_VERSIONS_RESPONSE;\n         }\n-        return createApiVersionsResponse(throttleTimeMs, maxMagic);\n+        return createApiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, finalizedFeatures, finalizedFeaturesEpoch);\n     }\n \n-    public static ApiVersionsResponse createApiVersionsResponse(int throttleTimeMs, final byte minMagic) {\n+    public static ApiVersionsResponse createApiVersionsResponseWithEmptyFeatures(\n+        int throttleTimeMs,\n+        final byte minMagic) {\n+        return createApiVersionsResponse(\n+            throttleTimeMs,\n+            minMagic,\n+            Features.emptySupportedFeatures(),\n+            Features.emptyFinalizedFeatures(),\n+            UNKNOWN_FINALIZED_FEATURES_EPOCH);\n+    }\n+\n+    public static ApiVersionsResponse createApiVersionsResponse(\n+        int throttleTimeMs,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NTc4OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437855789", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:39:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0MzMzMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NTI1MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437145250", "bodyText": "I overlooked this case, let's maintain this static constructor without renaming it, since it is public.", "author": "abbccdda", "createdAt": "2020-06-09T05:26:56Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ApiVersionsResponse.java", "diffHunk": "@@ -113,14 +128,45 @@ public static ApiVersionsResponse fromStruct(Struct struct, short version) {\n         }\n     }\n \n-    public static ApiVersionsResponse apiVersionsResponse(int throttleTimeMs, byte maxMagic) {\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures) {\n+        return apiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, Features.emptyFinalizedFeatures(), UNKNOWN_FINALIZED_FEATURES_EPOCH);\n+    }\n+\n+    public static ApiVersionsResponse apiVersionsResponse(\n+        int throttleTimeMs,\n+        byte maxMagic,\n+        Features<SupportedVersionRange> latestSupportedFeatures,\n+        Features<FinalizedVersionRange> finalizedFeatures,\n+        int finalizedFeaturesEpoch) {\n         if (maxMagic == RecordBatch.CURRENT_MAGIC_VALUE && throttleTimeMs == DEFAULT_THROTTLE_TIME) {\n             return DEFAULT_API_VERSIONS_RESPONSE;\n         }\n-        return createApiVersionsResponse(throttleTimeMs, maxMagic);\n+        return createApiVersionsResponse(\n+            throttleTimeMs, maxMagic, latestSupportedFeatures, finalizedFeatures, finalizedFeaturesEpoch);\n     }\n \n-    public static ApiVersionsResponse createApiVersionsResponse(int throttleTimeMs, final byte minMagic) {\n+    public static ApiVersionsResponse createApiVersionsResponseWithEmptyFeatures(", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NTY0Mw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437855643", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:38:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NTI1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NjU3Ng==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437146576", "bodyText": "nit: instead of using comments, better to build this into the test name, for example:\ntestInvalidSuppportedFeaturesWithMissingMaxVersion", "author": "abbccdda", "createdAt": "2020-06-09T05:31:25Z", "path": "clients/src/test/java/org/apache/kafka/common/feature/FeaturesTest.java", "diffHunk": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.feature;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.junit.Test;\n+\n+import static org.apache.kafka.common.utils.Utils.mkEntry;\n+import static org.apache.kafka.common.utils.Utils.mkMap;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n+\n+public class FeaturesTest {\n+\n+    @Test\n+    public void testEmptyFeatures() {\n+        Map<String, Map<String, Short>> emptyMap = new HashMap<>();\n+\n+        Features<FinalizedVersionRange> emptyFinalizedFeatures = Features.emptyFinalizedFeatures();\n+        assertTrue(emptyFinalizedFeatures.features().isEmpty());\n+        assertTrue(emptyFinalizedFeatures.toMap().isEmpty());\n+        assertEquals(emptyFinalizedFeatures, Features.fromFinalizedFeaturesMap(emptyMap));\n+\n+        Features<SupportedVersionRange> emptySupportedFeatures = Features.emptySupportedFeatures();\n+        assertTrue(emptySupportedFeatures.features().isEmpty());\n+        assertTrue(emptySupportedFeatures.toMap().isEmpty());\n+        assertEquals(emptySupportedFeatures, Features.fromSupportedFeaturesMap(emptyMap));\n+    }\n+\n+    @Test\n+    public void testNullFeatures() {\n+        assertThrows(\n+            NullPointerException.class,\n+            () -> Features.finalizedFeatures(null));\n+        assertThrows(\n+            NullPointerException.class,\n+            () -> Features.supportedFeatures(null));\n+    }\n+\n+    @Test\n+    public void testGetAllFeaturesAPI() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures =\n+            mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(allFeatures, features.features());\n+    }\n+\n+    @Test\n+    public void testGetAPI() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+        assertEquals(v1, features.get(\"feature_1\"));\n+        assertEquals(v2, features.get(\"feature_2\"));\n+        assertNull(features.get(\"nonexistent_feature\"));\n+    }\n+\n+    @Test\n+    public void testFromFeaturesMapToFeaturesMap() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+\n+        Map<String, Map<String, Short>> expected = mkMap(\n+            mkEntry(\"feature_1\", mkMap(mkEntry(\"min_version\", (short) 1), mkEntry(\"max_version\", (short) 2))),\n+            mkEntry(\"feature_2\", mkMap(mkEntry(\"min_version\", (short) 3), mkEntry(\"max_version\", (short) 4))));\n+        assertEquals(expected, features.toMap());\n+        assertEquals(features, Features.fromSupportedFeaturesMap(expected));\n+    }\n+\n+    @Test\n+    public void testFromToFinalizedFeaturesMap() {\n+        FinalizedVersionRange v1 = new FinalizedVersionRange((short) 1, (short) 2);\n+        FinalizedVersionRange v2 = new FinalizedVersionRange((short) 3, (short) 4);\n+        Map<String, FinalizedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<FinalizedVersionRange> features = Features.finalizedFeatures(allFeatures);\n+\n+        Map<String, Map<String, Short>> expected = mkMap(\n+            mkEntry(\"feature_1\", mkMap(mkEntry(\"min_version_level\", (short) 1), mkEntry(\"max_version_level\", (short) 2))),\n+            mkEntry(\"feature_2\", mkMap(mkEntry(\"min_version_level\", (short) 3), mkEntry(\"max_version_level\", (short) 4))));\n+        assertEquals(expected, features.toMap());\n+        assertEquals(features, Features.fromFinalizedFeaturesMap(expected));\n+    }\n+\n+    @Test\n+    public void testToStringFinalizedFeatures() {\n+        FinalizedVersionRange v1 = new FinalizedVersionRange((short) 1, (short) 2);\n+        FinalizedVersionRange v2 = new FinalizedVersionRange((short) 3, (short) 4);\n+        Map<String, FinalizedVersionRange> allFeatures = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<FinalizedVersionRange> features = Features.finalizedFeatures(allFeatures);\n+\n+        assertEquals(\n+            \"Features{(feature_1 -> FinalizedVersionRange[min_version_level:1, max_version_level:2]), (feature_2 -> FinalizedVersionRange[min_version_level:3, max_version_level:4])}\",\n+            features.toString());\n+    }\n+\n+    @Test\n+    public void testToStringSupportedFeatures() {\n+        SupportedVersionRange v1 = new SupportedVersionRange((short) 1, (short) 2);\n+        SupportedVersionRange v2 = new SupportedVersionRange((short) 3, (short) 4);\n+        Map<String, SupportedVersionRange> allFeatures\n+            = mkMap(mkEntry(\"feature_1\", v1), mkEntry(\"feature_2\", v2));\n+\n+        Features<SupportedVersionRange> features = Features.supportedFeatures(allFeatures);\n+\n+        assertEquals(\n+            \"Features{(feature_1 -> SupportedVersionRange[min_version:1, max_version:2]), (feature_2 -> SupportedVersionRange[min_version:3, max_version:4])}\",\n+            features.toString());\n+    }\n+\n+    @Test\n+    public void testSupportedFeaturesFromMapFailure() {\n+        // This is invalid because 'max_version' key is missing.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjE5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437856190", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:41:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE0NjU3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzM3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437153379", "bodyText": "nit: {} not necessary", "author": "abbccdda", "createdAt": "2020-06-09T05:53:27Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler}\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        featureZNode.status match {\n+          case FeatureZNodeStatus.Disabled => {\n+            info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n+            FinalizedFeatureCache.clear()\n+          }\n+          case FeatureZNodeStatus.Enabled => {\n+            FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)\n+          }\n+          case _ => throw new IllegalStateException(s\"Unexpected FeatureZNodeStatus found in $featureZNode\")\n+        }\n+      }\n+\n+      maybeNotifyOnce.foreach(notifier => notifier.countDown())\n+    }\n+\n+    /**\n+     * Waits until at least a single updateLatestOrThrow completes successfully. This method returns\n+     * immediately if an updateLatestOrThrow call had already completed successfully.\n+     *\n+     * @param waitTimeMs   the timeout for the wait operation\n+     *\n+     * @throws             TimeoutException if the wait can not be completed in waitTimeMs\n+     *                     milli seconds\n+     */\n+    def awaitUpdateOrThrow(waitTimeMs: Long): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (!notifier.await(waitTimeMs, TimeUnit.MILLISECONDS)) {\n+          throw new TimeoutException(\n+            s\"Timed out after waiting for ${waitTimeMs}ms for FeatureCache to be updated.\")\n+        }\n+      })\n+    }\n+  }\n+\n+  /**\n+   * A shutdownable thread to process feature node change notifications that are populated into the\n+   * queue. If any change notification can not be processed successfully (unless it is due to an\n+   * interrupt), the thread treats it as a fatal event and triggers Broker exit.\n+   *\n+   * @param name   name of the thread\n+   */\n+  private class ChangeNotificationProcessorThread(name: String) extends ShutdownableThread(name = name) {\n+    override def doWork(): Unit = {\n+      try {\n+        ignoring(classOf[InterruptedException]) {\n+          queue.take.updateLatestOrThrow()\n+        }\n+      } catch {\n+        case e: Exception => {\n+          error(\"Failed to process feature ZK node change event. The broker will eventually exit.\", e)\n+          throw new FatalExitError(1)\n+        }\n+      }\n+    }\n+  }\n+\n+  // Feature ZK node change handler.\n+  object FeatureZNodeChangeHandler extends ZNodeChangeHandler {\n+    override val path: String = FeatureZNode.path\n+\n+    override def handleCreation(): Unit = {\n+      info(s\"Feature ZK node created at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDataChange(): Unit = {\n+      info(s\"Feature ZK node updated at path: $path\")\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+\n+    override def handleDeletion(): Unit = {\n+      warn(s\"Feature ZK node deleted at path: $path\")\n+      // This event may happen, rarely (ex: ZK corruption or operational error).\n+      // In such a case, we prefer to just log a warning and treat the case as if the node is absent,\n+      // and populate the FinalizedFeatureCache with empty finalized features.\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+  }\n+\n+  object ZkStateChangeHandler extends StateChangeHandler {\n+    val path: String = FeatureZNode.path\n+\n+    override val name: String = path\n+\n+    override def afterInitializingSession(): Unit = {\n+      queue.add(new FeatureCacheUpdater(path))\n+    }\n+  }\n+\n+  private val queue = new LinkedBlockingQueue[FeatureCacheUpdater]\n+\n+  private val thread = new ChangeNotificationProcessorThread(\"feature-zk-node-event-process-thread\")\n+\n+  /**\n+   * This method initializes the feature ZK node change listener. Optionally, it also ensures to\n+   * update the FinalizedFeatureCache once with the latest contents of the feature ZK node\n+   * (if the node exists). This step helps ensure that feature incompatibilities (if any) in brokers\n+   * are conveniently detected before the initOrThrow() method returns to the caller. If feature\n+   * incompatibilities are detected, this method will throw an Exception to the caller, and the Broker\n+   * will exit eventually.\n+   *\n+   * @param waitOnceForCacheUpdateMs   # of milli seconds to wait for feature cache to be updated once.\n+   *                                   If this parameter <= 0, no wait operation happens.\n+   *\n+   * @throws Exception if feature incompatibility check could not be finished in a timely manner\n+   */\n+  def initOrThrow(waitOnceForCacheUpdateMs: Long): Unit = {\n+    if (waitOnceForCacheUpdateMs <= 0) {\n+      throw new IllegalArgumentException(\n+        s\"Expected waitOnceForCacheUpdateMs > 0, but provided: $waitOnceForCacheUpdateMs\")\n+    }\n+\n+    thread.start()\n+    zkClient.registerStateChangeHandler(ZkStateChangeHandler)\n+    zkClient.registerZNodeChangeHandlerAndCheckExistence(FeatureZNodeChangeHandler)\n+    val ensureCacheUpdateOnce = new FeatureCacheUpdater(\n+      FeatureZNodeChangeHandler.path, Some(new CountDownLatch(1)))\n+    queue.add(ensureCacheUpdateOnce)\n+    try {\n+      ensureCacheUpdateOnce.awaitUpdateOrThrow(waitOnceForCacheUpdateMs)\n+    } catch {\n+      case e: Exception => {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1ODEzMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437858130", "bodyText": "It's a 2-line block.", "author": "kowshik", "createdAt": "2020-06-10T04:49:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzE1MzM3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4MDU2NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437580565", "bodyText": "nit: use Introduced to align with previous comment?", "author": "abbccdda", "createdAt": "2020-06-09T16:57:16Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -98,7 +98,9 @@ object ApiVersion {\n     // No new APIs, equivalent to 2.4-IV1\n     KAFKA_2_5_IV0,\n     // Introduced StopReplicaRequest V3 containing the leader epoch for each partition (KIP-570)\n-    KAFKA_2_6_IV0\n+    KAFKA_2_6_IV0,\n+    // Introduce feature versioning support (KIP-584)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjI5OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437856298", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:41:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4MDU2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4MjAwNw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437582007", "bodyText": "it's -> its", "author": "abbccdda", "createdAt": "2020-06-09T16:59:26Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,102 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A common mutable cache containing the latest finalized features and epoch. By default the contents of\n+ * the cache are empty. This cache needs to be populated at least once for it's contents to become", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjM4NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437856385", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:41:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4MjAwNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4Mjk0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437582948", "bodyText": "could be simplified as the latest known FinalizedFeaturesAndEpoch or empty if not defined in the cache", "author": "abbccdda", "createdAt": "2020-06-09T17:00:59Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureCache.scala", "diffHunk": "@@ -0,0 +1,102 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import kafka.utils.Logging\n+import org.apache.kafka.common.feature.{Features, FinalizedVersionRange}\n+\n+// Raised whenever there was an error in updating the FinalizedFeatureCache with features.\n+class FeatureCacheUpdateException(message: String) extends RuntimeException(message) {\n+}\n+\n+// Helper class that represents finalized features along with an epoch value.\n+case class FinalizedFeaturesAndEpoch(features: Features[FinalizedVersionRange], epoch: Int) {\n+  override def toString(): String = {\n+    \"FinalizedFeaturesAndEpoch(features=%s, epoch=%s)\".format(features, epoch)\n+  }\n+}\n+\n+/**\n+ * A common mutable cache containing the latest finalized features and epoch. By default the contents of\n+ * the cache are empty. This cache needs to be populated at least once for it's contents to become\n+ * non-empty. Currently the main reader of this cache is the read path that serves an ApiVersionsRequest,\n+ * returning the features information in the response.\n+ *\n+ * @see FinalizedFeatureChangeListener\n+ */\n+object FinalizedFeatureCache extends Logging {\n+  @volatile private var featuresAndEpoch: Option[FinalizedFeaturesAndEpoch] = Option.empty\n+\n+  /**\n+   * @return   the latest known FinalizedFeaturesAndEpoch. If the returned value is empty, it means", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1NjUxMA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437856510", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:42:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4Mjk0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4NDc1OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437584758", "bodyText": "I think we could remove If the cache update is not successful, then, a suitable exception is raised... which is pretty obvious.", "author": "abbccdda", "createdAt": "2020-06-09T17:04:11Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler}\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg1Nzc5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437857795", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T04:47:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4NDc1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4NjIzNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437586235", "bodyText": "Sorry it's been a while since my last review, but have we discussed the recovery path when we hit a data corruption exception for the cluster? Is there a way to turn off the feature versioning completely to unblock, or we have a mechanism to wipe out ZK data?", "author": "abbccdda", "createdAt": "2020-06-09T17:06:41Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler}\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg4MzM3NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437883374", "bodyText": "It's very rare especially when controller is the only entity writing to the ZK node. I have now modified the code to handle this case and clear the cache. Perhaps that's better than crashing the broker in such a case. Remediation will need human intervention in fixing the ZK node. We can provide tooling if required.", "author": "kowshik", "createdAt": "2020-06-10T06:17:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU4NjIzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU5MjA1MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437592050", "bodyText": "Being a bit paranoid here, would it be possible to have out-of-order updates from ZK, such that the version number is not monotonically increasing? I'm thinking even we could throw in FinalizedFeatureCache, do we really want to kill the broker, or we should just log a warning and proceed.", "author": "abbccdda", "createdAt": "2020-06-09T17:16:33Z", "path": "core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala", "diffHunk": "@@ -0,0 +1,243 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server\n+\n+import java.util.concurrent.{CountDownLatch, LinkedBlockingQueue, TimeUnit}\n+\n+import kafka.utils.{Logging, ShutdownableThread}\n+import kafka.zk.{FeatureZNode, FeatureZNodeStatus, KafkaZkClient, ZkVersion}\n+import kafka.zookeeper.{StateChangeHandler, ZNodeChangeHandler}\n+import org.apache.kafka.common.internals.FatalExitError\n+\n+import scala.concurrent.TimeoutException\n+import scala.util.control.Exception.ignoring\n+\n+/**\n+ * Listens to changes in the ZK feature node, via the ZK client. Whenever a change notification\n+ * is received from ZK, the feature cache in FinalizedFeatureCache is asynchronously updated\n+ * to the latest features read from ZK. The cache updates are serialized through a single\n+ * notification processor thread.\n+ *\n+ * @param zkClient     the Zookeeper client\n+ */\n+class FinalizedFeatureChangeListener(zkClient: KafkaZkClient) extends Logging {\n+\n+  /**\n+   * Helper class used to update the FinalizedFeatureCache.\n+   *\n+   * @param featureZkNodePath   the path to the ZK feature node to be read\n+   * @param maybeNotifyOnce     an optional latch that can be used to notify the caller when an\n+   *                            updateOrThrow() operation is over\n+   */\n+  private class FeatureCacheUpdater(featureZkNodePath: String, maybeNotifyOnce: Option[CountDownLatch]) {\n+\n+    def this(featureZkNodePath: String) = this(featureZkNodePath, Option.empty)\n+\n+    /**\n+     * Updates the feature cache in FinalizedFeatureCache with the latest features read from the\n+     * ZK node in featureZkNodePath. If the cache update is not successful, then, a suitable\n+     * exception is raised.\n+     *\n+     * NOTE: if a notifier was provided in the constructor, then, this method can be invoked exactly\n+     * once successfully. A subsequent invocation will raise an exception.\n+     *\n+     * @throws   IllegalStateException, if a non-empty notifier was provided in the constructor, and\n+     *           this method is called again after a successful previous invocation.\n+     * @throws   FeatureCacheUpdateException, if there was an error in updating the\n+     *           FinalizedFeatureCache.\n+     * @throws   RuntimeException, if there was a failure in reading/deserializing the\n+     *           contents of the feature ZK node.\n+     */\n+    def updateLatestOrThrow(): Unit = {\n+      maybeNotifyOnce.foreach(notifier => {\n+        if (notifier.getCount != 1) {\n+          throw new IllegalStateException(\n+            \"Can not notify after updateLatestOrThrow was called more than once successfully.\")\n+        }\n+      })\n+\n+      debug(s\"Reading feature ZK node at path: $featureZkNodePath\")\n+      val (mayBeFeatureZNodeBytes, version) = zkClient.getDataAndVersion(featureZkNodePath)\n+\n+      // There are 4 cases:\n+      //\n+      // (empty dataBytes, valid version)       => The empty dataBytes will fail FeatureZNode deserialization.\n+      //                                           FeatureZNode, when present in ZK, can not have empty contents.\n+      // (non-empty dataBytes, valid version)   => This is a valid case, and should pass FeatureZNode deserialization\n+      //                                           if dataBytes contains valid data.\n+      // (empty dataBytes, unknown version)     => This is a valid case, and this can happen if the FeatureZNode\n+      //                                           does not exist in ZK.\n+      // (non-empty dataBytes, unknown version) => This case is impossible, since, KafkaZkClient.getDataAndVersion\n+      //                                           API ensures that unknown version is returned only when the\n+      //                                           ZK node is absent. Therefore dataBytes should be empty in such\n+      //                                           a case.\n+      if (version == ZkVersion.UnknownVersion) {\n+        info(s\"Feature ZK node at path: $featureZkNodePath does not exist\")\n+        FinalizedFeatureCache.clear()\n+      } else {\n+        val featureZNode = FeatureZNode.decode(mayBeFeatureZNodeBytes.get)\n+        featureZNode.status match {\n+          case FeatureZNodeStatus.Disabled => {\n+            info(s\"Feature ZK node at path: $featureZkNodePath is in disabled status.\")\n+            FinalizedFeatureCache.clear()\n+          }\n+          case FeatureZNodeStatus.Enabled => {\n+            FinalizedFeatureCache.updateOrThrow(featureZNode.features, version)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NDM5NQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437874395", "bodyText": "Re: out-of-order updates from ZK:\nI don't understand. When a watch fires from ZK, we react by issuing a ZK read operation to obtain the latest value of the ZK node (see L75). It is impossible that we get a stale read from ZK after watch fires on the client side.\n\n\nRe: broker death:\nThe exception thrown here almost always indicates a feature incompatibility, and, that means the broker can cause damage if it sticks around (because feature bumps are breaking changes and you can not allow an incompatible broker to stick around in the cluster). That is why I felt it is better to kill the broker in such a rare incompatibility case. Note that after the controller has finalized features, there should be no brokers in the cluster with incompatibilites, so death here makes sense.\n\n\nNote: I have also explained point #2 in this comment: https://github.com/apache/kafka/pull/8680/files#r434443007.", "author": "kowshik", "createdAt": "2020-06-10T05:50:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzU5MjA1MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNDAxNQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437604015", "bodyText": "nit: space", "author": "abbccdda", "createdAt": "2020-06-09T17:36:59Z", "path": "core/src/main/scala/kafka/zk/ZkData.scala", "diffHunk": "@@ -744,6 +781,165 @@ object DelegationTokenInfoZNode {\n   def decode(bytes: Array[Byte]): Option[TokenInformation] = DelegationTokenManager.fromBytes(bytes)\n }\n \n+/**\n+ * Represents the status of the FeatureZNode.\n+ *\n+ * Enabled  -> This status means the feature versioning system (KIP-584) is enabled, and, the\n+ *             finalized features stored in the FeatureZNode are active. This status is written by\n+ *             the controller to the FeatureZNode only when the broker IBP config is greater than\n+ *             or equal to KAFKA_2_7_IV0.\n+ *\n+ * Disabled -> This status means the feature versioning system (KIP-584) is disabled, and, the\n+ *             the finalized features stored in the FeatureZNode is not relevant. This status is\n+ *             written by the controller to the FeatureZNode only when the broker IBP config\n+ *             is less than KAFKA_2_7_IV0.\n+ *\n+ * The purpose behind the FeatureZNodeStatus is that it helps differentiates between the following\n+ * cases:\n+ *\n+ * 1. New cluster bootstrap:\n+ *    For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster\n+ *    with all the possible supported features finalized immediately. The new cluster will almost\n+ *    never be started with an old IBP config that\u2019s less than KAFKA_2_7_IV0. In such a case, the\n+ *    controller will start up and notice that the FeatureZNode is absent in the new cluster.\n+ *    To handle the requirement, the controller will create a FeatureZNode (with enabled status)\n+ *    containing the entire list of supported features as its finalized features.\n+ *\n+ * 2. Cluster upgrade:\n+ *    Imagine there is an existing Kafka cluster with IBP config less than KAFKA_2_7_IV0, but\n+ *    the Broker binary has been upgraded to a state where it supports the feature versioning\n+ *    system (KIP-584). This means the user is upgrading from an earlier version of the Broker\n+ *    binary. In this case, we want to start with no finalized features and allow the user to enable\n+ *    them whenever they are ready i.e. in the future whenever the user sets IBP config\n+ *    to be greater than or equal to KAFKA_2_7_IV0. The reason is that enabling all the possible\n+ *    features immediately after an upgrade could be harmful to the cluster.\n+ *    In such a case:\n+ *      - Before the Broker upgrade (i.e. IBP config set to less than KAFKA_2_7_IV0), the controller\n+ *        will start up and check if the FeatureZNode is absent. If true, then it will react by\n+ *        creating a FeatureZNode with disabled status and empty features.\n+ *      - After the Broker upgrade (i.e. IBP config set to greater than or equal to KAFKA_2_7_IV0),\n+ *        when the controller starts up it will check if the FeatureZNode exists and whether it is\n+ *        disabled. In such a case, it won\u2019t upgrade all features immediately. Instead it will just\n+ *        switch the FeatureZNode status to enabled status. This lets the user finalize the features\n+ *        later.\n+ *\n+ * 3. Cluster downgrade:\n+ *    Imagine that a Kafka cluster exists already and the IBP config is greater than or equal to\n+ *    KAFKA_2_7_IV0. Then, the user decided to downgrade the cluster by setting IBP config to a\n+ *    value less than KAFKA_2_7_IV0. This means the user is also disabling the feature versioning\n+ *    system (KIP-584). In this case, when the controller starts up with the lower IBP config, it\n+ *    will switch the FeatureZNode status to disabled with empty features.\n+ */\n+object FeatureZNodeStatus extends Enumeration {\n+  val Disabled, Enabled = Value\n+\n+  def withNameOpt(value: Int): Option[Value] = {\n+    values.find(_.id == value)\n+  }\n+}\n+\n+/**\n+ * Represents the contents of the ZK node containing finalized feature information.\n+ *\n+ * @param status     the status of the ZK node\n+ * @param features   the cluster-wide finalized features\n+ */\n+case class FeatureZNode(status: FeatureZNodeStatus.Value, features: Features[FinalizedVersionRange]) {\n+}\n+\n+object FeatureZNode {\n+  private val VersionKey = \"version\"\n+  private val StatusKey = \"status\"\n+  private val FeaturesKey = \"features\"\n+\n+  // V1 contains 'version', 'status' and 'features' keys.\n+  val V1 = 1\n+  val CurrentVersion = V1\n+\n+  def path = \"/feature\"\n+\n+  def asJavaMap(scalaMap: Map[String, Map[String, Short]]): util.Map[String, util.Map[String, java.lang.Short]] = {\n+    scalaMap\n+      .view.mapValues(_.view.mapValues(scalaShort => java.lang.Short.valueOf(scalaShort)).toMap.asJava)\n+      .toMap\n+      .asJava\n+  }\n+\n+  /**\n+   * Encodes a FeatureZNode to JSON.\n+   *\n+   * @param featureZNode   FeatureZNode to be encoded\n+   *\n+   * @return               JSON representation of the FeatureZNode, as an Array[Byte]\n+   */\n+  def encode(featureZNode: FeatureZNode): Array[Byte] = {\n+    val jsonMap = collection.mutable.Map(\n+      VersionKey -> CurrentVersion,\n+      StatusKey -> featureZNode.status.id,\n+      FeaturesKey -> featureZNode.features.toMap)\n+    Json.encodeAsBytes(jsonMap.asJava)\n+  }\n+\n+  /**\n+   * Decodes the contents of the feature ZK node from Array[Byte] to a FeatureZNode.\n+   *\n+   * @param jsonBytes   the contents of the feature ZK node\n+   *\n+   * @return            the FeatureZNode created from jsonBytes\n+   *\n+   * @throws IllegalArgumentException   if the Array[Byte] can not be decoded.\n+   */\n+  def decode(jsonBytes: Array[Byte]): FeatureZNode = {\n+    Json.tryParseBytes(jsonBytes) match {\n+      case Right(js) =>\n+        val featureInfo = js.asJsonObject\n+        val version = featureInfo(VersionKey).to[Int]\n+        if (version < V1) {\n+          throw new IllegalArgumentException(s\"Unsupported version: $version of feature information: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+\n+        val featuresMap = featureInfo\n+          .get(FeaturesKey)\n+          .flatMap(_.to[Option[Map[String, Map[String, Int]]]])\n+\n+        if (featuresMap.isEmpty) {\n+          throw new IllegalArgumentException(\"Features map can not be absent in: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+        val features = asJavaMap(\n+          featuresMap\n+            .map(theMap => theMap.view.mapValues(_.view.mapValues(_.asInstanceOf[Short]).toMap).toMap)\n+            .getOrElse(Map[String, Map[String, Short]]()))\n+\n+        val statusInt = featureInfo\n+          .get(StatusKey)\n+          .flatMap(_.to[Option[Int]])\n+        if (statusInt.isEmpty) {\n+          throw new IllegalArgumentException(\"Status can not be absent in feature information: \" +\n+            s\"${new String(jsonBytes, UTF_8)}\")\n+        }\n+        val status = FeatureZNodeStatus.withNameOpt(statusInt.get)\n+        if (status.isEmpty) {\n+          throw new IllegalArgumentException(\n+            s\"Malformed status: $statusInt  found in feature information: ${new String(jsonBytes, UTF_8)}\")", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NTg2NA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437875864", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T05:54:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNDAxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNTQxMw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437605413", "bodyText": "V4 doesn't have feature right? What's the purpose of this test?", "author": "abbccdda", "createdAt": "2020-06-09T17:39:17Z", "path": "core/src/test/scala/unit/kafka/cluster/BrokerEndPointTest.scala", "diffHunk": "@@ -149,6 +153,53 @@ class BrokerEndPointTest {\n     assertEquals(None, broker.rack)\n   }\n \n+  @Test\n+  def testFromJsonV4WithNoFeatures(): Unit = {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NjQxOQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437876419", "bodyText": "It checks backwards compatibility i.e. it checks whether the deserialization code (V5-based) can correctly deserialize V4 such that features are assigned empty value by default..", "author": "kowshik", "createdAt": "2020-06-10T05:56:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNTQxMw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNjc0OA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437606748", "bodyText": "add the space back", "author": "abbccdda", "createdAt": "2020-06-09T17:41:34Z", "path": "core/src/test/scala/unit/kafka/server/KafkaConfigTest.scala", "diffHunk": "@@ -755,7 +755,7 @@ class KafkaConfigTest {\n         case KafkaConfig.SaslLoginRefreshMinPeriodSecondsProp =>\n         case KafkaConfig.SaslLoginRefreshBufferSecondsProp =>\n \n-        // Security config\n+        //Security config", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NzM5MA==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437877390", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T05:59:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNjc0OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNzg2Nw==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437607867", "bodyText": "nit: we could add a minor test to verify a negative waitOnceForCacheUpdateMs will throw", "author": "abbccdda", "createdAt": "2020-06-09T17:43:22Z", "path": "core/src/test/scala/unit/kafka/server/FinalizedFeatureChangeListenerTest.scala", "diffHunk": "@@ -0,0 +1,215 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package kafka.server", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzg3NzE3OQ==", "url": "https://github.com/apache/kafka/pull/8680#discussion_r437877179", "bodyText": "Done.", "author": "kowshik", "createdAt": "2020-06-10T05:58:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNzYwNzg2Nw=="}], "type": "inlineReview"}, {"oid": "033c12710b421434c812dd7a4f2c460bbde1e3b6", "url": "https://github.com/apache/kafka/commit/033c12710b421434c812dd7a4f2c460bbde1e3b6", "message": "Implement read path for feature versioning system", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "8718b7ef21ec294d92b27e422ec38ab00212c6b7", "url": "https://github.com/apache/kafka/commit/8718b7ef21ec294d92b27e422ec38ab00212c6b7", "message": "Address comments from Boyang", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "ad2255ff5561bc2099058144e97aa2501a02988e", "url": "https://github.com/apache/kafka/commit/ad2255ff5561bc2099058144e97aa2501a02988e", "message": "Fix doc", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "3faf5081dfb9c3a121e943b9e3268fed30e10499", "url": "https://github.com/apache/kafka/commit/3faf5081dfb9c3a121e943b9e3268fed30e10499", "message": "Add doc", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "321cc2db88b5250d3919482c9e990e148c9a6045", "url": "https://github.com/apache/kafka/commit/321cc2db88b5250d3919482c9e990e148c9a6045", "message": "Fix doc", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "a4fab822f722b845b0ec40fa0237c737521e54a2", "url": "https://github.com/apache/kafka/commit/a4fab822f722b845b0ec40fa0237c737521e54a2", "message": "Address comments from Boyang", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "b166d213dc05c80325557b5a0b9694277b93d6f9", "url": "https://github.com/apache/kafka/commit/b166d213dc05c80325557b5a0b9694277b93d6f9", "message": "Better logging", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "ee2f29aaf07c45fb39becee8bbf5bf48e4675a68", "url": "https://github.com/apache/kafka/commit/ee2f29aaf07c45fb39becee8bbf5bf48e4675a68", "message": "Better logging", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "17aab22ada1ff636d2fefec4b99eccd3b51ec147", "url": "https://github.com/apache/kafka/commit/17aab22ada1ff636d2fefec4b99eccd3b51ec147", "message": "Address comments from Boyang", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "8184ea30ce0d75f6d5f23b04fd764ee9c9dff9ba", "url": "https://github.com/apache/kafka/commit/8184ea30ce0d75f6d5f23b04fd764ee9c9dff9ba", "message": "Cosmetic change", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "a1958946f994600b890618f0147e5fa7f526d8f6", "url": "https://github.com/apache/kafka/commit/a1958946f994600b890618f0147e5fa7f526d8f6", "message": "Cosmetic change", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "9a4c3f2808f29bd3511058bf620385f0a74a6841", "url": "https://github.com/apache/kafka/commit/9a4c3f2808f29bd3511058bf620385f0a74a6841", "message": "Cosmetics", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "bc3b2dd36bf68556c517303a063a5d5d9268cad9", "url": "https://github.com/apache/kafka/commit/bc3b2dd36bf68556c517303a063a5d5d9268cad9", "message": "Address comments from Jun Rao", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "6fc73ca8544308eb4c8e433f600112aa7001d85f", "url": "https://github.com/apache/kafka/commit/6fc73ca8544308eb4c8e433f600112aa7001d85f", "message": "Minor fix", "committedDate": "2020-06-11T05:58:47Z", "type": "commit"}, {"oid": "fc87eb880d8f25b4970d96a3c5ec7d99e09d207a", "url": "https://github.com/apache/kafka/commit/fc87eb880d8f25b4970d96a3c5ec7d99e09d207a", "message": "Fix test", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "12e41dfa9eae3dd470cdaa8a077a44f203feecdb", "url": "https://github.com/apache/kafka/commit/12e41dfa9eae3dd470cdaa8a077a44f203feecdb", "message": "Minor: change label of a test", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "657e256f35cbf6971f8d446b42b73c66a72746c2", "url": "https://github.com/apache/kafka/commit/657e256f35cbf6971f8d446b42b73c66a72746c2", "message": "Address comments from Jun", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "a556548912d154bda4a72998a5e134afe59033ac", "url": "https://github.com/apache/kafka/commit/a556548912d154bda4a72998a5e134afe59033ac", "message": "Address comments from Jun", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "4b785b980adc9f654a1a2aac2ed72caa644c47d3", "url": "https://github.com/apache/kafka/commit/4b785b980adc9f654a1a2aac2ed72caa644c47d3", "message": "Address comments from Boyang", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "8e3ef56c552da13226c1fcc472c8d339fb98c7a8", "url": "https://github.com/apache/kafka/commit/8e3ef56c552da13226c1fcc472c8d339fb98c7a8", "message": "Fix checkstyle issues", "committedDate": "2020-06-11T05:58:48Z", "type": "commit"}, {"oid": "8e3ef56c552da13226c1fcc472c8d339fb98c7a8", "url": "https://github.com/apache/kafka/commit/8e3ef56c552da13226c1fcc472c8d339fb98c7a8", "message": "Fix checkstyle issues", "committedDate": "2020-06-11T05:58:48Z", "type": "forcePushed"}]}