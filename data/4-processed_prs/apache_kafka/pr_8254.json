{"pr_number": 8254, "pr_title": "KIP-557: Add Emit On Change Support", "pr_createdAt": "2020-03-08T18:29:07Z", "pr_url": "https://github.com/apache/kafka/pull/8254", "timeline": [{"oid": "80de5bba0621aa71e5dcf29cfd30465bc1571bc3", "url": "https://github.com/apache/kafka/commit/80de5bba0621aa71e5dcf29cfd30465bc1571bc3", "message": "[KIP-557] Add emit on change support", "committedDate": "2020-03-08T18:27:30Z", "type": "commit"}, {"oid": "1aaaa01b4b410ce06eb879dcb14d8cef243b638b", "url": "https://github.com/apache/kafka/commit/1aaaa01b4b410ce06eb879dcb14d8cef243b638b", "message": "Adding some class modifications", "committedDate": "2020-03-14T03:37:11Z", "type": "commit"}, {"oid": "ec46f57050bb75cd662fd07664f8ab7470b638a5", "url": "https://github.com/apache/kafka/commit/ec46f57050bb75cd662fd07664f8ab7470b638a5", "message": "Adding class", "committedDate": "2020-03-14T03:38:24Z", "type": "commit"}, {"oid": "2c19bf7d21a022b7fc59b313f451a49de07f51ed", "url": "https://github.com/apache/kafka/commit/2c19bf7d21a022b7fc59b313f451a49de07f51ed", "message": "Merge branch 'trunk' into EMIT-ON-CHANGE", "committedDate": "2020-03-14T03:40:01Z", "type": "commit"}, {"oid": "e3ffa93503b2429ef7d3c3149eae06030c9425cb", "url": "https://github.com/apache/kafka/commit/e3ffa93503b2429ef7d3c3149eae06030c9425cb", "message": "Adding test", "committedDate": "2020-03-14T17:36:46Z", "type": "commit"}, {"oid": "a14725adcd4efa4dc29b3e2cc0b97cc28c42e20f", "url": "https://github.com/apache/kafka/commit/a14725adcd4efa4dc29b3e2cc0b97cc28c42e20f", "message": "Merge branch 'EMIT-ON-CHANGE' of https://github.com/ConcurrencyPractitioner/kafka into EMIT-ON-CHANGE", "committedDate": "2020-03-14T17:38:05Z", "type": "commit"}, {"oid": "6d3c7ab0c879e36900273a5e4eb19485da5b5e6f", "url": "https://github.com/apache/kafka/commit/6d3c7ab0c879e36900273a5e4eb19485da5b5e6f", "message": "Adding sensor", "committedDate": "2020-03-15T03:21:47Z", "type": "commit"}, {"oid": "81c19bd7329dce0aa2bb0148a95e9e65afb7ad7e", "url": "https://github.com/apache/kafka/commit/81c19bd7329dce0aa2bb0148a95e9e65afb7ad7e", "message": "Add working sensor", "committedDate": "2020-03-15T21:44:05Z", "type": "commit"}, {"oid": "209d37662114b871cbdb9a7ee36632daad53ca08", "url": "https://github.com/apache/kafka/commit/209d37662114b871cbdb9a7ee36632daad53ca08", "message": "Bumping processor level", "committedDate": "2020-03-15T21:58:25Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1OTI1Mg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r401859252", "bodyText": "Maybe we can be a little more conservative and avoid creating the new type TimestampedSerializedKeyValueStore, and therefore avoid changing the decorators at all. I'd like to avoid any reasonable possibility of users encountering the new methods.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            store = (TimestampedSerializedKeyValueStore<K, V>) context.getStateStore(queryableName);\n          \n          \n            \n                            final StateStore stateStore = context.getStateStore(queryableName);\n          \n          \n            \n                            try {\n          \n          \n            \n                                store = ((WrappedStateStore<MeteredTimestampedKeyValueStore<K, V>, K, V>) stateStore).wrapped();\n          \n          \n            \n                            } catch (final ClassCastException e) {\n          \n          \n            \n                                throw new IllegalStateException(\"Unexpected store type: \" + stateStore.getClass() + \" for store: \" + queryableName, e);\n          \n          \n            \n                            }\n          \n      \n    \n    \n  \n\nNote, I know this is an awkward hack, but the whole state store hierarchy is a mess, which will take some work to clean up. Until then, I'm personally comfortable saying that internally we can depend on always having a single layer of wrapper over the MeteredTimestamped store and just unwrapping it to get access to the new methods. The advantage is that we really minimize the surface area we have to change to implement this feature (and therefore minimize building a lot more on top of the mess we already have). The risk is small: we don't get compiler checking, but any/all of our tests would fail if we broke this assumption.", "author": "vvcephei", "createdAt": "2020-04-01T19:33:00Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java", "diffHunk": "@@ -86,12 +90,16 @@ public void init(final ProcessorContext context) {\n             metrics = (StreamsMetricsImpl) context.metrics();\n             droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(Thread.currentThread().getName(), context.taskId().toString(), metrics);\n             if (queryableName != null) {\n-                store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);\n+                store = (TimestampedSerializedKeyValueStore<K, V>) context.getStateStore(queryableName);", "originalCommit": "209d37662114b871cbdb9a7ee36632daad53ca08", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTkzNDY4Nw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r401934687", "bodyText": "@vvcephei  Yeah, I definitely noticed too. I've just had a thought. Should we do a refactoring for the state store hierarchy first before we further proceed with this PR?  That way, we can avoid a lot of the wacky wrangling we are doing here.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-01T21:58:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1OTI1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjAzNjAyOQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r402036029", "bodyText": "Nooooo :)\nI've tried tackling that particular dragon a couple of times. It's an absolutely enormous undertaking. Not too \"difficult\" per se, but designing for compatibility is a bit hairy. The real problem is just the massive diff that results.\nMy plan is to finish up the implementation of KIP-478 and then take it on, since 478 will make it a bit safer to make massive internal changes like that.\nIt's a good thought, but I'd rather let both efforts proceed in parallel. Especially since the code I proposed above, wile mildly offensive, is still pretty safe.", "author": "vvcephei", "createdAt": "2020-04-02T03:47:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg1OTI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg3NTYyMQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r401875621", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                @Test\n          \n          \n            \n                public void testKTableEmitOnChange() {\n          \n          \n            \n                    final StreamsBuilder builder = new StreamsBuilder();\n          \n          \n            \n                    final String topic1 = \"topic1\";\n          \n          \n            \n            \n          \n          \n            \n                    final KTable<String, Integer> table1 =\n          \n          \n            \n                        builder.table(topic1, Consumed.with(Serdes.String(), Serdes.Integer()), Materialized.as(\"store\"));\n          \n          \n            \n            \n          \n          \n            \n                    final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();\n          \n          \n            \n                    table1.toStream().process(supplier);\n          \n          \n            \n            \n          \n          \n            \n                    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n          \n          \n            \n                        final TestInputTopic<String, Integer> inputTopic =\n          \n          \n            \n                                driver.createInputTopic(topic1, new StringSerializer(), new IntegerSerializer());\n          \n          \n            \n                        inputTopic.pipeInput(\"A\", 1, 10L);\n          \n          \n            \n                        inputTopic.pipeInput(\"B\", 2, 11L);\n          \n          \n            \n                        inputTopic.pipeInput(\"A\", 1, 10L);\n          \n          \n            \n                        inputTopic.pipeInput(\"B\", 3, 13L);\n          \n          \n            \n            \n          \n          \n            \n                        assertEquals(1.0,\n          \n          \n            \n                            getMetricByName(driver.metrics(), \"idempotent-update-skip-total\", \"stream-processor-node-metrics\").metricValue());\n          \n          \n            \n                    }\n          \n          \n            \n            \n          \n          \n            \n                    assertEquals(\n          \n          \n            \n                        asList(new KeyValueTimestamp<>(\"A\", 1, 10L),\n          \n          \n            \n                            new KeyValueTimestamp<>(\"B\", 2, 11L),\n          \n          \n            \n                            new KeyValueTimestamp<>(\"B\", 3, 13L)),\n          \n          \n            \n                        supplier.theCapturedProcessor().processed);\n          \n          \n            \n                }\n          \n          \n            \n                @Test\n          \n          \n            \n                public void testKTableEmitOnChange() {\n          \n          \n            \n                    final StreamsBuilder builder = new StreamsBuilder();\n          \n          \n            \n                    final String topic1 = \"topic1\";\n          \n          \n            \n            \n          \n          \n            \n                    builder.table(topic1, Consumed.with(Serdes.String(), Serdes.Integer()), Materialized.as(\"store\"))\n          \n          \n            \n                           .toStream()\n          \n          \n            \n                           .to(\"output\");\n          \n          \n            \n            \n          \n          \n            \n                    try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n          \n          \n            \n                        final TestInputTopic<String, Integer> inputTopic =\n          \n          \n            \n                            driver.createInputTopic(topic1, new StringSerializer(), new IntegerSerializer());\n          \n          \n            \n                        final TestOutputTopic<String, Integer> outputTopic =\n          \n          \n            \n                            driver.createOutputTopic(\"output\", new StringDeserializer(), new IntegerDeserializer());\n          \n          \n            \n            \n          \n          \n            \n                        inputTopic.pipeInput(\"A\", 1, 10L);\n          \n          \n            \n                        inputTopic.pipeInput(\"B\", 2, 11L);\n          \n          \n            \n                        inputTopic.pipeInput(\"A\", 1, 12L);\n          \n          \n            \n                        inputTopic.pipeInput(\"B\", 3, 13L);\n          \n          \n            \n            \n          \n          \n            \n                        assertThat(\n          \n          \n            \n                            getMetricByName(driver.metrics(), \"idempotent-update-skip-total\", \"stream-processor-node-metrics\").metricValue(),\n          \n          \n            \n                            is(1.0)\n          \n          \n            \n                        );\n          \n          \n            \n            \n          \n          \n            \n                        assertThat(\n          \n          \n            \n                            outputTopic.readRecordsToList(),\n          \n          \n            \n                            is(\n          \n          \n            \n                                asList(new TestRecord<>(\"A\", 1, Instant.ofEpochMilli(10L)),\n          \n          \n            \n                                       new TestRecord<>(\"B\", 2, Instant.ofEpochMilli(11L)),\n          \n          \n            \n                                       new TestRecord<>(\"B\", 3, Instant.ofEpochMilli(13L)))\n          \n          \n            \n                            )\n          \n          \n            \n                        );\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nVery nice test!\nTwo suggestions for the price of one:\n\nIt's basically the same either way, but since we're using TopologyTestDriver and TestInputTopic, let's use TestOutputTopic as well instead of MockProcessorSupplier for the verification.\nIn the KTable context, we agreed to consider the update idempotent even if the timestamp is different, as long as the value is the same. Therefore, I've changed the update to A to have timestamp 12 instead of 10.\n\nOf course, we'd need several more tests to really verify all the angles of this feature.", "author": "vvcephei", "createdAt": "2020-04-01T20:02:24Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java", "diffHunk": "@@ -85,6 +85,36 @@ public void testKTable() {\n             supplier.theCapturedProcessor().processed);\n     }\n \n+    @Test\n+    public void testKTableEmitOnChange() {\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        final String topic1 = \"topic1\";\n+\n+        final KTable<String, Integer> table1 =\n+            builder.table(topic1, Consumed.with(Serdes.String(), Serdes.Integer()), Materialized.as(\"store\"));\n+\n+        final MockProcessorSupplier<String, Integer> supplier = new MockProcessorSupplier<>();\n+        table1.toStream().process(supplier);\n+\n+        try (final TopologyTestDriver driver = new TopologyTestDriver(builder.build(), props)) {\n+            final TestInputTopic<String, Integer> inputTopic =\n+                    driver.createInputTopic(topic1, new StringSerializer(), new IntegerSerializer());\n+            inputTopic.pipeInput(\"A\", 1, 10L);\n+            inputTopic.pipeInput(\"B\", 2, 11L);\n+            inputTopic.pipeInput(\"A\", 1, 10L);\n+            inputTopic.pipeInput(\"B\", 3, 13L);\n+\n+            assertEquals(1.0,\n+                getMetricByName(driver.metrics(), \"idempotent-update-skip-total\", \"stream-processor-node-metrics\").metricValue());\n+        }\n+\n+        assertEquals(\n+            asList(new KeyValueTimestamp<>(\"A\", 1, 10L),\n+                new KeyValueTimestamp<>(\"B\", 2, 11L),\n+                new KeyValueTimestamp<>(\"B\", 3, 13L)),\n+            supplier.theCapturedProcessor().processed);\n+    }", "originalCommit": "209d37662114b871cbdb9a7ee36632daad53ca08", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMTg3ODEyNA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r401878124", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                public boolean putIfDifferent(final K key,\n          \n          \n            \n                                              final ValueAndTimestamp<V> newValue,\n          \n          \n            \n                                              final byte[] oldSerializedValue) {\n          \n          \n            \n                    final Bytes serializedNewValueBytes = Bytes.wrap(serdes.rawValue(newValue));\n          \n          \n            \n                    final Bytes serializedOldValueBytes = Bytes.wrap(oldSerializedValue);\n          \n          \n            \n                    if (serializedNewValueBytes == null ||\n          \n          \n            \n                        !serializedNewValueBytes.equals(serializedOldValueBytes)) {\n          \n          \n            \n                        super.put(key, newValue);\n          \n          \n            \n                        return true;\n          \n          \n            \n                    }\n          \n          \n            \n                    return false;\n          \n          \n            \n                }\n          \n          \n            \n                public boolean putIfDifferentValues(final K key,\n          \n          \n            \n                                                    final ValueAndTimestamp<V> newValue,\n          \n          \n            \n                                                    final byte[] oldSerializedValue) {\n          \n          \n            \n                    try {\n          \n          \n            \n                        return maybeMeasureLatency(\n          \n          \n            \n                            () -> {\n          \n          \n            \n                                final byte[] newSerializedValue = serdes.rawValue(newValue);\n          \n          \n            \n                                if (ValueAndTimestampSerializer.maskTimestampAndCompareValues(oldSerializedValue, newSerializedValue)) {\n          \n          \n            \n                                    return false;\n          \n          \n            \n                                } else {\n          \n          \n            \n                                    wrapped().put(keyBytes(key), newSerializedValue);\n          \n          \n            \n                                    return true;\n          \n          \n            \n                                }\n          \n          \n            \n                            },\n          \n          \n            \n                            time,\n          \n          \n            \n                            putSensor\n          \n          \n            \n                        );\n          \n          \n            \n                    } catch (final ProcessorStateException e) {\n          \n          \n            \n                        final String message = String.format(e.getMessage(), key, newValue);\n          \n          \n            \n                        throw new ProcessorStateException(message, e);\n          \n          \n            \n                    }\n          \n          \n            \n                }\n          \n      \n    \n    \n  \n\nRecommending a few tweaks here as well:\n\nWe need to measure the put latency including serialization\nWe should avoid serializing twice (which happens when you delegate to super.put)\nFor KTables at least, we should ignore the timestamp when deciding whether to really put it or not, so I renamed the method to clarify it's only comparing the value, and I also created an extra comparison method in the ValueAndTimestampSerializer (since it manages the schema, it can safely skip over the timestamp in the serialized data).\n\nHere's what I sketched for the ValueAndTimestampSerializer method:\n    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {\n        // adapted from Arrays.equals\n        if (left == right)\n            return true;\n        if (left ==null || right ==null)\n            return false;\n\n        final int length = left.length;\n        if (right.length != length)\n            return false;\n\n        // skip the timestamp when comparing just the values\n        for (int i=Long.BYTES; i<length; i++)\n            if (left[i] != right[i])\n                return false;\n\n        return true;\n    }", "author": "vvcephei", "createdAt": "2020-04-01T20:07:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java", "diffHunk": "@@ -53,4 +56,28 @@ void initStoreSerde(final ProcessorContext context) {\n             keySerde == null ? (Serde<K>) context.keySerde() : keySerde,\n             valueSerde == null ? new ValueAndTimestampSerde<>((Serde<V>) context.valueSerde()) : valueSerde);\n     }\n+\n+    public RawAndDeserializedValue<V> getWithBinary(final K key) {\n+        try {\n+            final byte[] serializedValue = wrapped().get(keyBytes(key));\n+            return new RawAndDeserializedValue<V>(serializedValue,\n+                maybeMeasureLatency(() -> outerValue(serializedValue), time, getSensor));\n+        } catch (final ProcessorStateException e) {\n+            final String message = String.format(e.getMessage(), key);\n+            throw new ProcessorStateException(message, e);\n+        }\n+    }\n+\n+    public boolean putIfDifferent(final K key,\n+                                  final ValueAndTimestamp<V> newValue,\n+                                  final byte[] oldSerializedValue) {\n+        final Bytes serializedNewValueBytes = Bytes.wrap(serdes.rawValue(newValue));\n+        final Bytes serializedOldValueBytes = Bytes.wrap(oldSerializedValue);\n+        if (serializedNewValueBytes == null ||\n+            !serializedNewValueBytes.equals(serializedOldValueBytes)) {\n+            super.put(key, newValue);\n+            return true;\n+        }\n+        return false;\n+    }", "originalCommit": "209d37662114b871cbdb9a7ee36632daad53ca08", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8fef99c9f69143a89547fe36de6b76b908c5db90", "url": "https://github.com/apache/kafka/commit/8fef99c9f69143a89547fe36de6b76b908c5db90", "message": "Update streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-04-02T15:59:34Z", "type": "commit"}, {"oid": "1d7c4ad32ea71d1dca26037447ec7ab69fab63e2", "url": "https://github.com/apache/kafka/commit/1d7c4ad32ea71d1dca26037447ec7ab69fab63e2", "message": "Update streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-04-02T16:04:47Z", "type": "commit"}, {"oid": "d062bfaeb5bec8df605f36748f32b8bb917728e3", "url": "https://github.com/apache/kafka/commit/d062bfaeb5bec8df605f36748f32b8bb917728e3", "message": "Update streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java\n\nCo-Authored-By: John Roesler <vvcephei@users.noreply.github.com>", "committedDate": "2020-04-02T16:10:23Z", "type": "commit"}, {"oid": "d0737f583eedc4b27014f59c3bcf32846d9fd77e", "url": "https://github.com/apache/kafka/commit/d0737f583eedc4b27014f59c3bcf32846d9fd77e", "message": "Fixing compilation errors and other issues", "committedDate": "2020-04-02T16:37:43Z", "type": "commit"}, {"oid": "afdde96abfe246fa7dd70059f1d71111728ba81f", "url": "https://github.com/apache/kafka/commit/afdde96abfe246fa7dd70059f1d71111728ba81f", "message": "Removing unneeded class", "committedDate": "2020-04-02T16:38:54Z", "type": "commit"}, {"oid": "35c16b11cae35ee04a064b2044dc388c59299503", "url": "https://github.com/apache/kafka/commit/35c16b11cae35ee04a064b2044dc388c59299503", "message": "Removing all vestiges of new class", "committedDate": "2020-04-02T16:41:58Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU5MjU4OQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r402592589", "bodyText": "Ah, missed this the last time though. We should also perform the wrapped().get inside the lambda, so that the time to perform the get is included in the latency.", "author": "vvcephei", "createdAt": "2020-04-02T20:41:02Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java", "diffHunk": "@@ -53,4 +56,47 @@ void initStoreSerde(final ProcessorContext context) {\n             keySerde == null ? (Serde<K>) context.keySerde() : keySerde,\n             valueSerde == null ? new ValueAndTimestampSerde<>((Serde<V>) context.valueSerde()) : valueSerde);\n     }\n-}\n\\ No newline at end of file\n+\n+    public RawAndDeserializedValue<V> getWithBinary(final K key) {\n+        try {\n+            final byte[] serializedValue = wrapped().get(keyBytes(key));\n+            return new RawAndDeserializedValue<V>(serializedValue,\n+                maybeMeasureLatency(() -> outerValue(serializedValue), time, getSensor));", "originalCommit": "35c16b11cae35ee04a064b2044dc388c59299503", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMzEzMzQ0MQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r403133441", "bodyText": "Cool, I will make the change.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-03T16:36:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwMjU5MjU4OQ=="}], "type": "inlineReview"}, {"oid": "9e7649b61352d118f90bfb765dc522b942504681", "url": "https://github.com/apache/kafka/commit/9e7649b61352d118f90bfb765dc522b942504681", "message": "Adding more inclusive lambda", "committedDate": "2020-04-03T16:38:28Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY4MTE5Ng==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404681196", "bodyText": "req: Please change formatting to comply with code style guidelines\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                            skippedIdempotentUpdatesSensor = skippedIdempotentUpdatesSensor(Thread.currentThread().getName(),\n          \n          \n            \n                                                                                            context.taskId().toString(),\n          \n          \n            \n                                                                                            ((InternalProcessorContext) context).currentNode().name(),\n          \n          \n            \n                                                                                            metrics);\n          \n          \n            \n                            skippedIdempotentUpdatesSensor = skippedIdempotentUpdatesSensor(\n          \n          \n            \n                                Thread.currentThread().getName(), \n          \n          \n            \n                                context.taskId().toString(), \n          \n          \n            \n                                ((InternalProcessorContext) context).currentNode().name(), \n          \n          \n            \n                                metrics\n          \n          \n            \n                            );", "author": "cadonna", "createdAt": "2020-04-07T09:49:56Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java", "diffHunk": "@@ -86,12 +91,21 @@ public void init(final ProcessorContext context) {\n             metrics = (StreamsMetricsImpl) context.metrics();\n             droppedRecordsSensor = droppedRecordsSensorOrSkippedRecordsSensor(Thread.currentThread().getName(), context.taskId().toString(), metrics);\n             if (queryableName != null) {\n-                store = (TimestampedKeyValueStore<K, V>) context.getStateStore(queryableName);\n+                final StateStore stateStore = context.getStateStore(queryableName);\n+                try {\n+                    store = ((WrappedStateStore<MeteredTimestampedKeyValueStore<K, V>, K, V>) stateStore).wrapped();\n+                } catch (final ClassCastException e) {\n+                    throw new IllegalStateException(\"Unexpected store type: \" + stateStore.getClass() + \" for store: \" + queryableName, e);\n+                }\n                 tupleForwarder = new TimestampedTupleForwarder<>(\n                     store,\n                     context,\n                     new TimestampedCacheFlushListener<>(context),\n                     sendOldValues);\n+                skippedIdempotentUpdatesSensor = skippedIdempotentUpdatesSensor(Thread.currentThread().getName(),\n+                                                                                context.taskId().toString(),\n+                                                                                ((InternalProcessorContext) context).currentNode().name(),\n+                                                                                metrics);", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY4MjI2Nw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404682267", "bodyText": "req: Please add a unit test in ProcessorNodeMetricsTest.", "author": "cadonna", "createdAt": "2020-04-07T09:51:39Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetrics.java", "diffHunk": "@@ -108,6 +114,22 @@ public static Sensor suppressionEmitSensor(final String threadId,\n         );\n     }\n \n+    public static Sensor skippedIdempotentUpdatesSensor(final String threadId,\n+            final String taskId,\n+            final String processorNodeId,\n+            final StreamsMetricsImpl streamsMetrics) {\n+        return throughputSensor(\n+            threadId,\n+            taskId,\n+            processorNodeId,\n+            IDEMPOTENT_UPDATE_SKIP,\n+            IDEMPOTENT_UPDATE_SKIP_RATE_DESCRIPTION,\n+            IDEMPOTENT_UPDATE_SKIP_TOTAL_DESCRIPTION,\n+            RecordingLevel.DEBUG,\n+            streamsMetrics\n+        );\n+    }\n+", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzY5ODYxNA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407698614", "bodyText": "Done.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-13T20:23:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY4MjI2Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MDUzMQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404690531", "bodyText": "prop: Delete this comment since it is not very useful.", "author": "cadonna", "createdAt": "2020-04-07T10:05:22Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java", "diffHunk": "@@ -34,6 +34,25 @@\n         timestampSerializer = new LongSerializer();\n     }\n \n+    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {\n+        // adapted from Arrays.equals", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5MzUzNA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404693534", "bodyText": "prop: Please use braces to delimit the scopes of the loop and the ifs.", "author": "cadonna", "createdAt": "2020-04-07T10:10:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java", "diffHunk": "@@ -34,6 +34,25 @@\n         timestampSerializer = new LongSerializer();\n     }\n \n+    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {\n+        // adapted from Arrays.equals\n+        if (left == right)\n+            return true;\n+        if (left == null || right == null)\n+            return false;\n+\n+        final int length = left.length;\n+        if (right.length != length)\n+            return false;\n+\n+        // skip the timestamp when comparing just the values\n+        for (int i = Long.BYTES; i < length; i++)\n+            if (left[i] != right[i])\n+                return false;", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDY5NjkwMA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404696900", "bodyText": "prop: Instead of adding the comment, factor out this code to a method named skipTimestampAndCompareValues().", "author": "cadonna", "createdAt": "2020-04-07T10:16:08Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java", "diffHunk": "@@ -34,6 +34,25 @@\n         timestampSerializer = new LongSerializer();\n     }\n \n+    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {\n+        // adapted from Arrays.equals\n+        if (left == right)\n+            return true;\n+        if (left == null || right == null)\n+            return false;\n+\n+        final int length = left.length;\n+        if (right.length != length)\n+            return false;\n+\n+        // skip the timestamp when comparing just the values\n+        for (int i = Long.BYTES; i < length; i++)\n+            if (left[i] != right[i])\n+                return false;\n+\n+        return true;", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNDg1NA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404704854", "bodyText": "Q: If the record is out-of-order, shouldn't you put the value into the state store regardless of whether it is different from the old value or not? I think this would be more correct because the old value is actually the idempotent update to the new out-of-order value. This is one of the rare cases where we can correct the order. WDYT?", "author": "cadonna", "createdAt": "2020-04-07T10:29:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java", "diffHunk": "@@ -119,8 +134,13 @@ public void process(final K key, final V value) {\n                 } else {\n                     oldValue = null;\n                 }\n-                store.put(key, ValueAndTimestamp.make(value, context().timestamp()));\n-                tupleForwarder.maybeForward(key, value, oldValue);\n+                final boolean isDifferentValue = \n+                    store.putIfDifferentValues(key, ValueAndTimestamp.make(value, context().timestamp()), tuple.serializedValue);", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwNTU4NQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r405005585", "bodyText": "Thanks @cadonna. I'd like to ask for clarification:\nDo you mean, given the following sequence of input records:\nA: 2 @time3\nA: 2 @time4\nA: 2 @time1\n\nWe would take the following actions:\nput(A, 2 @time3) and forward the update\nskip idempotent update (A is still 2 @time3)\nput(A, 2 @time1) and forward the update (so now A is still 2, but the timestamp is updated to 1)\n\nThis might be controversial. I believe that when we \"complete\" the implementation of timestamp semantics in Streams, the current proposal is to actually drop disordered updates. In that context, it might seem like a step in the wrong direction to specifically implement the code to preserve them when we would otherwise drop them.\nOn the other hand, the current semantics do seem to be better maintained if we take your suggestion. Perhaps we should let the future take care of itself and implement correct semantics as we currently understand them. I.e., I'm +1 on your suggestion.", "author": "vvcephei", "createdAt": "2020-04-07T17:59:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNDg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTMzNTkwMg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r405335902", "bodyText": "Yes, this is what I mean and the current semantics were my motivation to ask the question.", "author": "cadonna", "createdAt": "2020-04-08T08:08:36Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNDg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzA5MDY1Mg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407090652", "bodyText": "These are good points! However, if this is the case, then we probably need to compare timestamps as well (i.e. to check if one is smaller than the other). We just need to modify the comparator in the serializer.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T17:43:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNDg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEyMTE2Ng==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407121166", "bodyText": "Also, added some changes to accomodate this in the test.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T23:09:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcwNDg1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMTUwNg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404711506", "bodyText": "Q: Why do you need the TopologyTestDriver here? I see that the other test use it. I guess you could simply instantiate a KTableSource, get the processor from it, and test directly the processor.", "author": "cadonna", "createdAt": "2020-04-07T10:42:17Z", "path": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableSourceTest.java", "diffHunk": "@@ -85,6 +89,40 @@ public void testKTable() {\n             supplier.theCapturedProcessor().processed);\n     }\n \n+    @Test\n+    public void testKTableSourceEmitOnChange() {", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzExODM2Nw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407118367", "bodyText": "I think that we can just follow the previous test format, so I think there's not too much need to change it at the moment.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T22:35:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMTUwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMTk4NQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404711985", "bodyText": "req: Please add unit tests for this method.", "author": "cadonna", "createdAt": "2020-04-07T10:43:07Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java", "diffHunk": "@@ -34,6 +34,25 @@\n         timestampSerializer = new LongSerializer();\n     }\n \n+    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEyMTE0NA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407121144", "bodyText": "Yep, got this done.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T23:08:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMTk4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMjI0Mw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404712243", "bodyText": "req: Please add unit tests for this method.", "author": "cadonna", "createdAt": "2020-04-07T10:43:38Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java", "diffHunk": "@@ -53,4 +56,48 @@ void initStoreSerde(final ProcessorContext context) {\n             keySerde == null ? (Serde<K>) context.keySerde() : keySerde,\n             valueSerde == null ? new ValueAndTimestampSerde<>((Serde<V>) context.valueSerde()) : valueSerde);\n     }\n-}\n\\ No newline at end of file\n+\n+    public RawAndDeserializedValue<V> getWithBinary(final K key) {", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEyNDE4Mw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407124183", "bodyText": "Done.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T23:50:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMjI0Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMjYyMw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r404712623", "bodyText": "req: Please add unit tests for this method.", "author": "cadonna", "createdAt": "2020-04-07T10:44:18Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java", "diffHunk": "@@ -53,4 +56,48 @@ void initStoreSerde(final ProcessorContext context) {\n             keySerde == null ? (Serde<K>) context.keySerde() : keySerde,\n             valueSerde == null ? new ValueAndTimestampSerde<>((Serde<V>) context.valueSerde()) : valueSerde);\n     }\n-}\n\\ No newline at end of file\n+\n+    public RawAndDeserializedValue<V> getWithBinary(final K key) {\n+        try {\n+            return maybeMeasureLatency(() -> { \n+                final byte[] serializedValue = wrapped().get(keyBytes(key));\n+                return new RawAndDeserializedValue<V>(serializedValue, outerValue(serializedValue));\n+            }, time, getSensor);\n+        } catch (final ProcessorStateException e) {\n+            final String message = String.format(e.getMessage(), key);\n+            throw new ProcessorStateException(message, e);\n+        }\n+    }\n+\n+    public boolean putIfDifferentValues(final K key,", "originalCommit": "9e7649b61352d118f90bfb765dc522b942504681", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNzEyNDE4MA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r407124180", "bodyText": "Done.", "author": "ConcurrencyPractitioner", "createdAt": "2020-04-11T23:49:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDcxMjYyMw=="}], "type": "inlineReview"}, {"oid": "21a734591ee0110c3b8131dd77b1b2fe0221235c", "url": "https://github.com/apache/kafka/commit/21a734591ee0110c3b8131dd77b1b2fe0221235c", "message": "Update streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java\n\nCo-Authored-By: Bruno Cadonna <bruno@confluent.io>", "committedDate": "2020-04-07T23:42:21Z", "type": "commit"}, {"oid": "c4257e1a3c4f8e36587ee8fa1f568df10fe9ce32", "url": "https://github.com/apache/kafka/commit/c4257e1a3c4f8e36587ee8fa1f568df10fe9ce32", "message": "Adding some modifications", "committedDate": "2020-04-11T22:50:22Z", "type": "commit"}, {"oid": "6ff8b650b10f404352eafee98eec1af4c764884b", "url": "https://github.com/apache/kafka/commit/6ff8b650b10f404352eafee98eec1af4c764884b", "message": "Merge branch 'EMIT-ON-CHANGE' of https://github.com/ConcurrencyPractitioner/kafka into EMIT-ON-CHANGE", "committedDate": "2020-04-11T22:50:54Z", "type": "commit"}, {"oid": "860d41be1f724795c4187d56497f95c24aeb7667", "url": "https://github.com/apache/kafka/commit/860d41be1f724795c4187d56497f95c24aeb7667", "message": "Pushing changes", "committedDate": "2020-04-11T22:52:55Z", "type": "commit"}, {"oid": "2871c6b5e284d0a6a7435d83acf9dc2a603dccc4", "url": "https://github.com/apache/kafka/commit/2871c6b5e284d0a6a7435d83acf9dc2a603dccc4", "message": "Realiging diffs", "committedDate": "2020-04-11T23:03:27Z", "type": "commit"}, {"oid": "367ebafe0a26941890d1e35ae1154f22b689b336", "url": "https://github.com/apache/kafka/commit/367ebafe0a26941890d1e35ae1154f22b689b336", "message": "Adding tests", "committedDate": "2020-04-11T23:32:58Z", "type": "commit"}, {"oid": "449efd96a0971fd8cf498850461e468237129c83", "url": "https://github.com/apache/kafka/commit/449efd96a0971fd8cf498850461e468237129c83", "message": "Adding test", "committedDate": "2020-04-13T20:21:44Z", "type": "commit"}, {"oid": "9cd3726269010fdd03d583d8733236e5ad785324", "url": "https://github.com/apache/kafka/commit/9cd3726269010fdd03d583d8733236e5ad785324", "message": "Adding fixed getWithBinary TEst", "committedDate": "2020-04-15T16:37:47Z", "type": "commit"}, {"oid": "cdad3484f9b8146beafaf02af7afcdaf010c94d4", "url": "https://github.com/apache/kafka/commit/cdad3484f9b8146beafaf02af7afcdaf010c94d4", "message": "Fixing description", "committedDate": "2020-04-15T16:44:56Z", "type": "commit"}, {"oid": "48cc4d93641cf96549debca0d08a300d824602d9", "url": "https://github.com/apache/kafka/commit/48cc4d93641cf96549debca0d08a300d824602d9", "message": "Making some test changes", "committedDate": "2020-04-17T16:12:22Z", "type": "commit"}, {"oid": "de0fc6d8cc5a0480d8ac3dc2ef0452795f2abc56", "url": "https://github.com/apache/kafka/commit/de0fc6d8cc5a0480d8ac3dc2ef0452795f2abc56", "message": "Merge branch 'trunk' into EMIT-ON-CHANGE", "committedDate": "2020-04-23T20:11:25Z", "type": "commit"}, {"oid": "d396fd4e60e29eb4a4d1c4c4963ada8809562839", "url": "https://github.com/apache/kafka/commit/d396fd4e60e29eb4a4d1c4c4963ada8809562839", "message": "Making some modifications to test", "committedDate": "2020-04-23T20:31:37Z", "type": "commit"}, {"oid": "7d931074dd15f1af94312fad27d5a3cd61321adf", "url": "https://github.com/apache/kafka/commit/7d931074dd15f1af94312fad27d5a3cd61321adf", "message": "Merge branch 'EMIT-ON-CHANGE' of https://github.com/ConcurrencyPractitioner/kafka into EMIT-ON-CHANGE", "committedDate": "2020-04-23T20:31:45Z", "type": "commit"}, {"oid": "07d40aff0ef33920e2cbdc24020ecae8929ef9d8", "url": "https://github.com/apache/kafka/commit/07d40aff0ef33920e2cbdc24020ecae8929ef9d8", "message": "Merge branch 'trunk' of https://github.com/apache/kafka into EMIT-ON-CHANGE", "committedDate": "2020-04-24T19:55:21Z", "type": "commit"}, {"oid": "4779b27b1c475cd8de848fbc62219a291e6ce3e2", "url": "https://github.com/apache/kafka/commit/4779b27b1c475cd8de848fbc62219a291e6ce3e2", "message": "Fixing details", "committedDate": "2020-04-24T19:57:16Z", "type": "commit"}, {"oid": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "url": "https://github.com/apache/kafka/commit/ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "message": "Catching bad modification", "committedDate": "2020-04-25T01:49:14Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxMzU2Nw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r416313567", "bodyText": "Ah, we'd better get rid of all the printlns before merging.", "author": "vvcephei", "createdAt": "2020-04-28T04:14:40Z", "path": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java", "diffHunk": "@@ -108,7 +126,9 @@ public void process(final K key, final V value) {\n             }\n \n             if (queryableName != null) {\n-                final ValueAndTimestamp<V> oldValueAndTimestamp = store.get(key);\n+                final RawAndDeserializedValue<V> tuple = store.getWithBinary(key);\n+                System.out.println(\"Old value found to be: \" + tuple.value);", "originalCommit": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNjU0NQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r416316545", "bodyText": "That last ); should go on a new line.", "author": "vvcephei", "createdAt": "2020-04-28T04:25:05Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/metrics/ProcessorNodeMetricsTest.java", "diffHunk": "@@ -97,6 +97,27 @@ public void shouldGetSuppressionEmitSensor() {\n             () -> ProcessorNodeMetrics.suppressionEmitSensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, streamsMetrics));\n     }\n \n+    @Test\n+    public void shouldGetIdempotentUpdateSkipSensor() {\n+        final String metricNamePrefix = \"idempotent-update-skip\";\n+        final String descriptionOfCount = \"The total number of skipped idempotent updates\";\n+        final String descriptionOfRate = \"The average number of skipped idempotent updates per second\";\n+        expect(streamsMetrics.nodeLevelSensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, metricNamePrefix, RecordingLevel.DEBUG))\n+            .andReturn(expectedSensor);\n+        expect(streamsMetrics.nodeLevelTagMap(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID)).andReturn(tagMap);\n+        StreamsMetricsImpl.addInvocationRateAndCountToSensor(\n+            expectedSensor,\n+            StreamsMetricsImpl.PROCESSOR_NODE_LEVEL_GROUP,\n+            tagMap,\n+            metricNamePrefix,\n+            descriptionOfRate,\n+            descriptionOfCount\n+        );\n+\n+        verifySensor(\n+            () -> ProcessorNodeMetrics.skippedIdempotentUpdatesSensor(THREAD_ID, TASK_ID, PROCESSOR_NODE_ID, streamsMetrics));", "originalCommit": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxNzM3Mg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r416317372", "bodyText": "Since you mocked the inner store get, you shouldn't need to actually do a put, right?", "author": "vvcephei", "createdAt": "2020-04-28T04:27:55Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java", "diffHunk": "@@ -181,6 +183,41 @@ public void shouldWriteBytesToInnerStoreAndRecordPutMetric() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetWithBinary() {\n+        expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);\n+\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        metered.put(key, valueAndTimestamp);", "originalCommit": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxOTczNg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r416319736", "bodyText": "It's kind of hard to read this test since it depends partly on externally constructed data and partially on data (like newValueAndTimestamp) created in the method itself.\nSince there were other comments that need to be addressed, I'll go ahead and also add a couple of nits, if you don't mind...\nInstead of testing two cases in one test method, can you split it into two test methods. I.e., one for L213, and another for L218. Also when you do that, can you just create all the values and serializedValues that you need in the test itself?\nThanks!", "author": "vvcephei", "createdAt": "2020-04-28T04:35:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java", "diffHunk": "@@ -181,6 +183,41 @@ public void shouldWriteBytesToInnerStoreAndRecordPutMetric() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetWithBinary() {\n+        expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);\n+\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        metered.put(key, valueAndTimestamp);\n+\n+        final RawAndDeserializedValue<String> valueWithBinary = metered.getWithBinary(key);\n+        assertEquals(valueWithBinary.value, valueAndTimestamp);\n+        assertEquals(valueWithBinary.serializedValue, valueAndTimestampBytes);\n+    }\n+\n+    @SuppressWarnings(\"resource\")\n+    @Test\n+    public void shouldPutIfDifferentValues() {\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));", "originalCommit": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNjMxOTkxMQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r416319911", "bodyText": "Can you also split this out into a separate method, please? Thanks!", "author": "vvcephei", "createdAt": "2020-04-28T04:36:04Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializerTest.java", "diffHunk": "@@ -50,6 +52,21 @@ public void shouldSerializeNonNullDataUsingTheInternalSerializer() {\n         assertThat(deserialized, is(valueAndTimestamp));\n     }\n \n+    @Test\n+    public void shouldCompareSerializedValuesWithoutTimestamp() {\n+        final String value = \"food\";\n+\n+        final ValueAndTimestamp<String> oldValueAndTimestamp = ValueAndTimestamp.make(value, TIMESTAMP);\n+        final byte[] oldSerializedValue = STRING_SERDE.serializer().serialize(TOPIC, oldValueAndTimestamp);\n+        final ValueAndTimestamp<String> newValueAndTimestamp = ValueAndTimestamp.make(value, TIMESTAMP + 1);\n+        final byte[] newSerializedValue = STRING_SERDE.serializer().serialize(TOPIC, newValueAndTimestamp);\n+        assertTrue(ValueAndTimestampSerializer.maskTimestampAndCompareValues(oldSerializedValue, newSerializedValue));\n+\n+        final ValueAndTimestamp<String> outOfOrderValueAndTimestamp = ValueAndTimestamp.make(value, TIMESTAMP - 1);\n+        final byte[] outOfOrderSerializedValue = STRING_SERDE.serializer().serialize(TOPIC, outOfOrderValueAndTimestamp);\n+        assertFalse(ValueAndTimestampSerializer.maskTimestampAndCompareValues(oldSerializedValue, outOfOrderSerializedValue));", "originalCommit": "ddbf2cf17c07a37959875e4ad79b1c4ad818c05a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "197ddd2c25311c611b8c456660d92130a778730b", "url": "https://github.com/apache/kafka/commit/197ddd2c25311c611b8c456660d92130a778730b", "message": "Resolving most comments", "committedDate": "2020-04-29T15:30:37Z", "type": "commit"}, {"oid": "527ba28787f3d3db7f4d36d406f33afdc5b4e1da", "url": "https://github.com/apache/kafka/commit/527ba28787f3d3db7f4d36d406f33afdc5b4e1da", "message": "Deleting massive amount of print statements", "committedDate": "2020-04-29T15:34:24Z", "type": "commit"}, {"oid": "bae2860750a042f3ab568ea301aff4caa9bab383", "url": "https://github.com/apache/kafka/commit/bae2860750a042f3ab568ea301aff4caa9bab383", "message": "Final removal", "committedDate": "2020-04-29T15:35:51Z", "type": "commit"}, {"oid": "bf5532f3946fd7064222e940a6469343f9d46a01", "url": "https://github.com/apache/kafka/commit/bf5532f3946fd7064222e940a6469343f9d46a01", "message": "Resolving last comment", "committedDate": "2020-04-29T15:50:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzMxMzM0Ng==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r423313346", "bodyText": "Ah, sorry to say, one more thing slipped by me before. We should verify(inner) at the end of both of these tests. It should actually fail for shouldNotPutIfSameValuesAndGreaterTimestamp because we should not call inner.put in this case. To fix that, we would just delete L202, where we set up the mock for inner.put. Then, the mock would be initialized to expect no calls, and the verification would fail if we did call it.", "author": "vvcephei", "createdAt": "2020-05-11T20:54:02Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java", "diffHunk": "@@ -181,6 +183,52 @@ public void shouldWriteBytesToInnerStoreAndRecordPutMetric() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetWithBinary() {\n+        expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);\n+\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        final RawAndDeserializedValue<String> valueWithBinary = metered.getWithBinary(key);\n+        assertEquals(valueWithBinary.value, valueAndTimestamp);\n+        assertEquals(valueWithBinary.serializedValue, valueAndTimestampBytes);\n+    }\n+\n+    @SuppressWarnings(\"resource\")\n+    @Test\n+    public void shouldNotPutIfSameValuesAndGreaterTimestamp() {\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        metered.put(key, valueAndTimestamp);\n+        final ValueAndTimestampSerde<String> stringSerde = new ValueAndTimestampSerde<>(Serdes.String());\n+        final byte[] encodedOldValue = stringSerde.serializer().serialize(\"TOPIC\", valueAndTimestamp);\n+\n+        final ValueAndTimestamp<String> newValueAndTimestamp = ValueAndTimestamp.make(\"value\", 98L);\n+        assertFalse(metered.putIfDifferentValues(key,\n+                                                 newValueAndTimestamp,\n+                                                 encodedOldValue));", "originalCommit": "bf5532f3946fd7064222e940a6469343f9d46a01", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzMxMzYyNg==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r423313626", "bodyText": "As I mentioned in the earlier comment, we're missing a verify(inner) call at the end of this test.", "author": "vvcephei", "createdAt": "2020-05-11T20:54:30Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java", "diffHunk": "@@ -181,6 +183,52 @@ public void shouldWriteBytesToInnerStoreAndRecordPutMetric() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetWithBinary() {\n+        expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);\n+\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        final RawAndDeserializedValue<String> valueWithBinary = metered.getWithBinary(key);\n+        assertEquals(valueWithBinary.value, valueAndTimestamp);\n+        assertEquals(valueWithBinary.serializedValue, valueAndTimestampBytes);\n+    }\n+\n+    @SuppressWarnings(\"resource\")\n+    @Test\n+    public void shouldNotPutIfSameValuesAndGreaterTimestamp() {\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        metered.put(key, valueAndTimestamp);\n+        final ValueAndTimestampSerde<String> stringSerde = new ValueAndTimestampSerde<>(Serdes.String());\n+        final byte[] encodedOldValue = stringSerde.serializer().serialize(\"TOPIC\", valueAndTimestamp);\n+\n+        final ValueAndTimestamp<String> newValueAndTimestamp = ValueAndTimestamp.make(\"value\", 98L);\n+        assertFalse(metered.putIfDifferentValues(key,\n+                                                 newValueAndTimestamp,\n+                                                 encodedOldValue));\n+    }\n+\n+    @SuppressWarnings(\"resource\")\n+    @Test\n+    public void shouldPutIfOutOfOrder() {\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        metered.put(key, valueAndTimestamp);\n+\n+        final ValueAndTimestampSerde<String> stringSerde = new ValueAndTimestampSerde<>(Serdes.String());\n+        final byte[] encodedOldValue = stringSerde.serializer().serialize(\"TOPIC\", valueAndTimestamp);\n+\n+        final ValueAndTimestamp<String> outOfOrderValueAndTimestamp = ValueAndTimestamp.make(\"value\", 95L);\n+        assertTrue(metered.putIfDifferentValues(key, outOfOrderValueAndTimestamp, encodedOldValue));", "originalCommit": "bf5532f3946fd7064222e940a6469343f9d46a01", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzMxNjU5Nw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r423316597", "bodyText": "Ah, upon reading that last test, I realized that I previously overlooked when you added the timestamp comparison to this method. We should change the method name for maintainability. It no longer just \"masks the timestamp and compares the values\". Can we instead call it \"compareValuesAndCheckForIncreasingTimestamp\" or something? I can almost guarantee that one or more people will be badly misled by the current method name.", "author": "vvcephei", "createdAt": "2020-05-11T21:00:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/ValueAndTimestampSerializer.java", "diffHunk": "@@ -34,6 +34,51 @@\n         timestampSerializer = new LongSerializer();\n     }\n \n+    private static boolean skipTimestampAndCompareValues(final byte[] left, final byte[] right) {\n+        for (int i = Long.BYTES; i < left.length; i++) {\n+            if (left[i] != right[i]) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    private static long extractTimestamp(final byte[] bytes) {\n+        final byte[] timestampBytes = new byte[Long.BYTES];\n+        for (int i = 0; i < Long.BYTES; i++) {\n+            timestampBytes[i] = bytes[i];\n+        }\n+        return ByteBuffer.wrap(timestampBytes).getLong();\n+    }\n+\n+    /**\n+     * @param left  the serialized byte array of the old record in state store\n+     * @param right the serialized byte array of the new record being processed\n+     * @return true if the two serialized values are the same (excluding timestamp) or \n+     *              if the timestamp of right is less than left (indicating out of order record)\n+     *         false otherwise\n+     */\n+    public static boolean maskTimestampAndCompareValues(final byte[] left, final byte[] right) {", "originalCommit": "bf5532f3946fd7064222e940a6469343f9d46a01", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "d9aa12d20bddadde43f4d64d7c6cd2ba597eb079", "url": "https://github.com/apache/kafka/commit/d9aa12d20bddadde43f4d64d7c6cd2ba597eb079", "message": "Addressing some last comments", "committedDate": "2020-05-11T23:53:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMzkzNDg2NA==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r423934864", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    expectLastCall();", "author": "vvcephei", "createdAt": "2020-05-12T18:10:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStoreTest.java", "diffHunk": "@@ -181,6 +183,53 @@ public void shouldWriteBytesToInnerStoreAndRecordPutMetric() {\n         verify(inner);\n     }\n \n+    @Test\n+    public void shouldGetWithBinary() {\n+        expect(inner.get(keyBytes)).andReturn(valueAndTimestampBytes);\n+\n+        inner.put(eq(keyBytes), aryEq(valueAndTimestampBytes));\n+        expectLastCall();\n+        init();\n+\n+        final RawAndDeserializedValue<String> valueWithBinary = metered.getWithBinary(key);\n+        assertEquals(valueWithBinary.value, valueAndTimestamp);\n+        assertEquals(valueWithBinary.serializedValue, valueAndTimestampBytes);\n+    }\n+\n+    @SuppressWarnings(\"resource\")\n+    @Test\n+    public void shouldNotPutIfSameValuesAndGreaterTimestamp() {\n+        expectLastCall();", "originalCommit": "d9aa12d20bddadde43f4d64d7c6cd2ba597eb079", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "2a3d93dbcd9324ad3a4393541a86258df61722de", "url": "https://github.com/apache/kafka/commit/2a3d93dbcd9324ad3a4393541a86258df61722de", "message": "minor test fix", "committedDate": "2020-05-12T18:10:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0MTE0NQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r424941145", "bodyText": "@ConcurrencyPractitioner @vvcephei I'm trying to understand this to debug some broken tests in ksql. Couple questions:\nWhen the timestamp of the newer value is lower (ignoring the value), why do we want to put the new value into the store? Surely the store should have the value with the newer timestamp? Otherwise we could wind up with a corrupt store.\nDon't we still want to put the value in the store (even if we don't forward it on to the next context) if the values are the same but the timestamp is newer? Otherwise if we get an out-of-order update with a different value, but a timestamp in between the rows with the same value, we'd incorrectly put that value into the store, e.g. the following updates:\nTS: 1, K: X, V: A\nTS: 3, K: X, V: A\nTS: 2, K: X, V: B\nwould result in the table containing K: X, V: B, which is wrong.", "author": "rodesai", "createdAt": "2020-05-14T07:57:11Z", "path": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredTimestampedKeyValueStore.java", "diffHunk": "@@ -53,4 +56,48 @@ void initStoreSerde(final ProcessorContext context) {\n             keySerde == null ? (Serde<K>) context.keySerde() : keySerde,\n             valueSerde == null ? new ValueAndTimestampSerde<>((Serde<V>) context.valueSerde()) : valueSerde);\n     }\n-}\n\\ No newline at end of file\n+\n+    public RawAndDeserializedValue<V> getWithBinary(final K key) {\n+        try {\n+            return maybeMeasureLatency(() -> { \n+                final byte[] serializedValue = wrapped().get(keyBytes(key));\n+                return new RawAndDeserializedValue<V>(serializedValue, outerValue(serializedValue));\n+            }, time, getSensor);\n+        } catch (final ProcessorStateException e) {\n+            final String message = String.format(e.getMessage(), key);\n+            throw new ProcessorStateException(message, e);\n+        }\n+    }\n+\n+    public boolean putIfDifferentValues(final K key,\n+                                        final ValueAndTimestamp<V> newValue,\n+                                        final byte[] oldSerializedValue) {\n+        try {\n+            return maybeMeasureLatency(\n+                () -> {\n+                    final byte[] newSerializedValue = serdes.rawValue(newValue);\n+                    if (ValueAndTimestampSerializer.compareValuesAndCheckForIncreasingTimestamp(oldSerializedValue, newSerializedValue)) {\n+                        return false;\n+                    } else {\n+                        wrapped().put(keyBytes(key), newSerializedValue);", "originalCommit": "2a3d93dbcd9324ad3a4393541a86258df61722de", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk3MTIyMw==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r424971223", "bodyText": "Why when the timestamp of the newer value is lower, do we want to put the new value into the store? Surely the store should have the value with the newer timestamp? Otherwise we could wind up with a corrupt store.\n\nThis behavior was there also before this PR. If a out-of-order record is encountered, a log message was written, but the record was nevertheless put into the state store (cf. \n  \n    \n      kafka/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableSource.java\n    \n    \n         Line 122\n      in\n      7624e62\n    \n    \n    \n    \n\n        \n          \n           store.put(key, ValueAndTimestamp.make(value, context().timestamp())); \n        \n    \n  \n\n). The only thing that changed is that if the serialized value of the new record is equal to the serialized value of the old value and the timestamp of the new record is equal or newer, we drop the record because it is a idempotent update.\nCould you elaborate on why a store should get corrupted because of this?\n\nwould result in the table containing K: X, V: B, which is wrong.\n\nAs said above, this behavior should not have been changed.", "author": "cadonna", "createdAt": "2020-05-14T08:46:23Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0MTE0NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk4NDU5OQ==", "url": "https://github.com/apache/kafka/pull/8254#discussion_r424984599", "bodyText": "Don't we still want to put the value in the store (even if we don't forward it on to the next context) if the values are the same but the timestamp is newer?\n\nIf we just put the value in the store but did not forward it, then the store would actually be corrupted, because the local state would not be consistent with downstream anymore.\nNot putting a record with the same value but a newer timestamp in the store and not forwarding it was the main point of this KIP.", "author": "cadonna", "createdAt": "2020-05-14T09:06:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNDk0MTE0NQ=="}], "type": "inlineReview"}]}