{"pr_number": 8327, "pr_title": "KAFKA-9743: Catch commit offset exception to eventually close dirty tasks", "pr_createdAt": "2020-03-22T07:02:29Z", "pr_url": "https://github.com/apache/kafka/pull/8327", "timeline": [{"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjE2MTY0Nw==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396161647", "bodyText": "I think we can consolidate the three blocks 1) prepareClose / committableOffsetsAndMetadata and 2) commitOffsetsOrTransaction, and 3) closeClean which are all wrapping runtime exceptions and updating exception / dirty tasks map into a single one.\nThis can be done in another PR.", "author": "guozhangwang", "createdAt": "2020-03-22T23:44:09Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -225,7 +225,16 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n         }\n \n         if (!consumedOffsetsAndMetadataPerTask.isEmpty()) {\n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            try {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396700644", "bodyText": "A CommitFaileException should result in a TaskMigratedException -- should we only catch this one? Catching a generic RuntimeException does seem to be a little coarse grained?\nAlso, I am wondering why this case is not handled by the caller, ie, the exception should finally pop out of Consumer#poll() and the thread would handle this case and close all tasks?\nAlso, why do we only handle it at this place explicitly? We call commitOffsetsOrTransaction on multiple places and never handle it, but just let upper layer take care?", "author": "mjsax", "createdAt": "2020-03-23T19:24:52Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -237,10 +237,19 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 }\n             }\n \n-            commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n+            try {\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n \n-            for (final Task task : additionalTasksForCommitting) {\n-                task.postCommit();\n+                for (final Task task : additionalTasksForCommitting) {\n+                    task.postCommit();\n+                }\n+            } catch (final RuntimeException e) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1OTY1Nw==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396759657", "bodyText": "The reason is because here we are removing the tasks before the commit block, so effectively this would be the last time we are going to see these tasks before we do a lostAll. I checked other commitOffsetsOrTransaction cases which are ok because there would be error handling in place or it is not inside a closing place. And if the current thread crashes for some non-recoverable exception during commit, these half-closed tasks are already polluting the rocksDBMetricsRecordingTrigger which later will bring down other stream threads as well.", "author": "abbccdda", "createdAt": "2020-03-23T21:12:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzNDgxNg==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396834816", "bodyText": "Ack. Thanks for clarifying.", "author": "mjsax", "createdAt": "2020-03-24T00:18:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzNjcyOQ==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396836729", "bodyText": "One more follow up question: With incremental rebalancing and eos-beta, we might commit RUNNING tasks (that were never suspended etc) and task that got assigned to another thread/instance within the same transaction. Hence, if committing fails, we would also need to rollback those RUNNING tasks (that we track via additionalTasksForCommitting) -- is this rolling back ensured by upper levels or do we need to do something special here, too?", "author": "mjsax", "createdAt": "2020-03-24T00:25:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg1Mjk4MA==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396852980", "bodyText": "My understanding is that the tasks in additionalTasksForCommitting are not wiped out during the task iterations, so if we through all the way up to the thread level, we should still have one last chance to call handleLostAll to clean them up.", "author": "abbccdda", "createdAt": "2020-03-24T01:25:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg1NTE0NA==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396855144", "bodyText": "I agree that we would not loose the reference to those tasks -- just wondering if the cleanup would be done correctly? (I believe yes, but worth to verify)", "author": "mjsax", "createdAt": "2020-03-24T01:33:35Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjcwMDY0NA=="}], "type": "inlineReview"}, {"oid": "424e1d2f7e842be5554273b24b97cdc4094a1f1e", "url": "https://github.com/apache/kafka/commit/424e1d2f7e842be5554273b24b97cdc4094a1f1e", "message": "Fix commit offsets exception", "committedDate": "2020-03-23T23:47:01Z", "type": "commit"}, {"oid": "3fb96faafd32a800852de21ec10333987d3c6ee4", "url": "https://github.com/apache/kafka/commit/3fb96faafd32a800852de21ec10333987d3c6ee4", "message": "unit test", "committedDate": "2020-03-23T23:47:01Z", "type": "commit"}, {"oid": "f7ec12ebd0d113e0f25563e88543401c0a5e884b", "url": "https://github.com/apache/kafka/commit/f7ec12ebd0d113e0f25563e88543401c0a5e884b", "message": "rebase", "committedDate": "2020-03-23T23:47:01Z", "type": "commit"}, {"oid": "f7ec12ebd0d113e0f25563e88543401c0a5e884b", "url": "https://github.com/apache/kafka/commit/f7ec12ebd0d113e0f25563e88543401c0a5e884b", "message": "rebase", "committedDate": "2020-03-23T23:47:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzNzI4OQ==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396837289", "bodyText": "nit: why use Task for task00 but StateMachineTask for task01 ? Can you unify this (it a little confusing)?", "author": "mjsax", "createdAt": "2020-03-24T00:27:29Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1198,6 +1198,48 @@ public void shouldCloseActiveTasksAndPropagateThreadProducerExceptionsOnCleanShu\n         verify(activeTaskCreator, changeLogReader);\n     }\n \n+    @Test\n+    public void shouldCloseActiveTasksDirtyAndPropagateCommitException() {\n+        final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(\n+            new Metrics(), \"clientId\", StreamsConfig.METRICS_LATEST);\n+        taskManager = new TaskManager(\n+            changeLogReader,\n+            UUID.randomUUID(),\n+            \"taskManagerTest\",\n+            streamsMetrics,\n+            activeTaskCreator,\n+            standbyTaskCreator,\n+            topologyBuilder,\n+            adminClient,\n+            stateDirectory,\n+            StreamThread.ProcessingMode.EXACTLY_ONCE_ALPHA\n+        );\n+        taskManager.setMainConsumer(consumer);\n+\n+        final Task task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);", "originalCommit": "f7ec12ebd0d113e0f25563e88543401c0a5e884b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njg2NTg3Ng==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396865876", "bodyText": "Because we don't have setCommitNeeded for Task I guess?", "author": "abbccdda", "createdAt": "2020-03-24T02:17:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzNzI4OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NjgzODEyNw==", "url": "https://github.com/apache/kafka/pull/8327#discussion_r396838127", "bodyText": "To make sure we close all tasks (not just the task that causes the error), we should add one more task, that we also close and ensure that it state goes to CLOSED?", "author": "mjsax", "createdAt": "2020-03-24T00:30:33Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/TaskManagerTest.java", "diffHunk": "@@ -1198,6 +1198,48 @@ public void shouldCloseActiveTasksAndPropagateThreadProducerExceptionsOnCleanShu\n         verify(activeTaskCreator, changeLogReader);\n     }\n \n+    @Test\n+    public void shouldCloseActiveTasksDirtyAndPropagateCommitException() {\n+        final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(\n+            new Metrics(), \"clientId\", StreamsConfig.METRICS_LATEST);\n+        taskManager = new TaskManager(\n+            changeLogReader,\n+            UUID.randomUUID(),\n+            \"taskManagerTest\",\n+            streamsMetrics,\n+            activeTaskCreator,\n+            standbyTaskCreator,\n+            topologyBuilder,\n+            adminClient,\n+            stateDirectory,\n+            StreamThread.ProcessingMode.EXACTLY_ONCE_ALPHA\n+        );\n+        taskManager.setMainConsumer(consumer);\n+\n+        final Task task00 = new StateMachineTask(taskId00, taskId00Partitions, true);\n+        final StateMachineTask task01 = new StateMachineTask(taskId01, taskId01Partitions, true);\n+        final Map<TopicPartition, OffsetAndMetadata> offsets = singletonMap(t1p1, new OffsetAndMetadata(0L, null));\n+        task01.setCommittableOffsetsAndMetadata(offsets);\n+        task01.setCommitNeeded();\n+        taskManager.tasks().put(taskId00, task00);\n+        taskManager.tasks().put(taskId01, task01);\n+\n+        expect(activeTaskCreator.streamsProducerForTask(taskId01)).andThrow(new RuntimeException(\"task 0_1 producer boom!\"));\n+        activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(eq(taskId01));\n+        EasyMock.expectLastCall();\n+\n+        replay(activeTaskCreator);\n+\n+        final RuntimeException thrown = assertThrows(RuntimeException.class,\n+            () -> taskManager.handleAssignment(mkMap(mkEntry(taskId00, taskId00Partitions)), Collections.emptyMap()));\n+        assertThat(thrown.getCause().getMessage(), is(\"task 0_1 producer boom!\"));\n+\n+        assertThat(task00.state(), is(Task.State.CREATED));\n+        assertThat(task01.state(), is(Task.State.CLOSED));", "originalCommit": "f7ec12ebd0d113e0f25563e88543401c0a5e884b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "3de186888785fdd16bc40ad29cd7f94de997b336", "url": "https://github.com/apache/kafka/commit/3de186888785fdd16bc40ad29cd7f94de997b336", "message": "address comments", "committedDate": "2020-03-24T03:46:21Z", "type": "commit"}]}