{"pr_number": 9401, "pr_title": "KAFKA-9628 Replace Produce request/response with automated protocol", "pr_createdAt": "2020-10-09T05:58:00Z", "pr_url": "https://github.com/apache/kafka/pull/9401", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMyNjAyMg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514326022", "bodyText": "Would it make sense to move this to the builder where we are already doing a pass over the partitions?", "author": "hachikuji", "createdAt": "2020-10-29T14:58:19Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -210,65 +142,42 @@ public String toString() {\n         }\n     }\n \n+    /**\n+     * We have to copy acks, timeout, transactionalId and partitionSizes from data since data maybe reset to eliminate\n+     * the reference to ByteBuffer but those metadata are still useful.\n+     */\n     private final short acks;\n     private final int timeout;\n     private final String transactionalId;\n-\n-    private final Map<TopicPartition, Integer> partitionSizes;\n-\n+    // visible for testing\n+    final Map<TopicPartition, Integer> partitionSizes;\n+    private boolean hasTransactionalRecords = false;\n+    private boolean hasIdempotentRecords = false;\n     // This is set to null by `clearPartitionRecords` to prevent unnecessary memory retention when a produce request is\n     // put in the purgatory (due to client throttling, it can take a while before the response is sent).\n     // Care should be taken in methods that use this field.\n-    private volatile Map<TopicPartition, MemoryRecords> partitionRecords;\n-    private boolean hasTransactionalRecords = false;\n-    private boolean hasIdempotentRecords = false;\n-\n-    private ProduceRequest(short version, short acks, int timeout, Map<TopicPartition, MemoryRecords> partitionRecords, String transactionalId) {\n-        super(ApiKeys.PRODUCE, version);\n-        this.acks = acks;\n-        this.timeout = timeout;\n-\n-        this.transactionalId = transactionalId;\n-        this.partitionRecords = partitionRecords;\n-        this.partitionSizes = createPartitionSizes(partitionRecords);\n+    private volatile ProduceRequestData data;\n \n-        for (MemoryRecords records : partitionRecords.values()) {\n-            setFlags(records);\n-        }\n-    }\n-\n-    private static Map<TopicPartition, Integer> createPartitionSizes(Map<TopicPartition, MemoryRecords> partitionRecords) {\n-        Map<TopicPartition, Integer> result = new HashMap<>(partitionRecords.size());\n-        for (Map.Entry<TopicPartition, MemoryRecords> entry : partitionRecords.entrySet())\n-            result.put(entry.getKey(), entry.getValue().sizeInBytes());\n-        return result;\n-    }\n-\n-    public ProduceRequest(Struct struct, short version) {\n+    public ProduceRequest(ProduceRequestData produceRequestData, short version) {\n         super(ApiKeys.PRODUCE, version);\n-        partitionRecords = new HashMap<>();\n-        for (Object topicDataObj : struct.getArray(TOPIC_DATA_KEY_NAME)) {\n-            Struct topicData = (Struct) topicDataObj;\n-            String topic = topicData.get(TOPIC_NAME);\n-            for (Object partitionResponseObj : topicData.getArray(PARTITION_DATA_KEY_NAME)) {\n-                Struct partitionResponse = (Struct) partitionResponseObj;\n-                int partition = partitionResponse.get(PARTITION_ID);\n-                MemoryRecords records = (MemoryRecords) partitionResponse.getRecords(RECORD_SET_KEY_NAME);\n-                setFlags(records);\n-                partitionRecords.put(new TopicPartition(topic, partition), records);\n-            }\n-        }\n-        partitionSizes = createPartitionSizes(partitionRecords);\n-        acks = struct.getShort(ACKS_KEY_NAME);\n-        timeout = struct.getInt(TIMEOUT_KEY_NAME);\n-        transactionalId = struct.getOrElse(NULLABLE_TRANSACTIONAL_ID, null);\n-    }\n-\n-    private void setFlags(MemoryRecords records) {\n-        Iterator<MutableRecordBatch> iterator = records.batches().iterator();\n-        MutableRecordBatch entry = iterator.next();\n-        hasIdempotentRecords = hasIdempotentRecords || entry.hasProducerId();\n-        hasTransactionalRecords = hasTransactionalRecords || entry.isTransactional();\n+        this.data = produceRequestData;\n+        this.data.topicData().forEach(topicProduceData -> topicProduceData.partitions()\n+            .forEach(partitionProduceData -> {\n+                MemoryRecords records = MemoryRecords.readableRecords(partitionProduceData.records());\n+                Iterator<MutableRecordBatch> iterator = records.batches().iterator();\n+                MutableRecordBatch entry = iterator.next();\n+                hasIdempotentRecords = hasIdempotentRecords || entry.hasProducerId();", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDQ2NzY1Mg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514467652", "bodyText": "Nevermind, I guess we have to do it here because the server needs to validate the request received from the client.", "author": "hachikuji", "createdAt": "2020-10-29T18:12:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMyNjAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDU5NDU5Ng==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514594596", "bodyText": "On the other hand, we might want to move this logic into a helper in KafkaApis so that these objects are dedicated only to serialization logic. Eventually we'll want to get rid of ProduceRequest and just use ProduceRequestData.", "author": "hachikuji", "createdAt": "2020-10-29T22:05:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMyNjAyMg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDgxOTMyNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514819326", "bodyText": "clients module has some tests which depends on it so I moves the helper to RequestUtils.", "author": "chia7712", "createdAt": "2020-10-30T04:10:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMyNjAyMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDMzNTcxNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514335716", "bodyText": "Kind of a pity to lose this. Can we move it to the class documentation?", "author": "hachikuji", "createdAt": "2020-10-29T15:10:31Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -17,179 +17,48 @@\n package org.apache.kafka.common.requests;\n \n import org.apache.kafka.common.TopicPartition;\n-import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.message.ProduceResponseData;\n+import org.apache.kafka.common.protocol.ByteBufferAccessor;\n import org.apache.kafka.common.protocol.Errors;\n-import org.apache.kafka.common.protocol.types.ArrayOf;\n-import org.apache.kafka.common.protocol.types.Field;\n-import org.apache.kafka.common.protocol.types.Schema;\n import org.apache.kafka.common.protocol.types.Struct;\n import org.apache.kafka.common.record.RecordBatch;\n-import org.apache.kafka.common.utils.CollectionUtils;\n \n import java.nio.ByteBuffer;\n-import java.util.ArrayList;\n+import java.util.AbstractMap;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-\n-import static org.apache.kafka.common.protocol.CommonFields.ERROR_CODE;\n-import static org.apache.kafka.common.protocol.CommonFields.PARTITION_ID;\n-import static org.apache.kafka.common.protocol.CommonFields.THROTTLE_TIME_MS;\n-import static org.apache.kafka.common.protocol.CommonFields.TOPIC_NAME;\n-import static org.apache.kafka.common.protocol.types.Type.INT64;\n+import java.util.stream.Collectors;\n \n /**\n- * This wrapper supports both v0 and v1 of ProduceResponse.\n+ * This wrapper supports both v0 and v8 of ProduceResponse.\n  */\n public class ProduceResponse extends AbstractResponse {\n-\n-    private static final String RESPONSES_KEY_NAME = \"responses\";\n-\n-    // topic level field names\n-    private static final String PARTITION_RESPONSES_KEY_NAME = \"partition_responses\";\n-\n     public static final long INVALID_OFFSET = -1L;\n+    private final ProduceResponseData data;\n+    private final Map<TopicPartition, PartitionResponse> responses;\n \n-    /**", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MDE3OA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514340178", "bodyText": "Hmm, not sure about making this ignorable. For transactional data, I think the broker would just fail if it cannot authorize the transactionalId.\nAlso, should we set the default to null?", "author": "hachikuji", "createdAt": "2020-10-29T15:16:09Z", "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"ignorable\": true, \"entityType\": \"transactionalId\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDkwMzAwMQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514903001", "bodyText": "you are right. will Roger that.", "author": "chia7712", "createdAt": "2020-10-30T06:57:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MDE3OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514342285", "bodyText": "nit: I kind of liked the original name to make the unit clear. We're probably not consistent on this convention though.", "author": "hachikuji", "createdAt": "2020-10-29T15:18:52Z", "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"ignorable\": true, \"entityType\": \"transactionalId\",\n       \"about\": \"The transactional ID, or null if the producer is not transactional.\" },\n     { \"name\": \"Acks\", \"type\": \"int16\", \"versions\": \"0+\",\n       \"about\": \"The number of acknowledgments the producer requires the leader to have received before considering a request complete. Allowed values: 0 for no acknowledgments, 1 for only the leader and -1 for the full ISR.\" },\n-    { \"name\": \"TimeoutMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+    { \"name\": \"Timeout\", \"type\": \"int32\", \"versions\": \"0+\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQ2Njc1NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r515466754", "bodyText": "This change is for compatibility. The name used by origin serialization is \"timeout\" rather than \"timeoutMs\" (see https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L54).", "author": "chia7712", "createdAt": "2020-10-31T07:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDg5MQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984891", "bodyText": "The names do not get serialized, so I think we can make them whatever we want.", "author": "hachikuji", "createdAt": "2020-11-06T20:18:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA2NjEzNA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519066134", "bodyText": "This is a major question when I am processing those protocol migration PRs.\nThe Struct deserialization depends on the \"name\" (\n  \n    \n      kafka/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java\n    \n    \n         Line 250\n      in\n      8e211eb\n    \n    \n    \n    \n\n        \n          \n           for (Object topicDataObj : struct.getArray(TOPIC_DATA_KEY_NAME)) { \n        \n    \n  \n\n). Struct.get(xxx) searches the index according to \"name\". Hence, changing the name breaks compatibility since the Struct serialization can not find the correct field. (That is to say, the previous protocol fails to deserialize the data from new auto-generated protocol)\nPlease correct me if I misunderstood protocol mechanism.", "author": "chia7712", "createdAt": "2020-11-07T00:21:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA4MTE5NQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519081195", "bodyText": "oh, I trace the code again. You are right. The name is not serialized.\nI will revise the field names as you suggested.", "author": "chia7712", "createdAt": "2020-11-07T02:02:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0MjI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNDM0NjI3OQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r514346279", "bodyText": "Wondering if we may as well rewrite this using ProduceResponseData.", "author": "hachikuji", "createdAt": "2020-10-29T15:24:01Z", "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -218,6 +218,7 @@ private void checkSimpleRequestResponse(NetworkClient networkClient) {\n                 request.apiKey().responseHeaderVersion(PRODUCE.latestVersion()));\n         Struct resp = new Struct(PRODUCE.responseSchema(PRODUCE.latestVersion()));\n         resp.set(\"responses\", new Object[0]);\n+        resp.set(CommonFields.THROTTLE_TIME_MS, 100);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3NjYwNQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518976605", "bodyText": "Is the plan to save this for a follow-up? It looks like it will be a bit of effort to trace down all the uses, but seems doable.", "author": "hachikuji", "createdAt": "2020-11-06T20:06:09Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +561,23 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // TODO: Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1MDgyMw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519250823", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10696", "author": "chia7712", "createdAt": "2020-11-08T03:27:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3NjYwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3ODg2Mw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518978863", "bodyText": "I wonder if we would get any benefit computing partitionSizes during this pass.", "author": "hachikuji", "createdAt": "2020-11-06T20:09:06Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -194,7 +107,27 @@ private ProduceRequest build(short version, boolean validate) {\n                     ProduceRequest.validateRecords(version, records);\n                 }\n             }\n-            return new ProduceRequest(version, acks, timeout, partitionRecords, transactionalId);\n+\n+            List<ProduceRequestData.TopicProduceData> tpd = partitionRecords", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4OTc1Ng==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518989756", "bodyText": "We could probably also compute partitionSizes lazily. I think the broker is the only one that uses it.", "author": "hachikuji", "createdAt": "2020-11-06T20:30:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3ODg2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA2NzY1NQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519067655", "bodyText": "The \"data\" may be cleared to null (for GC) so it is a risk if we lazily initialize the partition size.\nI considered to remove it as it is used by log only (toString).", "author": "chia7712", "createdAt": "2020-11-07T00:29:40Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3ODg2Mw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTA3OTE1Mw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519079153", "bodyText": "Right. I thought we might consider computing it just before the data gets cleared.", "author": "hachikuji", "createdAt": "2020-11-07T01:44:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk3ODg2Mw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDA2NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984064", "bodyText": "Not required, but this would be easier to follow up if we had some helpers.", "author": "hachikuji", "createdAt": "2020-11-06T20:16:48Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,78 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()\n+                .stream()\n+                .collect(Collectors.groupingBy(e -> e.getKey().topic()))\n+                .entrySet()\n+                .stream()\n+                .map(topicData -> new ProduceResponseData.TopicProduceResponse()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1MTY3MQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519251671", "bodyText": "Pardon me. why it is not required?", "author": "chia7712", "createdAt": "2020-11-08T03:40:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDA2NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDc0MDk1NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r520740954", "bodyText": "Oh, I was just emphasizing that it is a matter of taste. It's up to you if you agree or not.", "author": "hachikuji", "createdAt": "2020-11-10T17:28:15Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDA2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDM0OQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518984349", "bodyText": "Sounds good to refactor. Perhaps we can turn this TODO into a jira?", "author": "hachikuji", "createdAt": "2020-11-06T20:17:32Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,78 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()\n+                .stream()\n+                .collect(Collectors.groupingBy(e -> e.getKey().topic()))\n+                .entrySet()\n+                .stream()\n+                .map(topicData -> new ProduceResponseData.TopicProduceResponse()\n+                    .setTopic(topicData.getKey())\n+                    .setPartitionResponses(topicData.getValue()\n+                        .stream()\n+                        .map(p -> new ProduceResponseData.PartitionProduceResponse()\n+                            .setPartition(p.getKey().partition())\n+                            .setBaseOffset(p.getValue().baseOffset)\n+                            .setLogStartOffset(p.getValue().logStartOffset)\n+                            .setLogAppendTime(p.getValue().logAppendTime)\n+                            .setErrorMessage(p.getValue().errorMessage)\n+                            .setErrorCode(p.getValue().error.code())\n+                            .setRecordErrors(p.getValue().recordErrors\n+                                .stream()\n+                                .map(e -> new ProduceResponseData.BatchIndexAndErrorMessage()\n+                                    .setBatchIndex(e.batchIndex)\n+                                    .setBatchIndexErrorMessage(e.message))\n+                                .collect(Collectors.toList())))\n+                        .collect(Collectors.toList())))\n+                .collect(Collectors.toList()))\n+            .setThrottleTimeMs(throttleTimeMs));\n     }\n \n     /**\n-     * Constructor from a {@link Struct}.\n+     * Visible for testing.\n      */\n-    public ProduceResponse(Struct struct) {\n-        responses = new HashMap<>();\n-        for (Object topicResponse : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicRespStruct = (Struct) topicResponse;\n-            String topic = topicRespStruct.get(TOPIC_NAME);\n-\n-            for (Object partResponse : topicRespStruct.getArray(PARTITION_RESPONSES_KEY_NAME)) {\n-                Struct partRespStruct = (Struct) partResponse;\n-                int partition = partRespStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partRespStruct.get(ERROR_CODE));\n-                long offset = partRespStruct.getLong(BASE_OFFSET_KEY_NAME);\n-                long logAppendTime = partRespStruct.hasField(LOG_APPEND_TIME_KEY_NAME) ?\n-                        partRespStruct.getLong(LOG_APPEND_TIME_KEY_NAME) : RecordBatch.NO_TIMESTAMP;\n-                long logStartOffset = partRespStruct.getOrElse(LOG_START_OFFSET_FIELD, INVALID_OFFSET);\n-\n-                List<RecordError> recordErrors = Collections.emptyList();\n-                if (partRespStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    Object[] recordErrorsArray = partRespStruct.getArray(RECORD_ERRORS_KEY_NAME);\n-                    if (recordErrorsArray.length > 0) {\n-                        recordErrors = new ArrayList<>(recordErrorsArray.length);\n-                        for (Object indexAndMessage : recordErrorsArray) {\n-                            Struct indexAndMessageStruct = (Struct) indexAndMessage;\n-                            recordErrors.add(new RecordError(\n-                                    indexAndMessageStruct.getInt(BATCH_INDEX_KEY_NAME),\n-                                    indexAndMessageStruct.get(BATCH_INDEX_ERROR_MESSAGE_FIELD)\n-                            ));\n-                        }\n-                    }\n-                }\n-\n-                String errorMessage = partRespStruct.getOrElse(ERROR_MESSAGE_FIELD, null);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-                responses.put(tp, new PartitionResponse(error, offset, logAppendTime, logStartOffset, recordErrors, errorMessage));\n-            }\n-        }\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n-    }\n-\n     @Override\n-    protected Struct toStruct(short version) {\n-        Struct struct = new Struct(ApiKeys.PRODUCE.responseSchema(version));\n-\n-        Map<String, Map<Integer, PartitionResponse>> responseByTopic = CollectionUtils.groupPartitionDataByTopic(responses);\n-        List<Struct> topicDatas = new ArrayList<>(responseByTopic.size());\n-        for (Map.Entry<String, Map<Integer, PartitionResponse>> entry : responseByTopic.entrySet()) {\n-            Struct topicData = struct.instance(RESPONSES_KEY_NAME);\n-            topicData.set(TOPIC_NAME, entry.getKey());\n-            List<Struct> partitionArray = new ArrayList<>();\n-            for (Map.Entry<Integer, PartitionResponse> partitionEntry : entry.getValue().entrySet()) {\n-                PartitionResponse part = partitionEntry.getValue();\n-                short errorCode = part.error.code();\n-                // If producer sends ProduceRequest V3 or earlier, the client library is not guaranteed to recognize the error code\n-                // for KafkaStorageException. In this case the client library will translate KafkaStorageException to\n-                // UnknownServerException which is not retriable. We can ensure that producer will update metadata and retry\n-                // by converting the KafkaStorageException to NotLeaderOrFollowerException in the response if ProduceRequest version <= 3\n-                if (errorCode == Errors.KAFKA_STORAGE_ERROR.code() && version <= 3)\n-                    errorCode = Errors.NOT_LEADER_OR_FOLLOWER.code();\n-                Struct partStruct = topicData.instance(PARTITION_RESPONSES_KEY_NAME)\n-                        .set(PARTITION_ID, partitionEntry.getKey())\n-                        .set(ERROR_CODE, errorCode)\n-                        .set(BASE_OFFSET_KEY_NAME, part.baseOffset);\n-                partStruct.setIfExists(LOG_APPEND_TIME_KEY_NAME, part.logAppendTime);\n-                partStruct.setIfExists(LOG_START_OFFSET_FIELD, part.logStartOffset);\n-\n-                if (partStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    List<Struct> recordErrors = Collections.emptyList();\n-                    if (!part.recordErrors.isEmpty()) {\n-                        recordErrors = new ArrayList<>();\n-                        for (RecordError indexAndMessage : part.recordErrors) {\n-                            Struct indexAndMessageStruct = partStruct.instance(RECORD_ERRORS_KEY_NAME)\n-                                    .set(BATCH_INDEX_KEY_NAME, indexAndMessage.batchIndex)\n-                                    .set(BATCH_INDEX_ERROR_MESSAGE_FIELD, indexAndMessage.message);\n-                            recordErrors.add(indexAndMessageStruct);\n-                        }\n-                    }\n-                    partStruct.set(RECORD_ERRORS_KEY_NAME, recordErrors.toArray());\n-                }\n-\n-                partStruct.setIfExists(ERROR_MESSAGE_FIELD, part.errorMessage);\n-                partitionArray.add(partStruct);\n-            }\n-            topicData.set(PARTITION_RESPONSES_KEY_NAME, partitionArray.toArray());\n-            topicDatas.add(topicData);\n-        }\n-        struct.set(RESPONSES_KEY_NAME, topicDatas.toArray());\n-        struct.setIfExists(THROTTLE_TIME_MS, throttleTimeMs);\n+    public Struct toStruct(short version) {\n+        return data.toStruct(version);\n+    }\n \n-        return struct;\n+    public ProduceResponseData data() {\n+        return this.data;\n     }\n \n+    /**\n+     * this method is used by testing only.\n+     * TODO: refactor the tests which are using this method and then remove this method from production code.", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1MTEzMQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519251131", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10697", "author": "chia7712", "createdAt": "2020-11-08T03:32:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NDM0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NzE4MA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518987180", "bodyText": "This logic surprised me a little bit until I realized that we were trying to avoid redundant authorization calls. Might be worth adding a comment since I was almost ready to suggest moving this logic into the loop.", "author": "hachikuji", "createdAt": "2020-11-06T20:24:33Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NzU4Ng==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518987586", "bodyText": "That's a good question. I can't think of any great options. We ended up making FetchResponse generic to address a similar issue. I think the cast is reasonable for now. Can we move the TODO to a jira?", "author": "hachikuji", "createdAt": "2020-11-06T20:25:30Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())\n+\n+    produceRequest.dataOrException().topicData().forEach(topic => topic.data().forEach { partition =>\n+      val topicPartition = new TopicPartition(topic.topic(), partition.partition())\n+      // This caller assumes the type is MemoryRecords and that is true on current serialization\n+      // We cast the type to avoid causing big change to code base.\n+      // TODO: maybe we need to refactor code to avoid this casting", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxOTI1MTMyOA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r519251328", "bodyText": "https://issues.apache.org/jira/browse/KAFKA-10698", "author": "chia7712", "createdAt": "2020-11-08T03:35:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4NzU4Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODk4ODYxMg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r518988612", "bodyText": "nit: unnecessary parenthesis (a few of these around here)", "author": "hachikuji", "createdAt": "2020-11-06T20:28:03Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -517,19 +517,23 @@ class KafkaApis(val requestChannel: RequestChannel,\n       }\n       // Note that authorization to a transactionalId implies ProducerId authorization\n \n-    } else if (produceRequest.hasIdempotentRecords && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n+    } else if (RequestUtils.hasIdempotentRecords(produceRequest) && !authorize(request.context, IDEMPOTENT_WRITE, CLUSTER, CLUSTER_NAME)) {\n       sendErrorResponseMaybeThrottle(request, Errors.CLUSTER_AUTHORIZATION_FAILED.exception)\n       return\n     }\n \n-    val produceRecords = produceRequest.partitionRecordsOrFail.asScala\n     val unauthorizedTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val nonExistingTopicResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val invalidRequestResponses = mutable.Map[TopicPartition, PartitionResponse]()\n     val authorizedRequestInfo = mutable.Map[TopicPartition, MemoryRecords]()\n-    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRecords)(_._1.topic)\n-\n-    for ((topicPartition, memoryRecords) <- produceRecords) {\n+    val authorizedTopics = filterByAuthorized(request.context, WRITE, TOPIC, produceRequest.dataOrException().topicData().asScala)(_.topic())\n+\n+    produceRequest.dataOrException().topicData().forEach(topic => topic.data().forEach { partition =>\n+      val topicPartition = new TopicPartition(topic.topic(), partition.partition())", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwMjkzNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521902936", "bodyText": "I wonder if we could avoid all of this by requesting the Sender to create TopicProduceData directly. It seems that the Sender creates partitionRecords right before calling the builder: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L734. So we may be able to directly construct the expect data structure there.", "author": "dajac", "createdAt": "2020-11-12T08:00:06Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -194,7 +106,27 @@ private ProduceRequest build(short version, boolean validate) {\n                     ProduceRequest.validateRecords(version, records);\n                 }\n             }\n-            return new ProduceRequest(version, acks, timeout, partitionRecords, transactionalId);\n+\n+            List<ProduceRequestData.TopicProduceData> tpd = partitionRecords", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyNzcyMw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521927723", "bodyText": "nice suggestion.\nCould I address this in follow-up? I had filed jira (KAFKA-10696 ~ KAFKA-10698) to have Sender use auto-generated protocol directly.", "author": "chia7712", "createdAt": "2020-11-12T08:37:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwMjkzNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTk1NzIyMQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521957221", "bodyText": "oh. The jira I created does not cover this issue. open a new ticket (https://issues.apache.org/jira/browse/KAFKA-10709)", "author": "chia7712", "createdAt": "2020-11-12T09:23:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwMjkzNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwOTU4Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521909587", "bodyText": "It seems that we could create ProduceResponseData based on data. This avoids the cost of the group-by operation and the cost of constructing partitionSizes. That should bring the benchmark inline with what we had before. Would this work?", "author": "dajac", "createdAt": "2020-11-12T08:11:33Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java", "diffHunk": "@@ -323,27 +222,30 @@ public String toString(boolean verbose) {\n     @Override\n     public ProduceResponse getErrorResponse(int throttleTimeMs, Throwable e) {\n         /* In case the producer doesn't actually want any response */\n-        if (acks == 0)\n-            return null;\n-\n+        if (acks == 0) return null;\n         Errors error = Errors.forException(e);\n-        Map<TopicPartition, ProduceResponse.PartitionResponse> responseMap = new HashMap<>();\n-        ProduceResponse.PartitionResponse partitionResponse = new ProduceResponse.PartitionResponse(error);\n-\n-        for (TopicPartition tp : partitions())\n-            responseMap.put(tp, partitionResponse);\n-\n-        return new ProduceResponse(responseMap, throttleTimeMs);\n+        return new ProduceResponse(new ProduceResponseData()\n+            .setResponses(partitionSizes().keySet().stream().collect(Collectors.groupingBy(TopicPartition::topic)).entrySet()\n+                .stream()\n+                .map(entry -> new ProduceResponseData.TopicProduceResponse()\n+                    .setPartitionResponses(entry.getValue().stream().map(p -> new ProduceResponseData.PartitionProduceResponse()\n+                        .setIndex(p.partition())\n+                        .setRecordErrors(Collections.emptyList())\n+                        .setBaseOffset(INVALID_OFFSET)\n+                        .setLogAppendTimeMs(RecordBatch.NO_TIMESTAMP)\n+                        .setLogStartOffset(INVALID_OFFSET)\n+                        .setErrorMessage(e.getMessage())\n+                        .setErrorCode(error.code()))\n+                        .collect(Collectors.toList()))\n+                    .setName(entry.getKey()))\n+                .collect(Collectors.toList()))\n+            .setThrottleTimeMs(throttleTimeMs));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkzMTg4Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521931887", "bodyText": "I used  data to generate ProduceResponseData. However, the data may be null when create ProduceResponseData. That is to say, it require if-else to handle null data in getErrorResponse. It seems to me that is a bit ugly so not sure whether it is worth doing that.", "author": "chia7712", "createdAt": "2020-11-12T08:44:26Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkwOTU4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMTgzNw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r521921837", "bodyText": "As we care of performances here, I wonder if we should try not using the stream api here.\nAnother trick would be to  turn TopicProduceResponse in the ProduceResponse schema into a map by setting \"mapKey\": true for the topic name. This would allow to iterate over responses, get or create TopicProduceResponse for the topic, and add the PartitionProduceResponse into it.\nIt may be worth trying different implementation to compare their performances.", "author": "dajac", "createdAt": "2020-11-12T08:27:20Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -204,118 +75,79 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(new ProduceResponseData()\n+            .setResponses(responses.entrySet()", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjAwNTY1Mw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r522005653", "bodyText": "It may be worth trying different implementation to compare their performances.\n\nAs we all care for performance, I'm ok to say goodbye to stream api :)", "author": "chia7712", "createdAt": "2020-11-12T10:38:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMTgzNw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMjY3MDI5NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r522670294", "bodyText": "I have addressed your suggestion and it does improve the performance.", "author": "chia7712", "createdAt": "2020-11-13T05:54:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTkyMTgzNw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU4NjY5Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525586697", "bodyText": "nit: since we have the jira for tracking, can we remove the TODO? A few more of these in the PR.", "author": "hachikuji", "createdAt": "2020-11-17T23:11:05Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +562,24 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // TODO: Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse\n+                // https://issues.apache.org/jira/browse/KAFKA-10696", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTY4MzU5NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525683594", "bodyText": "copy that", "author": "chia7712", "createdAt": "2020-11-18T02:49:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU4NjY5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NTMyMw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525595323", "bodyText": "I think this test might be overkill. We haven't done anything like this for the other converted APIs. It's a little similar to MessageTest.testRequestSchemas, which was useful verifying the generated schemas when the message generator was being written. Soon testRequestSchemas will be redundant, so I guess we have to decide if we just trust the generator and our compatibility system tests or if we want some other canonical representation. Thoughts?", "author": "hachikuji", "createdAt": "2020-11-17T23:32:42Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceResponseTest.java", "diffHunk": "@@ -100,16 +113,92 @@ public void produceResponseRecordErrorsTest() {\n             ProduceResponse response = new ProduceResponse(responseData);\n             Struct struct = response.toStruct(ver);\n             assertEquals(\"Should use schema version \" + ver, ApiKeys.PRODUCE.responseSchema(ver), struct.schema());\n-            ProduceResponse.PartitionResponse deserialized = new ProduceResponse(struct).responses().get(tp);\n+            ProduceResponse.PartitionResponse deserialized = new ProduceResponse(new ProduceResponseData(struct, ver)).responses().get(tp);\n             if (ver >= 8) {\n                 assertEquals(1, deserialized.recordErrors.size());\n                 assertEquals(3, deserialized.recordErrors.get(0).batchIndex);\n                 assertEquals(\"Record error\", deserialized.recordErrors.get(0).message);\n                 assertEquals(\"Produce failed\", deserialized.errorMessage);\n             } else {\n                 assertEquals(0, deserialized.recordErrors.size());\n-                assertEquals(null, deserialized.errorMessage);\n+                assertNull(deserialized.errorMessage);\n             }\n         }\n     }\n+\n+    /**\n+     * the schema in this test is from previous code and the automatic protocol should be compatible to previous schema.\n+     */\n+    @Test\n+    public void testCompatibility() {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTY5MDQzNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525690436", "bodyText": "That makes sense to me. Will remove redundant test.", "author": "chia7712", "createdAt": "2020-11-18T02:57:04Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTU5NTMyMw=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk2NTA2NQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525965065", "bodyText": "nit: As we got rid of the streaming api in this section, would it make sense to also remove this one?", "author": "dajac", "createdAt": "2020-11-18T10:11:13Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "diffHunk": "@@ -560,13 +562,24 @@ private void handleProduceResponse(ClientResponse response, Map<TopicPartition,\n             log.trace(\"Received produce response from node {} with correlation id {}\", response.destination(), correlationId);\n             // if we have a response, parse it\n             if (response.hasResponse()) {\n+                // Sender should exercise PartitionProduceResponse rather than ProduceResponse.PartitionResponse\n+                // https://issues.apache.org/jira/browse/KAFKA-10696\n                 ProduceResponse produceResponse = (ProduceResponse) response.responseBody();\n-                for (Map.Entry<TopicPartition, ProduceResponse.PartitionResponse> entry : produceResponse.responses().entrySet()) {\n-                    TopicPartition tp = entry.getKey();\n-                    ProduceResponse.PartitionResponse partResp = entry.getValue();\n+                produceResponse.data().responses().forEach(r -> r.partitionResponses().forEach(p -> {\n+                    TopicPartition tp = new TopicPartition(r.name(), p.index());\n+                    ProduceResponse.PartitionResponse partResp = new ProduceResponse.PartitionResponse(\n+                            Errors.forCode(p.errorCode()),\n+                            p.baseOffset(),\n+                            p.logAppendTimeMs(),\n+                            p.logStartOffset(),\n+                            p.recordErrors()\n+                                .stream()\n+                                .map(e -> new ProduceResponse.RecordError(e.batchIndex(), e.batchIndexErrorMessage()))\n+                                .collect(Collectors.toList()),", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAyOTg0Mw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526029843", "bodyText": "The reason we got rid of streaming APIs is because it produces extra collection (groupBy). However, in this case we have to create a new collection to carry non-auto-generated data (and https://issues.apache.org/jira/browse/KAFKA-10696 will eliminate such conversion) even if we get rid of stream APIs. Hence, it should be fine to keep stream APIs here.", "author": "chia7712", "createdAt": "2020-11-18T11:55:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk2NTA2NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjA3NDc3Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526074777", "bodyText": "Yeah, I do agree. It won't change much from a performance point of view. I was more thinking about this from a code consistency point of view. I don't feel strong about this at all. It is just that I usually prefer not to mix paradigms. I recognise that this is a personal taste :).", "author": "dajac", "createdAt": "2020-11-18T13:12:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk2NTA2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3MTQ5Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525971497", "bodyText": "nit: Should we remove this usage of the stream api here as well?", "author": "dajac", "createdAt": "2020-11-18T10:20:53Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/ProduceResponse.java", "diffHunk": "@@ -203,119 +77,88 @@ public ProduceResponse(Map<TopicPartition, PartitionResponse> responses) {\n      * @param responses Produced data grouped by topic-partition\n      * @param throttleTimeMs Time in milliseconds the response was throttled\n      */\n+    @Deprecated\n     public ProduceResponse(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n-        this.responses = responses;\n-        this.throttleTimeMs = throttleTimeMs;\n+        this(toData(responses, throttleTimeMs));\n     }\n \n-    /**\n-     * Constructor from a {@link Struct}.\n-     */\n-    public ProduceResponse(Struct struct) {\n-        responses = new HashMap<>();\n-        for (Object topicResponse : struct.getArray(RESPONSES_KEY_NAME)) {\n-            Struct topicRespStruct = (Struct) topicResponse;\n-            String topic = topicRespStruct.get(TOPIC_NAME);\n-\n-            for (Object partResponse : topicRespStruct.getArray(PARTITION_RESPONSES_KEY_NAME)) {\n-                Struct partRespStruct = (Struct) partResponse;\n-                int partition = partRespStruct.get(PARTITION_ID);\n-                Errors error = Errors.forCode(partRespStruct.get(ERROR_CODE));\n-                long offset = partRespStruct.getLong(BASE_OFFSET_KEY_NAME);\n-                long logAppendTime = partRespStruct.hasField(LOG_APPEND_TIME_KEY_NAME) ?\n-                        partRespStruct.getLong(LOG_APPEND_TIME_KEY_NAME) : RecordBatch.NO_TIMESTAMP;\n-                long logStartOffset = partRespStruct.getOrElse(LOG_START_OFFSET_FIELD, INVALID_OFFSET);\n-\n-                List<RecordError> recordErrors = Collections.emptyList();\n-                if (partRespStruct.hasField(RECORD_ERRORS_KEY_NAME)) {\n-                    Object[] recordErrorsArray = partRespStruct.getArray(RECORD_ERRORS_KEY_NAME);\n-                    if (recordErrorsArray.length > 0) {\n-                        recordErrors = new ArrayList<>(recordErrorsArray.length);\n-                        for (Object indexAndMessage : recordErrorsArray) {\n-                            Struct indexAndMessageStruct = (Struct) indexAndMessage;\n-                            recordErrors.add(new RecordError(\n-                                    indexAndMessageStruct.getInt(BATCH_INDEX_KEY_NAME),\n-                                    indexAndMessageStruct.get(BATCH_INDEX_ERROR_MESSAGE_FIELD)\n-                            ));\n-                        }\n-                    }\n-                }\n+    @Override\n+    protected Send toSend(String destination, ResponseHeader header, short apiVersion) {\n+        return SendBuilder.buildResponseSend(destination, header, this.data, apiVersion);\n+    }\n \n-                String errorMessage = partRespStruct.getOrElse(ERROR_MESSAGE_FIELD, null);\n-                TopicPartition tp = new TopicPartition(topic, partition);\n-                responses.put(tp, new PartitionResponse(error, offset, logAppendTime, logStartOffset, recordErrors, errorMessage));\n+    private static ProduceResponseData toData(Map<TopicPartition, PartitionResponse> responses, int throttleTimeMs) {\n+        ProduceResponseData data = new ProduceResponseData().setThrottleTimeMs(throttleTimeMs);\n+        responses.forEach((tp, response) -> {\n+            ProduceResponseData.TopicProduceResponse tpr = data.responses().find(tp.topic());\n+            if (tpr == null) {\n+                tpr = new ProduceResponseData.TopicProduceResponse().setName(tp.topic());\n+                data.responses().add(tpr);\n             }\n-        }\n-        this.throttleTimeMs = struct.getOrElse(THROTTLE_TIME_MS, DEFAULT_THROTTLE_TIME);\n+            tpr.partitionResponses()\n+                .add(new ProduceResponseData.PartitionProduceResponse()\n+                    .setIndex(tp.partition())\n+                    .setBaseOffset(response.baseOffset)\n+                    .setLogStartOffset(response.logStartOffset)\n+                    .setLogAppendTimeMs(response.logAppendTime)\n+                    .setErrorMessage(response.errorMessage)\n+                    .setErrorCode(response.error.code())\n+                    .setRecordErrors(response.recordErrors\n+                        .stream()\n+                        .map(e -> new ProduceResponseData.BatchIndexAndErrorMessage()\n+                            .setBatchIndex(e.batchIndex)\n+                            .setBatchIndexErrorMessage(e.message))\n+                        .collect(Collectors.toList())));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAyOTk1OA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526029958", "bodyText": "ditto", "author": "chia7712", "createdAt": "2020-11-18T11:55:43Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3MTQ5Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3MzIxMw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525973213", "bodyText": "nit: Add a new line.", "author": "dajac", "createdAt": "2020-11-18T10:23:32Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/RequestUtils.java", "diffHunk": "@@ -48,4 +53,41 @@ public static ByteBuffer serialize(Struct headerStruct, Struct bodyStruct) {\n         buffer.rewind();\n         return buffer;\n     }\n-}\n+\n+    // visible for testing\n+    public static boolean hasIdempotentRecords(ProduceRequest request) {\n+        return flags(request).getKey();\n+    }\n+\n+    // visible for testing\n+    public static boolean hasTransactionalRecords(ProduceRequest request) {\n+        return flags(request).getValue();\n+    }\n+\n+    /**\n+     * Get both hasIdempotentRecords flag and hasTransactionalRecords flag from produce request.\n+     * Noted that we find all flags at once to avoid duplicate loop and record batch construction.\n+     * @return first flag is \"hasIdempotentRecords\" and another is \"hasTransactionalRecords\"\n+     */\n+    public static AbstractMap.SimpleEntry<Boolean, Boolean> flags(ProduceRequest request) {\n+        boolean hasIdempotentRecords = false;\n+        boolean hasTransactionalRecords = false;\n+        for (ProduceRequestData.TopicProduceData tpd : request.dataOrException().topicData()) {\n+            for (ProduceRequestData.PartitionProduceData ppd : tpd.partitionData()) {\n+                BaseRecords records = ppd.records();\n+                if (records instanceof Records) {\n+                    Iterator<? extends RecordBatch> iterator = ((Records) records).batches().iterator();\n+                    if (iterator.hasNext()) {\n+                        RecordBatch batch = iterator.next();\n+                        hasIdempotentRecords = hasIdempotentRecords || batch.hasProducerId();\n+                        hasTransactionalRecords = hasTransactionalRecords || batch.isTransactional();\n+                    }\n+                }\n+                // return early\n+                if (hasIdempotentRecords && hasTransactionalRecords)\n+                    return new AbstractMap.SimpleEntry<>(true, true);\n+            }\n+        }\n+        return new AbstractMap.SimpleEntry<>(hasIdempotentRecords, hasTransactionalRecords);\n+    }\n+}", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3NTg0OQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525975849", "bodyText": "nit: It seem that TransactionalId is null by default so we don't have to set it explicitly all the time. There are few cases in the this file and in others. I am not sure if this was intentional so I am also fine if you want to keep them.", "author": "dajac", "createdAt": "2020-11-18T10:27:21Z", "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -154,8 +155,11 @@ public void testClose() {\n         client.poll(1, time.milliseconds());\n         assertTrue(\"The client should be ready\", client.isReady(node, time.milliseconds()));\n \n-        ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000,\n-                Collections.emptyMap());\n+        ProduceRequest.Builder builder = ProduceRequest.forCurrentMagic(new ProduceRequestData()\n+                .setTopicData(new ProduceRequestData.TopicProduceDataCollection())\n+                .setAcks((short) 1)\n+                .setTimeoutMs(1000)\n+                .setTransactionalId(null));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAzMjcxNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526032716", "bodyText": "make sense to me. will remove this redundant setter", "author": "chia7712", "createdAt": "2020-11-18T12:00:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3NTg0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk3ODM5Nw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525978397", "bodyText": "nit: It may be better to use PRODUCE.latestVersion() to stay inline with L204. There are few cases like this in the file.", "author": "dajac", "createdAt": "2020-11-18T10:31:18Z", "path": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "diffHunk": "@@ -198,8 +202,9 @@ private void checkSimpleRequestResponse(NetworkClient networkClient) {\n         ResponseHeader respHeader =\n             new ResponseHeader(request.correlationId(),\n                 request.apiKey().responseHeaderVersion(PRODUCE.latestVersion()));\n-        Struct resp = new Struct(PRODUCE.responseSchema(PRODUCE.latestVersion()));\n-        resp.set(\"responses\", new Object[0]);\n+        Struct resp = new ProduceResponseData()\n+                .setThrottleTimeMs(100)\n+                .toStruct(ProduceResponseData.HIGHEST_SUPPORTED_VERSION);", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MTkyMA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525981920", "bodyText": "nit: I am not sure if this is important or not but we were using -1 previously.", "author": "dajac", "createdAt": "2020-11-18T10:36:43Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java", "diffHunk": "@@ -192,16 +258,21 @@ public void testMixedTransactionalData() {\n         final MemoryRecords txnRecords = MemoryRecords.withTransactionalRecords(CompressionType.NONE, producerId,\n                 producerEpoch, sequence, new SimpleRecord(\"bar\".getBytes()));\n \n-        final Map<TopicPartition, MemoryRecords> recordsByPartition = new LinkedHashMap<>();\n-        recordsByPartition.put(new TopicPartition(\"foo\", 0), txnRecords);\n-        recordsByPartition.put(new TopicPartition(\"foo\", 1), nonTxnRecords);\n-\n-        final ProduceRequest.Builder builder = ProduceRequest.Builder.forMagic(RecordVersion.current().value, (short) -1, 5000,\n-                recordsByPartition, transactionalId);\n+        ProduceRequest.Builder builder = ProduceRequest.forMagic(RecordBatch.CURRENT_MAGIC_VALUE,\n+                new ProduceRequestData()\n+                        .setTopicData(new ProduceRequestData.TopicProduceDataCollection(Arrays.asList(\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(0).setRecords(txnRecords))),\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(1).setRecords(nonTxnRecords))))\n+                                .iterator()))\n+                        .setAcks((short) 1)", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4MjUxMA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525982510", "bodyText": "We were setting transactionalId previously.", "author": "dajac", "createdAt": "2020-11-18T10:37:41Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/ProduceRequestTest.java", "diffHunk": "@@ -192,16 +258,21 @@ public void testMixedTransactionalData() {\n         final MemoryRecords txnRecords = MemoryRecords.withTransactionalRecords(CompressionType.NONE, producerId,\n                 producerEpoch, sequence, new SimpleRecord(\"bar\".getBytes()));\n \n-        final Map<TopicPartition, MemoryRecords> recordsByPartition = new LinkedHashMap<>();\n-        recordsByPartition.put(new TopicPartition(\"foo\", 0), txnRecords);\n-        recordsByPartition.put(new TopicPartition(\"foo\", 1), nonTxnRecords);\n-\n-        final ProduceRequest.Builder builder = ProduceRequest.Builder.forMagic(RecordVersion.current().value, (short) -1, 5000,\n-                recordsByPartition, transactionalId);\n+        ProduceRequest.Builder builder = ProduceRequest.forMagic(RecordBatch.CURRENT_MAGIC_VALUE,\n+                new ProduceRequestData()\n+                        .setTopicData(new ProduceRequestData.TopicProduceDataCollection(Arrays.asList(\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(0).setRecords(txnRecords))),\n+                                new ProduceRequestData.TopicProduceData().setName(\"foo\").setPartitionData(Collections.singletonList(\n+                                        new ProduceRequestData.PartitionProduceData().setIndex(1).setRecords(nonTxnRecords))))\n+                                .iterator()))\n+                        .setAcks((short) 1)\n+                        .setTimeoutMs(5000)\n+                        .setTransactionalId(null));", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4NzMzNQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r525987335", "bodyText": "I wonder if this one should be ignorable. It seems that we were ignoring it before when it was not present in the target version:\nstruct.setIfExists(NULLABLE_TRANSACTIONAL_ID, transactionalId)", "author": "dajac", "createdAt": "2020-11-18T10:45:07Z", "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"default\": \"null\", \"entityType\": \"transactionalId\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjAzOTg3Mg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526039872", "bodyText": "you are right.", "author": "chia7712", "createdAt": "2020-11-18T12:13:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4NzMzNQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI1MjU5NQ==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526252595", "bodyText": "Hmm I don't think it should be ignorable. The request would just fail if we drop it.", "author": "hachikuji", "createdAt": "2020-11-18T17:01:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTk4NzMzNQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3ODYxNg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526178616", "bodyText": "We should keep this one.", "author": "dajac", "createdAt": "2020-11-18T15:30:06Z", "path": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "diffHunk": "@@ -1730,7 +1768,6 @@ private DeleteTopicsResponse createDeleteTopicsResponse() {\n \n     private InitProducerIdRequest createInitPidRequest() {\n         InitProducerIdRequestData requestData = new InitProducerIdRequestData()\n-                .setTransactionalId(null)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE5MDUxMA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526190510", "bodyText": "thanks!!!", "author": "chia7712", "createdAt": "2020-11-18T15:44:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3ODYxNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3OTk4NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526179984", "bodyText": "We should keep this one as well.", "author": "dajac", "createdAt": "2020-11-18T15:31:47Z", "path": "clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java", "diffHunk": "@@ -580,7 +580,6 @@ synchronized void bumpIdempotentEpochAndResetIdIfNeeded() {\n             if (currentState != State.INITIALIZING && !hasProducerId()) {\n                 transitionTo(State.INITIALIZING);\n                 InitProducerIdRequestData requestData = new InitProducerIdRequestData()\n-                        .setTransactionalId(null)", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE4NzE2Ng==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526187166", "bodyText": "Sorry that I did not test them on my local before pushing :(", "author": "chia7712", "createdAt": "2020-11-18T15:40:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE3OTk4NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526260184", "bodyText": "I don't think it should be ignorable. Transactional requests require this in order to authorize.", "author": "hachikuji", "createdAt": "2020-11-18T17:11:55Z", "path": "clients/src/main/resources/common/message/ProduceRequest.json", "diffHunk": "@@ -33,21 +33,21 @@\n   \"validVersions\": \"0-8\",\n   \"flexibleVersions\": \"none\",\n   \"fields\": [\n-    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"0+\", \"entityType\": \"transactionalId\",\n+    { \"name\": \"TransactionalId\", \"type\": \"string\", \"versions\": \"3+\", \"nullableVersions\": \"3+\", \"default\": \"null\", \"ignorable\": true, \"entityType\": \"transactionalId\",", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI4MDA0Mg==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526280042", "bodyText": "It seems to me ignorable should be true in order to keep behavior consistency. With \"ignore=false\", setting value to TransactionalId can cause UnsupportedVersionException if the version is small than 3. The previous code (https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/ProduceRequest.java#L286) does not cause such exception.", "author": "chia7712", "createdAt": "2020-11-18T17:33:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMwOTIwOA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526309208", "bodyText": "The previous code probably relied on the range checking of the message format to imply support here. My point is that the request is doomed to fail if it holds transactional data and we drop the transactionalId. So we may as well fail fast.", "author": "hachikuji", "createdAt": "2020-11-18T18:03:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjMxNDY0OA==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526314648", "bodyText": "So we may as well fail fast.\n\nThat make sense. will revert this change.", "author": "chia7712", "createdAt": "2020-11-18T18:11:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM2ODk4Mw==", "url": "https://github.com/apache/kafka/pull/9401#discussion_r526368983", "bodyText": "Indeed, that makes sense. My bad!", "author": "dajac", "createdAt": "2020-11-18T19:38:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjI2MDE4NA=="}], "type": "inlineReview"}, {"oid": "f34e7c2e4df48316c18a7a956cb4616b22236a22", "url": "https://github.com/apache/kafka/commit/f34e7c2e4df48316c18a7a956cb4616b22236a22", "message": "KAFKA-9628 Replace Produce request with automated protocol", "committedDate": "2020-11-18T17:17:47Z", "type": "commit"}, {"oid": "f68102df76c76d1b3a5ec055def10a34e83ac128", "url": "https://github.com/apache/kafka/commit/f68102df76c76d1b3a5ec055def10a34e83ac128", "message": "Replace Produce response with automated protocol", "committedDate": "2020-11-18T17:17:47Z", "type": "commit"}, {"oid": "96e354aacdb37243a6d438fa70844171e49d8185", "url": "https://github.com/apache/kafka/commit/96e354aacdb37243a6d438fa70844171e49d8185", "message": "add compatibility test; benchmark; a bit refactor on server to use protocol data", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "0c8e4fc6781a0c14f63e8b668e550e9c1df1881b", "url": "https://github.com/apache/kafka/commit/0c8e4fc6781a0c14f63e8b668e550e9c1df1881b", "message": "optimize getErrorResponse; fix SchemaTestUtils", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "15a33f7d28eddf5442e56802d41c99821db257da", "url": "https://github.com/apache/kafka/commit/15a33f7d28eddf5442e56802d41c99821db257da", "message": "fix produceRequestGetErrorResponseTest", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "c7c4877642c9371e42f55470f5ded60ce385efb0", "url": "https://github.com/apache/kafka/commit/c7c4877642c9371e42f55470f5ded60ce385efb0", "message": "address review comments", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "a9f32e50ecb64f3ef98618d8b54eee4d6a55cf3b", "url": "https://github.com/apache/kafka/commit/a9f32e50ecb64f3ef98618d8b54eee4d6a55cf3b", "message": "add jira link and improve partitionSizes", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "c4bf6bcd948db2845e4528fb4b6e160bf4b95dea", "url": "https://github.com/apache/kafka/commit/c4bf6bcd948db2845e4528fb4b6e160bf4b95dea", "message": "git rid of stream APIs", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "b0b3259d0e292ea05a689c64450a617759ec592c", "url": "https://github.com/apache/kafka/commit/b0b3259d0e292ea05a689c64450a617759ec592c", "message": "use writable to replace struct", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "46449be74ba8cb58970073760f907a2a40d49e86", "url": "https://github.com/apache/kafka/commit/46449be74ba8cb58970073760f907a2a40d49e86", "message": "imporve conversion; add deprecation", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "f55116a4e06d2cc06bb39dc9645a61a6346b1829", "url": "https://github.com/apache/kafka/commit/f55116a4e06d2cc06bb39dc9645a61a6346b1829", "message": "remove deprecation reference of ProduceRequest", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "bc11f9012963ba11dd60189dbdbdffde31ec2478", "url": "https://github.com/apache/kafka/commit/bc11f9012963ba11dd60189dbdbdffde31ec2478", "message": "remove compatibility test and redundant TODO", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "a2ca27d3012eb7a5c0868bce295dd895176475de", "url": "https://github.com/apache/kafka/commit/a2ca27d3012eb7a5c0868bce295dd895176475de", "message": "address review comment", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "34b5b179e2baa4eb7a1fc7aeec05eb549cf5e7f2", "url": "https://github.com/apache/kafka/commit/34b5b179e2baa4eb7a1fc7aeec05eb549cf5e7f2", "message": "fix failed tests", "committedDate": "2020-11-18T17:17:48Z", "type": "commit"}, {"oid": "ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "url": "https://github.com/apache/kafka/commit/ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "message": "ignorable=false", "committedDate": "2020-11-18T18:12:36Z", "type": "commit"}, {"oid": "ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "url": "https://github.com/apache/kafka/commit/ffbe9a3ee73bc65253c0b319cf08b923dcce6138", "message": "ignorable=false", "committedDate": "2020-11-18T18:12:36Z", "type": "forcePushed"}]}