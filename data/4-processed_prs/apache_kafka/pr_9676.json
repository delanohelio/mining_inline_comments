{"pr_number": 9676, "pr_title": "KAFKA-10778: Fence appends after write failure", "pr_createdAt": "2020-12-02T15:48:24Z", "pr_url": "https://github.com/apache/kafka/pull/9676", "timeline": [{"oid": "ec6fbc4204350546883e5c664ed6ffd771327a3e", "url": "https://github.com/apache/kafka/commit/ec6fbc4204350546883e5c664ed6ffd771327a3e", "message": "KAFKA-10778: Fence appends after write failure", "committedDate": "2020-12-02T15:47:07Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUwMjI4Mg==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r534502282", "bodyText": "Perhaps we could move this to somewhere near the top? I don't think we get much benefit by delaying the check since duplicates would be a rare case. We probably don't want to have to trust the producer state anyway after an append failure.", "author": "hachikuji", "createdAt": "2020-12-02T21:45:52Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1219,6 +1219,9 @@ class Log(@volatile private var _dir: File,\n               appendInfo.logAppendTime = duplicate.timestamp\n               appendInfo.logStartOffset = logStartOffset\n             case None =>\n+              if (logDirFailureChannel.logDirIsFailed(parentDir)) {", "originalCommit": "ec6fbc4204350546883e5c664ed6ffd771327a3e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUwMjQ4Mg==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r534502482", "bodyText": "Maybe isOffline or hasFailed?", "author": "hachikuji", "createdAt": "2020-12-02T21:46:17Z", "path": "core/src/main/scala/kafka/server/LogDirFailureChannel.scala", "diffHunk": "@@ -49,6 +49,13 @@ class LogDirFailureChannel(logDirNum: Int) extends Logging {\n       offlineLogDirQueue.add(logDir)\n   }\n \n+  /*\n+   * Return whether the given log dir is offline.\n+   */\n+  def logDirIsFailed(logDir: String): Boolean = {", "originalCommit": "ec6fbc4204350546883e5c664ed6ffd771327a3e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUwMjc5MQ==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r534502791", "bodyText": "nit: we can use assertThrows or intercept", "author": "hachikuji", "createdAt": "2020-12-02T21:46:53Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2818,6 +2818,21 @@ class LogTest {\n       new SimpleRecord(RecordBatch.NO_TIMESTAMP, \"key\".getBytes, \"value\".getBytes)), leaderEpoch = 0)\n   }\n \n+  @Test\n+  def testAppendToLogInFailedLogDir(): Unit = {\n+    val log = createLog(logDir, LogConfig())\n+    log.appendAsLeader(TestUtils.singletonRecords(value = null), leaderEpoch = 0)\n+    assertEquals(0, readLog(log, 0, 4096).records.records.iterator.next().offset)\n+    log.logDirFailureChannel.maybeAddOfflineLogDir(logDir.getParent, \"Simulating failed log dir\", new IOException(\"Test failure\"))\n+    try {\n+      log.appendAsLeader(TestUtils.singletonRecords(value = null), leaderEpoch = 0)\n+      fail()", "originalCommit": "ec6fbc4204350546883e5c664ed6ffd771327a3e", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "8f9a63fac523f7f97828d20afb0ab88f79812606", "url": "https://github.com/apache/kafka/commit/8f9a63fac523f7f97828d20afb0ab88f79812606", "message": "Review comments + test failures", "committedDate": "2020-12-03T09:47:51Z", "type": "commit"}, {"oid": "57e439ac0b04049a4690ec62dc63a836b96c42c3", "url": "https://github.com/apache/kafka/commit/57e439ac0b04049a4690ec62dc63a836b96c42c3", "message": "Move up", "committedDate": "2020-12-03T09:59:51Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY5MTEwNw==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r535691107", "bodyText": "Maybe we can leave a more informative error message here? Sth like \"... dir has failed due to a previous IO exception\", just indicating it is not failed because of the current calling trace.", "author": "guozhangwang", "createdAt": "2020-12-03T22:30:44Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1219,6 +1219,9 @@ class Log(@volatile private var _dir: File,\n               appendInfo.logAppendTime = duplicate.timestamp\n               appendInfo.logStartOffset = logStartOffset\n             case None =>\n+              if (logDirFailureChannel.logDirIsOffline(parentDir)) {\n+                throw new KafkaStorageException(s\"The log dir $parentDir has failed.\");", "originalCommit": "8f9a63fac523f7f97828d20afb0ab88f79812606", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTk3Mjc0MA==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r535972740", "bodyText": "@guozhangwang good point. I also changed the language to 'offline' rather than 'failed', since this is more consistent with other usage.", "author": "tombentley", "createdAt": "2020-12-04T09:51:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY5MTEwNw=="}], "type": "inlineReview"}, {"oid": "22b7aa8964323d5cf278b8ed690043f464098354", "url": "https://github.com/apache/kafka/commit/22b7aa8964323d5cf278b8ed690043f464098354", "message": "Refine exception message", "committedDate": "2020-12-04T09:49:15Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4NDM4Ng==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r538684386", "bodyText": "Hmm.. In case there is an IOException on an append, we will release the lock and fail the log dir in maybeHandleIOException. There is a window for another append to sneak by. It looks like it should be possible to pull maybeHandleIOException into the locked section here since analyzeAndValidateRecords does not do any IO.", "author": "hachikuji", "createdAt": "2020-12-08T18:15:46Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1106,6 +1106,13 @@ class Log(@volatile private var _dir: File,\n         // they are valid, insert them in the log\n         lock synchronized {\n           checkIfMemoryMappedBufferClosed()\n+\n+          // check for offline log dir in case a retry following an IOException happens before the log dir", "originalCommit": "22b7aa8964323d5cf278b8ed690043f464098354", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY4Njc4NA==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r538686784", "bodyText": "I am not sure if it is really necessary, but since offline dirs are a rare situation, I'm wondering if makes sense to optimize for the common case to avoid the lookup. For example, maybe we could leave offlineLogDirs uninitialized until the first log dir failure.", "author": "hachikuji", "createdAt": "2020-12-08T18:18:06Z", "path": "core/src/main/scala/kafka/server/LogDirFailureChannel.scala", "diffHunk": "@@ -49,6 +49,13 @@ class LogDirFailureChannel(logDirNum: Int) extends Logging {\n       offlineLogDirQueue.add(logDir)\n   }\n \n+  /*\n+   * Return whether the given log dir is offline.\n+   */\n+  def logDirIsOffline(logDir: String): Boolean = {\n+    offlineLogDirs.containsKey(logDir)", "originalCommit": "22b7aa8964323d5cf278b8ed690043f464098354", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f2f67357f7c913d8cdb32fe496c38489079c737c", "url": "https://github.com/apache/kafka/commit/f2f67357f7c913d8cdb32fe496c38489079c737c", "message": "Review comments", "committedDate": "2020-12-09T16:18:13Z", "type": "commit"}, {"oid": "8d7c7974a3249064b63991278403677ff8ba470b", "url": "https://github.com/apache/kafka/commit/8d7c7974a3249064b63991278403677ff8ba470b", "message": "Switch to a flag", "committedDate": "2020-12-10T09:22:29Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYxMTI4NQ==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r544611285", "bodyText": "Seems more intuitive to move this check before the segment read. I don't think we can totally avoid race conditions with a failure in append since we don't have the lock here. Perhaps we could even move this check to maybeHandleIOException so that we handle all cases?", "author": "hachikuji", "createdAt": "2020-12-16T20:46:23Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1556,6 +1572,8 @@ class Log(@volatile private var _dir: File,\n           done = fetchDataInfo != null || segmentEntry == null\n         }\n \n+        checkForLogDirFailure()", "originalCommit": "8d7c7974a3249064b63991278403677ff8ba470b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTE0NDEzNA==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r545144134", "bodyText": "Yeah, I wondered about putting it in maybeHandleIOException too, and I've made this change. There are a few places where changing the order in which we do maybeHandleIOException with respect to lock synchronized would seem to avoid some possible races with no side effects other than increasing the duration the lock is held (for the try and the volatile read). I'm thinking specifically maybeIncrementLogStartOffset, roll, delete, truncateFullyAndStartAt and perhaps even truncateTo. But given that there will always exist some races maybe it's not worth it, WDYT?", "author": "tombentley", "createdAt": "2020-12-17T14:45:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYxMTI4NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYxMjkxNw==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r544612917", "bodyText": "Another way to trigger an IO exception is to rename the log file. This trick is used in testAppendToTransactionIndexFailure. Then we don't need to expose maybeHandleIOException for testing.", "author": "hachikuji", "createdAt": "2020-12-16T20:49:15Z", "path": "core/src/test/scala/unit/kafka/log/LogTest.scala", "diffHunk": "@@ -2818,6 +2818,22 @@ class LogTest {\n       new SimpleRecord(RecordBatch.NO_TIMESTAMP, \"key\".getBytes, \"value\".getBytes)), leaderEpoch = 0)\n   }\n \n+  @Test\n+  def testAppendToOrReadFromLogInFailedLogDir(): Unit = {\n+    val log = createLog(logDir, LogConfig())\n+    log.appendAsLeader(TestUtils.singletonRecords(value = null), leaderEpoch = 0)\n+    assertEquals(0, readLog(log, 0, 4096).records.records.iterator.next().offset)\n+    try {\n+      log.maybeHandleIOException(\"Simulating failed log dir\") {", "originalCommit": "8d7c7974a3249064b63991278403677ff8ba470b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NTE0NTAwNQ==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r545145005", "bodyText": "That only seems to work when writing to the transaction index (because the file opening is deferred so that it's only at the point where we try to append some transaction markers that the index file open is attempted and the path found to be a directory, which means that the exception propagates through Log). Renaming the segment file, on the other hand, doesn't propagate through Log, because the rename throws immediately and propagates directly from FileRecords to the test. Obviously when the exception doesn't propagate via Log the logDirOffline doesn't get set.\nMaking this change kind-of mixes transactional appends into a test for something with is orthogonal to transactional logs. In any case, I've done as you suggest, I just thought it worth mentioning.", "author": "tombentley", "createdAt": "2020-12-17T14:46:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0NDYxMjkxNw=="}], "type": "inlineReview"}, {"oid": "89d8687389fc335f23150d5bbbec1a03ae0bbae9", "url": "https://github.com/apache/kafka/commit/89d8687389fc335f23150d5bbbec1a03ae0bbae9", "message": "Move checkForLogDirFailure() into maybeHandleIOException", "committedDate": "2020-12-17T14:20:45Z", "type": "commit"}, {"oid": "40f0f5a4a99e7d0be4a7fa843b40bb71002457a1", "url": "https://github.com/apache/kafka/commit/40f0f5a4a99e7d0be4a7fa843b40bb71002457a1", "message": "Don't expose log.maybeHandleIOException to test", "committedDate": "2020-12-17T14:21:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MjA4NDMxOQ==", "url": "https://github.com/apache/kafka/pull/9676#discussion_r552084319", "bodyText": "Do we still need this since this check is in maybeHandleIOException?", "author": "hachikuji", "createdAt": "2021-01-05T17:34:19Z", "path": "core/src/main/scala/kafka/log/Log.scala", "diffHunk": "@@ -1093,19 +1097,25 @@ class Log(@volatile private var _dir: File,\n                      assignOffsets: Boolean,\n                      leaderEpoch: Int,\n                      ignoreRecordSize: Boolean): LogAppendInfo = {\n-    maybeHandleIOException(s\"Error while appending records to $topicPartition in dir ${dir.getParent}\") {\n-      val appendInfo = analyzeAndValidateRecords(records, origin, ignoreRecordSize)\n \n-      // return if we have no valid messages or if this is a duplicate of the last appended entry\n-      if (appendInfo.shallowCount == 0) appendInfo\n-      else {\n+    val appendInfo = analyzeAndValidateRecords(records, origin, ignoreRecordSize)\n \n-        // trim any invalid bytes or partial messages before appending it to the on-disk log\n-        var validRecords = trimInvalidBytes(records, appendInfo)\n+    // return if we have no valid messages or if this is a duplicate of the last appended entry\n+    if (appendInfo.shallowCount == 0) appendInfo\n+    else {\n \n-        // they are valid, insert them in the log\n-        lock synchronized {\n+      // trim any invalid bytes or partial messages before appending it to the on-disk log\n+      var validRecords = trimInvalidBytes(records, appendInfo)\n+\n+      // they are valid, insert them in the log\n+      lock synchronized {\n+        maybeHandleIOException(s\"Error while appending records to $topicPartition in dir ${dir.getParent}\") {\n           checkIfMemoryMappedBufferClosed()\n+\n+          // check for offline log dir in case a retry following an IOException happens before the log dir\n+          // is taken offline, which would result in inconsistent producer state\n+          checkForLogDirFailure()", "originalCommit": "40f0f5a4a99e7d0be4a7fa843b40bb71002457a1", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f66616a1e9832b8ab3f69911a180bff8d5e1339d", "url": "https://github.com/apache/kafka/commit/f66616a1e9832b8ab3f69911a180bff8d5e1339d", "message": "Review comment", "committedDate": "2021-01-06T10:05:19Z", "type": "commit"}]}