{"pr_number": 8900, "pr_title": "KAFKA-10169: swallow non-fatal KafkaException and don't abort transaction during clean close", "pr_createdAt": "2020-06-18T23:41:12Z", "pr_url": "https://github.com/apache/kafka/pull/8900", "timeline": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTQ1OQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442619459", "bodyText": "This was a sort-of bug: because we don't close things during handleRevocation, we want to make sure the TM will close this as dirty during handleAssignment. So we throw this just to force it to call closeDirty -- but it wasn't necessarily a fatal exception that caused commit to fail, so we should just throw TaskMigrated here.\nThat said, it doesn't really matter since the ConsumerCoordinator will save and rethrow only the first exception, which is the handleRevocation exception. Anything we throw in handleAssignment is \"lost\" -- but we should do the right thing anyway", "author": "ableegoldman", "createdAt": "2020-06-19T04:04:36Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -522,7 +522,7 @@ private void close(final boolean clean) {\n         if (clean && commitNeeded) {\n             log.debug(\"Tried to close clean but there was pending uncommitted data, this means we failed to\"\n                           + \" commit and should close as dirty instead\");\n-            throw new StreamsException(\"Tried to close dirty task as clean\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5ODQwMg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442998402", "bodyText": "This comment is worthy to be a comment on the code itself :)", "author": "guozhangwang", "createdAt": "2020-06-19T18:55:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTQ1OQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzgwODIyNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443808226", "bodyText": "ack", "author": "ableegoldman", "createdAt": "2020-06-22T20:30:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTQ1OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442619557", "bodyText": "My IDEA pointed out that this was never used", "author": "ableegoldman", "createdAt": "2020-06-19T04:05:05Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -69,7 +68,6 @@\n     private final ChangelogReader changelogReader;\n     private final UUID processId;\n     private final String logPrefix;\n-    private final StreamsMetricsImpl streamsMetrics;", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTEwOA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442999108", "bodyText": "Hmm, it is used in streamsMetrics.removeAllTaskLevelSensors(threadId, task.id().toString()); in cleanupTask?", "author": "guozhangwang", "createdAt": "2020-06-19T18:57:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODk0OA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443068948", "bodyText": "I looked into the code, and the answer is \"no\".  It's called within StreamTask / StandbyTask methods, closeClean, closeDirty, and closeAndRecycleState. So It seems fine?", "author": "mjsax", "createdAt": "2020-06-19T22:44:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIzNzUxMw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443237513", "bodyText": "You're right, I was looking at another branch (kafka-raft) again.. the jumping back-and-forth reviewing those PR keep confusing me, sorry.", "author": "guozhangwang", "createdAt": "2020-06-21T16:54:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzgwOTA0MA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443809040", "bodyText": "Lol, Guozhang, that's like the 3rd time in a week you've been looking at an old branch. I guess kafka-raft is not so up-to-date huh \ud83d\ude1b", "author": "ableegoldman", "createdAt": "2020-06-22T20:32:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0Nzg2Nw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443847867", "bodyText": "No it is not at all :) https://github.com/confluentinc/kafka/commits/kafka-raft", "author": "guozhangwang", "createdAt": "2020-06-22T21:55:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjYxOTU1Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442671271", "bodyText": "Should we avoid passing this exception to ProductionExceptionHandler as it never breaks sent now.", "author": "chia7712", "createdAt": "2020-06-19T07:13:19Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -267,7 +283,17 @@ public void close() {\n \n     private void checkForException() {\n         if (sendException != null) {\n-            throw sendException;\n+            if (sendException.getCause() instanceof KafkaException\n+                && sendException.getCause().getMessage().equals(\"Failing batch since transaction was aborted\")) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5Nzc2MA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442997760", "bodyText": "I think we should not add this handling for now based on the conclusion that after one task caused abortTxn is called, no other tasks should ever call recordCollector#flush/send/close anymore right?", "author": "guozhangwang", "createdAt": "2020-06-19T18:53:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2NzIxMQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443067211", "bodyText": "In the current code, we might still need to close tasks, right? If a TX is aborted, we need to \"reset\" all active tasks accordingly and this would imply, closing and reviving them? And while closing we would call checkForException and crash without this guard?\nWhat makes we wonder, if we should actually checkForException in closeDirty() above? If a TX is aborted, and we closeDirty and don't call checkForException it seems we don't need this guard? (In general, this guard seems a little bit hacky and it would be great if we could avoid it IMHO.)", "author": "mjsax", "createdAt": "2020-06-19T22:36:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIzNzM3Nw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443237377", "bodyText": "Makes sense, I think we can avoid checkForException in closeDirty, and if that is sufficient we can remove the exception handling based on error message.", "author": "guozhangwang", "createdAt": "2020-06-21T16:52:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzgyODM0NA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443828344", "bodyText": "That sounds right, we should never see this exception outside of closeDirty since we should then close all tasks dirty if the transaction is aborted.\nBut as for whether to check it in closeDirty, I think we would need to at the very least check it so we can reset the exception afterwards. Or do you think it's \"safe\" to just blindly reset the exception in the case of a dirty close, no matter what it was?", "author": "ableegoldman", "createdAt": "2020-06-22T21:10:41Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg0OTk0NQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443849945", "bodyText": "If we are calling from closeDirty, then exceptions from recordCollector.close would be swallowed and logged.\n\n\nIf we are calling from closeAndRecycleState, then the upper caller from TaskManager would capture all RE and move the task to toCloseDirty.\n\n\nThe only thing that needs to be careful is calling from closeClean as Matthias pointed as an example above.\n\n\nSo looking from that side, to keep the error logged down maybe we should still always checkForException in RecordCollector.close while at the same time keep this hacky error message handling inside RC.", "author": "guozhangwang", "createdAt": "2020-06-22T22:00:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg2NDMxMA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443864310", "bodyText": "I think, that we should never hit this exception during closeClean, that is, we should never call closeClean on a task/record collector that was part of an aborted transaction.\nThe transaction will be aborted if a task had to be closed dirty due to:\n\nsuspend/preCommit/commit failed: in this. case, any task that has an ongoing transaction will throw an exception at the beginning of closeClean and then be closed dirty\npostCommit/closeClean fails: in this case, one task might throw during postCommit or closeClean and cause it to be closed dirty while all other tasks are closed clean. But by definition, we have just committed the transaction, so there is no in-flight txn to be aborted\n\nSo, I think it is ok to just do this special exception handling inside closeDirty, and reset the sendException to null if we find this exception. WDYT?", "author": "ableegoldman", "createdAt": "2020-06-22T22:40:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3MTcxMg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443871712", "bodyText": "I think, that we should never hit this exception during closeClean, that is, we should never call closeClean on a task/record collector that was part of an aborted transaction.\n\nAgreed.\nI was hoping we don't need this hack at all? Why do we want to call checkForException in close dirty?", "author": "mjsax", "createdAt": "2020-06-22T23:05:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3Mzg0Mw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443873843", "bodyText": "Well, to still log the error message while swallowing it :)", "author": "guozhangwang", "createdAt": "2020-06-22T23:11:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3NTEwOA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443875108", "bodyText": "Ok, we can do that, but we only get the first exception back anyway. -- And for this, we also don't need the \"hack\" to check for a specific exception. In closeDirty() we just do\ntry {\n  checkForException():\n} catch (final RuntimeException logAndSwallow) {\n   log.error(...);\n}", "author": "mjsax", "createdAt": "2020-06-22T23:15:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3NzgzMA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443877830", "bodyText": "If we revive a task, we don't recreate the record collector AFAICT. So there may still be a sendException hanging around even after we close the record collector. If this was a truly-fatal exception, we'll check and throw it. But we shouldn't rethrow this particular non-fatal exception. Therefore, we need to check for it and reset the sendException iff we find this exact exception", "author": "ableegoldman", "createdAt": "2020-06-22T23:25:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzODcyNQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443938725", "bodyText": "What about clearing the exception in checkForException instead:\nprivate void checkForException() {\n    if (sendException != null) {\n        final KafkaException rethrow = sendException;\n        sendException = null;\n        throw rethrow;\n    }\n}\n\nWe need to fix sendException to make it thread safe though.", "author": "mjsax", "createdAt": "2020-06-23T03:20:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjY3MTI3MQ=="}], "type": "inlineReview"}, {"oid": null, "url": null, "message": null, "committedDate": null, "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r442999752", "bodyText": "Why only add active tasks here, not standby tasks?", "author": "guozhangwang", "createdAt": "2020-06-19T18:58:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTQxMA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069410", "bodyText": "Standbys done affect the TX-producer and thus they can be closed dirty without side effect.", "author": "mjsax", "createdAt": "2020-06-19T22:46:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIzNzkxNw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443237917", "bodyText": "You mean they can be closed clean right? In that case I'd agree :) Realized that we are still closing standby as clean if one of the active task is causing all other active tasks to be closed dirty. The looping over all tasks above is a bit confusing to me.\nMaybe a subjective, nit suggestion here is to first loop over active, and then loop over standby and in the second loop we do not need the one-spoils-all logic anymore. Although it is a bit duplicated code it would make logic cleaner.", "author": "guozhangwang", "createdAt": "2020-06-21T16:59:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc1OTgwNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443759806", "bodyText": "You mean they can be closed clean right? In that case I'd agree :)\n\nI guess :)", "author": "mjsax", "createdAt": "2020-06-22T18:51:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1MDUzNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443850536", "bodyText": "Sounds good. Will refactor a bit to clarify the active/standby handling", "author": "ableegoldman", "createdAt": "2020-06-22T22:01:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk5OTc1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODEwMA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443068100", "bodyText": "It seems this is the only place we call closeDirty, thus, I am wondering if it might be better to use a boolean flag ie, RecordCollector#close(boolean) and just call () -> recordCollector(clean) here?", "author": "mjsax", "createdAt": "2020-06-19T22:40:21Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java", "diffHunk": "@@ -542,7 +542,12 @@ private void close(final boolean clean) {\n                     \"state manager close\",\n                     log);\n \n-                executeAndMaybeSwallow(clean, recordCollector::close, \"record collector close\", log);\n+                executeAndMaybeSwallow(\n+                    clean,\n+                    clean ? recordCollector::closeClean : recordCollector::closeDirty,", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1MTA0MQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443851041", "bodyText": "I'm just following the pattern we use elsewhere with closeDirty/closeClean. Personally I think it does make the code a bit more readable so you don't have to then go and look up what the boolean argument to RecordCollector#close actually is", "author": "ableegoldman", "createdAt": "2020-06-22T22:02:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODEwMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg3MTk1OQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443871959", "bodyText": "Sure. Was just an idea.", "author": "mjsax", "createdAt": "2020-06-22T23:05:45Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2ODEwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTIxNw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069217", "bodyText": "typo: closed", "author": "mjsax", "createdAt": "2020-06-19T22:45:17Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTkwNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443069906", "bodyText": "Should we do an else here and only call commitOffsetsOrTransaction (and do the postCommit loop) in the else branch (I understand that the current code is correct, as commitOffsetOrTx() would be a no-op for this case -- just wondering, if the code would be easier to read?)", "author": "mjsax", "createdAt": "2020-06-19T22:48:50Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());\n+                consumedOffsetsAndMetadataPerTask.clear();\n             }", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1MjY3Ng==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443852676", "bodyText": "Ack", "author": "ableegoldman", "createdAt": "2020-06-22T22:07:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA2OTkwNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDk1OA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443070958", "bodyText": "Similar question as above: should we move this loop into the same try-catch as commitOffsetsOrTransaction ?This would make it clear that postCommit() should only be executed if committing was successful (even if the current code is correct as taskToCommit would empty if an error occurred.", "author": "mjsax", "createdAt": "2020-06-19T22:53:30Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,58 +675,75 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseClean = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n         final Set<Task> tasksToCommit = new HashSet<>();\n         final Map<TaskId, Map<TopicPartition, OffsetAndMetadata>> consumedOffsetsAndMetadataPerTask = new HashMap<>();\n \n-        for (final Task task : tasks.values()) {\n-            if (clean) {\n+        if (clean) {\n+            for (final Task task : tasks.values()) {\n                 try {\n                     task.suspend();\n                     if (task.commitNeeded()) {\n-                        tasksToCommit.add(task);\n                         final Map<TopicPartition, OffsetAndMetadata> committableOffsets = task.prepareCommit();\n+                        tasksToCommit.add(task);\n                         if (task.isActive()) {\n                             consumedOffsetsAndMetadataPerTask.put(task.id(), committableOffsets);\n                         }\n                     }\n-                    tasksToClose.add(task);\n+                    tasksToCloseClean.add(task);\n                 } catch (final TaskMigratedException e) {\n                     // just ignore the exception as it doesn't matter during shutdown\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 } catch (final RuntimeException e) {\n                     firstException.compareAndSet(null, e);\n-                    closeTaskDirty(task);\n+                    tasksToCloseDirty.add(task);\n                 }\n-            } else {\n-                closeTaskDirty(task);\n             }\n-        }\n \n-        try {\n-            if (clean) {\n-                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n-                for (final Task task : tasksToCommit) {\n-                    try {\n-                        task.postCommit();\n-                    } catch (final RuntimeException e) {\n-                        log.error(\"Exception caught while post-committing task \" + task.id(), e);\n-                        firstException.compareAndSet(null, e);\n-                    }\n-                }\n+            // If any active tasks have to be clsoed dirty and can't be committed, none of them can be\n+            if (!filterActive(tasksToCloseDirty).isEmpty()) {\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.removeAll(filterActive(tasksToCommit));\n+                tasksToCloseDirty.addAll(activeTaskIterable());\n+                consumedOffsetsAndMetadataPerTask.clear();\n             }\n-        } catch (final RuntimeException e) {\n-            log.error(\"Exception caught while committing tasks during shutdown\", e);\n-            firstException.compareAndSet(null, e);\n-        }\n \n-        for (final Task task : tasksToClose) {\n             try {\n-                completeTaskCloseClean(task);\n+                commitOffsetsOrTransaction(consumedOffsetsAndMetadataPerTask);\n             } catch (final RuntimeException e) {\n+                log.error(\"Exception caught while committing tasks during shutdown\", e);\n                 firstException.compareAndSet(null, e);\n-                closeTaskDirty(task);\n+\n+                // If the commit fails, everyone who participated in it must be closed dirty\n+                tasksToCloseDirty.addAll(filterActive(tasksToCommit));\n+                tasksToCloseClean.removeAll(filterActive(tasksToCommit));\n+                tasksToCommit.clear();\n+            }\n+\n+            for (final Task task : tasksToCommit) {", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzIzODcyNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443238726", "bodyText": "Regarding If any active tasks have to be closed dirty and can't be committed, none of them can be, is that only for eos-beta?", "author": "guozhangwang", "createdAt": "2020-06-21T17:10:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzc2MDYxOQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443760619", "bodyText": "I guess, technically it's only required for eos-beta, but we to avoid too many different cases, we might just want to do the same thing for all cases?", "author": "mjsax", "createdAt": "2020-06-22T18:52:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDk1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1MjE1NA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443852154", "bodyText": "Yeah I agree with Matthias, with eos-alpha we don't currently differentiate which tasks may have succeeded the commit and. which may have failed, so this would require a larger refactoring. My take is that it doesn't really matter much, since practically speaking it's unlikely that one task would successfully commit while other tasks would fail\n(ack on moving into the same try-catch)", "author": "ableegoldman", "createdAt": "2020-06-22T22:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MDk1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTEyMA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071120", "bodyText": "Good thinking!\nWe should mention RecordCollectorImpl as it's the class the relies on the error message.", "author": "mjsax", "createdAt": "2020-06-19T22:54:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTgzNQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071835", "bodyText": "Why do we need to set clientId?", "author": "mjsax", "createdAt": "2020-06-19T22:58:01Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NDEwNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443854106", "bodyText": "I guess we don't", "author": "ableegoldman", "createdAt": "2020-06-22T22:11:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTgzNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MTk3Mw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443071973", "bodyText": "Should we use try-with-resource? Or at least try-final and call producer.close in the finally block?", "author": "mjsax", "createdAt": "2020-06-19T22:58:54Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjExNQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443072115", "bodyText": "Do we need this?", "author": "mjsax", "createdAt": "2020-06-19T22:59:35Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());\n+\n+        final ProducerRecord<String, String> record = new ProducerRecord<>(SINGLE_PARTITION_INPUT_TOPIC, \"value\");\n+\n+        producer.initTransactions();\n+        producer.beginTransaction();\n+\n+        producer.send(record);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NDc1MA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443854750", "bodyText": "Nope, that just slipped in there", "author": "ableegoldman", "createdAt": "2020-06-22T22:13:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjExNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjMxOA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443072318", "bodyText": "Do we need to make them producer config changes to ensure that the producer does not flush the record by chance? (eg, increase linger.ms to MAX_VALUE or similar?)", "author": "mjsax", "createdAt": "2020-06-19T23:00:38Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/EosIntegrationTest.java", "diffHunk": "@@ -203,6 +208,34 @@ public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws\n         runSimpleCopyTest(1, MULTI_PARTITION_INPUT_TOPIC, MULTI_PARTITION_THROUGH_TOPIC, MULTI_PARTITION_OUTPUT_TOPIC, false, eosConfig);\n     }\n \n+    // This is technically a purely producer-client test, but since we're relying on the specific error message being\n+    // thrown we should make sure it can't change without us noticing. Once KAFKA-10186 is resolved we should fix this\n+    @Test\n+    public void testExceptionForPendingUnflushedDataWhenTransactionIsAborted()  {\n+        final Map<String, Object> configs = new HashMap<>();\n+        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());\n+        configs.put(\"client.id\", \"client-1\");\n+        configs.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, \"txnId\");\n+\n+        final KafkaProducer<String, String> producer =\n+            new KafkaProducer<>(configs, new StringSerializer(), new StringSerializer());\n+\n+        final ProducerRecord<String, String> record = new ProducerRecord<>(SINGLE_PARTITION_INPUT_TOPIC, \"value\");\n+\n+        producer.initTransactions();\n+        producer.beginTransaction();\n+\n+        producer.send(record);\n+\n+        final AtomicReference<Exception> receivedException = new AtomicReference<>(null);\n+        producer.send(record, (recordMetadata, exception) -> receivedException.compareAndSet(null, exception));", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NTk3Ng==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443855976", "bodyText": "Hm, yeah, good call. I didn't realize this defaults to 0", "author": "ableegoldman", "createdAt": "2020-06-22T22:16:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA3MjMxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NTYwMw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443855603", "bodyText": "nit: clean close", "author": "abbccdda", "createdAt": "2020-06-22T22:15:12Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -250,10 +250,26 @@ public void flush() {\n      * @throws TaskMigratedException recoverable error that would cause the task to be removed\n      */\n     @Override\n-    public void close() {\n-        log.info(\"Closing record collector\");\n+    public void closeClean() {\n+        log.info(\"Closing record collector clean\");\n+\n+        // No need to abort transaction during a clean  close: either we have successfully committed the ongoing", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzg1NTczMQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443855731", "bodyText": "nit: period at the end", "author": "abbccdda", "createdAt": "2020-06-22T22:15:34Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "diffHunk": "@@ -250,10 +250,26 @@ public void flush() {\n      * @throws TaskMigratedException recoverable error that would cause the task to be removed\n      */\n     @Override\n-    public void close() {\n-        log.info(\"Closing record collector\");\n+    public void closeClean() {\n+        log.info(\"Closing record collector clean\");\n+\n+        // No need to abort transaction during a clean  close: either we have successfully committed the ongoing\n+        // transaction during handleRevocation and thus there is no transaction in flight, or else none of the revoked\n+        // tasks had any data in the current transaction and therefore there is no need to commit or abort it", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDY0Ng==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443950646", "bodyText": "Saw this fail locally so just did a minor flaky test fix on the side", "author": "ableegoldman", "createdAt": "2020-06-23T04:12:27Z", "path": "streams/src/test/java/org/apache/kafka/streams/integration/RestoreIntegrationTest.java", "diffHunk": "@@ -359,7 +359,9 @@ public void shouldRecycleStateFromStandbyTaskPromotedToActiveTaskAndNotRestore()\n         waitForStandbyCompletion(client1, 1, 30 * 1000L);\n         waitForStandbyCompletion(client2, 1, 30 * 1000L);\n \n-        assertThat(CloseCountingInMemoryStore.numStoresClosed(), CoreMatchers.equalTo(0));\n+        // Sometimes the store happens to have already been closed sometime during startup, so just keep track\n+        // of where it started and make sure it doesn't happen more times from there\n+        final int initialStoreCloseCount = CloseCountingInMemoryStore.numStoresClosed();", "originalCommit": null, "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r443950906", "bodyText": "These tests used to rely on the fact that the sendException was never forgotten by just setting it once and then asserting that multiple subsequent calls also threw it. So now we need to call send before each to re-insert the exception", "author": "ableegoldman", "createdAt": "2020-06-23T04:13:37Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "diffHunk": "@@ -474,6 +492,7 @@ public void shouldThrowTaskMigratedExceptionOnSubsequentCallWhenProducerFencedIn\n                 \" indicating the task may be migrated out; it means all tasks belonging to this thread should be migrated.\")\n         );\n \n+        collector.send(topic, \"3\", \"0\", null, null, stringSerializer, stringSerializer, streamPartitioner);", "originalCommit": null, "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNjg0NA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444526844", "bodyText": "For those particular test, considering their names, it seem the tests are void now?", "author": "mjsax", "createdAt": "2020-06-23T21:45:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyODYyMg==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444528622", "bodyText": "Maybe I misinterpreted this, but I took the OnSubsequentCall in the name to mean that it would throw on the next (ie subsequent) call after the send, not that it would continue to throw on all subsequent calls. ie I think it should actually be several different tests (one for each \"call\" that should throw) but got mashed into just one", "author": "ableegoldman", "createdAt": "2020-06-23T21:49:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUzMDk4OA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444530988", "bodyText": "I guess I should just break these up into different tests then, huh. Will do", "author": "ableegoldman", "createdAt": "2020-06-23T21:55:03Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUzODcxMQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444538711", "bodyText": "Not 100% sure how OnSubsequentCall is meant either. But what you say seems to make sense and thus it should be different test. Thanks for the extra mile splitting them up!", "author": "mjsax", "createdAt": "2020-06-23T22:15:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mzk1MDkwNg=="}], "type": "inlineReview"}, {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "url": "https://github.com/apache/kafka/commit/4185c54b001ac381a8737683e8ae1a4aceb4a875", "message": "swallow this exception, dont abort during a clean close, and tests", "committedDate": "2020-06-23T17:08:01Z", "type": "commit"}, {"oid": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "url": "https://github.com/apache/kafka/commit/4185c54b001ac381a8737683e8ae1a4aceb4a875", "message": "swallow this exception, dont abort during a clean close, and tests", "committedDate": "2020-06-23T17:08:01Z", "type": "forcePushed"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNjg1OA==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444516858", "bodyText": "Should we fix this in older branches (2.5/2.4), too? (ie follow up PR)", "author": "mjsax", "createdAt": "2020-06-23T21:23:44Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "diffHunk": "@@ -152,9 +152,9 @@ public StreamsMetadata getLocalMetadata() {\n         }\n \n         if (globalStores.contains(storeName)) {\n-            // global stores are on every node. if we dont' have the host info\n+            // global stores are on every node. if we don't have the host info\n             // for this host then just pick the first metadata\n-            if (thisHost == UNKNOWN_HOST) {\n+            if (thisHost.equals(UNKNOWN_HOST)) {", "originalCommit": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyMjU0Mw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444522543", "bodyText": "Will do", "author": "ableegoldman", "createdAt": "2020-06-23T21:36:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNjg1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUzNzI5Nw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444537297", "bodyText": "#8919", "author": "ableegoldman", "createdAt": "2020-06-23T22:11:37Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNjg1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNzc0Ng==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444517746", "bodyText": "If tasksToCloseDirty is not empty, should we close dirty, too, ie pass in clean && tasksToCloseDirty.isEmpty() ?", "author": "mjsax", "createdAt": "2020-06-23T21:25:43Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,", "originalCommit": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUyNTk5Nw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444525997", "bodyText": "I think this is more in line with the general code flow elsewhere. Note that if we started out clean but became dirty and had to close some tasks as such, we would have already caught an exception somewhere. So AtomicReference#compareAndSet would be a no-op, and it actually doesn't matter what we pass in here", "author": "ableegoldman", "createdAt": "2020-06-23T21:44:07Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNzc0Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxNzkwNw==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444517907", "bodyText": "As above?", "author": "mjsax", "createdAt": "2020-06-23T21:26:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,\n+                () -> activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id()),\n+                e -> firstException.compareAndSet(null, e),\n+                e -> log.warn(\"Ignoring an exception while closing task \" + task.id() + \" producer.\", e)\n+            );\n+        }\n+\n+        executeAndMaybeSwallow(\n+            clean,", "originalCommit": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDUxODMyOQ==", "url": "https://github.com/apache/kafka/pull/8900#discussion_r444518329", "bodyText": "As above.", "author": "mjsax", "createdAt": "2020-06-23T21:26:57Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -679,92 +675,166 @@ private void cleanupTask(final Task task) {\n     void shutdown(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n \n-        final Set<Task> tasksToClose = new HashSet<>();\n+        final Set<Task> tasksToCloseDirty = new HashSet<>();\n+        tasksToCloseDirty.addAll(tryCloseCleanAllActiveTasks(clean, firstException));\n+        tasksToCloseDirty.addAll(tryCloseCleanAllStandbyTasks(clean, firstException));\n+\n+        for (final Task task : tasksToCloseDirty) {\n+            closeTaskDirty(task);\n+        }\n+\n+        for (final Task task : activeTaskIterable()) {\n+            executeAndMaybeSwallow(\n+                clean,\n+                () -> activeTaskCreator.closeAndRemoveTaskProducerIfNeeded(task.id()),\n+                e -> firstException.compareAndSet(null, e),\n+                e -> log.warn(\"Ignoring an exception while closing task \" + task.id() + \" producer.\", e)\n+            );\n+        }\n+\n+        executeAndMaybeSwallow(\n+            clean,\n+            activeTaskCreator::closeThreadProducerIfNeeded,\n+            e -> firstException.compareAndSet(null, e),\n+            e -> log.warn(\"Ignoring an exception while closing thread producer.\", e)\n+        );\n+\n+        tasks.clear();\n+\n+\n+        // this should be called after closing all tasks, to make sure we unlock the task dir for tasks that may\n+        // have still been in CREATED at the time of shutdown, since Task#close will not do so\n+        executeAndMaybeSwallow(\n+            clean,", "originalCommit": "4185c54b001ac381a8737683e8ae1a4aceb4a875", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "20321fd1325d402405a4b5899c22c83a51732b1c", "url": "https://github.com/apache/kafka/commit/20321fd1325d402405a4b5899c22c83a51732b1c", "message": "split tests up", "committedDate": "2020-06-23T22:08:59Z", "type": "commit"}]}