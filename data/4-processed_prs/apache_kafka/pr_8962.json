{"pr_number": 8962, "pr_title": "KAFKA-10166: checkpoint recycled standbys and ignore empty rocksdb base directory", "pr_createdAt": "2020-06-30T22:30:47Z", "pr_url": "https://github.com/apache/kafka/pull/8962", "timeline": [{"oid": "14290ff9225571028d70b9cb596a943304e76b3c", "url": "https://github.com/apache/kafka/commit/14290ff9225571028d70b9cb596a943304e76b3c", "message": "write checkpoint for recycled standbys, clean up task producer", "committedDate": "2020-06-30T20:48:59Z", "type": "commit"}, {"oid": "e8791a4bcd89c4e63b564006780594dbb727d5b4", "url": "https://github.com/apache/kafka/commit/e8791a4bcd89c4e63b564006780594dbb727d5b4", "message": "fix stateDirEmpty condition", "committedDate": "2020-06-30T21:57:38Z", "type": "commit"}, {"oid": "47e1f4c86e44730416652ea5631f2da4e54e3c25", "url": "https://github.com/apache/kafka/commit/47e1f4c86e44730416652ea5631f2da4e54e3c25", "message": "remove unused import", "committedDate": "2020-06-30T21:59:21Z", "type": "commit"}, {"oid": "2c7a2b6e4ac2b94ca4192960dbfef71d8258c5ea", "url": "https://github.com/apache/kafka/commit/2c7a2b6e4ac2b94ca4192960dbfef71d8258c5ea", "message": "add unit test", "committedDate": "2020-06-30T22:12:43Z", "type": "commit"}, {"oid": "25e4429f11281c2cd24e7e6750c101ce1026a30d", "url": "https://github.com/apache/kafka/commit/25e4429f11281c2cd24e7e6750c101ce1026a30d", "message": "move cleanupTaskProducer", "committedDate": "2020-06-30T22:20:44Z", "type": "commit"}, {"oid": "89dd4bd12427cae711fce736053d56da66213b22", "url": "https://github.com/apache/kafka/commit/89dd4bd12427cae711fce736053d56da66213b22", "message": "improve emptiness check", "committedDate": "2020-06-30T22:56:45Z", "type": "commit"}, {"oid": "101aa1e304a3e13f09f9e60f741f37f0adafda2c", "url": "https://github.com/apache/kafka/commit/101aa1e304a3e13f09f9e60f741f37f0adafda2c", "message": "hack for rocksdb", "committedDate": "2020-06-30T23:51:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448044329", "bodyText": "This is admittedly quite hacky, and of course does not solve the problem for custom state stores that might write some non-data files upon open. A \"better\" fix would probably be to write some sentinel value in the checkpoint ala OFFSET_UNKNOWN, so we do have an entry in there if the store was opened but does not yet have any data.\nBut, I wanted to keep things simple (a very relative term here, I know) and low-risk before the 2.6 release. We can discuss better solutions once we're not at the doorstep of the release (and blocking the door, I might add)", "author": "ableegoldman", "createdAt": "2020-07-01T00:03:41Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java", "diffHunk": "@@ -136,7 +143,59 @@ private boolean taskDirEmpty(final File taskDir) {\n                 !pathname.getName().equals(CHECKPOINT_FILE_NAME));\n \n         // if the task is stateless, storeDirs would be null\n-        return storeDirs == null || storeDirs.length == 0;\n+        if (storeDirs == null || storeDirs.length == 0) {\n+            return true;\n+        }\n+\n+        final List<File> baseSubDirectories = new LinkedList<>();\n+        for (final File file : storeDirs) {\n+            if (file.isDirectory()) {\n+                baseSubDirectories.add(file);\n+            } else {\n+                return false;\n+            }\n+        }\n+\n+        for (final File dir : baseSubDirectories) {\n+            final boolean isEmpty;\n+            if (dir.getName().equals(ROCKSDB_DIRECTORY_NAME)) {\n+                isEmpty = taskSubDirectoriesEmpty(dir, true);\n+            } else {\n+                isEmpty =  taskSubDirectoriesEmpty(dir, false);\n+            }\n+            if (!isEmpty) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    // BFS through the task directory to look for any files that are not more subdirectories\n+    private boolean taskSubDirectoriesEmpty(final File baseDir, final boolean sstOnly) {\n+        final Queue<File> subDirectories = new LinkedList<>();\n+        subDirectories.offer(baseDir);\n+\n+        final Set<File> visited = new HashSet<>();\n+        while (!subDirectories.isEmpty()) {\n+            final File dir = subDirectories.poll();\n+            if (!visited.contains(dir)) {\n+                final  File[] files = dir.listFiles();\n+                if (files == null) {\n+                    continue;\n+                }\n+                for (final File file : files) {\n+                    if (file.isDirectory()) {\n+                        subDirectories.offer(file);\n+                    } else if (sstOnly && file.getName().endsWith(ROCKSDB_SST_SUFFIX)) {", "originalCommit": "101aa1e304a3e13f09f9e60f741f37f0adafda2c", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5NzY5MA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448597690", "bodyText": "Thanks for this. What's the impact of the exception that we're avoiding here? It might be better to just not do anything right now than to introduce assumptions about the implementation of the default persistent store implementation here. If those assumptions become false later, it could be pretty bad.", "author": "vvcephei", "createdAt": "2020-07-01T20:25:29Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY4MDU4Ng==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448680586", "bodyText": "Well, I did add a unit test with a real RocksDB store to verify, how could any change possibly slip past us \ud83d\ude09", "author": "ableegoldman", "createdAt": "2020-07-02T00:17:48Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY4MDY2Mg==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448680662", "bodyText": "But yeah: the extra TaskCorruptedException shouldn't have any correctness implications, and I don't think it will have any bad side effects at all except for confusing ourselves and users. If this seems too risky then we can absolutely take it out. It's just annoying since we pretty much always hit this on startup.\nFor example in the EosBetaUpgradeIntegrationTest (where I noticed this), we start a second client who then gets a standby while the first client revokes the active task. The second client initializes the standby but doesn't get to processing any data before the followup rebalance is triggered, where it receives the active task. So basically every task hits TaskCorruptedException. Of course this is an edge case where we happen to be within the acceptable recovery lag", "author": "ableegoldman", "createdAt": "2020-07-02T00:18:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcwMzcwOQ==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448703709", "bodyText": "Ok I discussed offline with Guozhang, we should go forward with the \"write sentinel values for unknown offset\" fix since it will also be needed for other work soon in 2.7. I'll remove this hacky emptiness check", "author": "ableegoldman", "createdAt": "2020-07-02T01:48:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcwNDc3OA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448704778", "bodyText": "Ok, sounds good. Thanks!", "author": "vvcephei", "createdAt": "2020-07-02T01:52:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA0NDMyOQ=="}], "type": "inlineReview"}, {"oid": "eb890804d38b163fb162a5ce5e44e293d5472065", "url": "https://github.com/apache/kafka/commit/eb890804d38b163fb162a5ce5e44e293d5472065", "message": "fix StateDirectory tests", "committedDate": "2020-07-01T17:05:11Z", "type": "commit"}, {"oid": "67ae92ff41b7ebb9e4bf65062ed0a02a0f5dfd69", "url": "https://github.com/apache/kafka/commit/67ae92ff41b7ebb9e4bf65062ed0a02a0f5dfd69", "message": "add realistic RocksDB test", "committedDate": "2020-07-01T17:18:13Z", "type": "commit"}, {"oid": "0e4e213414b0b406210ecc3bffa58f348c74d2d7", "url": "https://github.com/apache/kafka/commit/0e4e213414b0b406210ecc3bffa58f348c74d2d7", "message": "unused import", "committedDate": "2020-07-01T17:22:10Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448598854", "bodyText": "This comment makes me a bit twitchy. Can we assert it instead? I mention it because I assume that active tasks also don't need to be committed because it should have happened already. Can we assert that as well?", "author": "vvcephei", "createdAt": "2020-07-01T20:28:01Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "diffHunk": "@@ -270,8 +270,11 @@ public void handleAssignment(final Map<TaskId, Set<TopicPartition>> activeTasks,\n                 if (oldTask.isActive()) {\n                     final Set<TopicPartition> partitions = standbyTasksToCreate.remove(oldTask.id());\n                     newTask = standbyTaskCreator.createStandbyTaskFromActive((StreamTask) oldTask, partitions);\n+                    cleanUpTaskProducer(oldTask, taskCloseExceptions);\n                 } else {\n                     oldTask.suspend(); // Only need to suspend transitioning standbys, actives should be suspended already", "originalCommit": "0e4e213414b0b406210ecc3bffa58f348c74d2d7", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY4MTc5Nw==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448681797", "bodyText": "Just to be clear, we do assert/enforce this, but in the StreamTask and not in the TaskManager. At this point the TaskManager is actually completely agnostic to task state* and all assertions and branching based on state is internal to the Task implementation.\n*except for in handleRevocation, where I just noticed we still filter the commit based on state, which is now unnecessary", "author": "ableegoldman", "createdAt": "2020-07-02T00:22:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcwNjQxMg==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448706412", "bodyText": "Your words sound good to me, but they are directly in contradiction to the code here, which skips suspending and committing active tasks because it assumes they have already happened. In other words, there is an assumption here that the active tasks are in some kind of \"committed\" state, while the standbys are in either \"created\" or \"running\".\nIf we're going to have a branch that explicitly assumes the task is already committed, then I'd like to verify it, otherwise experience says it will become false after refactoring, and we'd wind up trying to track down an IllegalStateException later on.\nOn the other hand, if these transitions are idempotent, we can just include them in both branches (or rather move them outside the conditional block).", "author": "vvcephei", "createdAt": "2020-07-02T01:59:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxMjUzNg==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448712536", "bodyText": "While suspend is technically idempotent, the safer thing is actually to not also call it for active tasks here. But let me back up a bit and try to further clarify my earlier response. I realize I made this claim without any hint as to what specifically I was referring to:\nBy we do assert/enforce this, but in the StreamTask I meant that if we try to close a task without first suspending it, we will get an IllegalStateException in the Task#close method. The TaskManager is responsible for \"managing\" the operations on the task, but the task is ultimately responsible for verifying that its lifecycle makes sense and its state/transitions are valid. If we just blindly suspend everything here, it seems worse than getting an IllegalStateException because we lose the check that the task was prepared for close (ie suspended + committed) earlier during handleRevocation.\n\nI'd like to verify it, otherwise experience says it will become false after refactoring, and we'd wind up trying to track down an IllegalStateException later on\n\nOn that note, I'm slightly confused by your proposal:  what do you mean by \"verify\" if not to throw an IllegalStateException if it's not what's expected? Or do you just mean we'd have to track down an IllegalStateException slightly farther from the root cause", "author": "ableegoldman", "createdAt": "2020-07-02T02:24:55Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxNDU5MA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448714590", "bodyText": "Also, the commit case is trickier than the suspend as it's possible we actually don't commit a task before closing it, if the commit or any commit preparation fails. So we can't just assert that the task is committed.\nWe can assert that we attempted to commit it by keeping track of tasks we tried to commit/revoke between handleRevocation and handleAssignment, but I'm quite confident that would quickly get out of control.\nWe could introduce a #commitAttempted method on the Task but that also seems to invite bugs\nWe could leave it up to the Task to make sure everything is done safely during the close procedure. What the task currently does is verify that no commit is needed if a clean close is attempted -- if we try to close clean but a commit is still needed, it means the commit failed, and we can throw an exception to force the TM to closeDirty", "author": "ableegoldman", "createdAt": "2020-07-02T02:32:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTc5OTExMA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r449799110", "bodyText": "I think @vvcephei 's confusion comes from a change that we now require all tasks to be transited to suspended before transiting to close: previously, we allow e.g. a running task to be closed immediately and inside the task itself the logic actually did the \"running -> suspend -> close\" logic, i.e. it is totally agnostic to the TM. In a refactoring with eos-beta we changed it. So now the responsibility is kinda split between the two: TM needs to make sure the transition is valid and the task verifies it. By doing this we avoided the \"pre-/post-\" functions.", "author": "guozhangwang", "createdAt": "2020-07-04T19:27:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDM3NjAyNg==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r450376026", "bodyText": "Hi all, thanks for the discussion.\nTo start at the beginning, yes, I was advocating for throwing an IllegalStateException at the earliest possible moment when we can detect the illegal state. Right here in the code, we are making an invalid assumption. Namely, that if a task to be recycled is active, then it has already been suspended and committed, and if it is a standby, then it still needs to be suspended and committed. Why should this be true? Because some other code deep inside another set of nested conditionals thirty lines above looks like it does that right now? Experience says that this situation will not survive refactoring. We cannot test every branch, so we need to make assertions about the state being valid when it's in doubt.\nWe could make an argument that if this assumption becomes incorrect, than we'll throw an exception later on before any state becomes corrupted, which would be good. We could make a stronger argument that the exception we throw later on will be perfectly crystal clear about the cause and therefore we won't spend weeks poking around a flaky test or a user bug report trying to figure out what happened. But both of those arguments would depend on even further assumptions about stuff that may or may not happen elsewhere in the code base. The best thing to do at all times is validate potentially dangerous assumptions. This looks very much like a potentially dangerous assumption. I'm asking that we validate it.", "author": "vvcephei", "createdAt": "2020-07-06T17:32:12Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQyOTY1NQ==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r450429655", "bodyText": "I won't hold up this PR on the point, but I just don't see how checking the state of the task here and throwing an exception if it's not in suspended at this random point in the TM code is any more clear than checking the state in the task and throwing an exception if it's not in suspended in Task#close\nThe latter makes it very clear what the problem is -- tasks need to be suspended before they are closed, end of story. The former seems to enforce suspension at an arbitrary point in the TM.\n\nWhy should this be true? Because some other code deep inside another set of nested conditionals thirty lines above looks like it does that right now\n\nThis makes me wonder if I'm misinterpreting you, though, at least in part. Are you saying that it's fine to make this assumption thirty lines up when dealing with tasks that are actually being closed, but it's not ok to make this assumption here specifically when we are recycling tasks? Or are you saying we should add this check in both places where we make this assumption in TM#handleAssignment, and referring to where we do the suspension by \"thirty lines up\" (it's technically down, so maybe I'm being too literal here)\nI guess my point is just that I don't agree with the claim that we're making a \"potentially dangerous assumption\". We do check the assumption very directly when closing the task.", "author": "ableegoldman", "createdAt": "2020-07-06T19:18:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDQ3MTcwMQ==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r450471701", "bodyText": "Hey folks, taking a step back here regarding this, here are a few different principles we can exercise regarding the class hierarchy:\n\nTM to be totally agnostic to the task's state, and the task's method would transit multiple state if necessary: e.g. if TM calls close on a running task, then task would first transit to suspended, and then to closed; if TM calls suspend on a suspended task, then it would be a no-op. In this case, then here TM could just blindly call close or suspend for closing / recycling tasks, and the Task's methods would handle state transition.\n\nThis would be ideal, but the current issue is that we need to commit post suspending a task today (i.e. handleRevocation would commit those suspended active tasks, while handleAssignment would commit those suspended standby tasks), and hence if we blindly suspend and commit a task, we may unnecessarily double committing a suspended task.\nRight now to work around this issue, we choose to:\n\nLetting TM to be aware of the task's state transition rules and tries to obey it. Similarly, we can also let TM to check the task's state, and then call suspend and commit conditionally -- at the moment, only standby tasks would not be suspended, so we would effectively end up the same logic as to only suspend standby tasks. So either checking task.isActive or task.isSuspended would work the same.\n\nPersonally I think if we want the TM to be purely agnostic to task state transition, an alternative approach could be, that we make preCommit / postCommit to be \"idempotent\" as well, e.g. if the task remembers since last preCommit we do not have processed any records, then a second call to preCommit would be a no-op and returns empty map, and similarly postCommit can also be a no-op if there's no processed records since its last checkpointing. And then in TM, we can blindly call:\ntask.suspend();\naggregatOffsets(task.prepreCommit());\nif (!offsetsMap.isEmpty()) call sendOffsets();\npostOffsets(task.postCommit());\n\nif (isActive()) {\n   recycle as standby;\n} else {\n   recycle as active;\n}\n\nAnd similarly for tasksToClose we can blindly call suspend / preCommit / postCommit first. WDYT?\nAnyways, I think this discussion can continue in a follow-up PR (maybe I can incorporate whatever we've agreed on in the decoupling flushing/committing PR) and we can merge this one as-is.", "author": "guozhangwang", "createdAt": "2020-07-06T20:48:21Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDUxMzI4OA==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r450513288", "bodyText": "Thanks, all!\nThanks for adding the check, @ableegoldman . To answer your question, I do think there may be some kind of misunderstanding. Almost certainly, I failed to make myself clear.\nI agree that the reason we enforce the valid transitions inside the task is so that the TM doesn't have to check every little thing. For example, that we call suspend in one place and close in another is not a big deal. As you pointed out, if we failed to call suspend, then the close call would let us know.\nHowever, in this case, we are checking something. But what we're checking is only vaguely related to what we do next. In other words, if (! isSuspended() ) suspend() makes way more sense than if ( isStandby() ) suspend(). How are we supposed to know that standby == not suspended at line 275 of a 700-line class? At least, with the log you added, if any quadrant of the implication is false, we'll have an error log telling us where it went wrong.\nNote, I think I was especially uneasy because we're now also committing the task in this block, and \"committed\" or \"not committed\" isn't a checked state, which is how we wound up with the subtle bug that you're fixing here. I think your fix is fine; I just felt the pre-existing structure of the code needed improvement.\nAnd thanks for the alternative proposals, @guozhangwang . I agree that either one of them would resolve my concern, and that we can take care of it in a follow-on PR.", "author": "vvcephei", "createdAt": "2020-07-06T22:32:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU5ODg1NA=="}], "type": "inlineReview"}, {"oid": "f314c79d3902c57e1ddf2e40c00deaa3c6254c03", "url": "https://github.com/apache/kafka/commit/f314c79d3902c57e1ddf2e40c00deaa3c6254c03", "message": "go with different approach; sentinel values in checkpoint", "committedDate": "2020-07-02T02:33:27Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODcxNTI1OQ==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r448715259", "bodyText": "This is a little awkward, and it might be cleaner to just replace the use of null with this OFFSET_UNKNOWN sentinel throughout the ProcessorStateManager/StoreChangelogReader -- but, I wanted to keep the changes as short and simple as possible for now", "author": "ableegoldman", "createdAt": "2020-07-02T02:35:51Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -578,4 +580,14 @@ private StateStoreMetadata findStore(final TopicPartition changelogPartition) {\n \n         return found.isEmpty() ? null : found.get(0);\n     }\n+\n+    // Pass in a sentinel value to checkpoint when the changelog offset is not yet initialized/known\n+    private long checkpointableOffsetFromChangelogOffset(final Long offset) {\n+        return offset != null ? offset : OFFSET_UNKNOWN;", "originalCommit": "f314c79d3902c57e1ddf2e40c00deaa3c6254c03", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "402b91b74351a259d76c015978b97a3871065bb6", "url": "https://github.com/apache/kafka/commit/402b91b74351a259d76c015978b97a3871065bb6", "message": "remove extra space", "committedDate": "2020-07-02T02:36:13Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIyNzczMg==", "url": "https://github.com/apache/kafka/pull/8962#discussion_r449227732", "bodyText": "This is kind of awkward, we forgot actually read the checkpoint here \ud83e\udd26\u200d\u2640\ufe0f", "author": "ableegoldman", "createdAt": "2020-07-02T19:34:03Z", "path": "streams/src/test/java/org/apache/kafka/streams/state/internals/OffsetCheckpointTest.java", "diffHunk": "@@ -91,20 +94,38 @@ public void shouldSkipNegativeOffsetsDuringRead() throws IOException {\n             offsets.put(new TopicPartition(topic, 0), -1L);\n \n             writeVersion0(offsets, file);\n+            assertTrue(checkpoint.read().isEmpty());", "originalCommit": "402b91b74351a259d76c015978b97a3871065bb6", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "ca3274ffc9a1973ed7a4977889e257cc8fc8b67e", "url": "https://github.com/apache/kafka/commit/ca3274ffc9a1973ed7a4977889e257cc8fc8b67e", "message": "add check", "committedDate": "2020-07-06T19:48:24Z", "type": "commit"}]}