{"pr_number": 8996, "pr_title": "KAFKA-10249: don't try to read un-checkpointed offsets of in-memory stores", "pr_createdAt": "2020-07-08T21:56:17Z", "pr_url": "https://github.com/apache/kafka/pull/8996", "timeline": [{"oid": "cb07cfc1f2fbe7a35f3ba1e8531b1d959930ff8b", "url": "https://github.com/apache/kafka/commit/cb07cfc1f2fbe7a35f3ba1e8531b1d959930ff8b", "message": "check forin-memory stores", "committedDate": "2020-07-08T21:19:10Z", "type": "commit"}, {"oid": "c209e8f9deaab42f1687e5b5ad6f1540322d4517", "url": "https://github.com/apache/kafka/commit/c209e8f9deaab42f1687e5b5ad6f1540322d4517", "message": "fix and  add tests", "committedDate": "2020-07-08T21:49:19Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451899726", "bodyText": "Don't we need to call store.setOffset(null) or this case?", "author": "mjsax", "createdAt": "2020-07-09T00:39:06Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -224,6 +224,9 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n             for (final StateStoreMetadata store : stores.values()) {\n                 if (store.changelogPartition == null) {\n                     log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else if (!store.stateStore.persistent()) {\n+                    log.info(\"Initializing to the starting offset for changelog {} of in-memory state store {}\",\n+                             store.changelogPartition, store.stateStore.name());", "originalCommit": "c209e8f9deaab42f1687e5b5ad6f1540322d4517", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMDY4Ng==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451900686", "bodyText": "Just wondering about https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L769-L802 (can't comment below).\nFor this case, if should hold that store.offset() == changelogOffsetFromCheckpointedOffset(loadedCheckpoints.remove(store.changelogPartition)) ?\nOr maybe >=?\nShould we add a sanity check? (Not related to this PR itself actually. -- Just wondering.)", "author": "mjsax", "createdAt": "2020-07-09T00:42:50Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyMjA0MA==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451922040", "bodyText": "Don't we need to call store.setOffset(null) or this case?\n\nWell, either it's a recycled task in which case no, we don't want to wipe out the existing offset, or it's a new task in which case it's initialized to null anyway.\nI'm also not sure what you mean in the second comment. Did you maybe paste the link to the wrong code? (just guessing since you linked to that same code earlier in John's PR)", "author": "ableegoldman", "createdAt": "2020-07-09T02:04:32Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNjY2NQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451926665", "bodyText": "Ups. Wrong link. This one: https://github.com/apache/kafka/pull/8996/files#diff-cc98a6c20f2a8483e1849aea6921c34dL251-L255", "author": "mjsax", "createdAt": "2020-07-09T02:22:06Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNzA5Nw==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451927097", "bodyText": "Well, either it's a recycled task in which case no,\n\nSo a active->standby or standby->active conversion. I was wondering about corrupted tasks? So we don't call initializeStoreOffsetsFromCheckpoint when reviving a corrupted task?", "author": "mjsax", "createdAt": "2020-07-09T02:23:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTk0NTQ5NQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451945495", "bodyText": "No, because we close the state manager completely and clear all this data before reviving a corrupted task.\nI've had to re-convince myself of this several times already. Maybe we can also add a check that none of them are marked corrupted while initializing offsets", "author": "ableegoldman", "createdAt": "2020-07-09T03:38:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTg5OTcyNg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMTk3NQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451901975", "bodyText": "Not sure if I understand the test: don't we write a checkpoint in L795 for the persistent store?", "author": "mjsax", "createdAt": "2020-07-09T00:47:55Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "diffHunk": "@@ -784,7 +784,7 @@ public void close() {\n     }\n \n     @Test\n-    public void shouldThrowTaskCorruptedWithoutCheckpointNonEmptyDir() throws IOException {\n+    public void shouldThrowTaskCorruptedWithoutPersistentStoreCheckpointAndNonEmptyDir() throws IOException {\n         final long checkpointOffset = 10L;\n \n         final Map<TopicPartition, Long> offsets = mkMap(", "originalCommit": "c209e8f9deaab42f1687e5b5ad6f1540322d4517", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyMjQyMQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451922421", "bodyText": "Yeah, I think that's the point of the test: we wrote a checkpoint but it was missing the offset for one of the persistent stores, thus, we should throw a TaskCorruptedException  in initializeStoreOffsetsFromCheckpoint", "author": "ableegoldman", "createdAt": "2020-07-09T02:05:51Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMTk3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyODE3MA==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451928170", "bodyText": "Not clear from the context of the code snipped (maybe need to go back to IntelliJ and read the full test setup) that there are multiple partitions. Also wondering why we include nonPersistentStorePartition and irrelevantPartition? How do we ensure that those don't case the TaskCorruptedException?", "author": "mjsax", "createdAt": "2020-07-09T02:27:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMTk3NQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM3MjM1Nw==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452372357", "bodyText": "Sorry yeah the relevant part doesn't show up on github. Basically we register\nstateMgr.registerStore(persistentStore, persistentStore.stateRestoreCallback);\t            \nstateMgr.registerStore(persistentStoreTwo, persistentStoreTwo.stateRestoreCallback);\t            \nstateMgr.registerStore(nonPersistentStore, nonPersistentStore.stateRestoreCallback);\n\nbut only write the checkpoint for the persistentStorePartition, nonPersistentStorePartition and irrelevantPartition . I think the point of the irrelevantPartition is to make sure that we detect that the persistentStoreTwoPartition offset is missing even though the checkpoint technically has the correct number of offsets in total. ie, that we actually map the offsets to a registered changelog", "author": "ableegoldman", "createdAt": "2020-07-09T17:19:14Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMTk3NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkwMjEzMQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451902131", "bodyText": "WithWithout ?", "author": "mjsax", "createdAt": "2020-07-09T00:48:35Z", "path": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "diffHunk": "@@ -813,6 +813,62 @@ public void shouldThrowTaskCorruptedWithoutCheckpointNonEmptyDir() throws IOExce\n         }\n     }\n \n+    @Test\n+    public void shouldNotThrowTaskCorruptedWithWithoutInMemoryStoreCheckpointAndNonEmptyDir() throws IOException {", "originalCommit": "c209e8f9deaab42f1687e5b5ad6f1540322d4517", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "0556668161951b51a7652d6916c44b92ac8bed66", "url": "https://github.com/apache/kafka/commit/0556668161951b51a7652d6916c44b92ac8bed66", "message": "fix extra 'With' in test name", "committedDate": "2020-07-09T02:07:21Z", "type": "commit"}, {"oid": "e272df4f05a2e33199a9999dda8546231d171653", "url": "https://github.com/apache/kafka/commit/e272df4f05a2e33199a9999dda8546231d171653", "message": "always wipe store on dirty close", "committedDate": "2020-07-09T02:12:44Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNTYyNg==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451925626", "bodyText": "Seems weird to skip deleting the dirty state on a dirty close just because we caught an exception while closing the state (probably in most cases a dirty close would mean something would throw, for example calling flush when we've dropped out of the group)", "author": "ableegoldman", "createdAt": "2020-07-09T02:17:58Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -104,17 +104,16 @@ static void closeStateManager(final Logger log,\n             if (stateDirectory.lock(id)) {\n                 try {\n                     stateMgr.close();\n-\n+                } catch (final ProcessorStateException e) {\n+                    firstException.compareAndSet(null, e);\n+                } finally {\n                     if (wipeStateStore) {\n                         log.debug(\"Wiping state stores for {} task {}\", taskType, id);\n                         // we can just delete the whole dir of the task, including the state store images and the checkpoint files,\n                         // and then we write an empty checkpoint file indicating that the previous close is graceful and we just\n                         // need to re-bootstrap the restoration from the beginning\n                         Utils.delete(stateMgr.baseDir());", "originalCommit": "e272df4f05a2e33199a9999dda8546231d171653", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNzcxNA==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r451927714", "bodyText": "Well, it could be some other issue and we would wipe the state on \"resuming\" the task anyway. Not sure if there are other things to consider? \\cc @guozhangwang", "author": "mjsax", "createdAt": "2020-07-09T02:26:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNTYyNg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjM3MDUxNw==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452370517", "bodyText": "Right, it's not a correctness issue but it's additional needless overhead to go through the whole cycle of initializing a task, getting a TaskCorrupted, wiping it then, and finally restarting it. Of course if we keep hitting an issue during closeDirty then we might never wipe the state, which does seem like a real problem. For example if there's some issue with the state, like the files are actually corrupted or something", "author": "ableegoldman", "createdAt": "2020-07-09T17:15:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTkyNTYyNg=="}], "type": "inlineReview"}, {"oid": "298cf5dcf8752fb887bc025926dc5ebd3a9f255c", "url": "https://github.com/apache/kafka/commit/298cf5dcf8752fb887bc025926dc5ebd3a9f255c", "message": "adding StateManagerUtil test", "committedDate": "2020-07-09T17:29:52Z", "type": "commit"}, {"oid": "6f6db6d6dea1a2164ee59895e46ccc268e6a477d", "url": "https://github.com/apache/kafka/commit/6f6db6d6dea1a2164ee59895e46ccc268e6a477d", "message": "add PSM test", "committedDate": "2020-07-09T17:32:31Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMDc3MQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452430771", "bodyText": "I think we should also remove the changelogPartition from loadedCheckpoints, if it exists. Otherwise, we'll spuriously warn in L267.", "author": "vvcephei", "createdAt": "2020-07-09T19:07:04Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "diffHunk": "@@ -222,8 +222,16 @@ void initializeStoreOffsetsFromCheckpoint(final boolean storeDirIsEmpty) {\n             log.trace(\"Loaded offsets from the checkpoint file: {}\", loadedCheckpoints);\n \n             for (final StateStoreMetadata store : stores.values()) {\n+                if (store.corrupted) {\n+                    log.error(\"Tried to initialize store offsets for corrupted store {}\", store);\n+                    throw new IllegalStateException(\"Should not initialize offsets for a corrupted task\");\n+                }\n+\n                 if (store.changelogPartition == null) {\n                     log.info(\"State store {} is not logged and hence would not be restored\", store.stateStore.name());\n+                } else if (!store.stateStore.persistent()) {", "originalCommit": "6f6db6d6dea1a2164ee59895e46ccc268e6a477d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ4MzUyNQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452483525", "bodyText": "I'm not sure I'd call it a spurious warning -- if we don't expect to have checkpointed in-memory stores, and we happen to have an offset for one in the checkpoint file, it seems reasonable to log a warning", "author": "ableegoldman", "createdAt": "2020-07-09T20:53:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMDc3MQ=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ4NzA3NQ==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452487075", "bodyText": "Fair enough.", "author": "vvcephei", "createdAt": "2020-07-09T21:00:01Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMDc3MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMzE4OA==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452433188", "bodyText": "I take it this block can also throw an exception? We shouldn't throw exceptions inside a finally block because it's not defined when the exception will be thrown, or in the case where the first try block threw, which exception is ultimately thrown is also undefined.\nTo make this simpler to grapple with, we added org.apache.kafka.streams.state.internals.ExceptionUtils#executeAll", "author": "vvcephei", "createdAt": "2020-07-09T19:11:58Z", "path": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StateManagerUtil.java", "diffHunk": "@@ -104,26 +104,27 @@ static void closeStateManager(final Logger log,\n             if (stateDirectory.lock(id)) {\n                 try {\n                     stateMgr.close();\n-\n-                    if (wipeStateStore) {\n-                        log.debug(\"Wiping state stores for {} task {}\", taskType, id);\n-                        // we can just delete the whole dir of the task, including the state store images and the checkpoint files,\n-                        // and then we write an empty checkpoint file indicating that the previous close is graceful and we just\n-                        // need to re-bootstrap the restoration from the beginning\n-                        Utils.delete(stateMgr.baseDir());\n-                    }\n                 } catch (final ProcessorStateException e) {\n                     firstException.compareAndSet(null, e);\n                 } finally {\n-                    stateDirectory.unlock(id);\n+                    try {\n+                        if (wipeStateStore) {\n+                            log.debug(\"Wiping state stores for {} task {}\", taskType, id);\n+                            // we can just delete the whole dir of the task, including the state store images and the checkpoint files,\n+                            // and then we write an empty checkpoint file indicating that the previous close is graceful and we just\n+                            // need to re-bootstrap the restoration from the beginning\n+                            Utils.delete(stateMgr.baseDir());\n+                        }\n+                    } finally {\n+                        stateDirectory.unlock(id);\n+                    }", "originalCommit": "6f6db6d6dea1a2164ee59895e46ccc268e6a477d", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ3ODg4Mg==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452478882", "bodyText": "Well, I figured it didn't matter since these both just throw IOException which we catch in the outer block. The point was to make sure we unlock it. But I'll check out ExceptionUtils#executeAll", "author": "ableegoldman", "createdAt": "2020-07-09T20:43:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMzE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ5MTE3Mw==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452491173", "bodyText": "I can't use ExceptionUtils#executeAll because the compiler complains that we don't handle the IOException unless we surround each Runnable with its own try-catch block, at which point #executeAll isn't really doing anything", "author": "ableegoldman", "createdAt": "2020-07-09T21:08:30Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMzE4OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQ5NDA3Mw==", "url": "https://github.com/apache/kafka/pull/8996#discussion_r452494073", "bodyText": "IMHO, the code is good as-is.\nThanks for rewriting to a nested try-final structure!", "author": "mjsax", "createdAt": "2020-07-09T21:14:44Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjQzMzE4OA=="}], "type": "inlineReview"}]}