{"pr_number": 9100, "pr_title": "Add AlterISR RPC and use it for ISR modifications", "pr_createdAt": "2020-07-29T21:05:27Z", "pr_url": "https://github.com/apache/kafka/pull/9100", "timeline": [{"oid": "37f7c63a47da0e5524b43e521ff4d37864073ad2", "url": "https://github.com/apache/kafka/commit/37f7c63a47da0e5524b43e521ff4d37864073ad2", "message": "Checking in RPCs and stubbing out basic functionality", "committedDate": "2020-07-29T21:04:11Z", "type": "commit"}, {"oid": "19dd93af3b7b08c5b9e5a32231c462738ef42e54", "url": "https://github.com/apache/kafka/commit/19dd93af3b7b08c5b9e5a32231c462738ef42e54", "message": "Update the ISR from the controller", "committedDate": "2020-07-30T00:44:57Z", "type": "commit"}, {"oid": "5482529f542554e7468965d6a6147b6cfc0cde2a", "url": "https://github.com/apache/kafka/commit/5482529f542554e7468965d6a6147b6cfc0cde2a", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-07-30T13:47:27Z", "type": "commit"}, {"oid": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "url": "https://github.com/apache/kafka/commit/6fb2551366c4c3baf89956357a838cea5bb43e4a", "message": "Start working on broker-side of AlterIsr", "committedDate": "2020-07-31T18:23:49Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwNTI3Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464705276", "bodyText": "Probably reasonable to handle it the same way other inter-broker RPCs are handled.", "author": "hachikuji", "createdAt": "2020-08-03T23:00:19Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -2956,6 +2956,22 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleAlterIsrRequest(request: RequestChannel.Request): Unit = {\n+    val alterIsrRequest = request.body[AlterIsrRequest]\n+\n+    authorizeClusterOperation(request, CLUSTER_ACTION);\n+\n+    // TODO do we need throttling for this response?", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwODMwMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464708303", "bodyText": "Good question. Might be fair to assume the controller is correct and use STALE_BROKER_EPOCH. Once kip-500 is all done, it would be totally fair since the controller will be guaranteed to have the latest state. The other question is what the broker should do if it sees STALE_BROKER_EPOCH...", "author": "hachikuji", "createdAt": "2020-08-03T23:10:04Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1771,6 +1775,127 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  // TODO is it okay to pull message classes down into the controller?\n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    //val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(alterIsrRequest.brokerId())\n+    /*if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker ${alterIsrRequest.brokerId()}\")\n+      // TODO is INVALID_REQUEST a reasonable error here?", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwOTM0MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464709341", "bodyText": "Hmm.. This adds a delay of 2.5s to every ISR change, which is a bit annoying. I guess the point is to allow batching? I think a better approach might be to send requests immediately on arrival, but set a limit on the maximum number of in-flight requests (maybe just 1) and let the changes accumulate when there is a request in-flight. Then we can still get a big batching benefit when there are a large number of ISR changes that need to be sent in a hurry.", "author": "hachikuji", "createdAt": "2020-08-03T23:13:24Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,121 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.TimeUnit\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates += alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    scheduler.schedule(\"alter-isr-send\", maybePropagateIsrChanges _, period = 2500L, unit = TimeUnit.MILLISECONDS)", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNjA0MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464716040", "bodyText": "That makes sense. I'll change that (this was pulled in from the previous ISR notification code in ReplicaManager)", "author": "mumrah", "createdAt": "2020-08-03T23:35:25Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcwOTM0MQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464710258", "bodyText": "Hmm.. Not sure it's worth doing these validations up-front. These checks could fail between the time that the event is enqueued and the time it is processed.", "author": "hachikuji", "createdAt": "2020-08-03T23:16:19Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1771,6 +1775,127 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  // TODO is it okay to pull message classes down into the controller?\n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    //val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(alterIsrRequest.brokerId())\n+    /*if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker ${alterIsrRequest.brokerId()}\")\n+      // TODO is INVALID_REQUEST a reasonable error here?\n+      callback.apply(new AlterIsrResponseData().setErrorCode(Errors.INVALID_REQUEST.code))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(alterIsrRequest.brokerEpoch())) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch ${alterIsrRequest.brokerEpoch()} for broker ${alterIsrRequest.brokerId()}\")\n+      callback.apply(new AlterIsrResponseData().setErrorCode(Errors.STALE_BROKER_EPOCH.code))\n+      return\n+    }*/\n+\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    val resp = new AlterIsrResponseData()\n+    resp.setTopics(new util.ArrayList())\n+\n+    alterIsrRequest.topics().forEach(topicReq => {\n+      val topicResp = new AlterIsrResponseTopics()\n+        .setName(topicReq.name())\n+        .setPartitions(new util.ArrayList())\n+      resp.topics().add(topicResp)\n+\n+      topicReq.partitions().forEach(partitionReq => {\n+        val partitionResp = new AlterIsrResponsePartitions()\n+          .setPartitionIndex(partitionReq.partitionIndex())\n+        topicResp.partitions().add(partitionResp)\n+\n+        // For each partition who's ISR we are altering, let's do some upfront validation for the broker response", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxNzE3MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464717171", "bodyText": "The main rationale for validating in the request handler is so we can return meaningful partition-level errors to the broker (fenced leader, not leader or follower, etc). Although, I'm not sure the broker could do anything useful with these errors since it probably has stale metadata in these cases.\nThe KIP calls out four partition-level errors. Do we actually need them?", "author": "mumrah", "createdAt": "2020-08-03T23:39:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxODgxOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r464718819", "bodyText": "To be clear, I'm not questioning the need for the validation, just the fact that it is done before enqueueing the event instead of when the event is processed.", "author": "hachikuji", "createdAt": "2020-08-03T23:44:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTAyOTM1Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465029352", "bodyText": "Ah, I see what you mean. Initially, I was concerned about blocking for too long while waiting for a response, but it looks like there is precedent for this pattern for some requests (reassignment, leader election, controlled shutdown). I'll move this validation and the callback down into the event processor method", "author": "mumrah", "createdAt": "2020-08-04T12:56:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcxMTk2Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465711962", "bodyText": "This is also a concurrency bug since you can't access stuff like the controllerContext except from the controller thread itself (it would be multi-threaded access without a lock)", "author": "cmccabe", "createdAt": "2020-08-05T13:06:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0MzkyMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465743922", "bodyText": "@cmccabe good to know about controllerContext \ud83d\udc4d", "author": "mumrah", "createdAt": "2020-08-05T13:54:54Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDcxMDI1OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcwODgzNw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465708837", "bodyText": "It seems like we need to set the inSyncReplicaIds here, since we don't do it in shrinkIsr.", "author": "cmccabe", "createdAt": "2020-08-05T13:00:48Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1212,17 +1221,32 @@ class Partition(val topicPartition: TopicPartition,\n \n   private def expandIsr(newIsr: Set[Int]): Unit = {\n     val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n-    val zkVersionOpt = stateStore.expandIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    //val zkVersionOpt = stateStore.expandIsr(controllerEpoch, newLeaderAndIsr)\n+    //maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    alterIsrChannelManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, handleIsrUpdate))\n+    alterIsrInFlight = true\n+\n+    info(\"ISR updated to [%s]\".format(newIsr.mkString(\",\")))\n+    inSyncReplicaIds = newIsr\n   }\n \n   private[cluster] def shrinkIsr(newIsr: Set[Int]): Unit = {\n     val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n-    val zkVersionOpt = stateStore.shrinkIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    //val zkVersionOpt = stateStore.shrinkIsr(controllerEpoch, newLeaderAndIsr)\n+    //maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    alterIsrChannelManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, handleIsrUpdate))\n+    alterIsrInFlight = true\n   }\n \n-  private[cluster] def maybeUpdateIsrAndVersion(isr: Set[Int], zkVersionOpt: Option[Int]): Unit = {\n+  private def handleIsrUpdate(error: Errors): Unit = {\n+    error match {\n+      case NONE => println(s\"Controller accepted ISR for $topicPartition\")\n+      case _ => println(s\"Had an error sending ISR for $topicPartition : $error\")\n+    }\n+    alterIsrInFlight = false\n+  }", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcxMjk0Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465712943", "bodyText": "This also needs to call NetworkClient#wake in case we are blocking inside NetworkClient#poll", "author": "cmccabe", "createdAt": "2020-08-05T13:07:48Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManager.scala", "diffHunk": "@@ -113,8 +113,8 @@ class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache\n       brokerToControllerListenerName, time, threadName)\n   }\n \n-  private[server] def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n-                                  callback: RequestCompletionHandler): Unit = {\n+  def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                  callback: RequestCompletionHandler): Unit = {\n     requestQueue.put(BrokerToControllerQueueItem(request, callback))", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcxNTc3MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465715770", "bodyText": "It would be good to find a better name for this.  When I read \"AlterIsrChannelManager\" I assumed it had its own separate channel, rather than using the ControllerChannelManager.", "author": "cmccabe", "createdAt": "2020-08-05T13:12:19Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,121 @@\n+package kafka.server", "originalCommit": "6fb2551366c4c3baf89956357a838cea5bb43e4a", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0ODUwNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465748504", "bodyText": "Yea, maybe just \"AlterIsrManager\"?", "author": "mumrah", "createdAt": "2020-08-05T14:01:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTcxNTc3MA=="}], "type": "inlineReview"}, {"oid": "5955b1d7bab29729b3daed23dfbc7c132c4baba3", "url": "https://github.com/apache/kafka/commit/5955b1d7bab29729b3daed23dfbc7c132c4baba3", "message": "Add throttle time to AlterIsr, fix error counts", "committedDate": "2020-08-05T13:45:59Z", "type": "commit"}, {"oid": "afeb18ede91081e9ea72a3f34a4e56e21f81e1cf", "url": "https://github.com/apache/kafka/commit/afeb18ede91081e9ea72a3f34a4e56e21f81e1cf", "message": "Broker to controller AlterIsr fixes\n\n* Make AlterIsr block until Controller process it fully\n* Batch AlterIsr requests for a short while on broker side\n* Don't attempt to restrict to a single in-flight request", "committedDate": "2020-08-05T13:47:14Z", "type": "commit"}, {"oid": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "url": "https://github.com/apache/kafka/commit/627b26aef3c623c13066a5f7e617318a49cc0a5b", "message": "Add \"effective\" ISR to Partition", "committedDate": "2020-08-05T13:52:04Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NjE2Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465746162", "bodyText": "It might be simpler just to use AlterIsrRequestData and AlterIsrResponseData throughout this code (rather than converting to Map[TopicPartition, LeaderAndIsr] and Map[TopicPartition, Errors])", "author": "mumrah", "createdAt": "2020-08-05T13:58:00Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1771,6 +1776,141 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzgxNzYyNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477817624", "bodyText": "The conversion logic is a tad annoying, but it makes the rest of the code nicer. I'm ok with it. That said, could we use scala conventions, e.g.:\n    alterIsrRequest.topics.forEach { topicReq => \n      topicReq.partitions.forEach { partitionReq =>", "author": "hachikuji", "createdAt": "2020-08-27T00:52:47Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0NjE2Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0ODEyMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465748120", "bodyText": "I think we need to send LeaderAndIsr for all the given partitions whether we updated the ISR or not. In cases where we failed due, the leaders likely have stale metadata. This way we can proactively send them the latest state.", "author": "mumrah", "createdAt": "2020-08-05T14:00:36Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1771,6 +1776,141 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name(), partitionReq.partitionIndex())\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId(), partitionReq.leaderEpoch(), newIsr, partitionReq.currentIsrVersion()))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code())\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic()).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics().add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions().add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition())\n+                  .setErrorCode(error.code()))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId(), alterIsrRequest.brokerEpoch(), isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val partitionErrors: mutable.Map[TopicPartition, Errors] = mutable.HashMap[TopicPartition, Errors]()\n+\n+    val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+      case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+        val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          case Some(leaderIsrAndControllerEpoch) =>\n+            val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+            if (newLeaderAndIsr.leader != currentLeaderAndIsr.leader) {\n+              Errors.NOT_LEADER_OR_FOLLOWER\n+            } else if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+              Errors.FENCED_LEADER_EPOCH\n+            } else {\n+              val currentAssignment = controllerContext.partitionReplicaAssignment(tp)\n+              if (!newLeaderAndIsr.isr.forall(replicaId => currentAssignment.contains(replicaId))) {\n+                warn(s\"Some of the proposed ISR are not in the assignment for partition $tp. Proposed ISR=$newLeaderAndIsr.isr assignment=$currentAssignment\")\n+                Errors.INVALID_REQUEST\n+              } else if (!newLeaderAndIsr.isr.forall(replicaId => controllerContext.isReplicaOnline(replicaId, tp))) {\n+                warn(s\"Some of the proposed ISR are offline for partition $tp. Proposed ISR=$newLeaderAndIsr.isr\")\n+                Errors.INVALID_REQUEST\n+              } else {\n+                Errors.NONE\n+              }\n+            }\n+          case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+        }\n+        if (partitionError == Errors.NONE) {\n+          // Bump the leaderEpoch for partitions that we're going to write\n+          Some(tp -> newLeaderAndIsr.newEpochAndZkVersion)\n+        } else {\n+          partitionErrors.put(tp, partitionError)\n+          None\n+        }\n+    }\n+\n+    // Do the updates in ZK\n+    info(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+    val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) =  zkClient.updateLeaderAndIsr(\n+      adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+    val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+      case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+      isrOrError match {\n+        case Right(updatedIsr) =>\n+          info(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))\n+          Some(partition -> updatedIsr)\n+        case Left(error) =>\n+          warn(s\"Failed to update ISR for partition $partition\", error)\n+          partitionErrors.put(partition, Errors.forException(error))\n+          None\n+      }\n+    }\n+\n+    badVersionUpdates.foreach(partition => {\n+      warn(s\"Failed to update ISR for partition $partition, bad ZK version\")\n+      partitionErrors.put(partition, Errors.INVALID_ISR_VERSION)\n+    })\n+\n+    // Update our cache\n+    updateLeaderAndIsrCache(successfulUpdates.keys.toSeq)\n+\n+    // Send back AlterIsr response\n+    callback.apply(Left(partitionErrors))\n+\n+    // Send out LeaderAndIsr for successful updates\n+    brokerRequestBatch.newBatch()\n+\n+    // Send LeaderAndIsr for all requested partitions", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc0OTU0Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465749546", "bodyText": "Should probably get rid of this and change the method to\nenqueueIsrUpdate(TopicPartition, LeaderAndIsr)", "author": "mumrah", "createdAt": "2020-08-05T14:02:38Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1MDQ0NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465750445", "bodyText": "Remove this", "author": "mumrah", "createdAt": "2020-08-05T14:03:55Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition())\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          println(response.responseBody().toString(response.requestHeader().apiVersion()))", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTc1MDYzMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465750632", "bodyText": "newline", "author": "mumrah", "createdAt": "2020-08-05T14:04:10Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition())\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          println(response.responseBody().toString(response.requestHeader().apiVersion()))\n+          val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+          val data: AlterIsrResponseData = body.data()\n+          Errors.forCode(data.errorCode()) match {\n+            case Errors.NONE => info(s\"Controller handled AlterIsr request\")\n+            case e: Errors => warn(s\"Controller returned an error when handling AlterIsr request: $e\")\n+          }\n+          data.topics().forEach(topic => {\n+            topic.partitions().forEach(topicPartition => {\n+              Errors.forCode(topicPartition.errorCode()) match {\n+                case Errors.NONE => info(s\"Controller handled AlterIsr for $topicPartition\")\n+                case e: Errors => warn(s\"Controlled had an error handling AlterIsr for $topicPartition: $e\")\n+              }\n+            })\n+          })\n+        }\n+\n+        info(\"Sending AlterIsr to controller\")\n+        controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+\n+        pendingIsrUpdates.clear()\n+        lastIsrPropagationMs.set(now)\n+      }\n+      scheduledRequest = None\n+    }\n+  }\n+}", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "url": "https://github.com/apache/kafka/commit/fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-08-07T20:31:40Z", "type": "commit"}, {"oid": "07716ceb16a4d15e66bda92dac378fd8f8fae0c9", "url": "https://github.com/apache/kafka/commit/07716ceb16a4d15e66bda92dac378fd8f8fae0c9", "message": "PartitionTest and ReplicaManagerTest passing", "committedDate": "2020-08-12T15:34:39Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3MjM0NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469372344", "bodyText": "With KIP-500, I imagine we could end up with other cases where we end up using optimistic concurrency control. Does it make sense to make this error a little more generic? Maybe INVALID_UPDATE_VERSION or something like that..", "author": "hachikuji", "createdAt": "2020-08-12T16:03:52Z", "path": "clients/src/main/java/org/apache/kafka/common/errors/InvalidIsrVersionException.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.common.errors;\n+\n+public class InvalidIsrVersionException extends ApiException {", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3MzMzOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469373339", "bodyText": "nit: maybe drop the parameters if they do not need to be documented", "author": "hachikuji", "createdAt": "2020-08-12T16:05:24Z", "path": "clients/src/main/java/org/apache/kafka/common/requests/AlterIsrRequest.java", "diffHunk": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.common.requests;\n+\n+import org.apache.kafka.common.message.AlterIsrRequestData;\n+import org.apache.kafka.common.message.AlterIsrResponseData;\n+import org.apache.kafka.common.protocol.ApiKeys;\n+import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.protocol.types.Struct;\n+\n+public class AlterIsrRequest extends AbstractRequest {\n+\n+    private final AlterIsrRequestData data;\n+\n+    public AlterIsrRequest(AlterIsrRequestData data, short apiVersion) {\n+        super(ApiKeys.ALTER_ISR, apiVersion);\n+        this.data = data;\n+    }\n+\n+    public AlterIsrRequestData data() {\n+        return data;\n+    }\n+\n+    @Override\n+    protected Struct toStruct() {\n+        return data.toStruct(version());\n+    }\n+\n+    /**\n+     * Get an error response for a request with specified throttle time in the response if applicable\n+     *\n+     * @param throttleTimeMs", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3OTQ4OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469379488", "bodyText": "nit: info feels a bit high for a message like this", "author": "hachikuji", "createdAt": "2020-08-12T16:14:57Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -569,8 +575,8 @@ class Partition(val topicPartition: TopicPartition,\n       val oldLeaderEpoch = leaderEpoch\n       // record the epoch of the controller that made the leadership decision. This is useful while updating the isr\n       // to maintain the decision maker controller's epoch in the zookeeper path\n-      controllerEpoch = partitionState.controllerEpoch\n \n+      info(s\"Follower ignoring ISR for $topicPartition\")", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDA1MzE3Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r470053173", "bodyText": "yea, lots of these will be lowered, was just doing this during development", "author": "mumrah", "createdAt": "2020-08-13T15:51:39Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM3OTQ4OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4MjcyOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469382729", "bodyText": "We might need to be careful about performance here since this would get called on every follower fetch.", "author": "hachikuji", "createdAt": "2020-08-12T16:20:13Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -255,6 +255,10 @@ class Partition(val topicPartition: TopicPartition,\n \n   def isAddingReplica(replicaId: Int): Boolean = assignmentState.isAddingReplica(replicaId)\n \n+  // For advancing the HW we assume the largest ISR even if the controller hasn't made the change yet\n+  // This set includes the latest ISR (as we learned from LeaderAndIsr) and any replicas from a pending ISR expansion\n+  def effectiveInSyncReplicaIds: Set[Int] = inSyncReplicaIds | pendingInSyncReplicaIds", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY3ODEyMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r470678123", "bodyText": "Since we are now only allowing one in-flight AlterIsr, I changed the semantics of pendingInSyncReplicaIds to be the maximal \"effective\" ISR. This way we don't need to compute it each time.", "author": "mumrah", "createdAt": "2020-08-14T14:58:58Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4MjcyOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4MzU3Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469383572", "bodyText": "The usage is a bit surprising given the \"pending\" name. I wonder if it would be clearer if we used a type of Option[Set[Int]] so that we could use None when there is no pending ISR change.\nOne more thing. It's worth double-checking the threading assumptions here. It looks like updateAssignmentAndIsr is only called while holding the write side of leaderIsrUpdateLock. On the other hand, I don't see any lock held in updateFollowerFetchState. It's worth stepping through that logic to make sure that we do not depend on inSyncReplicaIds and pendingInSyncReplicaIds getting set atomically.", "author": "hachikuji", "createdAt": "2020-08-12T16:21:34Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -691,6 +700,7 @@ class Partition(val topicPartition: TopicPartition,\n     else\n       assignmentState = SimpleAssignmentState(assignment)\n     inSyncReplicaIds = isr\n+    pendingInSyncReplicaIds = isr", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4NTM3NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469385374", "bodyText": "I think the answer is no. The pending ISR set is not guaranteed, so we cannot depend on it to enforce min.isr.", "author": "hachikuji", "createdAt": "2020-08-12T16:24:32Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -990,6 +997,7 @@ class Partition(val topicPartition: TopicPartition,\n       leaderLogIfLocal match {\n         case Some(leaderLog) =>\n           val minIsr = leaderLog.config.minInSyncReplicas\n+          // TODO is it safe to use pending ISR here?", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM4ODA5NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469388095", "bodyText": "Related to the other comment, but we need to be careful with the min.isr check below. I think it is correct to wait for effectiveInSyncReplicaIds before acknowledging the produce request, but we should probably use the size of inSyncReplicaIds in the min.isr check since that is the only set we can guarantee.", "author": "hachikuji", "createdAt": "2020-08-12T16:29:06Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -748,7 +755,7 @@ class Partition(val topicPartition: TopicPartition,\n     leaderLogIfLocal match {\n       case Some(leaderLog) =>\n         // keep the current immutable replica list reference\n-        val curInSyncReplicaIds = inSyncReplicaIds\n+        val curInSyncReplicaIds = effectiveInSyncReplicaIds", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5OTAzNw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469399037", "bodyText": "There is a \"classic\" edge case in Kafka which goes as follows:\n\nLeader is 1, ISR is [1, 2, 3]\nBroker 3 begins controlled shutdown. While awaiting shutdown, it continues fetching.\nController bumps epoch and shrinks ISR to [1, 2] and notifies replicas\nBefore controlled shutdown completes and 3 stops fetching, the leader adds it back to the ISR.\n\nThis bug was fixed by KIP-320 which added epoch validation to the Fetch API. After shrinking the ISR in step 3, the controller will send LeaderAndIsr with the updated epoch to [1, 2] and StopReplica to [3]. So 3 will not send any fetches with the updated epoch, which means it's impossible for the leader to add 3 back after observing the shrink to [1, 2].\nI just want to make sure whether above is correct and whether AlterIsr changes it in any way. I think the answer is no as long as ISR expansion is only done in response to a fetch request, but it's worth double-checking.", "author": "hachikuji", "createdAt": "2020-08-12T16:47:34Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -485,13 +490,11 @@ class Partition(val topicPartition: TopicPartition,\n   def makeLeader(partitionState: LeaderAndIsrPartitionState,", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDA2MzgzMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r470063833", "bodyText": "I don't think AlterIsr changes anything since we're now just sending the async ISR update where we were previously directly updating ZK.\nLooking at the usages, updateFollowerFetchState is only called following a read (Partition#readRecords). These reads only happen on fetch requests and from the alter log dirs fetcher. I'm not sure about the alter log dirs flow, but as long as it sends the leader epoch, it should be safe.", "author": "mumrah", "createdAt": "2020-08-13T16:08:16Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTM5OTAzNw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQwNjk3Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469406972", "bodyText": "I think it's worth adding a comment in the cases we rely on effectiveInSyncReplicaIds to explain why.", "author": "hachikuji", "createdAt": "2020-08-12T17:00:32Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -807,7 +814,7 @@ class Partition(val topicPartition: TopicPartition,\n       var newHighWatermark = leaderLog.logEndOffsetMetadata\n       remoteReplicasMap.values.foreach { replica =>\n         if (replica.logEndOffsetMetadata.messageOffset < newHighWatermark.messageOffset &&\n-          (curTime - replica.lastCaughtUpTimeMs <= replicaLagTimeMaxMs || inSyncReplicaIds.contains(replica.brokerId))) {\n+          (curTime - replica.lastCaughtUpTimeMs <= replicaLagTimeMaxMs || effectiveInSyncReplicaIds.contains(replica.brokerId))) {", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxMjU4Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469412587", "bodyText": "I think the implementation here is actually different than what was in the model. Consider the following case:\n\nInitial state: isr=[1, 2], pendingIsr=[1, 2]\nLeader expands ISR. isr=[1, 2], pendingIsr=[1, 2, 3]\nLeader shrinks ISR. isr=[1, 2], pendingIsr=[1, 2]\n\nWe don't know which of the updates in 2) or 3) will be accepted, but after 3), we will not assume that broker 3 could be in the ISR, which could lead to a correctness violation if the update in 2) is accepted by the controller.\nIn the model, we always assumed the maximal ISR across any potential update to protect from this edge case. Maybe in the end it is simpler to not allow multiple in-flight updates.", "author": "hachikuji", "createdAt": "2020-08-12T17:10:15Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,28 +1218,20 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n-    val zkVersionOpt = stateStore.expandIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n-  }\n+  private def expandIsr(newInSyncReplica: Int): Unit = {\n+    pendingInSyncReplicaIds += newInSyncReplica\n+    info(s\"Adding new in-sync replica $newInSyncReplica. Pending ISR updated to [${pendingInSyncReplicaIds.mkString(\",\")}]\")\n \n-  private[cluster] def shrinkIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n-    val zkVersionOpt = stateStore.shrinkIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, pendingInSyncReplicaIds.toList, zkVersion)\n+    alterIsrChannelManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr))\n   }\n \n-  private[cluster] def maybeUpdateIsrAndVersion(isr: Set[Int], zkVersionOpt: Option[Int]): Unit = {\n-    zkVersionOpt match {\n-      case Some(newVersion) =>\n-        inSyncReplicaIds = isr\n-        zkVersion = newVersion\n-        info(\"ISR updated to [%s] and zkVersion updated to [%d]\".format(isr.mkString(\",\"), zkVersion))\n+  private[cluster] def shrinkIsr(outOfSyncReplicas: Set[Int]): Unit = {\n+    pendingInSyncReplicaIds --= outOfSyncReplicas", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ2Nzg4MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469467881", "bodyText": "I'm currently looking at the effective ISR to find new out of sync replicas. This can include new ISR members which haven't made it into the \"true\" ISR via LeaderAndIsr yet (like broker=3 in your example). Maybe we should only consider removing ISR members iff they are in the true ISR. IOW changing from\nval candidateReplicaIds = effectiveInSyncReplicaIds - localBrokerId\nto\nval candidateReplicaIds = inSyncReplicaIds - localBrokerId\nAlso, I wonder if the batching that's happening in AlterIsrChannelManager violates the model. It sends the request asynchronously with a small delay, so multiple ISR changes can be batched into one AlterIsr.", "author": "mumrah", "createdAt": "2020-08-12T18:46:27Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxMjU4Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxMzIwNQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469413205", "bodyText": "nit: remove parenthesis for simpler getters like code. A few more of these", "author": "hachikuji", "createdAt": "2020-08-12T17:11:21Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1761,141 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name(), partitionReq.partitionIndex())\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId(), partitionReq.leaderEpoch(), newIsr, partitionReq.currentIsrVersion()))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code())", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxMzQ1NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469413455", "bodyText": "Missing license header in this file.", "author": "hachikuji", "createdAt": "2020-08-12T17:11:47Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNDcxOA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469414718", "bodyText": "nit: avoid loaded terminology like \"blackout\" (see https://cwiki.apache.org/confluence/display/KAFKA/KIP-629%3A+Use+racially+neutral+terms+in+our+codebase). Do we actually need this or IsrChangePropagationInterval below?", "author": "hachikuji", "createdAt": "2020-08-12T17:13:57Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQ2MDQ2Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469460462", "bodyText": "We don't, these were copied over from the ReplicaManager's ISR propagation logic. I'll clean this up", "author": "mumrah", "createdAt": "2020-08-12T18:33:11Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNDcxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNTU0Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469415543", "bodyText": "We should use Time", "author": "hachikuji", "createdAt": "2020-08-12T17:15:14Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxNzYxMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469417613", "bodyText": "Probably need to reduce the log level here and below.", "author": "hachikuji", "createdAt": "2020-08-12T17:18:30Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition())\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          println(response.responseBody().toString(response.requestHeader().apiVersion()))\n+          val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+          val data: AlterIsrResponseData = body.data()\n+          Errors.forCode(data.errorCode()) match {\n+            case Errors.NONE => info(s\"Controller handled AlterIsr request\")", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTQ1Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469419452", "bodyText": "I think the basic approach here is to ignore successful responses and wait for the LeaderAndIsr update. I am wondering how we should handle the case when the update failed. Say for example that our update fails with the INVALID_VERSION error. Inside Partition, we will still have the pendingIsr set. Do we need to clear it? How about other errors?", "author": "hachikuji", "createdAt": "2020-08-12T17:21:31Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition())\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDEwNjU5MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r474106591", "bodyText": "I found a race during the system tests when a follower is shutting down. The controller handles the shut down before it handles an AlterIsr. If the proposed ISR includes the now-offline replica, the controller refuses to update that ISR change and returns an error for that partition. It then sends out the current LeaderAndIsr.\nThe problem is that the broker ignores this LeaderAndIsr since it has the same leader epoch. This is easy enough to fix, we can bump the leader epoch in the controller (and ZK) before sending it out.\nHowever, there's still the case of failing to update ZK. I think we should probably treat this the same way as an offline replica. If we simply return an error in AlterIsr response and let the leader reset the pending ISR state, the leader will just retry with stale metadata and the update will fail again.\nI think in all these error cases we must bump the leader epoch to force the leader to accept the new LeaderAndIsr. Thoughts?", "author": "mumrah", "createdAt": "2020-08-20T16:17:31Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTQ1Mg=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDk1OTE3Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r474959176", "bodyText": "Continued in #9100 (comment)", "author": "mumrah", "createdAt": "2020-08-21T20:50:34Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQxOTQ1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMTAzMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469421031", "bodyText": "nit: more useful as a debug if we add request details to the message", "author": "hachikuji", "createdAt": "2020-08-12T17:24:17Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition())\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          println(response.responseBody().toString(response.requestHeader().apiVersion()))\n+          val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+          val data: AlterIsrResponseData = body.data()\n+          Errors.forCode(data.errorCode()) match {\n+            case Errors.NONE => info(s\"Controller handled AlterIsr request\")\n+            case e: Errors => warn(s\"Controller returned an error when handling AlterIsr request: $e\")\n+          }\n+          data.topics().forEach(topic => {\n+            topic.partitions().forEach(topicPartition => {\n+              Errors.forCode(topicPartition.errorCode()) match {\n+                case Errors.NONE => info(s\"Controller handled AlterIsr for $topicPartition\")\n+                case e: Errors => warn(s\"Controlled had an error handling AlterIsr for $topicPartition: $e\")\n+              }\n+            })\n+          })\n+        }\n+\n+        info(\"Sending AlterIsr to controller\")", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMTgwMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469421800", "bodyText": "The broker epoch is not a constant. It gets reinitialized whenever the broker has to create a new session.", "author": "hachikuji", "createdAt": "2020-08-12T17:25:29Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY3MzQxMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r470673412", "bodyText": "Fixed this by adding getBrokerEpoch to KafkaZkClient", "author": "mumrah", "createdAt": "2020-08-14T14:51:00Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMTgwMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyMzgzNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469423834", "bodyText": "The term \"pending\" again is a little unclear. Perhaps \"unsentIsrUpdates\" would make the usage clearer.", "author": "hachikuji", "createdAt": "2020-08-12T17:28:57Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNTg5OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469425898", "bodyText": "Removal from this set won't prevent BrokerToControllerRequestThread from retrying in-flight requests. I'm considering whether we should have a way to cancel requests that we are still awaiting.", "author": "hachikuji", "createdAt": "2020-08-12T17:32:22Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDY3NTMwMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r470675300", "bodyText": "With the latest changes to prevent multiple in-flight requests, I don't think this should happen for a given partition. Even if it did, the retried in-flight request from BrokerToControllerRequestThread would fail on the controller with an old version.\nI'm wondering if we even need this clearPending behavior. Since I changed the AlterIsr request to fire at most after 50ms, it's a narrow window between enqueueing an ISR update and receiving a LeaderAndIsr.", "author": "mumrah", "createdAt": "2020-08-14T14:54:10Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNTg5OA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDkzNDgzMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r474934833", "bodyText": "Update: after some discussion and looking over failed system tests, we ended up with the following error handling:\n\nREPLICA_NOT_AVAILABLE and INVALID_REPLICA_ASSIGNMENT will clear the pending ISR to let the leader retry. This covers a case where a leader tries to add a replica to the ISR which is offline because it (the follower) just finished shutdown.\nINVALID_UPDATE_VERSION will not clear the pending ISR since the broker has stale metadata.\nFENCED_LEADER_EPOCH, NOT_LEADER_OR_FOLLOWER, UNKNOWN_TOPIC_OR_PARTITION will not clear the pending state and therefor will not retry. We presume here that the controller is correct and the leader has old metadata. By not clearing the pending ISR, the leader will await LeaderAndIsr before attempting any further ISR changes\nOther unspecified errors: clear the pending state and let the leader retry. Not sure what cases could cause other errors, but it is probably better to be in a retry loop than to be completely stuck", "author": "mumrah", "createdAt": "2020-08-21T20:20:56Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTQyNTg5OA=="}], "type": "inlineReview"}, {"oid": "10b7d638e868bf44317ffeb05dcf4496859efb8a", "url": "https://github.com/apache/kafka/commit/10b7d638e868bf44317ffeb05dcf4496859efb8a", "message": "PR feedback and progress on unit tests", "committedDate": "2020-08-13T21:04:30Z", "type": "commit"}, {"oid": "da33368773462fb0a53c448bfd45e449832a0ef1", "url": "https://github.com/apache/kafka/commit/da33368773462fb0a53c448bfd45e449832a0ef1", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-08-14T14:55:38Z", "type": "commit"}, {"oid": "1d98ada3e2e6623bfc49d304113550ec7df22222", "url": "https://github.com/apache/kafka/commit/1d98ada3e2e6623bfc49d304113550ec7df22222", "message": "PR feedback, minor nits mostly", "committedDate": "2020-08-17T17:04:35Z", "type": "commit"}, {"oid": "b7577f1795b0f737eacce874159c6178e544000b", "url": "https://github.com/apache/kafka/commit/b7577f1795b0f737eacce874159c6178e544000b", "message": "Gate AlterIsr on IBP", "committedDate": "2020-08-17T19:31:48Z", "type": "commit"}, {"oid": "9c7afa1d2e3d5940c1a37d09329dbe8241acc225", "url": "https://github.com/apache/kafka/commit/9c7afa1d2e3d5940c1a37d09329dbe8241acc225", "message": "Add back ReplicaManager's ISR propagation thread", "committedDate": "2020-08-18T14:00:32Z", "type": "commit"}, {"oid": "4cc58a30599e54003c571b6a689faa055009ee6c", "url": "https://github.com/apache/kafka/commit/4cc58a30599e54003c571b6a689faa055009ee6c", "message": "WIP tests", "committedDate": "2020-08-18T18:02:01Z", "type": "commit"}, {"oid": "f557a86acdb5e854a1298464ffd293ad851246c7", "url": "https://github.com/apache/kafka/commit/f557a86acdb5e854a1298464ffd293ad851246c7", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-08-18T18:02:37Z", "type": "commit"}, {"oid": "8c9183b9425f72c7834015cf91e9a887358d2210", "url": "https://github.com/apache/kafka/commit/8c9183b9425f72c7834015cf91e9a887358d2210", "message": "Back something out", "committedDate": "2020-08-18T18:22:39Z", "type": "commit"}, {"oid": "1e20e9ccc2fb094f307002e46c69e610809d6607", "url": "https://github.com/apache/kafka/commit/1e20e9ccc2fb094f307002e46c69e610809d6607", "message": "Tests WIP", "committedDate": "2020-08-18T19:49:10Z", "type": "commit"}, {"oid": "93ac6301e8443ffd057adde2d56337b33ce7bd3d", "url": "https://github.com/apache/kafka/commit/93ac6301e8443ffd057adde2d56337b33ce7bd3d", "message": "Add some debug logging for test", "committedDate": "2020-08-19T15:52:29Z", "type": "commit"}, {"oid": "ccda5352339208b01efa00eda63069c3b8b2d38d", "url": "https://github.com/apache/kafka/commit/ccda5352339208b01efa00eda63069c3b8b2d38d", "message": "Make sure to always send LeaderAndIsr after AlterIsr", "committedDate": "2020-08-19T17:00:22Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTg4ODE4Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r465888182", "bodyText": "nit: could move these operations to the AlterIsrRequest as helpers.", "author": "abbccdda", "createdAt": "2020-08-05T17:28:23Z", "path": "core/src/main/scala/kafka/server/AlterIsrChannelManager.scala", "diffHunk": "@@ -0,0 +1,132 @@\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrChannelManager {\n+  val IsrChangePropagationBlackOut = 5000L\n+  val IsrChangePropagationInterval = 60000L\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr)\n+\n+class AlterIsrChannelManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                                 val zkClient: KafkaZkClient,\n+                                 val scheduler: Scheduler,\n+                                 val brokerId: Int,\n+                                 val brokerEpoch: Long) extends AlterIsrChannelManager with Logging with KafkaMetricsGroup {\n+\n+  private val pendingIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    pendingIsrUpdates synchronized {\n+      pendingIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(System.currentTimeMillis())\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    pendingIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      pendingIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()\n+  }\n+\n+  override def shutdown(): Unit = {\n+    controllerChannelManager.shutdown()\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = System.currentTimeMillis()\n+    pendingIsrUpdates synchronized {\n+      if (pendingIsrUpdates.nonEmpty) {\n+        // Max ISRs to send?\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        pendingIsrUpdates.values.groupBy(_.topicPartition.topic()).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()", "originalCommit": "627b26aef3c623c13066a5f7e617318a49cc0a5b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDkzMDMyOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r474930329", "bodyText": "It's a little tricky since LeaderAndIsr isn't visible to AlterIsrRequest.", "author": "mumrah", "createdAt": "2020-08-21T20:15:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTg4ODE4Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTUxNjM3NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r469516375", "bodyText": "nit: new line", "author": "abbccdda", "createdAt": "2020-08-12T20:20:11Z", "path": "clients/src/main/resources/common/message/AlterIsrRequest.json", "diffHunk": "@@ -0,0 +1,44 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"AlterIsrRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",\n+  \"fields\": [\n+    { \"name\": \"BrokerId\", \"type\": \"int32\", \"versions\": \"0+\", \"entityType\": \"brokerId\",\n+      \"about\": \"The ID of the requesting broker\" },\n+    { \"name\": \"BrokerEpoch\", \"type\": \"int64\", \"versions\": \"0+\", \"default\": \"-1\",\n+      \"about\": \"The epoch of the requesting broker\" },\n+    { \"name\": \"Topics\", \"type\": \"[]AlterIsrRequestTopics\", \"versions\": \"0+\", \"fields\": [\n+      { \"name\":  \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to alter ISRs for\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]AlterIsrRequestPartitions\", \"versions\": \"0+\", \"fields\": [\n+        { \"name\": \"PartitionIndex\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index\" },\n+        { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\", \"entityType\": \"brokerId\",\n+          \"about\": \"The ID of the leader broker\" },\n+        { \"name\": \"LeaderEpoch\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The leader epoch of this partition\" },\n+        { \"name\": \"NewIsr\", \"type\": \"[]int32\", \"versions\": \"0+\",\n+          \"about\": \"The ISR for this partition\"},\n+        { \"name\": \"CurrentIsrVersion\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The expected version of ISR which is being updated\"}\n+      ]}\n+    ]}\n+  ]\n+}", "originalCommit": "fe99911f5d06d76b18a9446bf94b4e0ca7ce6b1b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzUxNTYzNQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r473515635", "bodyText": "Should we move the startup logic to KafkaServer? Note the channel is shared between different modules, so it makes sense to start and close inside the server.", "author": "abbccdda", "createdAt": "2020-08-20T01:36:09Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,149 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+\n+  def startup(): Unit\n+\n+  def shutdown(): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  private val unsentIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    unsentIsrUpdates synchronized {\n+      unsentIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(time.milliseconds)\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      unsentIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  override def startup(): Unit = {\n+    controllerChannelManager.start()", "originalCommit": "ccda5352339208b01efa00eda63069c3b8b2d38d", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "19d8e362edce6159faa273823840c1cb4cb56164", "url": "https://github.com/apache/kafka/commit/19d8e362edce6159faa273823840c1cb4cb56164", "message": "More logging for tests", "committedDate": "2020-08-20T13:01:50Z", "type": "commit"}, {"oid": "2bcf8b4b68cf3e59d86743e8927b867bd11970a7", "url": "https://github.com/apache/kafka/commit/2bcf8b4b68cf3e59d86743e8927b867bd11970a7", "message": "Clear the pending ISR state for certain errors", "committedDate": "2020-08-20T15:06:56Z", "type": "commit"}, {"oid": "5be92d0f38be1a6a969e6e483bf8614d3488955a", "url": "https://github.com/apache/kafka/commit/5be92d0f38be1a6a969e6e483bf8614d3488955a", "message": "Let Partition handle LeaderAndIsr even if epoch is unchanged", "committedDate": "2020-08-20T18:10:08Z", "type": "commit"}, {"oid": "868a779bc31edebac5803ba48ed4cebde9f5403f", "url": "https://github.com/apache/kafka/commit/868a779bc31edebac5803ba48ed4cebde9f5403f", "message": "Better logging for ISR updates, allow leader to retry in some cases", "committedDate": "2020-08-21T15:43:48Z", "type": "commit"}, {"oid": "69441c0ff1484c24b1135a51e8236cbeee05c78b", "url": "https://github.com/apache/kafka/commit/69441c0ff1484c24b1135a51e8236cbeee05c78b", "message": "Clear pending ISR state after unknown controller error", "committedDate": "2020-08-21T18:31:03Z", "type": "commit"}, {"oid": "d6ba8c25dc68d4f4888a74aab3ffb92683b89203", "url": "https://github.com/apache/kafka/commit/d6ba8c25dc68d4f4888a74aab3ffb92683b89203", "message": "Feedback from PR", "committedDate": "2020-08-21T20:53:40Z", "type": "commit"}, {"oid": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "url": "https://github.com/apache/kafka/commit/c886e8c5137b6a90099625e75268b8e4bcf30ee4", "message": "Cover more error cases, some cleanup", "committedDate": "2020-08-21T20:58:56Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYyNTExMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475625111", "bodyText": "This error message should be less specific", "author": "mumrah", "createdAt": "2020-08-24T13:53:27Z", "path": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "diffHunk": "@@ -325,7 +326,8 @@\n     UNSTABLE_OFFSET_COMMIT(88, \"There are unstable offsets that need to be cleared.\", UnstableOffsetCommitException::new),\n     THROTTLING_QUOTA_EXCEEDED(89, \"The throttling quota has been exceeded.\", ThrottlingQuotaExceededException::new),\n     PRODUCER_FENCED(90, \"There is a newer producer with the same transactionalId \" +\n-            \"which fences the current one.\", ProducerFencedException::new);\n+            \"which fences the current one.\", ProducerFencedException::new),\n+    INVALID_UPDATE_VERSION(91, \"The given ISR version was out-of-date.\", InvalidUpdateVersionException::new);", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYyNjU3Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475626573", "bodyText": "Need to revert this stuff, didn't mean to commit", "author": "mumrah", "createdAt": "2020-08-24T13:54:36Z", "path": "config/log4j.properties", "diffHunk": "@@ -61,8 +61,8 @@ log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\n log4j.logger.org.apache.zookeeper=INFO\n \n # Change the two lines below to adjust the general broker logging level (output to server.log and stdout)\n-log4j.logger.kafka=INFO\n-log4j.logger.org.apache.kafka=INFO\n+log4j.logger.kafka=DEBUG", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYyNzI5MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475627290", "bodyText": "Rename to alterIsrManager", "author": "mumrah", "createdAt": "2020-08-24T13:55:09Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -141,7 +142,8 @@ object Partition extends KafkaMetricsGroup {\n       stateStore = zkIsrBackingStore,\n       delayedOperations = delayedOperations,\n       metadataCache = replicaManager.metadataCache,\n-      logManager = replicaManager.logManager)\n+      logManager = replicaManager.logManager,\n+      alterIsrChannelManager = replicaManager.alterIsrManager)", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYyOTU5Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475629593", "bodyText": "Expand on this comment to discuss the maximal ISR", "author": "mumrah", "createdAt": "2020-08-24T13:57:02Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -635,7 +666,7 @@ class Partition(val topicPartition: TopicPartition,\n \n         // check if we need to expand ISR to include this replica\n         // if it is not in the ISR yet", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYzMDY5NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475630695", "bodyText": "Fix comment to refer to correct variable", "author": "mumrah", "createdAt": "2020-08-24T13:57:55Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -806,8 +840,9 @@ class Partition(val topicPartition: TopicPartition,\n       // avoid unnecessary collection generation\n       var newHighWatermark = leaderLog.logEndOffsetMetadata\n       remoteReplicasMap.values.foreach { replica =>\n+        // Note here we are using effectiveInSyncReplicaIds, see explanation above", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NTYzMjcxOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r475632719", "bodyText": "newline", "author": "mumrah", "createdAt": "2020-08-24T13:59:34Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1257,4 +1364,4 @@ class Partition(val topicPartition: TopicPartition,\n     }\n     partitionString.toString\n   }\n-}\n+}", "originalCommit": "c886e8c5137b6a90099625e75268b8e4bcf30ee4", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1691951e335ff8e1ab6558b50c6607218cfe08a7", "url": "https://github.com/apache/kafka/commit/1691951e335ff8e1ab6558b50c6607218cfe08a7", "message": "Revert \"Back something out\"\n\nThis reverts commit 8c9183b9425f72c7834015cf91e9a887358d2210.", "committedDate": "2020-08-24T14:01:35Z", "type": "commit"}, {"oid": "6b264a48779750c9ae05bb15b517260245e4b44b", "url": "https://github.com/apache/kafka/commit/6b264a48779750c9ae05bb15b517260245e4b44b", "message": "Cleanup", "committedDate": "2020-08-24T14:16:33Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxNjkxMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477616910", "bodyText": "We may as well add flexible version support for the request and response.", "author": "hachikuji", "createdAt": "2020-08-26T22:08:38Z", "path": "clients/src/main/resources/common/message/AlterIsrRequest.json", "diffHunk": "@@ -0,0 +1,44 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"request\",\n+  \"name\": \"AlterIsrRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxODA4Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477618083", "bodyText": "nit: I think AlterIsrResponseTopics should be singular (similarly for other arrays in both of these schemas).\nAlso, I wonder if it's reasonable to leave off the AlterIsr prefix. We could access it as AlterIsrResponse.TopicData or something like that.", "author": "hachikuji", "createdAt": "2020-08-26T22:11:44Z", "path": "clients/src/main/resources/common/message/AlterIsrResponse.json", "diffHunk": "@@ -0,0 +1,38 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 50,\n+  \"type\": \"response\",\n+  \"name\": \"AlterIsrResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"none\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+      \"about\": \"The top level response error code\" },\n+    { \"name\": \"Topics\", \"type\": \"[]AlterIsrResponseTopics\", \"versions\": \"0+\", \"fields\": [", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxODY0Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477618646", "bodyText": "nit: unneeded newline", "author": "hachikuji", "createdAt": "2020-08-26T22:13:13Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -133,6 +133,7 @@ object Partition extends KafkaMetricsGroup {\n       replicaManager.delayedFetchPurgatory,\n       replicaManager.delayedDeleteRecordsPurgatory)\n \n+", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyMDU0OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477620548", "bodyText": "Do we need this message? It seems the one on line 537 below has more detail already. It would be useful to include the zkVersion in the message on 537 as well.", "author": "hachikuji", "createdAt": "2020-08-26T22:18:02Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -492,6 +514,8 @@ class Partition(val topicPartition: TopicPartition,\n       val isr = partitionState.isr.asScala.map(_.toInt).toSet\n       val addingReplicas = partitionState.addingReplicas.asScala.map(_.toInt)\n       val removingReplicas = partitionState.removingReplicas.asScala.map(_.toInt)\n+      info(s\"Leader setting ISR to $isr for $topicPartition with leader epoch ${partitionState.leaderEpoch}\")", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyNTc3Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477625772", "bodyText": "I am wondering if we can split this into two separate methods:\n\neffectiveIsr: takes into account any pending changes which may or may not have happened (I could probably also be convinced to call this maximalIsr)\nconfirmedIsr: the latest known value from Zookeeper (or the Controller)\n\nThat makes the code easier to follow since we wouldn't have to interpret this flag. Some high-level comments might be helpful as well. For example, it's useful to mention somewhere that the high watermark is always treated with respect to the effective ISR.", "author": "hachikuji", "createdAt": "2020-08-26T22:32:18Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -255,6 +265,15 @@ class Partition(val topicPartition: TopicPartition,\n \n   def isAddingReplica(replicaId: Int): Boolean = assignmentState.isAddingReplica(replicaId)\n \n+  def inSyncReplicaIds(includeUncommittedReplicas: Boolean = false): Set[Int] = {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUyMTgxMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r486521810", "bodyText": "I think this sounds good, explict over implicit and all that. If we have two methods like this, should we then make inSyncReplicaIds a private member?", "author": "mumrah", "createdAt": "2020-09-10T17:43:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyNTc3Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyOTc2NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477629764", "bodyText": "Can we move this check earlier in the flow so that we can skip acquiring the write lock if there is an inflight AlterIsr? Maybe it can be part of needsExpandIsr and needsShrinkIsr for example.", "author": "hachikuji", "createdAt": "2020-08-26T22:41:43Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,19 +1243,66 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock\n+    if (pendingInSyncReplicaIds.isEmpty) {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTE2Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r486535166", "bodyText": "Yea, good idea. I'll leave the check here since we actually acquire and release the lock when checking if we should expand/shrink. It's possible that pendingInSyncReplicaIds is cleared by a LeaderAndIsr before we acquire the write lock to do the update", "author": "mumrah", "createdAt": "2020-09-10T18:05:57Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYyOTc2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc0ODQzMw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477748433", "bodyText": "I have been thinking a little bit about the semantics of min.isr. Basically I am wondering if should be treated as a guarantee on the state of the ISR at the time the high watermark is reached (i.e. how many replicas are in the ISR), or instead should it be a guarantee on the state of progress of replication (i.e. some number of replicas have reached a given offset)? We may not have ever formally decided this, but here we are taking a stance that it is the latter because we are using the effective (uncommitted) ISR.\nOne of the consequences of this view is that a leader may continue to accept appends satisfying min.isr even if the true ISR never reaches min.isr. For example, imagine we have the following state:\nreplicas: (1, 2, 3)\nisr: (1)\nleader: 1\nSuppose that replica 2 has caught up to the leader, but the leader is unable to expand the ISR because the controller is unavailable or unreachable. With the logic here, we will nevertheless continue to satisfy acks=all requests with a min.isr of 2.\nI am not sure there is much choice about it to be honest. If instead we used only the \"confirmed\" ISR, then we would have sort of an opposite problem. For example, consider this state:\nreplicas: (1, 2, 3)\nisr: (1, 2)\nleader: 1\nSuppose the leader wants to remove 2 from the ISR. The AlterIsr is received by the controller and the state is updated, but the controller fails to send the corresponding LeaderAndIsr. Then committing on the basis of the confirmed ISR would lead to a similar problem.\nHere is the current documentation for the config:\n  val MinInSyncReplicasDoc = \"When a producer sets acks to \\\"all\\\" (or \\\"-1\\\"), \" +\n    \"min.insync.replicas specifies the minimum number of replicas that must acknowledge \" +\n    \"a write for the write to be considered successful. If this minimum cannot be met, \" +\n    \"then the producer will raise an exception (either NotEnoughReplicas or \" +\n    \"NotEnoughReplicasAfterAppend).<br>When used together, min.insync.replicas and acks \" +\n    \"allow you to enforce greater durability guarantees. A typical scenario would be to \" +\n    \"create a topic with a replication factor of 3, set min.insync.replicas to 2, and \" +\n    \"produce with acks of \\\"all\\\". This will ensure that the producer raises an exception \" +\n    \"if a majority of replicas do not receive a write.\"\n\nEven though it is named in terms of the ISR, the documentation only discusses acks from other replicas, so it seems like the implementation here is consistent even if potentially surprising in some cases.", "author": "hachikuji", "createdAt": "2020-08-27T00:16:03Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -748,7 +776,7 @@ class Partition(val topicPartition: TopicPartition,\n     leaderLogIfLocal match {\n       case Some(leaderLog) =>\n         // keep the current immutable replica list reference\n-        val curInSyncReplicaIds = inSyncReplicaIds\n+        val curInSyncReplicaIds = inSyncReplicaIds(true)", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NTc1OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477785759", "bodyText": "Maybe trace would be better? This could get verbose while we have an inflight AlterIsr.", "author": "hachikuji", "createdAt": "2020-08-27T00:35:59Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,19 +1243,66 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock\n+    if (pendingInSyncReplicaIds.isEmpty) {\n+      // When expanding the ISR, we can safely assume the new replica will make it into the ISR since this puts us in\n+      // a more constrained state for advancing the HW.\n+      val newIsr = inSyncReplicaIds + newInSyncReplica\n+      pendingInSyncReplicaIds = Some(newIsr)\n+      debug(s\"Adding new in-sync replica $newInSyncReplica. Pending ISR updated to [${newIsr.mkString(\",\")}] for $topicPartition\")\n+      alterIsr(newIsr)\n+    } else {\n+      debug(s\"ISR update in-flight, not adding new in-sync replica $newInSyncReplica for $topicPartition\")", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NjYzMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477786630", "bodyText": "I still think we need a better name for pendingInSyncReplicaIds since it is misleading in this case. Maybe we could call it overrideInSyncReplicaIds or something like that?", "author": "hachikuji", "createdAt": "2020-08-27T00:36:26Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,19 +1243,66 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock\n+    if (pendingInSyncReplicaIds.isEmpty) {\n+      // When expanding the ISR, we can safely assume the new replica will make it into the ISR since this puts us in\n+      // a more constrained state for advancing the HW.\n+      val newIsr = inSyncReplicaIds + newInSyncReplica\n+      pendingInSyncReplicaIds = Some(newIsr)\n+      debug(s\"Adding new in-sync replica $newInSyncReplica. Pending ISR updated to [${newIsr.mkString(\",\")}] for $topicPartition\")\n+      alterIsr(newIsr)\n+    } else {\n+      debug(s\"ISR update in-flight, not adding new in-sync replica $newInSyncReplica for $topicPartition\")\n+    }\n+  }\n+\n+  private def expandIsrWithZk(newInSyncReplica: Int): Unit = {\n+    val newInSyncReplicaIds = inSyncReplicaIds + newInSyncReplica\n+    info(s\"Expanding ISR from ${inSyncReplicaIds.mkString(\",\")} to ${newInSyncReplicaIds.mkString(\",\")}\")\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newInSyncReplicaIds.toList, zkVersion)\n     val zkVersionOpt = stateStore.expandIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    maybeUpdateIsrAndVersionWithZk(newInSyncReplicaIds, zkVersionOpt)\n   }\n \n-  private[cluster] def shrinkIsr(newIsr: Set[Int]): Unit = {\n+  private[cluster] def shrinkIsr(outOfSyncReplicas: Set[Int]): Unit = {\n+    if (useAlterIsr) {\n+      shrinkIsrWithAlterIsr(outOfSyncReplicas)\n+    } else {\n+      shrinkIsrWithZk(inSyncReplicaIds -- outOfSyncReplicas)\n+    }\n+  }\n+\n+  private def shrinkIsrWithAlterIsr(outOfSyncReplicas: Set[Int]): Unit = {\n+    // This is called from maybeShrinkIsr which holds the ISR write lock\n+    if (pendingInSyncReplicaIds.isEmpty) {\n+      // When shrinking the ISR, we cannot assume that the update will succeed as this could erroneously advance the HW\n+      // We update pendingInSyncReplicaIds here simply to prevent any further ISR updates from occurring until we get\n+      // the next LeaderAndIsr\n+      pendingInSyncReplicaIds = Some(inSyncReplicaIds)", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTg2OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r486535868", "bodyText": "How about uncommittedInSyncReplicaIds?", "author": "mumrah", "createdAt": "2020-09-10T18:07:08Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NjYzMA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwNDU4Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489604587", "bodyText": "The problem is that it is a sort of worst-case ISR and not the intended ISR update itself. Tough to come up with a good name to describe that. Just for the sake of having an alternative, what if we used case classes to represent the current ISR state and pending update? For example:\nsealed trait IsrStatus {\n  def isr: Set[Int]\n  def maximalIsr: Set[Int]\n}\ncase class PendingExpand(isr: Set[Int], newInSyncReplicaId: Int) extends IsrStatus {\n  val maximalIsr = isr + newInSyncReplicaId\n}\ncase class PendingShrink(isr: Set[Int], outOfSync: Set[Int]) extends IsrStatus  {\n  val maximalIsr = isr\n}\ncase class Stable(isr: Set[Int]) extends IsrStatus {\n  val maximalIsr = isr\n}\nThen we can get rid of effectiveIsr, inSyncReplicaIds, and pendingInSyncReplicaIds.", "author": "hachikuji", "createdAt": "2020-09-16T17:29:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NjYzMA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzc4NzM1MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477787350", "bodyText": "nit: maybe sendAlterIsrRequest?", "author": "hachikuji", "createdAt": "2020-08-27T00:36:50Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1234,6 +1314,36 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def alterIsr(newIsr: Set[Int]): Unit = {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzgxMTMzNg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477811336", "bodyText": "Hmm.. I am not sure it is safe to reset pendingInSyncReplicaIds in any case except INVALID_UPDATE_VERSION. For example, imagine the following sequence:\n\nBroker sends AlterIsr\nController writes new ISR and crashes before sending response\nBroker hits session expiration\nBroker retries AlterIsr on new controller with old broker epoch\nController responds with STALE_BROKER_EPOCH\n\nIn this case, the ISR was updated, but the broker is going to revert to the old state. I think the only time we can reset pendingInSyncReplicaIds is when we know the change could not have been applied.", "author": "hachikuji", "createdAt": "2020-08-27T00:49:30Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1234,6 +1314,36 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def alterIsr(newIsr: Set[Int]): Unit = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        case Errors.NONE =>\n+          debug(s\"Controller accepted proposed ISR $newIsr for $topicPartition.\")\n+        case Errors.REPLICA_NOT_AVAILABLE | Errors.INVALID_REPLICA_ASSIGNMENT =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition. Some replicas were not online or \" +\n+            s\"in the partition assignment. Clearing pending ISR to allow leader to retry.\")\n+          pendingInSyncReplicaIds = None\n+        case Errors.FENCED_LEADER_EPOCH =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition since we have an old leader epoch. Not retrying.\")\n+        case Errors.NOT_LEADER_OR_FOLLOWER =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition since we are not the leader. Not retrying.\")\n+        case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition since this partition is unknown. Not retrying.\")\n+        case Errors.INVALID_UPDATE_VERSION =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition due to invalid ZK version. Not retrying.\")\n+        // Top-level errors that have been pushed down to partition level by AlterIsrManager\n+        case Errors.STALE_BROKER_EPOCH =>\n+          warn(s\"Controller rejected proposed ISR $newIsr for $topicPartition due to a stale broker epoch. \" +\n+            s\"Clearing pending ISR to allow leader to retry.\")\n+          pendingInSyncReplicaIds = None", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzgzMjE2MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477832161", "bodyText": "Don't forget the TODO!", "author": "hachikuji", "createdAt": "2020-08-27T01:00:46Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1762,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions.add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition)\n+                  .setErrorCode(error.code))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val partitionResponses: mutable.Map[TopicPartition, Errors] = mutable.HashMap[TopicPartition, Errors]()\n+\n+    // Determine which partitions we will accept the new ISR for\n+    val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+      case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+        val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          case Some(leaderIsrAndControllerEpoch) =>\n+            val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+            if (newLeaderAndIsr.leader != currentLeaderAndIsr.leader) {\n+              Errors.NOT_LEADER_OR_FOLLOWER\n+            } else if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+              Errors.FENCED_LEADER_EPOCH\n+            } else {\n+              val currentAssignment = controllerContext.partitionReplicaAssignment(tp)\n+              if (!newLeaderAndIsr.isr.forall(replicaId => currentAssignment.contains(replicaId))) {\n+                warn(s\"Some of the proposed ISR are not in the assignment for partition $tp. Proposed ISR=$newLeaderAndIsr.isr assignment=$currentAssignment\")\n+                Errors.INVALID_REPLICA_ASSIGNMENT\n+              } else if (!newLeaderAndIsr.isr.forall(replicaId => controllerContext.isReplicaOnline(replicaId, tp))) {\n+                warn(s\"Some of the proposed ISR are offline for partition $tp. Proposed ISR=$newLeaderAndIsr.isr\")\n+                Errors.REPLICA_NOT_AVAILABLE\n+              } else {\n+                Errors.NONE\n+              }\n+            }\n+          case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+        }\n+        if (partitionError == Errors.NONE) {\n+          partitionResponses(tp) = Errors.NONE\n+          // Bump the leaderEpoch for partitions that we're going to write\n+          Some(tp -> newLeaderAndIsr.newEpochAndZkVersion) // TODO only bump this for latest IBP", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg3NjIzMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477876231", "bodyText": "Should we try to make AlterIsr an idempotent operation? I think currently if we retry an update that was successfully applied, then we will see INVALID_VERSION.\nIn general, I'm a bit concerned about the number of errors that are possible through this API and how the leader is supposed to handle them. I am thinking it might make our lives easier if we return some additional information in the response about what the current state really is. Let's say that we always try to add the full state tuple to the response: (leaderId, epoch, ISR, zkVersion). Then we can go through a simple process of reconciliation?\n\nAm I still the leader?\nDo I have the latest epoch?\nHas the zkVersion been bumped?\nDid the ISR change get applied?\n\nBasically I'm looking for a reliable way to determine whether we should continue retrying the request and whether it is safe to clear the pending replica set. At the same time, I'm feeling a bit on-the-fence about relying exclusively on LeaderAndIsr for state changes. If we need to return the current state in the response anyway to properly handle errors, then perhaps we may as well allow the state to be updated as well? This would actually be closer to the flow that we have today, which is the following:\n\nLeader changes state in Zookeeper and updates current ISR directly.\nAfter some delay, it posts the ISR update to isr_change_notifications.\nController picks up the notification and sends UpdateMetadata to all the brokers.\n\nNotice that the controller does not send LeaderAndIsr to the followers in this flow. What we could do is something more like the following:\n\nLeader sends AlterIsr to controller.\nController applies the change and returns the updated state.\nLeader receives the response and applies the state change.\nAfter some delay, the controller sends UpdateMetadata to the brokers with the change.\n\nIf we did this, then we wouldn't need to have the controller bump the epoch when handling AlterIsr. Just as we do today, we can reserve epoch bumps for controller-initiated changes.\nThen we might be able to simplify the error handling to the following:\n\nIf the epoch is the same and we are still the leader, then apply the update\nIf the epoch is higher, leave pendingIsr set and do not bother retrying\nOtherwise just keep retrying\n\nWhat do you think?", "author": "hachikuji", "createdAt": "2020-08-27T01:32:23Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1762,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions.add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition)\n+                  .setErrorCode(error.code))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val partitionResponses: mutable.Map[TopicPartition, Errors] = mutable.HashMap[TopicPartition, Errors]()\n+\n+    // Determine which partitions we will accept the new ISR for\n+    val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+      case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+        val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          case Some(leaderIsrAndControllerEpoch) =>\n+            val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+            if (newLeaderAndIsr.leader != currentLeaderAndIsr.leader) {\n+              Errors.NOT_LEADER_OR_FOLLOWER\n+            } else if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+              Errors.FENCED_LEADER_EPOCH\n+            } else {\n+              val currentAssignment = controllerContext.partitionReplicaAssignment(tp)\n+              if (!newLeaderAndIsr.isr.forall(replicaId => currentAssignment.contains(replicaId))) {\n+                warn(s\"Some of the proposed ISR are not in the assignment for partition $tp. Proposed ISR=$newLeaderAndIsr.isr assignment=$currentAssignment\")\n+                Errors.INVALID_REPLICA_ASSIGNMENT\n+              } else if (!newLeaderAndIsr.isr.forall(replicaId => controllerContext.isReplicaOnline(replicaId, tp))) {\n+                warn(s\"Some of the proposed ISR are offline for partition $tp. Proposed ISR=$newLeaderAndIsr.isr\")\n+                Errors.REPLICA_NOT_AVAILABLE\n+              } else {\n+                Errors.NONE\n+              }\n+            }\n+          case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+        }\n+        if (partitionError == Errors.NONE) {\n+          partitionResponses(tp) = Errors.NONE\n+          // Bump the leaderEpoch for partitions that we're going to write\n+          Some(tp -> newLeaderAndIsr.newEpochAndZkVersion) // TODO only bump this for latest IBP\n+        } else {\n+          partitionResponses(tp) = partitionError\n+          None\n+        }\n+    }\n+\n+    // Do the updates in ZK\n+    info(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+    val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) =  zkClient.updateLeaderAndIsr(", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg4OTU1Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477889557", "bodyText": "Probably not a useful log message", "author": "hachikuji", "createdAt": "2020-08-27T01:43:00Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  private val unsentIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    unsentIsrUpdates synchronized {\n+      unsentIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(time.milliseconds)\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      unsentIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = time.milliseconds()\n+    unsentIsrUpdates synchronized {\n+      if (unsentIsrUpdates.nonEmpty) {\n+        val brokerEpoch: Long = zkClient.getBrokerEpoch(brokerId) match {\n+          case Some(brokerEpoch) => brokerEpoch\n+          case None => throw new RuntimeException(\"Cannot send AlterIsr because we cannot determine broker epoch\")\n+        }\n+\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        val callbacks = new mutable.HashMap[TopicPartition, Errors => Unit]()\n+        unsentIsrUpdates.values.groupBy(_.topicPartition.topic).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition)\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+            callbacks(item.topicPartition) = item.callback\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+          val data: AlterIsrResponseData = body.data\n+          Errors.forCode(data.errorCode) match {\n+            case Errors.NONE =>\n+              info(s\"Controller handled AlterIsr request\")", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg5MjgwMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477892802", "bodyText": "Not sure about this. Do we really want to put zk in the path to sending to the controller?", "author": "hachikuji", "createdAt": "2020-08-27T01:45:16Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  private val unsentIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    unsentIsrUpdates synchronized {\n+      unsentIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(time.milliseconds)\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      unsentIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = time.milliseconds()\n+    unsentIsrUpdates synchronized {\n+      if (unsentIsrUpdates.nonEmpty) {\n+        val brokerEpoch: Long = zkClient.getBrokerEpoch(brokerId) match {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODA2OTE0NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r478069145", "bodyText": "I wasn't too happy about this. Is there another way to get the current broker epoch? As I understand it, the broker epoch can change during the lifecycle of a broker.", "author": "mumrah", "createdAt": "2020-08-27T03:57:52Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Nzg5MjgwMg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkwMjk3Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477902972", "bodyText": "This should be fixed", "author": "hachikuji", "createdAt": "2020-08-27T01:52:41Z", "path": "core/src/test/scala/unit/kafka/cluster/PartitionLockTest.scala", "diffHunk": "@@ -281,10 +283,10 @@ class PartitionLockTest extends Logging {\n     when(stateStore.fetchTopicConfig()).thenReturn(createLogProperties(Map.empty))\n     when(offsetCheckpoints.fetch(ArgumentMatchers.anyString, ArgumentMatchers.eq(topicPartition)))\n       .thenReturn(None)\n-    when(stateStore.shrinkIsr(ArgumentMatchers.anyInt, ArgumentMatchers.any[LeaderAndIsr]))\n-      .thenReturn(Some(2))\n-    when(stateStore.expandIsr(ArgumentMatchers.anyInt, ArgumentMatchers.any[LeaderAndIsr]))\n-      .thenReturn(Some(2))\n+    //when(stateStore.shrinkIsr(ArgumentMatchers.anyInt, ArgumentMatchers.any[LeaderAndIsr]))", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkwNDQ3OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477904479", "bodyText": "As far as I can tell, we don't have any logic which tells us whether there is an inflight request. I am considering whether we should as a mechanism for batching/flow control. It might be simpler if we just allow one inflight request. While we are waiting for it to return, we can collect additional pending updates. In case we need to retry the request, we could coalesce the new updates into the request.\nNote that currently BrokerToControllerChannelManagerImpl currently sets max inflight requests to 1 anyway.", "author": "hachikuji", "createdAt": "2020-08-27T01:53:43Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  private val unsentIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    unsentIsrUpdates synchronized {\n+      unsentIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(time.milliseconds)\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODA2NzIyMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r478067221", "bodyText": "I think that sounds pretty reasonable. Would we need any kind of timeout at this layer, or just rely on the underlying channel to provide timeouts?", "author": "mumrah", "createdAt": "2020-08-27T03:55:53Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkwNDQ3OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkxMTc1Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477911757", "bodyText": "We seem to be losing some of the value of having a top-level error code here. As far as I can tell, the following top-level errors should be possible:\n\nNOT_CONTROLLER: should be retried (handled in BrokerToControllerChannelManagerImpl)\nSTALE_BROKER_EPOCH: should be retried (could we do that here?)\nCLUSTER_AUTHORIZATION_FAILED: probably should be fatal (can we handle that here?)\n\nSeems like it might simplify the error handling if we can handle them at a corresponding granularity.", "author": "hachikuji", "createdAt": "2020-08-27T01:59:01Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,145 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.{ScheduledFuture, TimeUnit}\n+import java.util.concurrent.atomic.AtomicLong\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.AlterIsrRequestData.{AlterIsrRequestPartitions, AlterIsrRequestTopics}\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Errors => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  private val unsentIsrUpdates: mutable.Map[TopicPartition, AlterIsrItem] = new mutable.HashMap[TopicPartition, AlterIsrItem]()\n+  private val lastIsrChangeMs = new AtomicLong(0)\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  @volatile private var scheduledRequest: Option[ScheduledFuture[_]] = None\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Unit = {\n+    unsentIsrUpdates synchronized {\n+      unsentIsrUpdates(alterIsrItem.topicPartition) = alterIsrItem\n+      lastIsrChangeMs.set(time.milliseconds)\n+      // Rather than sending right away, we'll delay at most 50ms to allow for batching of ISR changes happening\n+      // in fast succession\n+      if (scheduledRequest.isEmpty) {\n+        scheduledRequest = Some(scheduler.schedule(\"propagate-alter-isr\", propagateIsrChanges, 50, -1, TimeUnit.MILLISECONDS))\n+      }\n+    }\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates synchronized {\n+      // when we get a new LeaderAndIsr, we clear out any pending requests\n+      unsentIsrUpdates.remove(topicPartition)\n+    }\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    val now = time.milliseconds()\n+    unsentIsrUpdates synchronized {\n+      if (unsentIsrUpdates.nonEmpty) {\n+        val brokerEpoch: Long = zkClient.getBrokerEpoch(brokerId) match {\n+          case Some(brokerEpoch) => brokerEpoch\n+          case None => throw new RuntimeException(\"Cannot send AlterIsr because we cannot determine broker epoch\")\n+        }\n+\n+        val message = new AlterIsrRequestData()\n+          .setBrokerId(brokerId)\n+          .setBrokerEpoch(brokerEpoch)\n+          .setTopics(new util.ArrayList())\n+\n+        val callbacks = new mutable.HashMap[TopicPartition, Errors => Unit]()\n+        unsentIsrUpdates.values.groupBy(_.topicPartition.topic).foreachEntry((topic, items) => {\n+          val topicPart = new AlterIsrRequestTopics()\n+            .setName(topic)\n+            .setPartitions(new util.ArrayList())\n+          message.topics().add(topicPart)\n+          items.foreach(item => {\n+            topicPart.partitions().add(new AlterIsrRequestPartitions()\n+              .setPartitionIndex(item.topicPartition.partition)\n+              .setLeaderId(item.leaderAndIsr.leader)\n+              .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+              .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+              .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+            )\n+            callbacks(item.topicPartition) = item.callback\n+          })\n+        })\n+\n+        def responseHandler(response: ClientResponse): Unit = {\n+          val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+          val data: AlterIsrResponseData = body.data\n+          Errors.forCode(data.errorCode) match {\n+            case Errors.NONE =>\n+              info(s\"Controller handled AlterIsr request\")\n+              data.topics.forEach(topic => {\n+                topic.partitions().forEach(partition => {\n+                  callbacks(new TopicPartition(topic.name, partition.partitionIndex))(\n+                    Errors.forCode(partition.errorCode))\n+                })\n+              })\n+            case e: Errors =>\n+              // Need to propagate top-level errors back to all partitions so they can react accordingly\n+              warn(s\"Controller returned a top-level error when handling AlterIsr request: $e\")", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkxMjMxNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477912314", "bodyText": "nit: misaligned", "author": "hachikuji", "createdAt": "2020-08-27T01:59:22Z", "path": "core/src/main/scala/kafka/server/BrokerToControllerChannelManagerImpl.scala", "diffHunk": "@@ -113,9 +124,10 @@ class BrokerToControllerChannelManager(metadataCache: kafka.server.MetadataCache\n       brokerToControllerListenerName, time, threadName)\n   }\n \n-  private[server] def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n-                                  callback: RequestCompletionHandler): Unit = {\n+  override def sendRequest(request: AbstractRequest.Builder[_ <: AbstractRequest],\n+                  callback: RequestCompletionHandler): Unit = {", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkxNjU2NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477916564", "bodyText": "Any particular reason to change the order here?", "author": "hachikuji", "createdAt": "2020-08-27T02:02:29Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -278,10 +284,10 @@ class KafkaController(val config: KafkaConfig,\n   private def onControllerResignation(): Unit = {\n     debug(\"Resigning\")\n     // de-register listeners\n-    zkClient.unregisterZNodeChildChangeHandler(isrChangeNotificationHandler.path)\n     zkClient.unregisterZNodeChangeHandler(partitionReassignmentHandler.path)\n     zkClient.unregisterZNodeChangeHandler(preferredReplicaElectionHandler.path)\n     zkClient.unregisterZNodeChildChangeHandler(logDirEventNotificationHandler.path)\n+    zkClient.unregisterStateChangeHandler(isrChangeNotificationHandler.path)", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjUzNTk2NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r486535964", "bodyText": "nope, will revert", "author": "mumrah", "createdAt": "2020-09-10T18:07:20Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkxNjU2NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkxODU1Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477918557", "bodyText": "We should probably have a try/catch in here somewhere for the unhandled errors to make sure that the callback always gets applied.", "author": "hachikuji", "createdAt": "2020-08-27T02:03:54Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1762,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions.add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition)\n+                  .setErrorCode(error.code))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkyMDMxNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477920314", "bodyText": "Maybe debug is more suitable?", "author": "hachikuji", "createdAt": "2020-08-27T02:05:17Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1762,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions.add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition)\n+                  .setErrorCode(error.code))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val partitionResponses: mutable.Map[TopicPartition, Errors] = mutable.HashMap[TopicPartition, Errors]()\n+\n+    // Determine which partitions we will accept the new ISR for\n+    val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+      case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+        val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          case Some(leaderIsrAndControllerEpoch) =>\n+            val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+            if (newLeaderAndIsr.leader != currentLeaderAndIsr.leader) {\n+              Errors.NOT_LEADER_OR_FOLLOWER\n+            } else if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+              Errors.FENCED_LEADER_EPOCH\n+            } else {\n+              val currentAssignment = controllerContext.partitionReplicaAssignment(tp)\n+              if (!newLeaderAndIsr.isr.forall(replicaId => currentAssignment.contains(replicaId))) {\n+                warn(s\"Some of the proposed ISR are not in the assignment for partition $tp. Proposed ISR=$newLeaderAndIsr.isr assignment=$currentAssignment\")\n+                Errors.INVALID_REPLICA_ASSIGNMENT\n+              } else if (!newLeaderAndIsr.isr.forall(replicaId => controllerContext.isReplicaOnline(replicaId, tp))) {\n+                warn(s\"Some of the proposed ISR are offline for partition $tp. Proposed ISR=$newLeaderAndIsr.isr\")\n+                Errors.REPLICA_NOT_AVAILABLE\n+              } else {\n+                Errors.NONE\n+              }\n+            }\n+          case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+        }\n+        if (partitionError == Errors.NONE) {\n+          partitionResponses(tp) = Errors.NONE\n+          // Bump the leaderEpoch for partitions that we're going to write\n+          Some(tp -> newLeaderAndIsr.newEpochAndZkVersion) // TODO only bump this for latest IBP\n+        } else {\n+          partitionResponses(tp) = partitionError\n+          None\n+        }\n+    }\n+\n+    // Do the updates in ZK\n+    info(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzkyMTM5Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r477921396", "bodyText": "Could be debug perhaps?", "author": "hachikuji", "createdAt": "2020-08-27T02:06:04Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1756,6 +1762,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics().forEach(topicReq => topicReq.partitions().forEach(partitionReq => {\n+      val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+      val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+      isrsToAlter.put(tp, new LeaderAndIsr(partitionReq.leaderId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+    }))\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Errors], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitions: Map[TopicPartition, Errors]) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitions.groupBy(_._1.topic).foreachEntry((topic, partitionMap) => {\n+            val topicResp = new AlterIsrResponseTopics()\n+              .setName(topic)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            partitionMap.foreachEntry((partition, error) => {\n+              topicResp.partitions.add(\n+                new AlterIsrResponsePartitions()\n+                  .setPartitionIndex(partition.partition)\n+                  .setErrorCode(error.code))\n+            })\n+          })\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val partitionResponses: mutable.Map[TopicPartition, Errors] = mutable.HashMap[TopicPartition, Errors]()\n+\n+    // Determine which partitions we will accept the new ISR for\n+    val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+      case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+        val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          case Some(leaderIsrAndControllerEpoch) =>\n+            val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+            if (newLeaderAndIsr.leader != currentLeaderAndIsr.leader) {\n+              Errors.NOT_LEADER_OR_FOLLOWER\n+            } else if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+              Errors.FENCED_LEADER_EPOCH\n+            } else {\n+              val currentAssignment = controllerContext.partitionReplicaAssignment(tp)\n+              if (!newLeaderAndIsr.isr.forall(replicaId => currentAssignment.contains(replicaId))) {\n+                warn(s\"Some of the proposed ISR are not in the assignment for partition $tp. Proposed ISR=$newLeaderAndIsr.isr assignment=$currentAssignment\")\n+                Errors.INVALID_REPLICA_ASSIGNMENT\n+              } else if (!newLeaderAndIsr.isr.forall(replicaId => controllerContext.isReplicaOnline(replicaId, tp))) {\n+                warn(s\"Some of the proposed ISR are offline for partition $tp. Proposed ISR=$newLeaderAndIsr.isr\")\n+                Errors.REPLICA_NOT_AVAILABLE\n+              } else {\n+                Errors.NONE\n+              }\n+            }\n+          case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+        }\n+        if (partitionError == Errors.NONE) {\n+          partitionResponses(tp) = Errors.NONE\n+          // Bump the leaderEpoch for partitions that we're going to write\n+          Some(tp -> newLeaderAndIsr.newEpochAndZkVersion) // TODO only bump this for latest IBP\n+        } else {\n+          partitionResponses(tp) = partitionError\n+          None\n+        }\n+    }\n+\n+    // Do the updates in ZK\n+    info(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+    val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) =  zkClient.updateLeaderAndIsr(\n+      adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+    val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+      case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+      isrOrError match {\n+        case Right(updatedIsr) =>\n+          info(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))", "originalCommit": "6b264a48779750c9ae05bb15b517260245e4b44b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "f791645f8d5655c6b7775d79296d3c6bbea5e17a", "url": "https://github.com/apache/kafka/commit/f791645f8d5655c6b7775d79296d3c6bbea5e17a", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-09-09T15:56:01Z", "type": "commit"}, {"oid": "a3ab36cdab2db97cc14934bb6f32029e4296c43e", "url": "https://github.com/apache/kafka/commit/a3ab36cdab2db97cc14934bb6f32029e4296c43e", "message": "Lots of updates from Jason's review\n\n* Get broker epoch from KafkaController instead of ZK\n* Add additional fields to AlterIsr response\n* Update ISR in Partition from AlterIsr response\n* Controller no longer sends LeaderAndIsr, now sends UpdateMetadata", "committedDate": "2020-09-10T15:06:03Z", "type": "commit"}, {"oid": "aa15fd0269b25c0cb44f3c4ab008d0dcd5a6e194", "url": "https://github.com/apache/kafka/commit/aa15fd0269b25c0cb44f3c4ab008d0dcd5a6e194", "message": "Fix some Scala 2.12 problems", "committedDate": "2020-09-10T17:16:06Z", "type": "commit"}, {"oid": "7ef9a0de7b806852f560e1a43cf1ddc05ef2e3d6", "url": "https://github.com/apache/kafka/commit/7ef9a0de7b806852f560e1a43cf1ddc05ef2e3d6", "message": "More PR feedback, mostly style stuff", "committedDate": "2020-09-10T18:35:37Z", "type": "commit"}, {"oid": "4e5685ce50174e1d06c166465ff24b8084ee8388", "url": "https://github.com/apache/kafka/commit/4e5685ce50174e1d06c166465ff24b8084ee8388", "message": "Make sure we set throttle time on the response\n\nAlso fix some short circuit bugs in controller", "committedDate": "2020-09-11T15:46:57Z", "type": "commit"}, {"oid": "02e7378e240fa5727c92a35cac2b1153a3d03c27", "url": "https://github.com/apache/kafka/commit/02e7378e240fa5727c92a35cac2b1153a3d03c27", "message": "Use a queue instead of map inside AlterIsrManager", "committedDate": "2020-09-12T00:16:24Z", "type": "commit"}, {"oid": "31f91260c6fd052c8e26ddb2af1358f362a76646", "url": "https://github.com/apache/kafka/commit/31f91260c6fd052c8e26ddb2af1358f362a76646", "message": "Trying something different in AlterIsrManager", "committedDate": "2020-09-15T01:07:02Z", "type": "commit"}, {"oid": "d00c0666b7b08365afd088a5e3f2b4a0ff5fc8e2", "url": "https://github.com/apache/kafka/commit/d00c0666b7b08365afd088a5e3f2b4a0ff5fc8e2", "message": "Cleanup and adding more unit tests", "committedDate": "2020-09-15T14:55:33Z", "type": "commit"}, {"oid": "ff4f5b2efb92ebceaeeb53f8541cfe3cc01bce48", "url": "https://github.com/apache/kafka/commit/ff4f5b2efb92ebceaeeb53f8541cfe3cc01bce48", "message": "Fix unit tests", "committedDate": "2020-09-15T17:22:59Z", "type": "commit"}, {"oid": "392472e076454d3e250781fb528b31f6b1641ce6", "url": "https://github.com/apache/kafka/commit/392472e076454d3e250781fb528b31f6b1641ce6", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-09-15T17:24:24Z", "type": "commit"}, {"oid": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "url": "https://github.com/apache/kafka/commit/41053499d2cbcdcae21ff4bc70b1d46705394a32", "message": "fixup after merge", "committedDate": "2020-09-15T17:29:36Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU2OTQwMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489569400", "bodyText": "I know we've gone back and forth on including some of these fields. This is one I'm inclined to get rid of since we already include \"BrokerId\" at the top level and AlterIsr can only be sent by leaders.", "author": "hachikuji", "createdAt": "2020-09-16T16:29:34Z", "path": "clients/src/main/resources/common/message/AlterIsrRequest.json", "diffHunk": "@@ -0,0 +1,44 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 52,\n+  \"type\": \"request\",\n+  \"name\": \"AlterIsrRequest\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"BrokerId\", \"type\": \"int32\", \"versions\": \"0+\", \"entityType\": \"brokerId\",\n+      \"about\": \"The ID of the requesting broker\" },\n+    { \"name\": \"BrokerEpoch\", \"type\": \"int64\", \"versions\": \"0+\", \"default\": \"-1\",\n+      \"about\": \"The epoch of the requesting broker\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicData\", \"versions\": \"0+\", \"fields\": [\n+      { \"name\":  \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic to alter ISRs for\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionData\", \"versions\": \"0+\", \"fields\": [\n+        { \"name\": \"PartitionIndex\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index\" },\n+        { \"name\": \"LeaderId\", \"type\": \"int32\", \"versions\": \"0+\", \"entityType\": \"brokerId\",", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3MTMzMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489571330", "bodyText": "nit: shall we call this LeaderId in line with BrokerId in the request?", "author": "hachikuji", "createdAt": "2020-09-16T16:32:40Z", "path": "clients/src/main/resources/common/message/AlterIsrResponse.json", "diffHunk": "@@ -0,0 +1,46 @@\n+// Licensed to the Apache Software Foundation (ASF) under one or more\n+// contributor license agreements.  See the NOTICE file distributed with\n+// this work for additional information regarding copyright ownership.\n+// The ASF licenses this file to You under the Apache License, Version 2.0\n+// (the \"License\"); you may not use this file except in compliance with\n+// the License.  You may obtain a copy of the License at\n+//\n+//    http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+{\n+  \"apiKey\": 52,\n+  \"type\": \"response\",\n+  \"name\": \"AlterIsrResponse\",\n+  \"validVersions\": \"0\",\n+  \"flexibleVersions\": \"0+\",\n+  \"fields\": [\n+    { \"name\": \"ThrottleTimeMs\", \"type\": \"int32\", \"versions\": \"0+\",\n+      \"about\": \"The duration in milliseconds for which the request was throttled due to a quota violation, or zero if the request did not violate any quota.\" },\n+    { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+      \"about\": \"The top level response error code\" },\n+    { \"name\": \"Topics\", \"type\": \"[]TopicData\", \"versions\": \"0+\", \"fields\": [\n+      { \"name\":  \"Name\", \"type\": \"string\", \"versions\": \"0+\", \"entityType\": \"topicName\",\n+        \"about\": \"The name of the topic\" },\n+      { \"name\": \"Partitions\", \"type\": \"[]PartitionData\", \"versions\": \"0+\", \"fields\": [\n+        { \"name\": \"PartitionIndex\", \"type\": \"int32\", \"versions\": \"0+\",\n+          \"about\": \"The partition index\" },\n+        { \"name\": \"ErrorCode\", \"type\": \"int16\", \"versions\": \"0+\",\n+          \"about\": \"The partition level error code\" },\n+        { \"name\": \"Leader\", \"type\": \"int32\", \"versions\": \"0+\", \"entityType\": \"brokerId\",", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3MjQxOA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489572418", "bodyText": "Can we revert this change? I think the trace logging is intended, if a bit odd.", "author": "hachikuji", "createdAt": "2020-09-16T16:34:32Z", "path": "config/log4j.properties", "diffHunk": "@@ -76,8 +76,8 @@ log4j.additivity.kafka.request.logger=false\n log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender\n log4j.additivity.kafka.network.RequestChannel$=false\n \n-log4j.logger.kafka.controller=TRACE, controllerAppender\n-log4j.additivity.kafka.controller=false\n+log4j.logger.kafka.controller=DEBUG, controllerAppender", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTcyMDk3MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489720971", "bodyText": "Yup, my mistake, shouldn't have been committed", "author": "mumrah", "createdAt": "2020-09-16T20:00:38Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3MjQxOA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3Mjc0NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489572745", "bodyText": "We probably need another version since we bumped the Fetch protocol version yesterday.", "author": "hachikuji", "createdAt": "2020-09-16T16:35:06Z", "path": "core/src/main/scala/kafka/api/ApiVersion.scala", "diffHunk": "@@ -100,7 +100,9 @@ object ApiVersion {\n     // Introduced StopReplicaRequest V3 containing the leader epoch for each partition (KIP-570)\n     KAFKA_2_6_IV0,\n     // Introduced feature versioning support (KIP-584)\n-    KAFKA_2_7_IV0\n+    KAFKA_2_7_IV0,\n+    // Introduced AlterIsr (KIP-497)\n+    KAFKA_2_7_IV1", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3NzM5NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489577394", "bodyText": "nit: we use \"maximal\" and \"effective\" interchangeably in this PR. Maybe we can choose one term and use it consistently. I do sort of like \"maximal\" since it is more suggestive of the semantics.", "author": "hachikuji", "createdAt": "2020-09-16T16:42:48Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -617,9 +665,9 @@ class Partition(val topicPartition: TopicPartition,\n         // since the replica's logStartOffset may have incremented\n         val leaderLWIncremented = newLeaderLW > oldLeaderLW\n \n-        // check if we need to expand ISR to include this replica\n-        // if it is not in the ISR yet\n-        if (!inSyncReplicaIds.contains(followerId))\n+        // Check if this in-sync replica needs to be added to the ISR. We look at the \"maximal\" ISR here so we don't", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTcyMTMxNg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489721316", "bodyText": "\"maximal\" works for me \ud83d\udc4d", "author": "mumrah", "createdAt": "2020-09-16T20:01:18Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3NzM5NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU3OTU5MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489579590", "bodyText": "nit: maybe hasInFlightAlterIsr so that it's clearer what the return value indicates?", "author": "hachikuji", "createdAt": "2020-09-16T16:46:31Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -239,6 +249,34 @@ class Partition(val topicPartition: TopicPartition,\n \n   def isAddingReplica(replicaId: Int): Boolean = assignmentState.isAddingReplica(replicaId)\n \n+  /**\n+   * This set may include un-committed ISR members following an expansion. This \"effective\" ISR is used for advancing\n+   * the high watermark as well as determining which replicas are required for acks=all produce requests.\n+   *\n+   * Only applicable as of IBP 2.7-IV1, for older versions this simply returns the committed ISR\n+   *\n+   * @return the set of replica IDs which are in-sync\n+   */\n+  def effectiveIsr: Set[Int] = {\n+    if (useAlterIsr) {\n+      pendingInSyncReplicaIds.getOrElse(inSyncReplicaIds)\n+    } else {\n+      inSyncReplicaIds\n+    }\n+  }\n+\n+  /**\n+   * Check if we have an in-flight AlterIsr\n+   */\n+  def checkInFlightAlterIsr: Boolean = {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU4MDM4NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489580384", "bodyText": "nit: maybe we could rename curInSyncReplicaIds to curEffectiveIsr", "author": "hachikuji", "createdAt": "2020-09-16T16:47:54Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -732,7 +778,7 @@ class Partition(val topicPartition: TopicPartition,\n     leaderLogIfLocal match {\n       case Some(leaderLog) =>\n         // keep the current immutable replica list reference\n-        val curInSyncReplicaIds = inSyncReplicaIds\n+        val curInSyncReplicaIds = effectiveIsr", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU4Mjg3OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489582879", "bodyText": "nit: probably need to reword mention of LeaderAndIsr since the AlterIsr response is now used.", "author": "hachikuji", "createdAt": "2020-09-16T16:52:06Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -780,6 +826,11 @@ class Partition(val topicPartition: TopicPartition,\n    * follower's log end offset may keep falling behind the HW (determined by the leader's log end offset) and therefore\n    * will never be added to ISR.\n    *\n+   * With the addition of AlterIsr, we also consider newly added replicas as part of the ISR when advancing\n+   * the HW. These replicas have not yet been committed to the ISR by the controller and propagated via LeaderAndIsr.", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU5MzMwMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489593301", "bodyText": "nit: maybe we can check in-flight requests first (same in needsExpandIsr). Otherwise it's a little odd that getOutOfSyncReplicas may be based on the maximal ISR while we have an in-flight.", "author": "hachikuji", "createdAt": "2020-09-16T17:09:49Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -891,7 +942,7 @@ class Partition(val topicPartition: TopicPartition,\n   private def needsShrinkIsr(): Boolean = {\n     if (isLeader) {\n       val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)\n-      outOfSyncReplicaIds.nonEmpty\n+      outOfSyncReplicaIds.nonEmpty && !checkInFlightAlterIsr", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU5MzY1MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489593650", "bodyText": "nit: redundant comment", "author": "hachikuji", "createdAt": "2020-09-16T17:10:29Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1186,19 +1237,75 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  // This is called from maybeExpandIsr which holds the ISR write lock\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTU5NDY4NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489594684", "bodyText": "nit: you can take the topic partition out of this message since it is already included in logIdent. Same on line 1262 below.", "author": "hachikuji", "createdAt": "2020-09-16T17:12:13Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1186,19 +1237,75 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  // This is called from maybeExpandIsr which holds the ISR write lock\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock\n+    if (pendingInSyncReplicaIds.isEmpty) {\n+      // When expanding the ISR, we can safely assume the new replica will make it into the ISR since this puts us in\n+      // a more constrained state for advancing the HW.\n+      val newIsr = inSyncReplicaIds + newInSyncReplica\n+      if (sendAlterIsrRequest(newIsr)) {\n+        pendingInSyncReplicaIds = Some(newIsr)\n+        debug(s\"Adding new in-sync replica $newInSyncReplica. Pending ISR updated to [${newIsr.mkString(\",\")}] for $topicPartition\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwNjEyNw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489606127", "bodyText": "nit: \"... doesn't know about this topic or partition\"?", "author": "hachikuji", "createdAt": "2020-09-16T17:32:22Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwNzE2NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489607165", "bodyText": "Hmm... Why do we reset pendingInSyncReplicaIds if we are retrying? Unless we are guaranteed that the update failed, then I think we need to continue assuming the worst-case ISR. Maybe we could just could call enqueueIsrUpdate again to explicitly retry?", "author": "hachikuji", "createdAt": "2020-09-16T17:34:25Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")\n+            case Errors.FENCED_LEADER_EPOCH =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since we sent an old leader epoch. Giving up.\")\n+            case _ =>\n+              pendingInSyncReplicaIds = None", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTcwMzgxMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489703810", "bodyText": "I think this has been a long-standing bad assumption on my part in this PR. I've been (mis)treating pendingInSyncReplicaIds as a mechanism for initiating a retry along with its other semantics. You're right though, explicitly re-sending the ISR is definitely better.", "author": "mumrah", "createdAt": "2020-09-16T19:27:49Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwNzE2NQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwODUxMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489608511", "bodyText": "nit: We don't need topic partition here, but it would be nice if we could include the intended update.", "author": "hachikuji", "createdAt": "2020-09-16T17:36:44Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")\n+            case Errors.FENCED_LEADER_EPOCH =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since we sent an old leader epoch. Giving up.\")\n+            case _ =>\n+              pendingInSyncReplicaIds = None\n+              debug(s\"Controller failed to update ISR for $topicPartition due to $error. Retrying.\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwODc0MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489608740", "bodyText": "nit: as long as we're updating this, can we use $ substitutions?  Also can we mention that this update came from AlterIsr?", "author": "hachikuji", "createdAt": "2020-09-16T17:37:10Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")\n+            case Errors.FENCED_LEADER_EPOCH =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since we sent an old leader epoch. Giving up.\")\n+            case _ =>\n+              pendingInSyncReplicaIds = None\n+              debug(s\"Controller failed to update ISR for $topicPartition due to $error. Retrying.\")\n+          }\n+          case Right(leaderAndIsr: LeaderAndIsr) =>\n+            // Success from controller, still need to check a few things\n+            if (leaderAndIsr.leaderEpoch != leaderEpoch) {\n+              debug(s\"Ignoring ISR with ${leaderAndIsr} since we have a stale leader epoch.\")\n+            } else if (leaderAndIsr.zkVersion <= zkVersion) {\n+              debug(s\"Ignoring ISR with ${leaderAndIsr} since we have a new one.\")\n+            } else {\n+              inSyncReplicaIds = leaderAndIsr.isr.toSet\n+              zkVersion = leaderAndIsr.zkVersion\n+              pendingInSyncReplicaIds = None\n+              info(\"ISR updated to [%s] and version updated to [%d]\".format(inSyncReplicaIds.mkString(\",\"), zkVersion))", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYwOTg2Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489609866", "bodyText": "Maybe helpful if these messages indicate that this leaderAndIsr can from an AlterIsr response. Also, it may be useful to include the current (stale) leader epoch.", "author": "hachikuji", "createdAt": "2020-09-16T17:39:09Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")\n+            case Errors.FENCED_LEADER_EPOCH =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since we sent an old leader epoch. Giving up.\")\n+            case _ =>\n+              pendingInSyncReplicaIds = None\n+              debug(s\"Controller failed to update ISR for $topicPartition due to $error. Retrying.\")\n+          }\n+          case Right(leaderAndIsr: LeaderAndIsr) =>\n+            // Success from controller, still need to check a few things\n+            if (leaderAndIsr.leaderEpoch != leaderEpoch) {\n+              debug(s\"Ignoring ISR with ${leaderAndIsr} since we have a stale leader epoch.\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxMDUwOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489610509", "bodyText": "nit: similarly, we can include current zkVersion", "author": "hachikuji", "createdAt": "2020-09-16T17:40:19Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1210,6 +1317,37 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(newIsr: Set[Int]): Boolean = {\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+    alterIsrManager.enqueueIsrUpdate(AlterIsrItem(topicPartition, newLeaderAndIsr, result => {\n+      inWriteLock(leaderIsrUpdateLock) {\n+        result match {\n+          case Left(error: Errors) => error match {\n+            case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since it doesn't know about this partition. Giving up.\")\n+            case Errors.FENCED_LEADER_EPOCH =>\n+              debug(s\"Controller failed to update ISR for $topicPartition since we sent an old leader epoch. Giving up.\")\n+            case _ =>\n+              pendingInSyncReplicaIds = None\n+              debug(s\"Controller failed to update ISR for $topicPartition due to $error. Retrying.\")\n+          }\n+          case Right(leaderAndIsr: LeaderAndIsr) =>\n+            // Success from controller, still need to check a few things\n+            if (leaderAndIsr.leaderEpoch != leaderEpoch) {\n+              debug(s\"Ignoring ISR with ${leaderAndIsr} since we have a stale leader epoch.\")\n+            } else if (leaderAndIsr.zkVersion <= zkVersion) {\n+              debug(s\"Ignoring ISR with ${leaderAndIsr} since we have a new one.\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxMjM4NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489612385", "bodyText": "nit: maybe we could shorten this name to just enqueue since the fact that it is an ISR update is already implied by the argument and the name of the trait itself.", "author": "hachikuji", "createdAt": "2020-09-16T17:43:38Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNDEwNQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489614105", "bodyText": "nit: usually we write this as forEach { topic =>. Avoids the extra parenthesis.", "author": "hachikuji", "createdAt": "2020-09-16T17:46:39Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")\n+      case Errors.NONE =>\n+        // Collect partition-level responses to pass to the callbacks\n+        val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+          new mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+        debug(s\"Controller successfully handled AlterIsr request\")\n+        data.topics.forEach(topic => {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTczMDI2NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489730265", "bodyText": "ah, missed one ;)", "author": "mumrah", "createdAt": "2020-09-16T20:19:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNDEwNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNDczMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489614732", "bodyText": "nit: maybe split this into two separate methods?", "author": "hachikuji", "createdAt": "2020-09-16T17:47:43Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxNzcwOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489617709", "bodyText": "The use of a queue is a tad odd here. We could use ListBuffer? Also nit: use type inference.", "author": "hachikuji", "createdAt": "2020-09-16T17:52:57Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxOTM3Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489619376", "bodyText": "Still not super keen on this propagation delay. At least it would be nice if we did not have to wakeup the thread every 50ms when there's nothing to do. This is potentially something we can save for a follow-up since coming up with a good solution might require some experimentation and analysis.", "author": "hachikuji", "createdAt": "2020-09-16T17:55:53Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc0MDQ3OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489740479", "bodyText": "Currently we impose a 2.5s delay for the old ZK based ISR propagation method. We could probably increase this 50ms up to a few hundred without any ill-effects. We still benefit from fact that we assume the maximal ISR immediately. How about 200ms?\nLonger term we can look into having a single thread invocation that sits in a while loop trying to consume from a LinkedBlockingQueue or maybe even a SynchronousQueue. But agreed we should leave this for later.", "author": "mumrah", "createdAt": "2020-09-16T20:39:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxOTM3Ng=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYxOTc2Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489619767", "bodyText": "nit: use type inference", "author": "hachikuji", "createdAt": "2020-09-16T17:56:37Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTEyMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489621121", "bodyText": "nit: can we include the broker epoch that was sent in this message?", "author": "hachikuji", "createdAt": "2020-09-16T17:59:00Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTE2OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489621168", "bodyText": "Hmm.. Where does this exception get caught? Since it is in the response handler, I guess that NetworkClient just eats it. Perhaps we should just continue retrying so that the problem remains visible in the logs.", "author": "hachikuji", "createdAt": "2020-09-16T17:59:06Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc0NDU2MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489744561", "bodyText": "How about we raise this to an error log with the exception?", "author": "mumrah", "createdAt": "2020-09-16T20:47:19Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTE2OA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTgxNQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489621815", "bodyText": "nit: this message would be more useful if we include the response. Perhaps it would be better to log each partition update separately?", "author": "hachikuji", "createdAt": "2020-09-16T18:00:08Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")\n+      case Errors.NONE =>\n+        // Collect partition-level responses to pass to the callbacks\n+        val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+          new mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+        debug(s\"Controller successfully handled AlterIsr request\")", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc0NTUzOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489745539", "bodyText": "Should we drop it to trace in that case?", "author": "mumrah", "createdAt": "2020-09-16T20:49:17Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMTgxNQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMjAwMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489622001", "bodyText": "nit: unneeded parenthesis", "author": "hachikuji", "createdAt": "2020-09-16T18:00:29Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")\n+      case Errors.NONE =>\n+        // Collect partition-level responses to pass to the callbacks\n+        val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+          new mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+        debug(s\"Controller successfully handled AlterIsr request\")\n+        data.topics.forEach(topic => {\n+          topic.partitions().forEach(partition => {\n+            val tp = new TopicPartition(topic.name, partition.partitionIndex)\n+            if (partition.errorCode() == Errors.NONE.code()) {\n+              val newLeaderAndIsr = new LeaderAndIsr(partition.leader(), partition.leaderEpoch(),", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMzU0OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489623549", "bodyText": "Maybe we could log a warning and let the partition remain in unsentIsrUpdates so that it is retried until we get a response?", "author": "hachikuji", "createdAt": "2020-09-16T18:03:18Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")\n+      case Errors.NONE =>\n+        // Collect partition-level responses to pass to the callbacks\n+        val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+          new mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+        debug(s\"Controller successfully handled AlterIsr request\")\n+        data.topics.forEach(topic => {\n+          topic.partitions().forEach(partition => {\n+            val tp = new TopicPartition(topic.name, partition.partitionIndex)\n+            if (partition.errorCode() == Errors.NONE.code()) {\n+              val newLeaderAndIsr = new LeaderAndIsr(partition.leader(), partition.leaderEpoch(),\n+                partition.isr().asScala.toList.map(_.toInt), partition.currentIsrVersion)\n+              partitionResponses(tp) = Right(newLeaderAndIsr)\n+            } else {\n+              partitionResponses(tp) = Left(Errors.forCode(partition.errorCode()))\n+            }\n+          })\n+        })\n+\n+        // Iterate across the items we sent rather than what we received to ensure we run the callback even if a\n+        // partition was somehow erroneously excluded from the response. Note that these callbacks are run from\n+        // the leaderIsrUpdateLock write lock in Partition#sendAlterIsrRequest\n+        inflightAlterIsrItems.foreach(inflightAlterIsr => try {\n+          if (partitionResponses.contains(inflightAlterIsr.topicPartition)) {\n+            inflightAlterIsr.callback.apply(partitionResponses(inflightAlterIsr.topicPartition))\n+          } else {\n+            inflightAlterIsr.callback.apply(Left(Errors.UNKNOWN_SERVER_ERROR)) // TODO better error here?", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc0NzEwMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489747101", "bodyText": "Good idea. Another case not covered is if partitions are included in the response but weren't sent out. These will be ignored as things currently stand -- maybe that's ok", "author": "mumrah", "createdAt": "2020-09-16T20:52:28Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMzU0OQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyMzkyOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489623929", "bodyText": "nit: may as well convert to Errors since we do so below anyway", "author": "hachikuji", "createdAt": "2020-09-16T18:04:01Z", "path": "core/src/main/scala/kafka/server/AlterIsrManager.scala", "diffHunk": "@@ -0,0 +1,173 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.server\n+\n+import java.util\n+import java.util.concurrent.atomic.{AtomicBoolean, AtomicLong}\n+import java.util.concurrent.{ConcurrentHashMap, TimeUnit}\n+\n+import kafka.api.LeaderAndIsr\n+import kafka.metrics.KafkaMetricsGroup\n+import kafka.utils.{Logging, Scheduler}\n+import kafka.zk.KafkaZkClient\n+import org.apache.kafka.clients.ClientResponse\n+import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.message.{AlterIsrRequestData, AlterIsrResponseData}\n+import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.requests.{AlterIsrRequest, AlterIsrResponse}\n+import org.apache.kafka.common.utils.Time\n+\n+import scala.collection.mutable\n+import scala.jdk.CollectionConverters._\n+\n+/**\n+ * Handles the sending of AlterIsr requests to the controller. Updating the ISR is an asynchronous operation,\n+ * so partitions will learn about updates through LeaderAndIsr messages sent from the controller\n+ */\n+trait AlterIsrManager {\n+  def start(): Unit\n+\n+  def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean\n+\n+  def clearPending(topicPartition: TopicPartition): Unit\n+}\n+\n+case class AlterIsrItem(topicPartition: TopicPartition, leaderAndIsr: LeaderAndIsr, callback: Either[Errors, LeaderAndIsr] => Unit)\n+\n+class AlterIsrManagerImpl(val controllerChannelManager: BrokerToControllerChannelManager,\n+                          val zkClient: KafkaZkClient,\n+                          val scheduler: Scheduler,\n+                          val time: Time,\n+                          val brokerId: Int,\n+                          val brokerEpochSupplier: () => Long) extends AlterIsrManager with Logging with KafkaMetricsGroup {\n+\n+  // Used to allow only one pending ISR update per partition\n+  private val unsentIsrUpdates: util.Map[TopicPartition, AlterIsrItem] = new ConcurrentHashMap[TopicPartition, AlterIsrItem]()\n+\n+  // Used to allow only one in-flight request at a time\n+  private val inflightRequest: AtomicBoolean = new AtomicBoolean(false)\n+\n+  private val lastIsrPropagationMs = new AtomicLong(0)\n+\n+  override def start(): Unit = {\n+    scheduler.schedule(\"send-alter-isr\", propagateIsrChanges, 50, 50, TimeUnit.MILLISECONDS)\n+  }\n+\n+  override def enqueueIsrUpdate(alterIsrItem: AlterIsrItem): Boolean = {\n+    unsentIsrUpdates.putIfAbsent(alterIsrItem.topicPartition, alterIsrItem) == null\n+  }\n+\n+  override def clearPending(topicPartition: TopicPartition): Unit = {\n+    unsentIsrUpdates.remove(topicPartition)\n+  }\n+\n+  private def propagateIsrChanges(): Unit = {\n+    if (!unsentIsrUpdates.isEmpty && inflightRequest.compareAndSet(false, true)) {\n+      // Copy current unsent ISRs but don't remove from the map\n+      val inflightAlterIsrItems: mutable.Queue[AlterIsrItem] = new mutable.Queue[AlterIsrItem]()\n+      unsentIsrUpdates.values().forEach(item => inflightAlterIsrItems.enqueue(item))\n+\n+      val now = time.milliseconds()\n+      lastIsrPropagationMs.set(now)\n+\n+      buildAndSendRequest(inflightAlterIsrItems.toSeq)\n+    }\n+  }\n+\n+  def buildAndSendRequest(inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val message = new AlterIsrRequestData()\n+      .setBrokerId(brokerId)\n+      .setBrokerEpoch(brokerEpochSupplier.apply())\n+      .setTopics(new util.ArrayList())\n+\n+    inflightAlterIsrItems.groupBy(_.topicPartition.topic).foreach(entry => {\n+      val topicPart = new AlterIsrRequestData.TopicData()\n+        .setName(entry._1)\n+        .setPartitions(new util.ArrayList())\n+      message.topics().add(topicPart)\n+      entry._2.foreach(item => {\n+        topicPart.partitions().add(new AlterIsrRequestData.PartitionData()\n+          .setPartitionIndex(item.topicPartition.partition)\n+          .setLeaderId(item.leaderAndIsr.leader)\n+          .setLeaderEpoch(item.leaderAndIsr.leaderEpoch)\n+          .setNewIsr(item.leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+          .setCurrentIsrVersion(item.leaderAndIsr.zkVersion)\n+        )\n+      })\n+    })\n+\n+    def responseHandler(response: ClientResponse): Unit = {\n+      try {\n+        val body: AlterIsrResponse = response.responseBody().asInstanceOf[AlterIsrResponse]\n+        handleAlterIsrResponse(body, inflightAlterIsrItems)\n+      } finally {\n+        // Be sure to clear the in-flight flag to allow future requests\n+        if (!inflightRequest.compareAndSet(true, false)) {\n+          throw new IllegalStateException(\"AlterIsr response callback called when no requests were in flight\")\n+        }\n+      }\n+    }\n+\n+    debug(s\"Sending AlterIsr to controller $message\")\n+    controllerChannelManager.sendRequest(new AlterIsrRequest.Builder(message), responseHandler)\n+  }\n+\n+  def handleAlterIsrResponse(alterIsrResponse: AlterIsrResponse, inflightAlterIsrItems: Seq[AlterIsrItem]): Unit = {\n+    val data: AlterIsrResponseData = alterIsrResponse.data\n+\n+    Errors.forCode(data.errorCode) match {\n+      case Errors.STALE_BROKER_EPOCH =>\n+        warn(s\"Broker had a stale broker epoch, retrying.\")\n+      case Errors.CLUSTER_AUTHORIZATION_FAILED =>\n+        warn(s\"Broker is not authorized to send AlterIsr to controller\")\n+        throw Errors.CLUSTER_AUTHORIZATION_FAILED.exception(\"Broker is not authorized to send AlterIsr to controller\")\n+      case Errors.NONE =>\n+        // Collect partition-level responses to pass to the callbacks\n+        val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+          new mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+        debug(s\"Controller successfully handled AlterIsr request\")\n+        data.topics.forEach(topic => {\n+          topic.partitions().forEach(partition => {\n+            val tp = new TopicPartition(topic.name, partition.partitionIndex)\n+            if (partition.errorCode() == Errors.NONE.code()) {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNDg1NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489624854", "bodyText": "I think authorization should probably be the first thing we do.", "author": "hachikuji", "createdAt": "2020-09-16T18:05:34Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3045,6 +3045,24 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleAlterIsrRequest(request: RequestChannel.Request): Unit = {\n+    val alterIsrRequest = request.body[AlterIsrRequest]\n+\n+    if (!controller.isActive) {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc1Mzk0Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489753946", "bodyText": "This is actually a really good point. I filed a JIRA to fix this in other places in KafkaApis https://issues.apache.org/jira/browse/KAFKA-10491", "author": "mumrah", "createdAt": "2020-09-16T21:05:59Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNDg1NA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNTE4NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489625184", "bodyText": "nit: remove these lines", "author": "hachikuji", "createdAt": "2020-09-16T18:06:09Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -323,6 +330,8 @@ class ReplicaManager(val config: KafkaConfig,\n     val haltBrokerOnFailure = config.interBrokerProtocolVersion < KAFKA_1_0_IV0\n     logDirFailureHandler = new LogDirFailureHandler(\"LogDirFailureHandler\", haltBrokerOnFailure)\n     logDirFailureHandler.start()\n+", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNTYxMQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489625611", "bodyText": "Do we still need this change? I think we are trying to keep the current approach where the controller bumps the leader epoch for any controller-initiated change.", "author": "hachikuji", "createdAt": "2020-09-16T18:07:02Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1317,7 +1326,9 @@ class ReplicaManager(val config: KafkaConfig,\n             partitionOpt.foreach { partition =>\n               val currentLeaderEpoch = partition.getLeaderEpoch\n               val requestLeaderEpoch = partitionState.leaderEpoch\n-              if (requestLeaderEpoch > currentLeaderEpoch) {\n+              val currentZkVersion = partition.getZkVersion\n+              val requestZkVersion = partitionState.zkVersion\n+              if (requestLeaderEpoch > currentLeaderEpoch || requestZkVersion > currentZkVersion) {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc1NjA4Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489756083", "bodyText": "No, we don't need this anymore. This was added so a LeaderAndIsr could update the Partition state without a leader epoch bump, but we don't have that flow anymore so we can revert this.", "author": "mumrah", "createdAt": "2020-09-16T21:10:09Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNTYxMQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTYyNTgzNg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r489625836", "bodyText": "I guess we don't need this anymore.", "author": "hachikuji", "createdAt": "2020-09-16T18:07:25Z", "path": "core/src/main/scala/kafka/zk/KafkaZkClient.scala", "diffHunk": "@@ -452,6 +452,17 @@ class KafkaZkClient private[zk] (zooKeeperClient: ZooKeeperClient, isSecure: Boo\n     }\n   }\n \n+  def getBrokerEpoch(brokerId: Int): Option[Long] = {", "originalCommit": "41053499d2cbcdcae21ff4bc70b1d46705394a32", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "834718ecf326c367cb97d4f1299901c3d9813c7e", "url": "https://github.com/apache/kafka/commit/834718ecf326c367cb97d4f1299901c3d9813c7e", "message": "Partially revert startup ordering", "committedDate": "2020-09-16T19:21:54Z", "type": "commit"}, {"oid": "47878b092cd425376c6cbe7b9036667e0b91f45d", "url": "https://github.com/apache/kafka/commit/47878b092cd425376c6cbe7b9036667e0b91f45d", "message": "More PR feedback\n\n* Lots of cleanup, logging improvements\n* Changes to AlterIsr RPC field names\n* Consolidate on maximal ISR name\n* Introduce sealed trait for different ISR states\n* Tweaks to some error handling", "committedDate": "2020-09-17T13:07:57Z", "type": "commit"}, {"oid": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "url": "https://github.com/apache/kafka/commit/7a93d06f42f0ccbfbeb22c2db965cda59762789b", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-09-17T13:18:30Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2NDc1Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490464756", "bodyText": "nit: hasInflight?", "author": "hachikuji", "createdAt": "2020-09-17T18:20:10Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -157,6 +158,44 @@ case class OngoingReassignmentState(addingReplicas: Seq[Int],\n \n case class SimpleAssignmentState(replicas: Seq[Int]) extends AssignmentState\n \n+\n+\n+sealed trait IsrState {\n+  /**\n+   * Includes only the in-sync replicas which have been committed to ZK.\n+   */\n+  def isr: Set[Int]\n+\n+  /**\n+   * This set may include un-committed ISR members following an expansion. This \"effective\" ISR is used for advancing\n+   * the high watermark as well as determining which replicas are required for acks=all produce requests.\n+   *\n+   * Only applicable as of IBP 2.7-IV2, for older versions this will return the committed ISR\n+   *\n+   */\n+  def maximalIsr: Set[Int]\n+\n+  /**\n+   * Indicates if we have an AlterIsr request inflight.\n+   */\n+  def inflight: Boolean", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2NTE4MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490465180", "bodyText": "nit: It's surprising to have a side effect like this in a function like this. I think it would be better to include this logging at the caller when we are considering a specific change. That way we can also include in the log message information about the change that we were intending to make.", "author": "hachikuji", "createdAt": "2020-09-17T18:20:54Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -222,24 +265,36 @@ class Partition(val topicPartition: TopicPartition,\n   private val tags = Map(\"topic\" -> topic, \"partition\" -> partitionId.toString)\n \n   newGauge(\"UnderReplicated\", () => if (isUnderReplicated) 1 else 0, tags)\n-  newGauge(\"InSyncReplicasCount\", () => if (isLeader) inSyncReplicaIds.size else 0, tags)\n+  newGauge(\"InSyncReplicasCount\", () => if (isLeader) isrState.isr.size else 0, tags)\n   newGauge(\"UnderMinIsr\", () => if (isUnderMinIsr) 1 else 0, tags)\n   newGauge(\"AtMinIsr\", () => if (isAtMinIsr) 1 else 0, tags)\n   newGauge(\"ReplicasCount\", () => if (isLeader) assignmentState.replicationFactor else 0, tags)\n   newGauge(\"LastStableOffsetLag\", () => log.map(_.lastStableOffsetLag).getOrElse(0), tags)\n \n-  def isUnderReplicated: Boolean = isLeader && (assignmentState.replicationFactor - inSyncReplicaIds.size) > 0\n+  def isUnderReplicated: Boolean = isLeader && (assignmentState.replicationFactor - isrState.isr.size) > 0\n \n-  def isUnderMinIsr: Boolean = leaderLogIfLocal.exists { inSyncReplicaIds.size < _.config.minInSyncReplicas }\n+  def isUnderMinIsr: Boolean = leaderLogIfLocal.exists { isrState.isr.size < _.config.minInSyncReplicas }\n \n-  def isAtMinIsr: Boolean = leaderLogIfLocal.exists { inSyncReplicaIds.size == _.config.minInSyncReplicas }\n+  def isAtMinIsr: Boolean = leaderLogIfLocal.exists { isrState.isr.size == _.config.minInSyncReplicas }\n \n   def isReassigning: Boolean = assignmentState.isInstanceOf[OngoingReassignmentState]\n \n   def isAddingLocalReplica: Boolean = assignmentState.isAddingReplica(localBrokerId)\n \n   def isAddingReplica(replicaId: Int): Boolean = assignmentState.isAddingReplica(replicaId)\n \n+  /**\n+   * Check if we have an in-flight AlterIsr\n+   */\n+  def hasInFlightAlterIsr: Boolean = {\n+    if (isrState.inflight) {\n+      trace(s\"ISR update in-flight, skipping update\")", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2OTI1Mg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490469252", "bodyText": "Another possibility is that the replica is pending removal in which case another AlterIsr will be needed. I think it might be more intuitive to make this check:\nif (!isrState.inflight && !isrState.isr.contains(followerId))", "author": "hachikuji", "createdAt": "2020-09-17T18:28:13Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -618,9 +682,9 @@ class Partition(val topicPartition: TopicPartition,\n         // since the replica's logStartOffset may have incremented\n         val leaderLWIncremented = newLeaderLW > oldLeaderLW\n \n-        // check if we need to expand ISR to include this replica\n-        // if it is not in the ISR yet\n-        if (!inSyncReplicaIds.contains(followerId))\n+        // Check if this in-sync replica needs to be added to the ISR. We look at the \"maximal\" ISR here so we don't\n+        // send an additional Alter ISR request for the same replica", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU5NDMzNQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490594335", "bodyText": "Yea checking the maximal set isn't needed anymore since adding the sealed trait. I'll just update this to simply call maybeExpandIsr which will do the check you propose here", "author": "mumrah", "createdAt": "2020-09-17T22:21:46Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ2OTI1Mg=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ4ODYyNg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490488626", "bodyText": "I think we can refactor this a little bit to avoid some duplication and inconsistency. We have the following logic above when updating follower state:\n        if (!isrState.maximalIsr.contains(followerId))\n          maybeExpandIsr(followerReplica, followerFetchTimeMs)\nThis is a little inconsistent because here we are checking isrState.isr. I'd suggest splitting this method into something like the following:\ndef hasReachedHighWatermark(follower: Replica): Boolean = {\n  leaderLogIfLocal.exists { leaderLog =>\n    val leaderHighwatermark = leaderLog.highWatermark\n    isFollowerInSync(follower, leaderHighwatermark)\n  }\n}\n\ndef canAddToIsr(followerId: Int): Boolean = {\n  val current = isrState\n  !current.inflight && !current.isr.contains(followerId)\n}\n\ndef needsExpandIsr(follower: Replica): Boolean = {\n  canAddToIsr(follower.brokerId) && hasReachedHighWatermark(follower)\n}\nThen we can change the logic in maybeExpandIsr to the following:\n    val needsIsrUpdate = canAddToIsr(followerReplica) && inReadLock(leaderIsrUpdateLock) {\n...", "author": "hachikuji", "createdAt": "2020-09-17T19:03:35Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -700,19 +764,16 @@ class Partition(val topicPartition: TopicPartition,\n       inWriteLock(leaderIsrUpdateLock) {\n         // check if this replica needs to be added to the ISR\n         if (needsExpandIsr(followerReplica)) {\n-          val newInSyncReplicaIds = inSyncReplicaIds + followerReplica.brokerId\n-          info(s\"Expanding ISR from ${inSyncReplicaIds.mkString(\",\")} to ${newInSyncReplicaIds.mkString(\",\")}\")\n-          // update ISR in ZK and cache\n-          expandIsr(newInSyncReplicaIds)\n+          expandIsr(followerReplica.brokerId)\n         }\n       }\n     }\n   }\n \n   private def needsExpandIsr(followerReplica: Replica): Boolean = {\n-    leaderLogIfLocal.exists { leaderLog =>\n+    !hasInFlightAlterIsr && leaderLogIfLocal.exists { leaderLog =>", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5MjM0MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490492340", "bodyText": "Seems like we do not have a check for inflight AlterIsr after the write lock has been acquired.", "author": "hachikuji", "createdAt": "2020-09-17T19:10:10Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -858,10 +925,10 @@ class Partition(val topicPartition: TopicPartition,\n         case Some(leaderLog) =>\n           val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)\n           if (outOfSyncReplicaIds.nonEmpty) {\n-            val newInSyncReplicaIds = inSyncReplicaIds -- outOfSyncReplicaIds\n+            val newInSyncReplicaIds = isrState.isr -- outOfSyncReplicaIds", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5Mjc4MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490492780", "bodyText": "This is related to my comment above for the ISR expansion case, but it is a bit confusing to use maximal ISR when the expectation is that we will not shrink as long as we have a pending update inflight. Would it be better to check for inflights and document that this method will return an empty set as long as there is a pending AlterIsr request?", "author": "hachikuji", "createdAt": "2020-09-17T19:10:50Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -920,7 +986,7 @@ class Partition(val topicPartition: TopicPartition,\n      * is violated, that replica is considered to be out of sync\n      *\n      **/\n-    val candidateReplicaIds = inSyncReplicaIds - localBrokerId\n+    val candidateReplicaIds = isrState.maximalIsr - localBrokerId", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU2MTAyOA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490561028", "bodyText": "Makes sense, that will also satisfy your other comment about not checking for inflight requests within the write lock", "author": "mumrah", "createdAt": "2020-09-17T21:04:05Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5Mjc4MA=="}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU2MjYzMA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490562630", "bodyText": "Also, yes it's confusing to refer to maximalIsr here even though it should always equal the committed ISR at this point (assuming we check for inflight first).", "author": "mumrah", "createdAt": "2020-09-17T21:07:24Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5Mjc4MA=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5NDc3Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490494777", "bodyText": "It might be a little more intuitive to change the order here. Something like this:\n      val upatedIsrState = PendingShrinkIsr(isrState.isr, outOfSyncReplicas)\n      if (sendAlterIsrRequest(updatedIsrState)) {\n        isrState = updatedIsrState\n...", "author": "hachikuji", "createdAt": "2020-09-17T19:14:38Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1222,22 +1288,74 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n-  private def expandIsr(newIsr: Set[Int]): Unit = {\n-    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newIsr.toList, zkVersion)\n+  private[cluster] def expandIsr(newInSyncReplica: Int): Unit = {\n+    if (useAlterIsr) {\n+      expandIsrWithAlterIsr(newInSyncReplica)\n+    } else {\n+      expandIsrWithZk(newInSyncReplica)\n+    }\n+  }\n+\n+  private def expandIsrWithAlterIsr(newInSyncReplica: Int): Unit = {\n+    // This is called from maybeExpandIsr which holds the ISR write lock\n+    if (!isrState.inflight) {\n+      // When expanding the ISR, we can safely assume the new replica will make it into the ISR since this puts us in\n+      // a more constrained state for advancing the HW.\n+      if (sendAlterIsrRequest()) {\n+        // Only update our ISR state of AlterIsrManager accepts our update\n+        isrState = PendingExpandIsr(isrState.isr, newInSyncReplica)\n+        debug(s\"Adding new in-sync replica $newInSyncReplica. Pending ISR updated to [${isrState.maximalIsr.mkString(\",\")}]\")\n+      } else {\n+        throw new IllegalStateException(\"Failed to enqueue ISR expansion even though there was no apparent in-flight ISR changes\")\n+      }\n+    } else {\n+      trace(s\"ISR update in-flight, not adding new in-sync replica $newInSyncReplica\")\n+    }\n+  }\n+\n+  private def expandIsrWithZk(newInSyncReplica: Int): Unit = {\n+    val newInSyncReplicaIds = isrState.isr + newInSyncReplica\n+    info(s\"Expanding ISR from ${isrState.isr.mkString(\",\")} to ${newInSyncReplicaIds.mkString(\",\")}\")\n+    val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, newInSyncReplicaIds.toList, zkVersion)\n     val zkVersionOpt = stateStore.expandIsr(controllerEpoch, newLeaderAndIsr)\n-    maybeUpdateIsrAndVersion(newIsr, zkVersionOpt)\n+    maybeUpdateIsrAndVersionWithZk(newInSyncReplicaIds, zkVersionOpt)\n+  }\n+\n+  private[cluster] def shrinkIsr(outOfSyncReplicas: Set[Int]): Unit = {\n+    if (useAlterIsr) {\n+      shrinkIsrWithAlterIsr(outOfSyncReplicas)\n+    } else {\n+      shrinkIsrWithZk(isrState.isr -- outOfSyncReplicas)\n+    }\n+  }\n+\n+  private def shrinkIsrWithAlterIsr(outOfSyncReplicas: Set[Int]): Unit = {\n+    // This is called from maybeShrinkIsr which holds the ISR write lock\n+    if (!isrState.inflight) {\n+      // When shrinking the ISR, we cannot assume that the update will succeed as this could erroneously advance the HW\n+      // We update pendingInSyncReplicaIds here simply to prevent any further ISR updates from occurring until we get\n+      // the next LeaderAndIsr\n+      isrState = PendingShrinkIsr(isrState.isr, outOfSyncReplicas)\n+      if (sendAlterIsrRequest()) {", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDQ5NDk3NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490494975", "bodyText": "nit: can probably rework this as exists\nisrToSendOpt.exists { isrToSend =>\n...\n}", "author": "hachikuji", "createdAt": "2020-09-17T19:14:57Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1246,6 +1364,50 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(): Boolean = {\n+    val isrToSend: Option[Set[Int]] = isrState match {\n+      case PendingExpandIsr(isr, newInSyncReplicaId) => Some(isr + newInSyncReplicaId)\n+      case PendingShrinkIsr(isr, outOfSyncReplicaIds) => Some(isr -- outOfSyncReplicaIds)\n+      case CommittedIsr(_) =>\n+        error(s\"Asked to send AlterIsr but there are no pending updates\")\n+        None\n+    }\n+    if (isrToSend.isDefined) {", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUyNzYyNw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490527627", "bodyText": "Since INVALID_UPDATE_VERSION is one of the expected errors at this level, can we add a separate case for it? For unexpected errors, we might want to log at warn level.", "author": "hachikuji", "createdAt": "2020-09-17T20:00:06Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1246,6 +1364,50 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(): Boolean = {\n+    val isrToSend: Option[Set[Int]] = isrState match {\n+      case PendingExpandIsr(isr, newInSyncReplicaId) => Some(isr + newInSyncReplicaId)\n+      case PendingShrinkIsr(isr, outOfSyncReplicaIds) => Some(isr -- outOfSyncReplicaIds)\n+      case CommittedIsr(_) =>\n+        error(s\"Asked to send AlterIsr but there are no pending updates\")\n+        None\n+    }\n+    if (isrToSend.isDefined) {\n+      val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, isrToSend.get.toList, zkVersion)\n+      val callbackPartial = handleAlterIsrResponse(isrToSend.get, _ : Either[Errors, LeaderAndIsr])\n+      alterIsrManager.enqueue(AlterIsrItem(topicPartition, newLeaderAndIsr, callbackPartial))\n+    } else {\n+      false\n+    }\n+  }\n+\n+  private def handleAlterIsrResponse(proposedIsr: Set[Int], result: Either[Errors, LeaderAndIsr]): Unit = {\n+    inWriteLock(leaderIsrUpdateLock) {\n+      result match {\n+        case Left(error: Errors) => error match {\n+          case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since it doesn't know about this topic or partition. Giving up.\")\n+          case Errors.FENCED_LEADER_EPOCH =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since we sent an old leader epoch. Giving up.\")\n+          case _ =>", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUyODMzOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490528339", "bodyText": "Shall we include some details about the failed request?", "author": "hachikuji", "createdAt": "2020-09-17T20:01:27Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>\n+            val topicResp = new AlterIsrResponseData.TopicData()\n+              .setName(entry._1)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            entry._2.foreach { partitionEntry =>\n+              partitionEntry._2 match {\n+                case Left(error) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setErrorCode(error.code))\n+                case Right(leaderAndIsr) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setLeaderId(leaderAndIsr.leader)\n+                    .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                    .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                    .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+              }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+        mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+\n+      // Determine which partitions we will accept the new ISR for\n+      val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+        case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+            case Some(leaderIsrAndControllerEpoch) =>\n+              val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+              if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+                Errors.FENCED_LEADER_EPOCH\n+              } else {\n+                Errors.NONE\n+              }\n+            case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+          }\n+          if (partitionError == Errors.NONE) {\n+            Some(tp -> newLeaderAndIsr)\n+          } else {\n+            partitionResponses(tp) = Left(partitionError)\n+            None\n+          }\n+      }\n+\n+      // Do the updates in ZK\n+      debug(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+      val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) = zkClient.updateLeaderAndIsr(\n+        adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+      val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+        case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+          isrOrError match {\n+            case Right(updatedIsr) =>\n+              debug(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))\n+              partitionResponses(partition) = Right(updatedIsr)\n+              Some(partition -> updatedIsr)\n+            case Left(error) =>\n+              warn(s\"Failed to update ISR for partition $partition\", error)\n+              partitionResponses(partition) = Left(Errors.forException(error))\n+              None\n+          }\n+      }\n+\n+      badVersionUpdates.foreach(partition => {\n+        warn(s\"Failed to update ISR for partition $partition, bad ZK version\")\n+        partitionResponses(partition) = Left(Errors.INVALID_UPDATE_VERSION)\n+      })\n+\n+      def processUpdateNotifications(partitions: Seq[TopicPartition]): Unit = {\n+        val liveBrokers: Seq[Int] = controllerContext.liveOrShuttingDownBrokerIds.toSeq\n+        debug(s\"Sending MetadataRequest to Brokers: $liveBrokers for TopicPartitions: $partitions\")\n+        sendUpdateMetadataRequest(liveBrokers, partitions.toSet)\n+      }\n+\n+      // Update our cache and send out metadata updates\n+      updateLeaderAndIsrCache(successfulUpdates.keys.toSeq)\n+      processUpdateNotifications(isrsToAlter.keys.toSeq)\n+\n+      Left(partitionResponses)\n+    } catch {\n+      case e: Throwable =>\n+        error(s\"Error when processing AlterIsr request\", e)", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUyODc0NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490528744", "bodyText": "nit: rewrite with $", "author": "hachikuji", "createdAt": "2020-09-17T20:02:13Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>\n+            val topicResp = new AlterIsrResponseData.TopicData()\n+              .setName(entry._1)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            entry._2.foreach { partitionEntry =>\n+              partitionEntry._2 match {\n+                case Left(error) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setErrorCode(error.code))\n+                case Right(leaderAndIsr) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setLeaderId(leaderAndIsr.leader)\n+                    .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                    .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                    .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+              }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+        mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+\n+      // Determine which partitions we will accept the new ISR for\n+      val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+        case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+            case Some(leaderIsrAndControllerEpoch) =>\n+              val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+              if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+                Errors.FENCED_LEADER_EPOCH\n+              } else {\n+                Errors.NONE\n+              }\n+            case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+          }\n+          if (partitionError == Errors.NONE) {\n+            Some(tp -> newLeaderAndIsr)\n+          } else {\n+            partitionResponses(tp) = Left(partitionError)\n+            None\n+          }\n+      }\n+\n+      // Do the updates in ZK\n+      debug(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+      val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) = zkClient.updateLeaderAndIsr(\n+        adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+      val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+        case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+          isrOrError match {\n+            case Right(updatedIsr) =>\n+              debug(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUyOTI1Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490529253", "bodyText": "I think warn might be too high here. We should expect to see some of these even if the cluster is working properly. How about debug?", "author": "hachikuji", "createdAt": "2020-09-17T20:03:19Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>\n+            val topicResp = new AlterIsrResponseData.TopicData()\n+              .setName(entry._1)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            entry._2.foreach { partitionEntry =>\n+              partitionEntry._2 match {\n+                case Left(error) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setErrorCode(error.code))\n+                case Right(leaderAndIsr) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setLeaderId(leaderAndIsr.leader)\n+                    .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                    .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                    .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+              }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+        mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+\n+      // Determine which partitions we will accept the new ISR for\n+      val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+        case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+            case Some(leaderIsrAndControllerEpoch) =>\n+              val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+              if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+                Errors.FENCED_LEADER_EPOCH\n+              } else {\n+                Errors.NONE\n+              }\n+            case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+          }\n+          if (partitionError == Errors.NONE) {\n+            Some(tp -> newLeaderAndIsr)\n+          } else {\n+            partitionResponses(tp) = Left(partitionError)\n+            None\n+          }\n+      }\n+\n+      // Do the updates in ZK\n+      debug(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+      val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) = zkClient.updateLeaderAndIsr(\n+        adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+      val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+        case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+          isrOrError match {\n+            case Right(updatedIsr) =>\n+              debug(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))\n+              partitionResponses(partition) = Right(updatedIsr)\n+              Some(partition -> updatedIsr)\n+            case Left(error) =>\n+              warn(s\"Failed to update ISR for partition $partition\", error)\n+              partitionResponses(partition) = Left(Errors.forException(error))\n+              None\n+          }\n+      }\n+\n+      badVersionUpdates.foreach(partition => {\n+        warn(s\"Failed to update ISR for partition $partition, bad ZK version\")", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzMDQ5Ng==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490530496", "bodyText": "nit: can we avoid using _1 and _2? It's a lot easier to follow if they are named.", "author": "hachikuji", "createdAt": "2020-09-17T20:05:35Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzMTgxNA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490531814", "bodyText": "nit: use type inference. It's conventional to write this as\nval partitionResponses = mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]].empty()", "author": "hachikuji", "createdAt": "2020-09-17T20:08:12Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>\n+            val topicResp = new AlterIsrResponseData.TopicData()\n+              .setName(entry._1)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            entry._2.foreach { partitionEntry =>\n+              partitionEntry._2 match {\n+                case Left(error) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setErrorCode(error.code))\n+                case Right(leaderAndIsr) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setLeaderId(leaderAndIsr.leader)\n+                    .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                    .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                    .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+              }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzMjQ4MQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490532481", "bodyText": "nit: I think we can get rid of this. The logging in ControllerChannelManager.sendUpdateMetadataRequests is probably good enough.", "author": "hachikuji", "createdAt": "2020-09-17T20:09:31Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1763,6 +1768,143 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults.groupBy(_._1.topic).foreach { entry =>\n+            val topicResp = new AlterIsrResponseData.TopicData()\n+              .setName(entry._1)\n+              .setPartitions(new util.ArrayList())\n+            resp.topics.add(topicResp)\n+            entry._2.foreach { partitionEntry =>\n+              partitionEntry._2 match {\n+                case Left(error) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setErrorCode(error.code))\n+                case Right(leaderAndIsr) => topicResp.partitions.add(\n+                  new AlterIsrResponseData.PartitionData()\n+                    .setPartitionIndex(partitionEntry._1.partition)\n+                    .setLeaderId(leaderAndIsr.leader)\n+                    .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                    .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                    .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+              }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses: mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]] =\n+        mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+\n+      // Determine which partitions we will accept the new ISR for\n+      val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+        case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+            case Some(leaderIsrAndControllerEpoch) =>\n+              val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+              if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n+                Errors.FENCED_LEADER_EPOCH\n+              } else {\n+                Errors.NONE\n+              }\n+            case None => Errors.UNKNOWN_TOPIC_OR_PARTITION\n+          }\n+          if (partitionError == Errors.NONE) {\n+            Some(tp -> newLeaderAndIsr)\n+          } else {\n+            partitionResponses(tp) = Left(partitionError)\n+            None\n+          }\n+      }\n+\n+      // Do the updates in ZK\n+      debug(s\"Updating ISRs for partitions: ${adjustedIsrs.keySet}.\")\n+      val UpdateLeaderAndIsrResult(finishedUpdates, badVersionUpdates) = zkClient.updateLeaderAndIsr(\n+        adjustedIsrs, controllerContext.epoch, controllerContext.epochZkVersion)\n+\n+      val successfulUpdates: Map[TopicPartition, LeaderAndIsr] = finishedUpdates.flatMap {\n+        case (partition: TopicPartition, isrOrError: Either[Throwable, LeaderAndIsr]) =>\n+          isrOrError match {\n+            case Right(updatedIsr) =>\n+              debug(\"ISR for partition %s updated to [%s] and zkVersion updated to [%d]\".format(partition, updatedIsr.isr.mkString(\",\"), updatedIsr.zkVersion))\n+              partitionResponses(partition) = Right(updatedIsr)\n+              Some(partition -> updatedIsr)\n+            case Left(error) =>\n+              warn(s\"Failed to update ISR for partition $partition\", error)\n+              partitionResponses(partition) = Left(Errors.forException(error))\n+              None\n+          }\n+      }\n+\n+      badVersionUpdates.foreach(partition => {\n+        warn(s\"Failed to update ISR for partition $partition, bad ZK version\")\n+        partitionResponses(partition) = Left(Errors.INVALID_UPDATE_VERSION)\n+      })\n+\n+      def processUpdateNotifications(partitions: Seq[TopicPartition]): Unit = {\n+        val liveBrokers: Seq[Int] = controllerContext.liveOrShuttingDownBrokerIds.toSeq\n+        debug(s\"Sending MetadataRequest to Brokers: $liveBrokers for TopicPartitions: $partitions\")", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzNDA2MA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490534060", "bodyText": "nit: it's subjective, so feel free to ignore, but I find this a little easier to read if we handle the error cases first. So..\nif (!authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n      sendResponseMaybeThrottle(request, requestThrottleMs =>\n        alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.CLUSTER_AUTHORIZATION_FAILED.exception))Errors.CLUSTER_AUTHORIZATION_FAILED.exception))  \n} else if (!controller.isActive) {\n     sendResponseMaybeThrottle(request, requestThrottleMs =>\n        alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.NOT_CONTROLLER.exception()))\n} else {\n...\n}\nBasically we're discarding the error cases so that the successful path continues flowing downward and we're avoiding extra nesting. Like I said, it's subjective.", "author": "hachikuji", "createdAt": "2020-09-17T20:12:36Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3054,6 +3054,26 @@ class KafkaApis(val requestChannel: RequestChannel,\n     }\n   }\n \n+  def handleAlterIsrRequest(request: RequestChannel.Request): Unit = {\n+    val alterIsrRequest = request.body[AlterIsrRequest]\n+\n+    if (authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzNDI3NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490534274", "bodyText": "nit: unneeded newline", "author": "hachikuji", "createdAt": "2020-09-17T20:13:00Z", "path": "core/src/main/scala/kafka/server/KafkaServer.scala", "diffHunk": "@@ -298,9 +298,12 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         socketServer = new SocketServer(config, metrics, time, credentialProvider)\n         socketServer.startup(startProcessingRequests = false)\n \n+", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzNDk1NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490534954", "bodyText": "nit: not sure it makes sense to include this change any longer", "author": "hachikuji", "createdAt": "2020-09-17T20:14:26Z", "path": "core/src/main/scala/kafka/server/ReplicaManager.scala", "diffHunk": "@@ -1358,7 +1366,8 @@ class ReplicaManager(val config: KafkaConfig,\n                 stateChangeLogger.info(s\"Ignoring LeaderAndIsr request from \" +\n                   s\"controller $controllerId with correlation id $correlationId \" +\n                   s\"epoch $controllerEpoch for partition $topicPartition since its associated \" +\n-                  s\"leader epoch $requestLeaderEpoch matches the current leader epoch\")\n+                  s\"leader epoch $requestLeaderEpoch matches the current leader epoch \" +\n+                  s\"and the zk version $requestZkVersion matches the current zk version\")", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzNjgzOQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490536839", "bodyText": "I wonder if we should be exposing this. Would it be enough to have a def inSyncReplicaIds = isrState.isr? One thing we need to be a little careful of is the fact that we now have a volatile variable with multiple fields. So if you try to access two fields through the isrState reference, you could see inconsistent data.", "author": "hachikuji", "createdAt": "2020-09-17T20:17:56Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -200,9 +241,11 @@ class Partition(val topicPartition: TopicPartition,\n   // defined when this broker is leader for partition\n   @volatile private var leaderEpochStartOffsetOpt: Option[Long] = None\n   @volatile var leaderReplicaIdOpt: Option[Int] = None\n-  @volatile var inSyncReplicaIds = Set.empty[Int]\n+  @volatile var isrState: IsrState = CommittedIsr(Set.empty)", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU0Mjc0OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490542749", "bodyText": "Yea i was thinking we should move the ISR to a separate public accessor. I'll change this", "author": "mumrah", "createdAt": "2020-09-17T20:29:02Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzNjgzOQ=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDUzOTUyMg==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490539522", "bodyText": "Need to address the TODOs in this class.", "author": "hachikuji", "createdAt": "2020-09-17T20:23:10Z", "path": "core/src/test/scala/unit/kafka/cluster/PartitionTest.scala", "diffHunk": "@@ -1149,31 +1153,27 @@ class PartitionTest extends AbstractPartitionTest {\n       followerFetchTimeMs = time.milliseconds(),\n       leaderEndOffset = 6L)\n \n-    assertEquals(Set(brokerId), partition.inSyncReplicaIds)\n+    assertEquals(Set(brokerId), partition.isrState.isr)\n     assertEquals(3L, remoteReplica.logEndOffset)\n     assertEquals(0L, remoteReplica.logStartOffset)\n \n-    // The next update should bring the follower back into the ISR\n-    val updatedLeaderAndIsr = LeaderAndIsr(\n-      leader = brokerId,\n-      leaderEpoch = leaderEpoch,\n-      isr = List(brokerId, remoteBrokerId),\n-      zkVersion = 1)\n-    when(stateStore.expandIsr(controllerEpoch, updatedLeaderAndIsr)).thenReturn(Some(2))\n-\n     partition.updateFollowerFetchState(remoteBrokerId,\n       followerFetchOffsetMetadata = LogOffsetMetadata(10),\n       followerStartOffset = 0L,\n       followerFetchTimeMs = time.milliseconds(),\n       leaderEndOffset = 6L)\n \n-    assertEquals(Set(brokerId, remoteBrokerId), partition.inSyncReplicaIds)\n+    assertEquals(alterIsrManager.isrUpdates.size, 1)\n+    assertEquals(alterIsrManager.isrUpdates.dequeue().leaderAndIsr.isr, List(brokerId, remoteBrokerId))\n+    assertEquals(Set(brokerId), partition.isrState.isr)\n+    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.maximalIsr)\n     assertEquals(10L, remoteReplica.logEndOffset)\n     assertEquals(0L, remoteReplica.logStartOffset)\n   }\n \n   @Test\n   def testIsrNotExpandedIfUpdateFails(): Unit = {\n+    // TODO maybe remove this test now?", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU0MDc4OQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490540789", "bodyText": "I may have missed it, but do we have tests which verify error handling? I see tests which verify requests get sent, but at a quick glance I didn't see tests of responses.", "author": "hachikuji", "createdAt": "2020-09-17T20:25:23Z", "path": "core/src/test/scala/unit/kafka/cluster/PartitionTest.scala", "diffHunk": "@@ -1257,20 +1250,18 @@ class PartitionTest extends AbstractPartitionTest {\n \n     // On initialization, the replica is considered caught up and should not be removed\n     partition.maybeShrinkIsr()\n-    assertEquals(Set(brokerId, remoteBrokerId), partition.inSyncReplicaIds)\n+    assertEquals(Set(brokerId, remoteBrokerId), partition.isrState.isr)\n \n     // If enough time passes without a fetch update, the ISR should shrink\n     time.sleep(partition.replicaLagTimeMaxMs + 1)\n-    val updatedLeaderAndIsr = LeaderAndIsr(\n-      leader = brokerId,\n-      leaderEpoch = leaderEpoch,\n-      isr = List(brokerId),\n-      zkVersion = 1)\n-    when(stateStore.shrinkIsr(controllerEpoch, updatedLeaderAndIsr)).thenReturn(Some(2))\n \n+    // Shrink the ISR\n     partition.maybeShrinkIsr()\n-    assertEquals(Set(brokerId), partition.inSyncReplicaIds)\n-    assertEquals(10L, partition.localLogOrException.highWatermark)\n+    assertEquals(alterIsrManager.isrUpdates.size, 1)", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MDU0MTU1Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r490541557", "bodyText": "nit: sort of conventional to use a name like MockAlterIsrManager", "author": "hachikuji", "createdAt": "2020-09-17T20:26:45Z", "path": "core/src/test/scala/unit/kafka/utils/TestUtils.scala", "diffHunk": "@@ -1065,6 +1065,25 @@ object TestUtils extends Logging {\n                    logDirFailureChannel = new LogDirFailureChannel(logDirs.size))\n   }\n \n+  class TestAlterIsrManager extends AlterIsrManager {", "originalCommit": "7a93d06f42f0ccbfbeb22c2db965cda59762789b", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"oid": "1dde0e47d59d27e25a50bfd21264c0ec07d9115e", "url": "https://github.com/apache/kafka/commit/1dde0e47d59d27e25a50bfd21264c0ec07d9115e", "message": "More PR feedback\n\n* Cleaned up expand/shrink ISR methods\n* Style stuff\n* Logging stuff", "committedDate": "2020-09-17T23:04:11Z", "type": "commit"}, {"oid": "8c694d185df0f4e4d4e2eb7cddec4033f6428561", "url": "https://github.com/apache/kafka/commit/8c694d185df0f4e4d4e2eb7cddec4033f6428561", "message": "Two small test things", "committedDate": "2020-09-17T23:10:55Z", "type": "commit"}, {"oid": "1647c7115b83674ab1a20d21bacda43068f135ed", "url": "https://github.com/apache/kafka/commit/1647c7115b83674ab1a20d21bacda43068f135ed", "message": "Merge remote-tracking branch 'apache-github/trunk' into KAFKA-8836-alter-isr", "committedDate": "2020-09-23T16:54:08Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4MDg0Nw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494480847", "bodyText": "Consider the following scenario:\n\nbroker sends AlterIsr\nthe update succeeds but the response is lost\nbroker retries AlterIsr\n\nCurrently the leader will be stuck after 3) because it has no way to get the latest LeaderAndIsr state if the first attempt fails. To handle this, I think we need to add an idempotence check here. After we have validated the leader epoch, if the intended state matches the current state, then we can just return the current state.", "author": "hachikuji", "createdAt": "2020-09-24T17:13:35Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1764,6 +1769,145 @@ class KafkaController(val config: KafkaConfig,\n     }\n   }\n \n+  def alterIsrs(alterIsrRequest: AlterIsrRequestData, callback: AlterIsrResponseData => Unit): Unit = {\n+    val isrsToAlter = mutable.Map[TopicPartition, LeaderAndIsr]()\n+\n+    alterIsrRequest.topics.forEach { topicReq =>\n+      topicReq.partitions.forEach { partitionReq =>\n+        val tp = new TopicPartition(topicReq.name, partitionReq.partitionIndex)\n+        val newIsr = partitionReq.newIsr().asScala.toList.map(_.toInt)\n+        isrsToAlter.put(tp, new LeaderAndIsr(alterIsrRequest.brokerId, partitionReq.leaderEpoch, newIsr, partitionReq.currentIsrVersion))\n+      }\n+    }\n+\n+    def responseCallback(results: Either[Map[TopicPartition, Either[Errors, LeaderAndIsr]], Errors]): Unit = {\n+      val resp = new AlterIsrResponseData()\n+      results match {\n+        case Right(error) =>\n+          resp.setErrorCode(error.code)\n+        case Left(partitionResults) =>\n+          resp.setTopics(new util.ArrayList())\n+          partitionResults\n+            .groupBy { case (tp, _) => tp.topic }   // Group by topic\n+            .foreach { case (topic, partitions) =>\n+              // Add each topic part to the response\n+              val topicResp = new AlterIsrResponseData.TopicData()\n+                .setName(topic)\n+                .setPartitions(new util.ArrayList())\n+              resp.topics.add(topicResp)\n+              partitions.foreach { case (tp, errorOrIsr) =>\n+                // Add each partition part to the response (new ISR or error)\n+                errorOrIsr match {\n+                  case Left(error) => topicResp.partitions.add(\n+                    new AlterIsrResponseData.PartitionData()\n+                      .setPartitionIndex(tp.partition)\n+                      .setErrorCode(error.code))\n+                  case Right(leaderAndIsr) => topicResp.partitions.add(\n+                    new AlterIsrResponseData.PartitionData()\n+                      .setPartitionIndex(tp.partition)\n+                      .setLeaderId(leaderAndIsr.leader)\n+                      .setLeaderEpoch(leaderAndIsr.leaderEpoch)\n+                      .setIsr(leaderAndIsr.isr.map(Integer.valueOf).asJava)\n+                      .setCurrentIsrVersion(leaderAndIsr.zkVersion))\n+                }\n+            }\n+          }\n+      }\n+      callback.apply(resp)\n+    }\n+\n+    eventManager.put(AlterIsrReceived(alterIsrRequest.brokerId, alterIsrRequest.brokerEpoch, isrsToAlter, responseCallback))\n+  }\n+\n+  private def processAlterIsr(brokerId: Int, brokerEpoch: Long, isrsToAlter: Map[TopicPartition, LeaderAndIsr],\n+                              callback: AlterIsrCallback): Unit = {\n+\n+    // Handle a few short-circuits\n+    if (!isActive) {\n+      callback.apply(Right(Errors.NOT_CONTROLLER))\n+      return\n+    }\n+\n+    val brokerEpochOpt = controllerContext.liveBrokerIdAndEpochs.get(brokerId)\n+    if (brokerEpochOpt.isEmpty) {\n+      info(s\"Ignoring AlterIsr due to unknown broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    if (!brokerEpochOpt.contains(brokerEpoch)) {\n+      info(s\"Ignoring AlterIsr due to stale broker epoch $brokerEpoch for broker $brokerId\")\n+      callback.apply(Right(Errors.STALE_BROKER_EPOCH))\n+      return\n+    }\n+\n+    val response = try {\n+      val partitionResponses = mutable.HashMap[TopicPartition, Either[Errors, LeaderAndIsr]]()\n+\n+      // Determine which partitions we will accept the new ISR for\n+      val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n+        case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n+          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+            case Some(leaderIsrAndControllerEpoch) =>\n+              val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n+              if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {", "originalCommit": "1647c7115b83674ab1a20d21bacda43068f135ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUyMDI0NQ==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494520245", "bodyText": "I was trying to think some kind of race with a zombie leader trying to update the ISR, however this would get fenced by the leader epoch. This should be pretty easy to add", "author": "mumrah", "createdAt": "2020-09-24T18:21:33Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4MDg0Nw=="}], "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4MzgwOA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494483808", "bodyText": "It might make more sense to handle this case similarly to FENCED_LEADER_EPOCH. Retrying won't help since we know our version will be rejected Come to think of it, this would be kind of a strange error to hit in the current implementation which only allows one request inflight at a time. For controller-initiated changes, we'd expect to hit FENCED_LEADER_EPOCH. Anyway, I think it's still worth keeping the error.", "author": "hachikuji", "createdAt": "2020-09-24T17:18:40Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1246,6 +1351,51 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(proposedIsrState: IsrState): Boolean = {\n+    val isrToSendOpt: Option[Set[Int]] = proposedIsrState match {\n+      case PendingExpandIsr(isr, newInSyncReplicaId) => Some(isr + newInSyncReplicaId)\n+      case PendingShrinkIsr(isr, outOfSyncReplicaIds) => Some(isr -- outOfSyncReplicaIds)\n+      case CommittedIsr(_) =>\n+        error(s\"Asked to send AlterIsr but there are no pending updates\")\n+        None\n+    }\n+    isrToSendOpt.exists { isrToSend =>\n+      val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, isrToSend.toList, zkVersion)\n+      val callbackPartial = handleAlterIsrResponse(isrToSend, _ : Either[Errors, LeaderAndIsr])\n+      alterIsrManager.enqueue(AlterIsrItem(topicPartition, newLeaderAndIsr, callbackPartial))\n+    }\n+  }\n+\n+  private def handleAlterIsrResponse(proposedIsr: Set[Int], result: Either[Errors, LeaderAndIsr]): Unit = {\n+    inWriteLock(leaderIsrUpdateLock) {\n+      result match {\n+        case Left(error: Errors) => error match {\n+          case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since it doesn't know about this topic or partition. Giving up.\")\n+          case Errors.FENCED_LEADER_EPOCH =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since we sent an old leader epoch. Giving up.\")\n+          case Errors.INVALID_UPDATE_VERSION =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to invalid zk version. Retrying.\")\n+            sendAlterIsrRequest(isrState)", "originalCommit": "1647c7115b83674ab1a20d21bacda43068f135ed", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4NjU3OA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494486578", "bodyText": "Is there any way that we could end up retrying after the pending ISR state had already been reset? I know we have AlterIsrManager.clearPending, but that only removes the partition from the unsent queue. How do we handle inflight AlterIsr requests after the state has been reset. Seems like it might be worth adding a check here to validate whether the request is still needed.", "author": "hachikuji", "createdAt": "2020-09-24T17:23:03Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1246,6 +1351,51 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  private def sendAlterIsrRequest(proposedIsrState: IsrState): Boolean = {\n+    val isrToSendOpt: Option[Set[Int]] = proposedIsrState match {\n+      case PendingExpandIsr(isr, newInSyncReplicaId) => Some(isr + newInSyncReplicaId)\n+      case PendingShrinkIsr(isr, outOfSyncReplicaIds) => Some(isr -- outOfSyncReplicaIds)\n+      case CommittedIsr(_) =>\n+        error(s\"Asked to send AlterIsr but there are no pending updates\")\n+        None\n+    }\n+    isrToSendOpt.exists { isrToSend =>\n+      val newLeaderAndIsr = new LeaderAndIsr(localBrokerId, leaderEpoch, isrToSend.toList, zkVersion)\n+      val callbackPartial = handleAlterIsrResponse(isrToSend, _ : Either[Errors, LeaderAndIsr])\n+      alterIsrManager.enqueue(AlterIsrItem(topicPartition, newLeaderAndIsr, callbackPartial))\n+    }\n+  }\n+\n+  private def handleAlterIsrResponse(proposedIsr: Set[Int], result: Either[Errors, LeaderAndIsr]): Unit = {\n+    inWriteLock(leaderIsrUpdateLock) {\n+      result match {\n+        case Left(error: Errors) => error match {\n+          case Errors.UNKNOWN_TOPIC_OR_PARTITION =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since it doesn't know about this topic or partition. Giving up.\")\n+          case Errors.FENCED_LEADER_EPOCH =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since we sent an old leader epoch. Giving up.\")\n+          case Errors.INVALID_UPDATE_VERSION =>\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to invalid zk version. Retrying.\")\n+            sendAlterIsrRequest(isrState)\n+          case _ =>\n+            warn(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to $error. Retrying.\")\n+            sendAlterIsrRequest(isrState)", "originalCommit": "1647c7115b83674ab1a20d21bacda43068f135ed", "replyToReviewId": null, "replies": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDUyMzc3Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494523773", "bodyText": "True, we could see a new ISR from controller initiated changes via LeaderAndIsr while our request is in-flight. We have a check for this on successful responses, but we should also check here. Since our request failed, we don't have a leaderEpoch to check against so I think the best we can do is see if isrState is still pending before re-sending the request", "author": "mumrah", "createdAt": "2020-09-24T18:26:13Z", "replyToReviewId": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDQ4NjU3OA=="}], "type": "inlineReview"}, {"oid": "ac3eec11505bace6c3302ad982d6418d30f26313", "url": "https://github.com/apache/kafka/commit/ac3eec11505bace6c3302ad982d6418d30f26313", "message": "Next round of PR feedback\n\n* Make AlterIsr idempotent in the controller\n* Tweak some error handling\n* Add a few tests\n* Style feedback", "committedDate": "2020-09-24T19:13:34Z", "type": "commit"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY2MDA4NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494660084", "bodyText": "nit (for follow-up): fix grammar \"since due\"", "author": "hachikuji", "createdAt": "2020-09-24T23:13:45Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1375,11 +1381,14 @@ class Partition(val topicPartition: TopicPartition,\n           case Errors.FENCED_LEADER_EPOCH =>\n             debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} since we sent an old leader epoch. Giving up.\")\n           case Errors.INVALID_UPDATE_VERSION =>\n-            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to invalid zk version. Retrying.\")\n-            sendAlterIsrRequest(isrState)\n+            debug(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to invalid zk version. Giving up.\")\n           case _ =>\n-            warn(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to $error. Retrying.\")\n-            sendAlterIsrRequest(isrState)\n+            if (isrState.isInflight) {\n+              warn(s\"Controller failed to update ISR to ${proposedIsr.mkString(\",\")} due to $error. Retrying.\")\n+              sendAlterIsrRequest(isrState)\n+            } else {\n+              warn(s\"Ignoring failed ISR update to ${proposedIsr.mkString(\",\")} since due to $error since we have a committed ISR.\")", "originalCommit": "ac3eec11505bace6c3302ad982d6418d30f26313", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY2MDk4Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494660983", "bodyText": "nit: conventionally we prefer \"retriable\"", "author": "hachikuji", "createdAt": "2020-09-24T23:16:38Z", "path": "core/src/main/scala/kafka/cluster/Partition.scala", "diffHunk": "@@ -1366,6 +1366,12 @@ class Partition(val topicPartition: TopicPartition,\n     }\n   }\n \n+  /**\n+   * This is called for each partition in the body of an AlterIsr response. For errors which are non-retryable we simply", "originalCommit": "ac3eec11505bace6c3302ad982d6418d30f26313", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY2MTM0NA==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494661344", "bodyText": "It might be worth mentioning that this could happen in the case of a retry after a successful update.", "author": "hachikuji", "createdAt": "2020-09-24T23:17:43Z", "path": "core/src/main/scala/kafka/controller/KafkaController.scala", "diffHunk": "@@ -1847,21 +1847,22 @@ class KafkaController(val config: KafkaConfig,\n       // Determine which partitions we will accept the new ISR for\n       val adjustedIsrs: Map[TopicPartition, LeaderAndIsr] = isrsToAlter.flatMap {\n         case (tp: TopicPartition, newLeaderAndIsr: LeaderAndIsr) =>\n-          val partitionError: Errors = controllerContext.partitionLeadershipInfo(tp) match {\n+          controllerContext.partitionLeadershipInfo(tp) match {\n             case Some(leaderIsrAndControllerEpoch) =>\n               val currentLeaderAndIsr = leaderIsrAndControllerEpoch.leaderAndIsr\n               if (newLeaderAndIsr.leaderEpoch < currentLeaderAndIsr.leaderEpoch) {\n-                Errors.FENCED_LEADER_EPOCH\n+                partitionResponses(tp) = Left(Errors.FENCED_LEADER_EPOCH)\n+                None\n+              } else if (newLeaderAndIsr.equalsIgnoreZk(currentLeaderAndIsr)) {\n+                // If a partition is already in the desired state, just return it", "originalCommit": "ac3eec11505bace6c3302ad982d6418d30f26313", "replyToReviewId": null, "replies": null, "type": "inlineReview"}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDY2MTU4Mw==", "url": "https://github.com/apache/kafka/pull/9100#discussion_r494661583", "bodyText": "nit: leave off parenthesis after exception", "author": "hachikuji", "createdAt": "2020-09-24T23:18:19Z", "path": "core/src/main/scala/kafka/server/KafkaApis.scala", "diffHunk": "@@ -3066,20 +3066,18 @@ class KafkaApis(val requestChannel: RequestChannel,\n   def handleAlterIsrRequest(request: RequestChannel.Request): Unit = {\n     val alterIsrRequest = request.body[AlterIsrRequest]\n \n-    if (authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n-      if (!controller.isActive) {\n-        sendResponseMaybeThrottle(request, requestThrottleMs =>\n-          alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.NOT_CONTROLLER.exception()))\n-      } else {\n-        controller.alterIsrs(alterIsrRequest.data,\n-          alterIsrResp => sendResponseMaybeThrottle(request, requestThrottleMs =>\n-            new AlterIsrResponse(alterIsrResp.setThrottleTimeMs(requestThrottleMs))\n-          )\n-        )\n-      }\n-    } else {\n+    if (!authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {\n       sendResponseMaybeThrottle(request, requestThrottleMs =>\n         alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.CLUSTER_AUTHORIZATION_FAILED.exception))\n+    } else if (!controller.isActive) {\n+      sendResponseMaybeThrottle(request, requestThrottleMs =>\n+        alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.NOT_CONTROLLER.exception()))", "originalCommit": "ac3eec11505bace6c3302ad982d6418d30f26313", "replyToReviewId": null, "replies": null, "type": "inlineReview"}]}